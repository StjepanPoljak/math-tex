\documentclass [12pt]{article}
\usepackage{latexsym,amsfonts,amssymb,amsmath,amsbsy}
\usepackage{graphics}
\usepackage[croatian]{babel}
\usepackage[cp1250]{inputenc}
\usepackage{color}

\textwidth=15.5cm \textheight=24cm

\setlength{\oddsidemargin}{.25in}
\setlength{\evensidemargin}{.25in} \addtolength\topmargin{-.5in}

\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\thesection\arabic{subsection}.}
\renewcommand{\thesubsubsection}{\thesubsection\arabic{subsubsection}.}

\newtheorem{sadrzaj}{Sadr\v{z}aj}[section]
\newtheorem{zadatak}{Zadatak}[section]
\newtheorem{rjesenje}{Rje\v{s}enje}[section]
\newtheorem{teorem}{Teorem}[section]
\newtheorem{korolar}{Korolar}[section]
\newtheorem{propozicija}{Propozicija}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{primjedba}{Primjedba}[section]
\newtheorem{tvrdnja}{Tvrdnja}[section]
\newtheorem{lema}{Lema}[section]
\newtheorem{Def}{Definition}[section]
\renewcommand{\thesadrzaj}{\thesection\arabic{sadrzaj}}
\renewcommand{\thezadatak}{\thesection\arabic{zadatak}}
\renewcommand{\therjesenje}{\thesection\arabic{rjesenje}}
\renewcommand{\thetvrdnja}{\thesection\arabic{tvrdnja}}
\renewcommand{\theteorem}{\thesection\arabic{teorem}}
\renewcommand{\thekorolar}{\thesection\arabic{korolar}}
\renewcommand{\thepropozicija}{\thesection\arabic{propozicija}}
\renewcommand{\theproblem}{\thesection\arabic{problem}}
\renewcommand{\theprimjedba}{\thesection\arabic{primjedba}}
\renewcommand{\thelema}{\thesection\arabic{lema}}
\renewcommand{\theDef}{\thesection\arabic{Def}}

\newcommand{\ord}[1]{\textnormal{ord}\left(#1\right)}
\newcommand{\lcm}[1]{\textnormal{lcm}\left(#1\right)}
\newcommand{\dom}[1]{\textnormal{dom}\left(#1\right)}
\newcommand{\cod}[1]{\textnormal{cod}\left(#1\right)}
\renewcommand{\ker}[1]{\textnormal{ker}\left(#1\right)}
\newcommand{\ran}[1]{\textnormal{ran}\left(#1\right)}
\newcommand{\cyc}[1]{\left\langle#1\right\rangle}
\newcommand{\Aut}[1]{\textnormal{Aut}\left(#1\right)}
\newcommand{\cis}[1]{\textnormal{cis}\left(#1\right)}
\newcommand{\Orb}[2]{\textnormal{Orb}_{#2}\left(#1\right)}
\newcommand{\Stab}[2]{\textnormal{Stab}_{#2}\left(#1\right)}
\newcommand{\End}[1]{\textnormal{End}\left(#1\right)}
\newcommand{\Fix}[2]{\textnormal{Fix}_{#2}\left(#1\right)}
\newcommand{\Norm}[2]{\textnormal{N}_{#2}\left(#1\right)}
\newcommand{\Syl}[2]{\textnormal{Syl}_{#1}\left(#2\right)}
\newcommand{\rad}[1]{\textnormal{rad}\left(#1\right)}
\newcommand{\Ann}[1]{\textnormal{Ann}\left(#1\right)}
\newcommand{\zmod}[1]{\Z\slash #1\Z}
\newcommand{\zmodinv}[1]{\left(\Z\slash #1\Z\right)^{\ast}}
\newcommand{\homo}[3]{#1\underset{#3}{\longrightarrow}#2}
\newcommand{\primeideal}{\trianglelefteq_p}
\newcommand{\zmodel}[2]{\left[#1\right]_{#2}}
\newcommand{\rchar}[1]{\textnormal{char}\left(#1\right)}
\newcommand{\proj}[2]{\widehat{#1_#2}}
\newcommand{\unitset}[1]{#1^{\ast}}
\newcommand{\vek}[1]{{\bf #1}}
\newcommand{\base}[1]{\mathcal{B}\left(#1\right)}
\newcommand{\lcomb}[1]{\mathcal{C}\left(#1\right)}
\renewcommand{\dim}[1]{\textnormal{dim}\left(#1\right)}

\def\C{{\mathbb{C}}}
\def\R{{\mathbb{R}}}
\def\Z{{\mathbb{Z}}}
\def\N{{\mathbb{N}}}
\def\R{{\mathbb{R}}}
\def\Q{{\mathbb{Q}}}
\def\C{{\mathbb{C}}}
\def\N{{\mathbb{N}}}
\def\Z{{\mathbb{Z}}}


\font\tencyr=wncyr10

\def\cyr{\tencyr\cyracc}

\begin{document}
\baselineskip=17pt

\begin{center}
{\bf Abstract Algebra}\\
Stjepan Poljak
\end{center}

\vskip 0.5cm

\begin{center}
{\bf Introduction I.}
\end{center}

\vskip 0.5cm

There is no higher intention for this script than to serve as a collection of solutions to problems in {\it The Book of Abstract Algebra} by Charles C. Pinter. I will, however, write down important definitions and theorems which are either necessary for solving exercises in the book or are of certain interest to the author of these passages. If anyone is to find this script, I certainly do hope it serves him well.

\vskip 0.5cm

\begin{center}
{\bf Introduction II.}
\end{center}

\vskip 0.5cm

As my works expanded far beyond the book of Charles C. Pinter, and they include some of my thoughts and conclusions, I have felt proper, then, to add some basic introduction to category theory, some of my excercises and theorems. Long have I fought my weak mind to discover something new, and most of my theorems were derived and proved independently, as my own ideas. However, as I have the fortune of the most misfortunate men, often have I discovered, that my observations were already observed. As to the category theory and new discoveries, I would only add one further comment. Everything has been put under a microscope, I feel that there is nothing new; and category theory does not add one single new specimen for observation, it only shifts the focus of observation. Oh, and one more thing, most have asked me with rather astonishing curiosity why I am writing this in English. The reason is simple. To show my ability to handle English. Furthermore, most of the namings are, especially for category theory, unknown to me in Croatian language, as I have access (for the time being) only to English literature; to invent my own word in Croatian for the name of the definiens would be, from my point of view, highly irrational and downright dumb. Yet, one may argue that writing in English is more irrational than only inventing names that resemble Slavic origin, which is my mother language. I may then argue, that my mother language is ugly, meaningless, clumsy and irrationaly difficult (and difficulty is not a positive quality) and that I prefer any other language as my mother language. If I had the ability to handle modern mathematical notions in Latin, may Heaven be my witness, I would do so without any hesitation.

\newpage

\begin{center}
{\bf Groups}
\end{center}

\vskip 0.5cm

In this chapter we shall deal with operations which take only two elements (an ordered pair) from a set to produce a unique third element from the same set. Altough this can be generalized and in the further text it won't be ambiguous if we use the term {\it operation}, we will however call it a {\it binary operation} due to specificity (to avoid the confusion if the reader is familiar with other terms other than these).

\noindent\newline{\bf Definition.} Let $S$ be a non-empty set. {\it Binary operation} $\ast$ is every function of the form

\begin{equation*}
\ast:S\times S\rightarrow S.
\end{equation*}

\noindent\newline To further explain necessary conditions this definition imposes on us we have to take into consideration that every (binary) operation is necessary a {\it function}. Let us remember that a function is, non-formally speaking, a mapping from one set to the other such that every element from the first set (domain) is mapped in the second set (codomain) uniquely (in other words every element from domain must have one and only one corresponding element in the codomain). This must also hold for binary operations. Therefore, every binary operation (coming from the definition of the function) must satisfy these three conditions ({\it axioms of totality}\footnote{Axioms of totality would be more formally written by using first order logic semantics as:

\begin{enumerate}
\item $\left(\forall\left(x,y\right)\in S\right)\left(\exists\left(x\ast y\right)\right)$
\item $\left(\forall\left(x,y\right),\left(m,n\right)\in S\right)\left(\left(x\ast y\neq m\ast n\right)\rightarrow\left(\left(x,y\right)\neq\left(m,n\right)\right)\right)$
\item $\left(\forall\left(x,y\right)\in S\right)\left(\left(x\ast y\right)\in S\right)$
\end{enumerate}}):

\begin{enumerate}
\item Operation $\ast$ must be defined for every ordered pair in $S$ (e.g. division on $\R$ is not an operation for it is not defined when dividing with zero).

\item It must produce a unique element in $S$ (we cannot have two or more different elements resulting from the same operation).

\item The operation must be closed on $S$; in other words, the element it produces is necessarily in $S$ (e.g. subtracting on $\N$ is not an operation for it is not closed in $\N$; subtracting $5$ from $3$ yields $-2$ which is in $\Z$, but not in $\N$).
\end{enumerate}

If for a certain rule only the axioms of uniqueness and closure are satisfied, that rule is then called {\it partial operation}, as it is defined by a partial function (a type of relation which does not require the mapping to be defined for every element in domain as opposed to total function which is a synonime for a function). One last note on the condition of the requirement for $S$ to be a non-empty set. We do not want to deal with empty sets in this way, especially if we are considering its elements later in the definition, or propositions. For if $S$ were to be empty, then we enter the area of a vacuous truth: everything we claim about its elements is logically (if not epistemologically) true (and the negation of the claim) because there are no elements to which we can assign these truths (if I were to say that cats which live underwater have nine-tails that would be true - so would be the opposite; there are no cats which live underwater, so logically I can claim anything I want about them; on the other hand, from an epistemological view, I {\it know} from experience that there are no cats living underwater and I would discern such claim as non-sense). This is one of the reasons why we have to be careful when {\it thinking logically}. Thinking logically takes form into consideration more than it does contents, and as everything in this universe, they are inseparable. And mathematics being mostly concerned with more form than contents (compared to other sciences, the only exception being logic itself) we have to be extremely careful not to fall into a paradox (which some say is a bad thing). Let us now elaborate on properties\footnote{In FOL, these properties can be written as:

\begin{enumerate}
\item $\left(\forall a,b,c\in S\right)\left(\left(a\ast b\right)\ast c=a\ast\left(b\ast c\right)\right)$
\item $\left(\forall a,b\in S\right)\left(a\ast b=b\ast a\right)$
\item $\left(\exists e\in S\right)\left(\forall x\in S\right)\left(x\ast e=e\ast x=x\right)$
\item $\left(\forall x\in S\right)\left(\exists x^{-1}\in S\right)\left(x\ast x^{-1}=x^{-1}\ast x=e\right)$
\end{enumerate}} an operation defined as above can have:

\begin{enumerate}
\item {\it Associativity.} We say that operation $\ast$ is associative if $\left(a\ast b\right)\ast c=a\ast\left(b\ast c\right),\\\forall a,b\in S$. The importance of associativity comes from the fact that we can combine three elements using the same operation (but without changing their order; only their priorities) and obtain the same result. Associativity holds for addition, multiplication, composition, but not for subtraction and division.
\item {\it Commutativity.} Operation $\ast$ is commutative if $a\ast b=b\ast a,\ \forall a,b\in S$. Addition and multiplication are commutative, while composition, sutraction and division are not.
\item {\it Existence of identity.} We say that operation $\ast$ has an identity (neutral) element $e$ if $\exists e\in S:\ a\ast e=e\ast a=a,\ \forall a\in S$. Neutral element for addition is $0$, for multiplication $1$; when we apply neutral element to any element on which the operation is defined we produce that element (on which the neutral element was applied in respect to the defined operation). Notice that here we assumed that operation $\ast$ is commutative (at least when observing neutral element). If that is not the case, then an operation may have only a left or a right neutral element (in this case it has no neutral element, only exception being when left element equals the right element, in other words if it's commutative).
\item {\it Existence of inverse.} Operation $\ast$ has an inverse element $a^{-1}$ if $\exists a^{-1}:\ a\ast a^{-1}=a^{-1}\ast a=e,\ \forall a\in S$. Notice that for an inverse to exist it is necessary that the same operation already has satisfied the condition of a neutral element. For addition, the inverse element is $-a$ and for multiplication $a^{-1}$ (on the set of reals only zero has no inverse, therefore, only the set of reals without zero has an inverse). Similiar to identity, the inverse also assumes commutativity (at least for inverse; if that is not the case then we speak of left or right inverse element - if they are equal then we call it's simply an inverse element).
\end{enumerate}

\noindent{\bf Definition.} Let us take into consideration an ordered pair $(S,\ast)$. If such ordered pair satisfies conditions of associativity and existence of identity and inverse elements in respect to $\ast$ defined on $S$, then we call it a {\it group}. If a group also satisfies commutativity then we call it an {\it Abelian} or {\it commutative group}.

\noindent\newline{\bf Problem.} Which of the following rules are operations on the indicated set?

\begin{enumerate}
\item $a\ast b=\sqrt{\left|a b\right|}$, on the set $\Q$.
\item $a\ast b=a\ln{b}$, on the set $A=\left\{x\in\R:\ x>0\right\}$.
\item $a\ast b$ is a root of the equation $x^2-a^2 b^2=0$, on the set $\R$.
\item Subtraction, on the set $\Z$.
\item Subtraction, on the set $B=\left\{n\in\Z:\ n\geq0\right\}$.
\item $a\ast b=\left|a-b\right|$, on the set $C=\left\{n\in\Z:\ n\geq0\right\}$.
\end{enumerate}

\noindent{\bf Solution.} First rule, $a\ast b=\sqrt{\left|a b\right|}$ defined on the set $\Q$ is not an operation because the square root of the product of two rational numbers can be an irrational number, e.g. $\frac{1}{2}\ast 4=\sqrt{2}$; we see that $\frac{1}{2},4\in\Q$, but $\sqrt{2}\in\mathbb{I}$ (operation is not closed on $\Q$). It would, however, be an operation if we were to expand the set on which it is defined to the set of reals, $\R$.

Second rule, $a\ast b=a\ln{b}$, on the set $A$, is not an operation for the similar reasons as the first rule; if we take $1,\frac{1}{e}\in A$ (where $e$ denotes Euler's number), by operation $\ast$ we would obtain $1\ast \frac{1}{e}=1\cdot\ln{\frac{1}{e}}=-1\notin A$ (for $\log{\frac{1}{a}}=\log{a^{-1}}=-\log{a}$). Rule $\ast$ would be an operation if we were to take $A^{\ast}=\left\{x\in\R:\ x\geq e\right\}$; then, the smallest element (as natural logarithm is a strictly increasing function, and every $\ln{x}\geq 1,\ \forall x\in\R,\ x\geq e$) would be $e\ast e=e\ln{e}=e\in A^{\ast}$ ($e$ would therefore be a neutral element). All other elements would be larger then $e$ and therefore contained in $A^{\ast}$.

Third rule does not satisfy uniqueness. For the equation $x^2=a^2 b^2$ returns two solutions $x=\pm a b$; therefore $a\ast b=\pm a b$, meaning it is not uniquely definied as we get two different values in return. If we were to take $\R^{+}$ instead of $\R$, then it would be an operation, as we would observe only positive solutions.

It is trivial to notice that the fourth rule (subtraction) is operation on the set of integers, $\Z$, for it satisfies all three conditions. We will not, however, conduct any rigorous proofs at this point. It is also trivial to notice that the fifth rule is not an operation, for it does not satisfy the condition of closure, for example if we take $3,4\in B$, subtracting $4$ from $3$ produces $-1\notin B$.

Seventh rule is operation on the given set, which is easy to prove. Absolute value is defined for every real number (therefore also for every integer). Closure is satisfied because $\left|a-b\right|\geq0,\ \forall a,b\in\R$. Uniqueness is also satisfied as absolute value is always unique, in other words it never produces more than one element.

\noindent\newline{\bf Problem.} Each of the following is an operation $\ast$ on $\R$. Indicate whether or not it is commutative, associative, whether $\R$ has an identity element and an inverse element for every $x\in\R$ with respect to $\ast$.

\begin{enumerate}
\item $x\ast y=x+y+1$,
\item $x\ast y=x+2y+4$,
\item $x\ast y=x+2y-x y$,
\item $x\ast y=\left|x+y\right|$,
\item $x\ast y=\left|x-y\right|$,
\item $x\ast y=x y+1$,
\item $x\ast y=\max{\left\{x,y\right\}}$,
\item $x\ast y=\frac{x y}{x+y+1}$, on the set of positive real numbers.
\end{enumerate}

\noindent{\bf Solution.} For each example we will check all four properties in the following way, labeling associativity with "A", commutativity with "C", existence of neutral element with "N" and existence of inverses with "I".

\begin{enumerate}
\item $x\ast y=x+y+1$

\begin{itemize}
\item[A.] $\left(x\ast y\right)\ast z=\left(x+y+1\right)\ast z=x+y+1+z+1=x+\left(y+z+1\right)+1=x\ast\left(y+z+1\right)=x\ast\left(y\ast z\right)$.
\item[C.] Valid due to commutativity of addition, $x\ast y=x+y+1=y+x+1=y\ast x$.
\item[N.] We seek $e\in\R$ such that $x\ast e=x$, id est $x+e+1=x$. From this expression it is trivial to notice that $e=-1$; $x\ast\left(-1\right)=x+\left(-1\right)+1=x$.
\item[I.] Let us see if there exists $x^{-1}\in\R$ for every $x\in\R$ such that $x\ast x^{-1}=e$, i.e. $x\ast x^{-1}=-1$. From definition of operation follows that $x+x^{-1}+1=-1$ and we conclude that $x^{-1}=-x-2$, $x^{-1}\in\R,\ \forall x\in\R$. It is easy to verify that $x\ast\left(-x-2\right)=x-x-2+1=-1=e$, which also hold for $x^{-1}\ast x$ due to commutativity.
\end{itemize}

\noindent Ordered pair $\left(\R,\ast\right)$ therefore denotes a commutative group.

\item $x\ast y=x+2y+4$

\begin{itemize}
\item[A.] $\left(x\ast y\right)\ast z=\left(x+2y+4\right)\ast z=x+2y+4+2z+4=x+2y+2z+8$. This operation is not associative as expression $x\ast\left(y\ast z\right)=x\ast\left(y+2z+4\right)=x+2\left(y+4z+4\right)+4=x+2y+4z+12$ does not equal previous expression.
\item[C.] This operation is also not commutative as $x\ast y=x+2y+4$ differs from $y\ast x=y+2x+4$.
\item[N.] We can see that we have a right neutral element $x\ast e_r=x$, which is $x+2e_r+1=x$, id est $e_r=\frac{-1}{2}$ and that a left neutral element, $e_l+2x+1=x$, is $e_l=-x-1$. As $e_r\neq e_l$, there is no neutral element.
\item[I.] No element $x\in\R$ has an inverse because neutral element is non-existent.
\end{itemize}

\noindent Ordered pair, such as $\left(\R,\ast\right)$ in this exercise, which satisfies only the three axioms of totality is called a {\it magma} (or sometimes a {\it groupoid}). If only the axioms of closure and uniqueness are satisfied (operation $\ast$ being just a partial operation), the ordered pair is called {\it partial magma}.

\item $x\ast y=x+2y-x y$

\begin{itemize}
\item[A.] Property of associativity is not satisfied as $\left(x\ast y\right)\ast z=\left(x+2y-x y\right)\ast z=x+2y-x y+2z-\left(x+2y-x y\right)\cdot z=x+2y-x y+2z-x z-2y z+x y z$ does not equal $x\ast\left(y\ast z\right)=x\ast\left(y+2z-y z\right)=x+2y+4z-2y z+x\left(y+2z-y z\right)=x+2y+4z+2y+x y+2x z-x y z$.
\item[C.] It is easy to see that $x\ast y=x+2y-x y$ does not equal $y\ast x=y+2x-x y$.
\item[N.] Due to failure of commutativity for this operation, the property of existence of neutral element will also not be satisfied as $x\ast e\neq e\ast x$.
\item[I.] As there is no neutral element, the property of existence of inverses cannot be satisfied.
\end{itemize}

\noindent As in previous exercise, ordered pair $\left(\R,\ast\right)$ is a magma.

\item $x\ast y=\left|x+y\right|$

\begin{itemize}
\item[A.] As in previous exercises, $x\ast\left(y\ast z\right)=x\ast\left|y+z\right|=\left|x+\left|y+z\right|\right|$ differs from $\left(x\ast y\right)\ast z=\left|x+y\right|\ast z=\left|\left|x+y\right|+z\right|$, therefore associativity is not satisfied. We can think of a counterexample to further assure us in our thoughts; let us take $-5,4,2\in\R$. Then it follows $\left|-5+\left|4+2\right|\right|=\left|-5+6\right|=1$ and $\left|\left|-5+4\right|+2\right|=\left|1+2\right|=3$.
\item[C.] Operation is, however, commutative as $x\ast y=\left|x+y\right|=\left|y+x\right|=y\ast x$.
\item[N.] We obtain neutral element from $x\ast e=x$, i.e. $\left|x+e\right|=x$. This equation gives two solutions, one being $0$ and the other being $-2x$. Already it is obvious that $-2x$ is not a neutral element as we need a unique neutral element for all $x\in\R$, not the reverse. We can, however, test $e=0$. If we take $\left|x+0\right|=\left|0+x\right|=\left|x\right|$ we can see that this would be a neutral element only for non-negative real numbers, for if we take $-5\in\R$ we see that $\left|-5+0\right|=5\neq-5$. Therefore, there is no neutral element.
\item[I.] Existence of inverses does not come into question as there is no neutral element on which to observe, i.e. search for, inverses.
\end{itemize}

\noindent This ordered pair $\left(\R,\ast\right)$ is a {\it commutative magma}.

\item $x\ast y=\left|x-y\right|$

\begin{itemize}
\item[A.] This operation is analogous to the previous operation and it is easy to verify that associativity is not satisfied, for $\left|x-\left|y-z\right|\right|\neq\left|\left|x-y\right|-z\right|$. We can take a counterexample, e.g. $5,4,2\in\R$, to make sure our assertion is in order; $\left|5-\left|4-2\right|\right|=\left|5-2\right|=3$ and $\left|\left|5-4\right|-2\right|=\left|1-2\right|=1$.
\item[C.] Commutativity holds, because $\left|x-y\right|=\left|-\left(y-x\right)\right|=\left|y-x\right|$.
\item[N.] Neutral element would be, analogous to the previous example, $e=0$, but $\left|x-0\right|=\left|0-x\right|=\left|x\right|$, which can only hold for non-negative real numbers.
\item[I.] Inverses are non-existent, for there is no neutral element.
\end{itemize}

\noindent As in the previous case, ordered pair $\left(\R,\ast\right)$ is also a commutative magma.

\item $x\ast y=x y+1$

\begin{itemize}

\item[A.] We obtain that $\left(x\ast y\right)\ast z=\left(x y+1\right)\ast z=\left(x y+1\right)\cdot z+1=x y z+z+1$ and $x\ast\left(y\ast z\right)=x\ast\left(y z+1\right)=x\left(y z+1\right)+1=x y z+x+1$. Those two expressions being different, operation is not associative.
\item[C.] Operation is commutative as $x\ast y=x y+1=y x+1=y\ast x$ for all $x,y\in\R$.
\item[N.] From $x\ast e=x$ we get $x e+1=x$, i.e. $e=\frac{x-1}{x}$, $x\neq0$. Therefore, there is no neutral element in $\R$.
\item[I.] Property of existence of identity not being satisfied, there can be no inverses.
\end{itemize}

\noindent $\left(\R,\ast\right)$ is a commutative magma.

\item $x\ast y=\max{\left\{x,y\right\}}$

\begin{itemize}
\item[A.] Let us show that both $\max{\left\{\max{\left\{x,y\right\}},z\right\}}$ and $\max{\left\{x,\max{\left\{y,z\right\}}\right\}}$ both produce the same element - the one with the greatest value among $x,y,z$. It is sufficient that we observe only two cases, when $y$ is of the greatest value and when $x$ or $z$ are of the greatest vaule (those cases are analogous, because when we observe $x$ on left side, on the right side we get the case what would be for $z$ on the left side and conversely). Let us suppose that $y$ is of the greatest value; then on the left side we have $\max{\left\{x,y\right\}}=y$ and therefore $\max{\left\{y,z\right\}}=y$. On the right side, similarly it holds that $\max{\left\{y,z\right\}}=y$; then it follows that $\max{\left\{y,z\right\}}=y$, so in this case associativity is satisfied. But what if $x$ (or $z$) is the element of the greatest value? On the left side we would surely now have that $\max{\left\{x,y\right\}}=x$ and then that $\max{\left\{x,z\right\}}=x$. Yet, on the right side, following from condition that $x\geq y$ and $x\geq z$, we have $x\geq\max{\left\{y,z\right\}}=m$. Therefore, $\max{\left\{x,m\right\}}=x$, which proves associativity.
\item[C.] Commutativity is satisfied as $x\ast y=\max{\left\{x,y\right\}}=\max{\left\{y,x\right\}}=y\ast x$.
\item[N.] When will hold that $\max{\left\{x,e\right\}}=x$? It will hold if and only if $x\geq e$. In order for it to hold generally, we would need to take the smallest value possible for $e$ in the set of reals, but no matter how small we set it to be, there can always be smaller $x\in\R$. Therefore, as there is no smallest real number, there can be no neutral element for this operation.
\item[I.] There are no inverses as there is no neutral element.
\end{itemize}

\noindent Ordered pair $\left(\R,\ast\right)$, where associativity holds is called a {\it semigroup}. This one is also commutative (which is not necessary for it to be a semigroup).

\item $x\ast y=\frac{x y}{x+y+1}$

\begin{itemize}
\item[A.] Let us see if $x\ast\left(y\ast z\right)=\left(x\ast y\right)\ast z$. On the left side we have

\begin{eqnarray*}
x\ast\left(y\ast z\right)&=&x\ast\left(\frac{y z}{y+z+1}\right)=\frac{x\frac{y z}{y+z+1}}{x+\frac{y z}{y+z+1}+1}=\frac{\frac{x y z}{y+z+1}}{\frac{y z+x\left(y+z+1\right)+\left(y+z+1\right)}{y+z+1}}\\
&=&\frac{x y z}{y z+x y+x z+2z+y+1}.
\end{eqnarray*}

\noindent\newline On the right side,

\begin{eqnarray*}
\left(x\ast y\right)\ast z&=&\left(\frac{x y}{x+y+1}\right)\ast z=\frac{z\frac{x y}{x+y+1}}{\frac{x y}{x+y+1}+z+1}=\frac{\frac{x y z}{x+y+1}}{\frac{x y+z\left(x+y+1\right)+\left(x+y+1\right)}{x+y+1}}\\
&=&\frac{x y z}{x y+z x+z y+x+y+z+1}.
\end{eqnarray*}

\noindent\newline These two expressions differ, therefore associativity does not hold.

\item[C.] Commutativity, however, holds because $x\ast y=\frac{x y}{x+y+1}=\frac{y x}{y+x+1}=y\ast x$.

\item[N.] We look for neutral element by observing expression $x\ast e=x$, i.e. $\frac{x e}{x+e+1}=x$. We multiply it with $x+e+1$ (which is always $\neq 0$, for $x,e,1\in\R^{+}$) and obtain $x e=x^2+x e+x$. As $x e$ is cancelled, we are left with $x^2+x=0$, which does not say much about the nature of $e$. Therefore, there is no neutral element.

\item[I.] There is no neutral element, so there are also no inverses.
\end{itemize}

\noindent Ordered pair $\left(\R^{+},\ast\right)$ is a commutative magma.

\end{enumerate}

\noindent{\bf Problem.} Using the following three tables check the operation $\ast$ on the set $A=\left\{a,b\right\}$ for all four properties (note that $\left(i,j\right)$ element in the matrix represents one in the $i$-th row and $j$-th column).
\begin{center}
\parbox{.3\linewidth}{\begin{center}
1.\quad\begin{tabular}{c|cc}
  %\hline
  $\ast$ & $a$ & $b$ \\
  \hline
  $a$ & $a$ & $b$ \\
  $b$ & $b$ & $a$ \\
  %\hline
\end{tabular}
\end{center}}
\hfill
\parbox{.3\linewidth}{\begin{center}
2.\quad\begin{tabular}{c|cc}
  %\hline
  $\ast$ & $a$ & $b$ \\
  \hline
  $a$ & $b$ & $a$ \\
  $b$ & $b$ & $a$ \\
  %\hline
\end{tabular}
\end{center}}
\hfill
\parbox{.3\linewidth}{\begin{center}
3.\quad\begin{tabular}{c|cc}
  %\hline
  $\ast$ & $a$ & $b$ \\
  \hline
  $a$ & $b$ & $a$ \\
  $b$ & $b$ & $b$ \\
  %\hline
\end{tabular}
\end{center}}
\end{center}

\noindent{\bf Solution} (1.)

\begin{itemize}

\item[A.] Let us check associativity of this operation. We need to examine all eight cases (for we have three places where we can put two elements, that is $2\cdot2\cdot2=8$)\footnote{Notice that we always perform the operation in the form of row$\ast$column, e.g. in third example, $a\ast b=a$, where all other combinations equal $b$.}:

\begin{enumerate}
\item $a\ast\left(a\ast a\right)=a\ast a=a$\quad and\quad $\left(a\ast a\right)\ast a=a\ast a=a$,
\item $a\ast\left(a\ast b\right)=a\ast b=b$\quad and\quad $\left(a\ast a\right)\ast b=a\ast b=b$,
\item $a\ast\left(b\ast a\right)=a\ast b=b$\quad and\quad $\left(a\ast b\right)\ast a=b\ast a=b$,
\item $a\ast\left(b\ast b\right)=a\ast a=a$\quad and\quad $\left(a\ast b\right)\ast b=b\ast b=a$,
\item $b\ast\left(a\ast a\right)=b\ast a=b$\quad and\quad $\left(b\ast a\right)\ast a=b\ast a=b$,
\item $b\ast\left(a\ast b\right)=b\ast b=a$\quad and\quad $\left(b\ast a\right)\ast b=b\ast b=a$,
\item $b\ast\left(b\ast a\right)=b\ast b=a$\quad and\quad $\left(b\ast b\right)\ast a=a\ast a=a$,
\item $b\ast\left(b\ast b\right)=b\ast a=b$\quad and\quad $\left(b\ast b\right)\ast b=a\ast b=b$.
\end{enumerate}

\noindent As two expressions in all eight cases are equal, associativity is satisfied.

\item[C.] Commutativity is valid, which is obvious from the symmetry observed in the table (also, $a\ast b=b=b\ast a$).

\item[N.] Let us try to find a neutral element; we have only two candidates, $a$ and $b$. Let us suppose that $e=a$ is a neutral element. Then $a\ast e=a$ and $b\ast e=e\ast b=b$. If we also consider $e=b$, then $a\ast e=e\ast a=b$ and $b\ast e=a$; it is obvious that this is 
not valid, therefore $e=a$ is a neutral element in respect to $\ast$ for all $x\in A$ (this can also be observed from the fact that the column and row in the table corresponding to $a$ leaves other elements intact).

\item[I.] Let us verify that each element $x\in A$ has its corresponding inverse element. First, if we take $a\ast a^{-1}=e$, i.e. $a\ast a^{-1}=a$, we see that $a^{-1}=a$; it is also valid for $a^{-1}\ast a=e$ because $a\ast a=a$. Now, if we take $b\ast b^{-1}=e$ it follows that $b^{-1}=b$ as $b\ast b=a$. It is trivial to notice that this is valid also for $b^{-1}\ast b=e$. To conclude, every element in $A$ has its inverse element in $A$, in respect to $\ast$.
\end{itemize}

\noindent Ordered pair $\left(A,\ast\right)$ is an Abelian group.

\noindent\newline{\bf Solution.} (2.)

\begin{itemize}
\item[A.] We would examine all eight cases, but even in the first case we can see that associativity is not satisfied, beacuse $a\ast\left(a\ast a\right)=a\ast b=a$ differs from $\left(a\ast a\right)\ast a=b\ast a=b$.
\item[C.] Operation is not commutative which can be seen from the previous example; $a\ast b=a$, while $b\ast a=b$.
\item[N.] We look for neutral element in $a\ast e=e\ast a=a$ and $b\ast e=e\ast b=b$. Such $e$ does not exist, because, if we take $e=a$, then $a\ast a=b$ is valid, but $b\ast a=b$ does not hold because $a\ast b=a$. Similarly, if we take $e=b$, not even $b\ast b=a$ holds. In conclusion, there is no neutral element.
\item[I.] There are no inverses.
\end{itemize}

\noindent Ordered pair $\left(A,\ast\right)$ is a magma.

\noindent\newline{\bf Solution.} (3.)

\begin{itemize}
\item[A.] We can easily see that this operation is not associative as $a\ast\left(a\ast a\right)=a\ast b=a$\quad and\quad $\left(a\ast a\right)\ast a=b\ast a=b$.
\item[C.] From upper counterexample we can also see that this operation is not commutative ($a\ast b=a$ and $b\ast a=b$).
\item[N.] Neutral element is also non-existent because if we take $a\ast e=e\ast a=a$, we can have $e=b$ so that $a\ast b=a$, but $b\ast a=b$. If we take $e=a$, then we have the same thing with $b$.
\item[I.] There are no inverses.
\end{itemize}

\noindent Ordered pair $\left(A,\ast\right)$ is a magma.

\noindent\newline{\bf Problem.} Digital computers and related machines process information which is received in the form of input sequences. An {\it input sequence} is a finite sequence of symbols from some alphabet $A$. For instance, if $A=\left\{0,1\right\}$ (that is, if the alphabet consists of only the two symbols $0$ and $1$), then examples of input sequences are $011010$ and $10101111$. If $A=\left\{a,b,c\right\}$, then examples of input sequences are $babbcac$ and $cccabaa$. {\it Output sequences} are defined in the same way as input sequences. The set of all sequences of symbols in the alphabet $A$ is denoted by $A^{\ast}$.

There is an operation on $A^{\ast}$ called {\it concatenation}: If ${\bf a}$ and ${\bf b}$ are in $A^{\ast}$, say ${\bf a}=a_1 a_2 \ldots a_n$ and ${\bf b}=b_1 b_2 \ldots b_m$, then

\begin{equation*}
{\bf a b}=a_1 a_2\ldots a_n b_1 b_2\ldots b_m.
\end{equation*}

\noindent\newline In other words, the sequence ${\bf ab}$ consists of the two sequences ${\bf a}$ and ${\bf b}$ end to end. For example, in the alphabet $A=\left\{0,1\right\}$, if ${\bf a}=1001$ and ${\bf b}=010$, then ${\bf a b}=1001010$.

The symbol ${\bf \lambda}$ denotes the empty sequence.

\begin{enumerate}
\item Prove that the operation defined above is associative.
\item Explain why the operation is not commutative.
\item Prove that there is an identity element for this operation.
\end{enumerate}

\noindent{\bf Solution.} Let $A$ be an alphabet and $A^{\ast}$ the set of all sequences of symbols in $A$. Then, let ${\bf a},{\bf b},{\bf c}\in A^{\ast}$, in other words, ${\bf a}=a_1 a_2\ldots a_p$, ${\bf b}=b_1 b_2\ldots b_r$ and ${\bf c}=c_1 c_2\ldots c_s$, where $p,r,s\in\N$.

\begin{enumerate}
\item First we will prove associativity, i.e. that $\left({\bf a b}\right){\bf c}={\bf a}\left({\bf b c}\right)$. If we check the left hand side of the equation we obtain $a_1 a_2\ldots a_p b_1 b_2\ldots b_r {\bf c}$, that is

\begin{equation*}
a_1 a_2\ldots a_p b_1 b_2\ldots b_r c_1 c_2\ldots c_s.
\end{equation*}

\noindent\newline It is trivial to notice that the right hand side is the same and that associativity holds for this operation on $A^{\ast}$ as ${\bf a} b_1 b_2\ldots b_r c_1 c_2\ldots c_s$ equals

\begin{equation*}
a_1 a_2\ldots a_p\ldots b_1 b_2\ldots b_r\ldots c_1 c_2\ldots c_s.
\end{equation*}

\item The operation is not commutative as the result of the operation depends on which side the element was applied; it is easy to show that by a simple counterexample, such as 

\begin{equation*}
{\bf a b}=a_1 a_2\ldots a_p b_1 b_2\ldots b_r\neq b_1 b_2\ldots b_r a_1 a_2\ldots a_p={\bf b a}.
\end{equation*}

\item Let us suppose that an empty sequence $\lambda$ is the identity element for this operation. We have to show that ${\bf\lambda a}={\bf a\lambda}={\bf a}$. It is easy to see that ${\bf\lambda} a_1 a_2\ldots a_p=a_1 a_2\ldots a_p={\bf a}$ and that $a_1 a_2\ldots a_p {\bf\lambda}=a_1 a_2\ldots a_p={\bf a}$. Can there exist an inverse element for every ${\bf a}\in A^{\ast}$? No, because from any non-empty sequence ${\bf a}$ we can never obtain the empty sequence ${\bf\lambda}$ by contatenation. All we can do is accumulate more and more symbols by performing the operation of concatenation, but never take them away to get empty sequence. So, the ordered pair $\left(A^{\ast},\circ\right)$, where $\circ$ denotes the operation of concatenation, is at most a commutative {\it monoid} (an ordered pair of a set and of an operation that is defined on a set is called a monoid if associativity and identity hold; the only difference from a group is that there are some elements without inverses).

\end{enumerate}

\noindent{\bf Problem.} Prove that each of the following sets, with the indicated operation, is an Abelian group.

\begin{enumerate}
\item $x\ast y=x+y+k$, where $k$ is a fixed constant, on the set $\R$.
\item $x\ast y=\frac{x y}{2}$, on the set $A=\left\{x\in\R:\ x\neq0\right\}$.
\item $x\ast y=x+y+x y$, on the set $B=\left\{x\in\R:\ x\neq-1\right\}$.
\item $x\ast y=\frac{x+y}{x y+1}$, on the set $C=\left\{x\in\R:\ -1<x<1\right\}$.
\end{enumerate}

\noindent{\bf Solution.} Let us prove all properties, as in previous examples, for the ordered pairs above.

\begin{enumerate}
\item $x\ast y=x+y+k$, where $k$ is a fixed constant, on the set $\R$.
\begin{itemize}
\item[A.] It is trivial to show that $\left(x\ast y\right)\ast z=x\ast\left(y\ast z\right)$, i.e. associativity holds. For the left hand side we have $\left(x+y+k\right)\ast z=x+y+k+z+k=x+y+z+2k$. On the right hand side, we obtain $x\ast\left(y+z+k\right)=x+y+z+k+k=x+y+z+2k$.
\item[C.] Commutativity holds because addition is also commutative (and only operation used in $\ast$), that is, $x\ast y=x+y+k=y+x+k=y\ast x$.
\item[N.] Neutral element can be found from expression $x\ast e=e\ast x=x$. We take $x\ast e=x$, that is, $x+e+k=x$ and obtain $e=-k$. It is easy to see that it is also a left neutral element (and by that a neutral element), as $-k\ast x=-k+x+k=x$, for all $x\in\R$.
\item[I.] We will obtain inverses from expression $x\ast x^{-1}=e$, that is $x+x^{-1}+k=-k$. We see that $x^{-1}=-x-2k$. We can also show that $x^{-1}$ is a left inverse by showing that $x^{-1}\ast x=-x-2k+x+k=-k=e$, for all $x\in\R$.
\end{itemize}

\noindent Ordered pair $\left(\R,\ast\right)$ is truly an Abelian group.

\item $x\ast y=\frac{x y}{2}$, on the set $A=\R\backslash\left\{0\right\}$.

\begin{itemize}
\item[A.] First, $x\ast\left(y\ast z\right)=x\ast\left(\frac{y z}{2}\right)=\frac{x\frac{y z}{2}}{2}=\frac{x y z}{4}$. On the other hand, $\left(x\ast y\right)\ast z=\left(\frac{x y}{2}\right)\ast z=\frac{z \frac{x y}{2}}{2}=\frac{x y z}{4}$. Both sides being equal, associativity holds.
\item[C.] Operation is commutative as $x\ast y=\frac{x y}{2}=\frac{y x}{2}=y\ast x$ for all $x,y\in A$.
\item[N.] Let us consider $\frac{x e}{2}=x$. We have $x e=2x$, i.e. $e=2$ when we divide expression by $x\neq 0$. As operation $\ast$ is commutative, the existence of the neutral element in $\R\backslash\left\{0\right\}$ is proved.
\item[I.] Is there $x^{-1}\in A$ such that $\frac{x x^{-1}}{2}=2$? We have $x x^{-1}=4$, and when we divide everything with $x\neq 0$, we obtain $x^{-1}=\frac{4}{x}$, for all $x\in A$ (as operation is commutative).
\end{itemize}

\noindent Ordered pair $\left(\R\backslash\left\{0\right\},\ast\right)$ is an Abelian group.

\item $x\ast y=x+y+x y$, on the set $B=\R\backslash\left\{-1\right\}$.

\begin{itemize}
\item[A.] On the left hand side we have $x\ast\left(y\ast z\right)\\=x\ast\left(y+z+y z\right)=x+\left(y+z+y z\right)+x\left(y+z+y z\right)=x+y+z+y z+x y+x z+x y z$, while on the right hand side we have $\left(x\ast y\right)\ast z=\left(x+y+x y\right)\ast z=\left(x+y+x y\right)+z+z\left(x+y+x y\right)=x+y+z+x y+z x+y z+x y z$. Therefore, operation $\ast$ is associative.
\item[C.] Commutativity holds because $x\ast y=x+y+x y=y+x+y x=y\ast x$.
\item[N.] We'll try and find $e\in B$ such that $x\ast e=x$, i.e. $x+e+x e=x$. The two $x$ cancel out and we have $e\left(1+x\right)=0$. After dividing with $\left(1+x\right)\neq0$ we have $e=0$. As operation is commutative, neutral element is absolute for all $x\in B$.
\item[I.] We consider expression $x+x^{-1}+x x^{-1}=0$. We then have $x^{-1}\left(1+x\right)=-x$. After dividing expression with $\left(1+x\right)\neq0$ (notice that here is the condition that $x\neq-1$), we have $x^{-1}=-\frac{x}{1+x}$, for all $x\in B$ (operation being commutative, we don't have to check the existence and equality of left and right inverses).
\end{itemize}

\noindent Ordered pair $\left(\R\backslash\left\{-1\right\},\ast\right)$ is an Abelian group.

\item $x\ast y=\frac{x+y}{x y+1}$, on the set $C=\left\{x\in\R:\ -1<x<1\right\}$.

\begin{itemize}
\item[A.] First, we obtain $\left(x\ast y\right)\ast z=\left(\frac{x+y}{x y+1}\right)\ast z=\frac{\frac{x+y}{x y+1}+z}{\frac{x+y}{x y+1}z+1}=\frac{\frac{x+y+x y z+z}{x y+1}}{\frac{x z+y z+x y+1}{x y+1}}=\frac{x+y+x y z+z}{x z+y z+x y+1}$. Next, we have $x\ast\left(y\ast z\right)=x\ast\left(\frac{y+z}{y z+1}\right)=\frac{x+\frac{y+z}{y z+1}}{x \frac{y+z}{y z+1}+1}=\frac{\frac{x y z+x+y+z}{y z+1}}{\frac{x y+x z+y z+1}{y z+1}}=\frac{x+y+z+x y z}{x z+y z+x y+1}$. Therefore, associativity holds (note that the denominator must not equal zero; yet, on the set $C$, we are only allowed to use numbers whose absolute value is less than one, i.e. numbers of the form $\pm\frac{a}{b}$, where $a<b$ and $a,b\in\R^{+}_0$ and $b\neq 0$; therefore, if we multiply such numbers as in set $C$, we will always obtain a new number from set $C$, i.e. a number whose absolute value is less than $1$ so we can never get a zero denominator).

\item[C.] Operation is commutative as $x\ast y=\frac{x+y}{x y+1}=\frac{y+x}{y x+1}=y\ast x$.

\item[N.] Let us find $e\in C$ such that $\frac{e+x}{e x+1}=x$. We multiply the previous expression with denominator on the left-hand side and thus obtain $e+x=e x^2+x$. Now we have $e x^2-e=0$, i.e. $e (x^2-1)=0$. Either it is $e=0$ or $x=\pm 1$. From $x=\pm1$ we get no information about neutral element, therefore our only choice is that $e=0$. Indeed, it is easily verified that $\frac{0+x}{0\cdot x+1}=\frac{x+0}{x\cdot 0+1}=\frac{x}{1}=x$.

\item[I.] Now we have to find $x^{-1}$ such that $\frac{x+x^{-1}}{x x^{-1}+1}=0$. Denominator here serves no purpose as only nominator is allowed to be zero (and denominator cannot be zero in $C$ anyway). So $x+x^{-1}=0$ and $x^{-1}=-x$.
\end{itemize}

\noindent Ordered pair $\left(\left\langle-1,1\right\rangle,\ast\right)$ is an Abelian group.

\end{enumerate}

\noindent{\bf Problem.} Which of the following subsets of $\R\times\R$, with the indicated operation, is a group?

\begin{enumerate}
\item $\left(a,b\right)\ast\left(c,d\right)=\left(a d+b c, b d\right)$, on the set $\left\{\left(x,y\right)\in\R\times\R:\ y\neq0\right\}$.

\item $\left(a,b\right)\ast\left(c,d\right)=\left(a c, b c+d\right)$, on the set $\left\{\left(x,y\right)\in\R\times\R:\ x\neq0\right\}$.

\item Same operation as in part $2$, but on the set $\R\times\R$.

\item $\left(a,b\right)\ast\left(c,d\right)=\left(a c-b d, a d+b c\right)$, on the set $\R\times\R$ with the origin deleted.

\item Consider the operation of the preceeding problem on the set $\R\times\R$. Is this a group? Explain.

\end{enumerate}

\noindent\newline{\bf Solution.}

\begin{enumerate}
\item $\left(a,b\right)\ast\left(c,d\right)=\left(a d+b c, b d\right)$, on the set $\left\{\left(x,y\right)\in\R\times\R:\ y\neq0\right\}$

\begin{itemize}
\item[A.] First, we have $\left(\left(a,b\right)\ast\left(c,d\right)\right)\ast\left(e,f\right)=\left(a d+b c, b d\right)\ast\left(e,f\right)\\=\left(\left(a d+b c\right)f+b d e, b d f\right)=\left(a d f+b c f+b d e, b d f\right)$. On the other hand, we have $\left(a,b\right)\ast\left(\left(c,d\right)\ast\left(e,f\right)\right)=\left(a,b\right)\ast\left(c f+e d, d f\right)=\left(a d f+b\left(c f+e d\right),b d f\right)\\=\left(a d f+b c f+b d e, b d f\right)$. Therefore, associativity holds.
\item[C.] We can easily conclude that $\left(a,b\right)\ast\left(c,d\right)=\left(a d+b c, b d\right)=\left(c b+a d, b d\right)=\left(c,d\right)\ast\left(a,b\right)$ which proves commutativity.
\item[N.] Let us find $\left(e_1,e_2\right)$ such that $\left(e_1,e_2\right)\ast\left(a,b\right)=\left(a,b\right)$, i.e. $\left(e_1 b+e_2 a, e_2 b\right)=\left(a,b\right)$. We have two equations, first being $e_1 b+e_2 a=a$ and $e_2 b=b$. It is trivial to see that necessarily $e_2=1$. Now we have $e_1 b+a=a$ which yields $e_1=0$. Neutral element is $\left(e_1,e_2\right)=\left(0,1\right)$, as commutativity also holds we need not check $\left(a,b\right)\ast\left(0,1\right)=\left(a,b\right)$.
\item[I.] From expression $\left(a^{-1},b^{-1}\right)\ast\left(a,b\right)=\left(0,1\right)$ we shall derive the inverse elements. We have $\left(a^{-1}b+b^{-1}a,b^{-1} b\right)=\left(0,1\right)$ from which we have $a^{-1}b+b^{-1}a=0$ and $b^{-1}b=1$. Obviously, $b^{-1}=\frac{1}{b}$ ($b\neq0$ being consistent with the definition of the corresponding set), so by inserting new equality into first equation, we obtain $a^{-1}b+\frac{1}{b}a=0$. That is, $a^{-1}b=-\frac{a}{b}$, and by dividing the equation with $b\neq0$ we obtain $a^{-1}=-\frac{a}{b^2}$. Inverse elements are therefore defined for every element in the set as $\left(a^{-1},b^{-1}\right)=\left(-\frac{a}{b^2},\frac{1}{b}\right)$.
\end{itemize}

\noindent Ordered pair $\left(\R\times\left(\R\backslash\left\{0\right\}\right),\ast\right)$ is an Abelian group.

\item $\left(a,b\right)\ast\left(c,d\right)=\left(a c, b c+d\right)$, on the set $\left\{\left(x,y\right)\in\R\times\R:\ x\neq0\right\}$.

\begin{itemize}
\item[A.] On the left hand side, we have $\left(\left(a,b\right)\ast\left(c,d\right)\right)\ast\left(e,f\right)=\left(a c, b c+d\right)\ast\left(e,f\right)=\left(a c e, b c e+d e+f\right)$. On the right hand side, $\left(a,b\right)\ast\left(\left(c,d\right)\ast\left(e,f\right)\right)=\left(a,b\right)\ast\left(c e,d e+f\right)=\left(a c e, b c e+d e+f\right)$. This operation is associative.
\item[C.] It is obvious that $\left(a,b\right)\ast\left(c,d\right)=\left(a c, b c+d\right)\neq\left(a c, d a+b\right)=\left(c,d\right)\ast\left(a,b\right)$ so we conclude that the operation is not commutative.
\item[N.] First, we check the existence of $\left(e_1,e_2\right)$ on the set of $\R^2$ without $x=0$, such that $\left(e_1,e_2\right)\ast\left(a,b\right)=\left(a,b\right)$. We have $\left(e_1 a, e_2 a+b\right)=\left(a,b\right)$. It has to be $e_1 a=a$, i.e. $e_1=1$ and $e_2 a+b=b$, that is, $e_2=0$. Notice that it must be $a\neq0$. Now we can easily obtain $\left(a,b\right)\ast\left(e_1,e_2\right)=\left(a,b\right)\ast\left(1,0\right)=\left(a,b\right)$. Both sides being equal, $\left(1,0\right)\in\left(\R\backslash\left\{0\right\}\right)\times\R$ is a neutral element.
\item[I.] Let's find the inverses, if they exist, from $\left(a^{-1},b^{-1}\right)\ast\left(a,b\right)=\left(1,0\right)$. From that we get that $\left(a^{-1} a,b^{-1}a+b\right)=\left(1,0\right)$. Now we have $a^{-1}a=1$, that is $a^{-1}=\frac{1}{a}$ and $a\neq0$, which is okay, since we already do not allow the first member to be zero. From the second members in the ordered pair we have $b^{-1}a+b=0$. Now, $b^{-1}=-\frac{b}{a}$ and, again, $a\neq0$. As commutativity does not hold in general, we must verify $\left(a,b\right)\ast\left(a^{-1},b^{-1}\right)=\left(1,0\right)$, i.e. $\left(a,b\right)\ast\left(\frac{1}{a},-\frac{b}{a}\right)=\left(1,0\right)$. We have $\left(a\cdot\frac{1}{a},b\cdot\frac{1}{a}-\frac{b}{a}\right)$, which truly is $\left(1,0\right)$. Therefore, every element in the set on which the operation $\ast$ is defined has an inverse element.
\end{itemize}

\noindent Ordered pair $\left(\left(\R\backslash\left\{0\right\}\right)\times\R,\ast\right)$ is a group.

\item We need not check all the properties, for the condition that is changed, that is, the set, will affect only the inverses. Associativity still holds, commutativity does not and there is a unique neutral element for all $x\in\R^2$ because it's still valid even if we take $\left(0,b\right)\ast\left(1,0\right)=\left(0,b\right)$. Yet, by eliminating the condition that the first member in ordered pairs cannot be zero, we have no inverse element for, exempli gratia, $\left(0,b\right)$, where $b$ can be any real number. Notice that, the inverse would be $\left(\frac{1}{0},-\frac{b}{0}\right)$, which is undefined. Therefore, without inverses, operation $\ast$ on $\R^2$ is only a monoid.

\item $\left(a,b\right)\ast\left(c,d\right)=\left(a c-b d, a d+b c\right)$, on the set $\R^2\backslash\left\{\left(0,0\right)\right\}$.

\begin{itemize}
\item[A.] We can see that $\left(\left(a,b\right)\ast\left(c,d\right)\right)\ast\left(e,f\right)=\left(a c-b d, a d+b c\right)\ast\left(e,f\right)\\=\left(a c e-b d e-a d f-b c f, a c f-b d f+a d e+b c e\right)$ and that $\\\left(a,b\right)\ast\left(\left(c,d\right)\ast\left(e,f\right)\right)=\left(a,b\right)\ast\left(c e-d f,c f+d e\right)\\=\left(a c e-a d f-c f b-d e b, a c f+a d e+b c e-b d f\right)$; the members of the former and the latter ordered pair, by careful observation, coincide respectively, therefore, the operation is associative.
\item[C.] We have $\left(a,b\right)\ast\left(c,d\right)=\left(a c-b d, a d+b c\right)=\left(c a-d b, d a+c b\right)=\left(c,d\right)\ast\left(a,b\right)$ so commutativity holds.
\item[N.] Let us find neutral elements. We take $\left(e_1,e_2\right)\ast\left(a,b\right)=\left(a,b\right)$, i.e. $\\\left(e_1 a-e_2 b,e_1 b+e_2 a\right)=\left(a,b\right)$. So we are looking for $e_1$ and $e_2$ such that $e_1 a-e_2 b=a$ and $e_1 b+e_2 a=b$. If we take from the former equation $e_1=1+\frac{e_2 b}{a}$, where $a\neq 0$, and combine it with the latter equation, we get $b+e_2 b=b$, that is $e_2=0$ and $b\neq0$. Then it follows that $e_1=1$. There is no need to check the other condition, for commutativity holds in general.
\item[I.] Now, we try and find $a^{-1}$ and $b^{-1}$ such that $\left(a^{-1},b^{-1}\right)\ast\left(a,b\right)=\left(1,0\right)$. We now have two equations again, one being $a^{-1}a-b^{-1} b=1$ and the other $a^{-1} b+b^{-1} a=0$. From the former we get, e.g. that $a^{-1}=\frac{1+b^{-1}b}{a}$ and $a\neq0$. We combine it again with the latter equation and we get $\frac{b+b^{-1}b^2}{a}+b^{-1}a=0$. Now, we multiply it by $a$ and obtain $b+b^{-1}b^2+b^{-1}a^2=0$. Then, we get $b^{-1}=-\frac{b}{b^2+a^2}$, while $b^2+a^2\neq0$, that is both $a$ and $b$ must be different from zero. By inserting the result into the equation for $a^{-1}$ we see that $a^{-1}=\frac{1}{a}-\frac{b^2}{b^2+a^2}$. So, we found that the inverse for each element except origin is $\left(\frac{1}{a}-\frac{b^2}{b^2+a^2},-\frac{b}{b^2+a^2}\right)$.
\end{itemize}

\noindent Ordered pair $\left(\R^2\backslash\left\{\left(0,0\right)\right\},\ast\right)$ is Abelian group.

\item Again, as in previous examples, this cannot be a group if we allow origin to be in the set on which such operation is defined. Associativity and commutativity will hold and we can verify if we can have a neutral element for $\left(0,0\right)$. That would be $\left(0,0\right)\ast\left(1,0\right)=\left(0,0\right)$. But we cannot have an inverse element for $\left(0,0\right)$ as we would need denominators to be zero. That is impossible. Ordered pair $\left(\R^2,\ast\right)$ is a monoid.

\end{enumerate}

\noindent{\bf Problem.} Operation $A\Delta B=\left(A\backslash B\right)\cup\left(B\backslash A\right)$ is called a symmetric difference of sets $A$ and $B$. Let $D$ be a set and $\mathcal{P}_D$ power set of $D$ (the set containing all subsets of $D$). The operation $\Delta$ is to be regarded as an operation on $\mathcal{P}_D$.

\begin{enumerate}
\item Prove that there is an identity element with respect to the operation $\Delta$, which is the empty set, i.e. $\emptyset$.
\item Prove every subset $A$ of $D$ has an inverse with respect to $\Delta$, which is the set $A$ itself. Thus, $\left(\mathcal{P}_D,\Delta\right)$ is a group!
\item Let $D$ be the three-element set $D=\left\{a,b,c\right\}$. List the elements of $\mathcal{P}_D$. (For example, one element is $\{a\}$, another is $\{a,b\}$, and so on. Do not forget the empty set and the whole set $D$.) Then write the operation table for $\left(\mathcal{P}_D,\Delta\right)$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item Noting that $A\backslash\emptyset=A$ and $\emptyset\backslash A=\emptyset$ and $A\cup\emptyset=A$ for any set $A$ in $\mathcal{P}_D$, it's easy to verify that $A\Delta\emptyset=\left(A\backslash\emptyset\right)\cup\left(\emptyset\backslash A\right)=A\cup\emptyset=A$ and $\emptyset\Delta A=A$ due to commutativity of set union operation.
\item By using the fact that $A\backslash A=\emptyset$ from expression $A\Delta A=\left(A\backslash A\right)\cup\left(A\backslash A\right)=\emptyset\cup\emptyset=\emptyset$ we see that every subset $A$ of $D$ has an inverse $A$ (every element of $\mathcal{P}_D$ is an inverse of itself regarding operation $\Delta$).
\item Power set of $D$ is $\mathcal{P}_D=\left\{\emptyset,\{a\},\{b\},\{c\},\{a,b\},\{a,c\},\{b,c\},D\right\}$. We can write the operation table as follows:

\begin{center}
\begin{tabular}{c|cccccccc}
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  $\Delta$ & $\emptyset$ & $\{a\}$ & $\{b\}$ & $\{c\}$ & $\{a,b\}$ & $\{a,c\}$ & $\{b,c\}$ & $D$\\
  \hline
  $\emptyset$ & $\emptyset$ & $\{a\}$ & $\{b\}$ & $\{c\}$ & $\{a,b\}$ & $\{a,c\}$ & $\{b,c\}$ & $D$\\
  $\{a\}$ & $\{a\}$ & $\emptyset$ & $\{a,b\}$ & $\{a,c\}$ & $\{b\}$ & $\{c\}$ & $D$ & $\{b,c\}$\\
  $\{b\}$ & $\{b\}$ & $\{a,b\}$ & $\emptyset$ & $\{b,c\}$ & $\{a\}$ & $D$ & $\{c\}$ & $\{a,c\}$\\
  $\{c\}$ & $\{c\}$ & $\{a,c\}$ & $\{b,c\}$ & $\emptyset$ & $D$ & $\{a\}$ & $\{b\}$ & $\{a,b\}$\\
  $\{a,b\}$ & $\{a,b\}$ & $\{b\}$ & $\{a\}$ & $D$ & $\emptyset$ & $\{b,c\}$ & $\{a,c\}$ & $\{c\}$\\
  $\{a,c\}$ & $\{a,c\}$ & $\{c\}$ & $D$ & $\{a\}$ & $\{b\}$ & $\emptyset$ & $\{a,b\}$ & $\{b\}$\\
  $\{b,c\}$ & $\{b,c\}$ & $D$ & $\{c\}$ & $\{b\}$ & $\{a,c\}$ & $\{a,b\}$ & $\emptyset$ & $\{a\}$\\
  $D$ & $D$ & $\{b,c\}$ & $\{a,c\}$ & $\{a,b\}$ & $\{c\}$ & $\{b\}$ & $\{a\}$ & $\emptyset$\\
\end{tabular}
\end{center}
\end{enumerate}

\noindent{\bf Theorem.} Let $\left(S,\ast\right)$ be a commutative magma. If there exists a left or a right neutral element, then there exists a neutral element. Furthermore, if $\left(S,\ast\right)$ is a commutative magma with a neutral element and with a left or a right inverse, then every element has its inverse element.

\noindent\newline{\bf Proof.} Without loss of generality let us suppose that $\left(S,\ast\right)$ has a right neutral element $e_r$. Then for every $x\in S$ holds that $x\ast e_r=x$. Applying commutativity, we have $x\ast e_r=e_r\ast x=x$, therefore $e_r=e_l$ and $e_r=e_l=e$, i.e. there exist a unique neutral element $e$ for every $x\in S$. Now, if $x^{-1}_r$ is a right inverse, again without the loss of generality, we have $x\ast x^{-1}_r=e$. With commutativity, it's $x\ast x^{-1}_r=x^{-1}_r\ast x=e$, i.e. $x^{-1}_r=x^{-1}_l=x$, which means that for every $x\in S$ there exists an inverse element $x^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $\Phi=\left\{\bot,\top\right\}\cup\left\{F_1, F_2,\ldots\right\}$ be a set which contains all well-formed formulae $F_i$ and logical constants $\top$ (true) and $\bot$ (false). Check what structure following operations define on $\Phi$:

\begin{enumerate}
\item Conjunction $F\land G$, which is true when and only when both $F$ and $G$ are true.
\item Disjunction $F\lor G$, which is false when and only when both $F$ and $G$ are false.
\item Implication $F\rightarrow G$, which is false when and only when $F$ is true and $G$ false.
\item Biconditional $F\leftrightarrow G$, which is true when and only when $F$ and $G$ have equal truth values.
\end{enumerate}

\noindent{\bf Solution.} It's trivial to see that both association and commutativity is valid for conjunction, disjunction and biconditional. Therefore, for these operations, we shall only check for neutral and inverse elements.

\begin{enumerate}

\item {\it Conjunction.} Neutral element for conjunction is $\top$, as $F\land\top$ and $\top\land F$ produce $F$ ($\top$ is always true, so all truth values are dependant on $F$ being true or false; we could not consider $\bot$ as we would never get the case when both $F$ and $\bot$ are true). However, there are no inverse elements, as any formula $G$ we can try, cannot make $F\land G$ true when $F$ is false; we cannot get every case to be true because of this. Ordered pair $\left(\Phi,\land\right)$ is a commutative monoid.

\item {\it Disjunction.} For disjunction, neutral element is $\bot$, as $F\lor\bot$ and $\bot\lor F$ produce $F$ ($\bot$ is always false, so all truth values are dependant on $F$; we could not consider $\top$ as we would never get the case when both $F$ and $\top$ are false). There are no inverse elements as $F$ is always going to be true if $F$ is true, and that is something we cannot undo with any formula to get $\bot$. Ordered pair $\left(\Phi,\lor\right)$ is a commutative monoid.

\item {\it Implication.} Associativity is not valid. For $F\rightarrow\left(G\rightarrow H\right)$ is false when and only when $F$ is true and $G\rightarrow H$ is false, which is in turn false only when $G$ is true and $H$ is false. So, for this case is false when and only when $F$ and $G$ are true and $H$ is false. But $\left(F\rightarrow G\right)\rightarrow H$ is false when and only when $H$ is false, meaning that the falsness of this expression does not depend on $F$ and $G$ at all. Therefore it can be false when $F$, $G$ and $H$ are all false, which is not the case for the first expression. Operation is also not commutative as $F\rightarrow G$ is true when and only when $F$ is true and $G$ false and $G\rightarrow F$ false when and only when $G$ is true and $F$ is false. Neutral element does not exist. For the neutral element to be unique for all formulae, we need look for it only in constants $\bot$ and $\top$. It's not $\bot$ as $F\rightarrow\bot$ is false when and only when $F$ is true, and that would yield $\neg F$, not $F$. It's not $\top$ either as $F\rightarrow\top$ would never be false, that is, it would be equivalent to $\top$. We need not check the reverse cases ($\top\rightarrow F$ or $\bot\rightarrow F$), as even the necessary conditions that $F\rightarrow E$ (where $E$ is presumed neutral element) is equivalent to $F$ is not satisfied. There are no inverses as there is no unique neutral element. Therefore, ordered pair $\left(\phi,\rightarrow\right)$ is a magma.

\item {\it Biconditional.} Neutral element for biconditional is actually $\top$, as $F\leftrightarrow\top$ is true when and only when $F$ is true, meaning that expression would yield $F$. The operation is commutative, therefore, $\top$ is definitely a neutral element for this operation on $\phi$. Inverse element is actually $F$ itself as $F\leftrightarrow F$ is always true, for $F\leftrightarrow F$ is true, or $\top$, when and only when truth values of $F$ and $F$ are equal, and that is something that will always be, considering it's the same formula. Ordered pair $\left(\phi,\leftrightarrow\right)$ is an Abelian group.

\end{enumerate}

\noindent{\bf Problem.} Prove that if closure (from axioms of totality) does not hold, then so does not associativity.

\noindent\newline{\bf Solution.} Let $S$ be a set, and $\ast$ an operation which does not satisfy the condition of closure on set $S$. That means that there exist $a,b\in S$ such that $a\ast b\notin S$. Now, if we try to prove associativity, $x\ast(y\ast z)=(x\ast y)\ast z$, it has to be valid for all elements $x,y,z\in S$. But, as we can take $x=a$ and $y=b$, then $a\ast(b\ast z)$ may be defined (as $b\ast z$ might yield some element $c\in S$ and $a\ast c$ might also be in $S$), but $(a\ast b)\ast z$ is certainly not in $S$, as we cannot take an element that is not in $S$ and perform an operation with some element $z$ that is in $S$ (as binary operation demands that both operands be from the same set).

\noindent\newline{\bf Problem.} Name the structure that the following operations define on set $\N$ (the set of natural numbers):

\begin{enumerate}
\item Addition.
\item Subtraction.
\item Multiplication.
\item Division.
\end{enumerate}

\noindent{\bf Solution.} It's trivial to notice that associativity and commutativity hold for addition and multiplication, and that associativity and commutativity do not hold for subtraction and division. So, let's observe only the inverse and neutral elements.

\begin{enumerate}
\item{\it Addition.} Neutral element for addition is such $e\in\N$ that $x+e=e+x=x$, that would be $e=0$, but $0\notin\N$, so there is no neutral element for addition in $\N$. There can be no inverses without a neutral element. Ordered pair $\left(\N,+\right)$ is a commutative semigroup.

\item{\it Subtraction.} Operation of subtraction does not satisfy closure because, e.g. $4-5=-1\notin\N$ and $4,5\in\N$. Associativity then also does not hold, following from the previous problem. Neutral element would be $e=0$ for $x-0=x$, but $0\notin\N$ and $0-x=-x\neq x=x-0$ (if $x\in\N$, then definitely $-x\notin\N$; here we are presuming that $-x$ is defined in $\Z\supset\N$ so we can talk about notion of opposite numbers, though outside of $\N$). There are no neutral elements and no inverses. Ordered pair $\left(\N,-\right)$ is not even a partial magma.

\item{\it Multiplication.} Neutral element for multiplication is $e=1$ as $x\cdot 1=1\cdot x=x$, and $1\in\N$. However, there are no inverse elements for some $x\in\N$, e.g. there exist no $x\in\N$ such that $5\cdot x=1$ (obviously $5\in\N$; presuming that the reader considers $\Q$ then, $x$ would be $\frac{1}{5}\in\Q$, but not in $\N$). Ordered pair $\left(\N,\cdot\right)$ is a commutative monoid.

\item{\it Division.} Closure is not satisfied as, e.g. $5:3\notin\N$ (it is in $\Q$, however). Therefore, we cannot consider associativity. Neutral element might be $1\in\N$ for $x:1=x$, but $1:x=\frac{1}{x}$ (which is in $\Q\backslash\{0\}$, but not in $\N$). Plus, division by zero is not even defined. There are no inverses, so, as with subtraction, ordered pair $\left(\N,:\right)$ is not even a partial magma.

\end{enumerate}

\noindent{\bf Problem.} Name the structure that the following operations define on set $\Z$ (the set of integers):

\begin{enumerate}
\item Addition.
\item Subtraction.
\item Multiplication.
\item Division.
\end{enumerate}

\noindent{\bf Solution.} We will grant associativity and commutativity for addition and multiplication in $\Z$, for it's a trivial thing. Also, it's obvious to see that subtraction and division are not associative nor commutative in $\Z$.

\begin{enumerate}
\item {\it Addition on $\Z$.} Associativity and commutativity holds, as stated above. Now, $0$ is obviously a neutral element as $x+0=x$ for all $x\in\Z$. And for every $x\in\Z$ there is an inverse $-x\in\Z$, as $x+(-x)=0$. $\left(\Z,+\right)$ is an Abelian group! Yay!
\item {\it Subtraction on $\Z$.} Closure holds this time, as $a-b$ is defined on $\Z$ for all $a,b\in\Z$. There are no neutral elements and therefore no inverses, as $x-0=x$ but $0-x=-x$ (our only candidate for a neutral element). $\left(\Z,-\right)$ is a magma.
\item {\it Multiplication on $\Z$.} Neutral element exists, that is $e=1$. Inverses do not, generally, exist, e.g. for $3\cdot x=1$ there is no such $x$ that would satisfy this equation (it would be $\frac{1}{3}\in\Q$). $\left(\Z,\cdot\right)$ is a commutative monoid.
\item {\it Division on $\Z$.} Not even closure holds here and for $0$ divisor it is not even defined. $\left(\Z,:\right)$ is not even a partial magma, again.
\end{enumerate}

\noindent{\bf Problem.} Name the structure that the following operations define on set $\Q$ (the set of rational numbers):

\begin{enumerate}
\item Addition.
\item Subtraction.
\item Multiplication.
\item Division.
\end{enumerate}

\noindent{\bf Solution.} As in previous problems, we will check only for neutral elements and inverses.

\begin{enumerate}
\item {\it Addition on $\Q$}. Neutral element is again $e=0$ and inverses are $-x\in\Q$. $\left(\Q,+\right)$ is an Abelian group.
\item {\it Subtraction on $\Q$}. As in previous example, there is no neutral element, and no inverse elements, therefore $\left(\Q,-\right)$ is a magma.
\item {\it Multiplication on $\Q$}. Neutral element is $e=1$, and inverse for every $x\neq 0$, $x\in\Q$ exists and that is $\frac{1}{x}\in\Q$. Alas! The zero has no inverse, therefore $\left(\Q,\cdot\right)$ is a commutative monoid. But! All is not lost, as we can remove the zero (or can we perhaps define some element $\zeta$ to satisfy $0\cdot\zeta=1$?) and get an ordered pair $\left(\Q\backslash\{0\},\cdot\right)$ to be an Abelian group.
\item {\it Division on $\Q$}. Again, division by zero is not defined, but even if we took $\Q\backslash\{0\}$, could we find a neutral element or inverses? It would have to be $x:e=e:x=x$, the only possibility would be $e=1$ as $x:1=x$, but $1:x\neq x$. Therefore, $\left(\Q,:\right)$ is a partial magma, but $\left(\Q\backslash\{0\},:\right)$ is a magma.
\end{enumerate}

\noindent{\bf Problem.} Does even one operation defined on the set of irrational numbers satisfy at least closure?

\noindent\newline{\bf Solution.} If we add two irrational numbers we can get a rational number. {\it Example.} Obviously $\sqrt{2}$ is a rational number, as is $(-\sqrt{2})$. Yet, $\sqrt{2}+(-\sqrt{2})=0$ which is a whole number. The same goes for subtraction. As for multiplication, $\sqrt{2}\cdot\sqrt{2}=2$, which is a natural number. For division, $\sqrt{2}:\sqrt{2}=1$, which is, again, a natural number. Therefore not even one of the four standard arithmetic operations satisfy the axioms of totality.

\noindent\newline{\bf Problem.} Name the structure that the following operations define on set $\R$ (the set of real numbers):

\begin{enumerate}
\item Addition.
\item Subtraction.
\item Multiplication.
\item Division.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Addition on $\R$.} Neutral element is $e=0$ and inverse is $-x$, both of which are in $\R$. Ordered pair $\left(\R,+\right)$ is an Abelian group.
\item {\it Subtraction on $\R$.} As with a previous example, there are no neutral elements and no inverses. Axioms of totality are satisfied, however. Ordered pair $\left(\R,-\right)$ is a magma.
\item {\it Multiplication on $\R$.} Neutral element is $e=1$, but as in a previous example, there is no inverse for $0\in\R$, therefore $\left(\R,\cdot\right)$ is a commutative monoid and $\left(\R\backslash\{0\},\cdot\right)$ is an Abelian group.
\item {\it Division on $\R$.} There are no neutral elements, again, as in $\Q$, and no inverses. Division by zero is not defined so $\left(\R,:\right)$ is a partial magma, but $\left(\R\backslash\{0\},:\right)$ is a magma.
\end{enumerate}

\noindent{\bf Problem.} Let $\alpha=\{0,1\}$ be an alphabet and $\alpha_n$ a set of all words of length $n\geq 1$, derived by combining elements of $\alpha$. Name the structure that following operations define on $\alpha_n$:

\begin{enumerate}
\item (Bitwise) AND.
\item (Bitwise) OR.
\item (Bitwise) XOR.
\item (Bitwise) NOR (Sheffer stroke).
\item (Bitwise) NAND (Pierce arrow).
\end{enumerate}

\noindent{\bf Solution.} Before we begin, we will add a following table which will help us to solve all the problems above. For every $A,B\in\alpha$ we can define all the possibilities as follows:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$A$ & $B$ & $A\wedge_B B$ & $A\vee_B B$ & $A\underline{\vee}_B B$ & $A\uparrow_B B$ & $A\downarrow_B B$\\
\hline
1 & 1 & 1 & 1 & 0 & 0 & 0\\
1 & 0 & 0 & 1 & 1 & 1 & 0\\
0 & 1 & 0 & 1 & 1 & 1 & 0\\
0 & 0 & 0 & 0 & 0 & 1 & 1\\
\hline
\end{tabular}
\end{center}

\noindent\newline Now, these operations can be observed if we look upon $A$ and $B$ as ordered $n$-tuples $(a_1,\ldots,a_n)$ and $(b_1,\ldots,b_n)$ and then define $A\circ_B B$, where $\circ_B\in\{\wedge_B,\vee_B,\underline{\vee}_B,\uparrow_B,\downarrow_B\}$ as $(a_1\circ b_1,\ldots,a_n\circ b_n)$. Then, e.g. if $A=1001$ and $B=1010$, then $A\wedge_B B=1001\wedge_B 1010=1000$. See that every new "digit" corresponds to the conjuction of corresponding "digits" of $A$ and $B$. Obviously, this time, axioms of totality are satisfied as we are always bound to get a word and that word will be of length equal to $n$.

\begin{enumerate}
\item {\it Bitwise AND.} Associativity and commutativity hold because conjuction is associative and commutative. Reader can easily verify that claim. Neutral element is a word which contains only digit $1$. Now we need $A^{-1}\in\alpha_n$ such that $\left(a_0\wedge a^{-1},\ldots,a_n\wedge a_n^{-1}\right)=\left(1,1\ldots,1\right)$. Obviously the inverse does not exist if some $a_i=0$. Through conjuction digit $1$ cannot be obtained. Ordered pair $\left(\alpha_n,\wedge_B\right)$ is a commutative monoid.
\item {\it Bitwise OR.} Same thing as with AND, except that the neutral element is a word containing only zero digits. There is no inverse also, as if there is a $1$ in a word, we never get a zero by disjunction. Ordered pair $\left(\alpha_n,\vee_B\right)$ is a commutative monoid.
\item {\it Bitwise XOR.} Operation is associative and commutative (actually every operation of this kind that has same value for different propositional values). Now, there is no neutral element and there are no inverses, so ordered pair $\left(\alpha_n,\underline{\vee}_B\right)$ is a commutative semigroup.
\item {\it Bitwise NOR (Sheffer stroke).} Operation is commutative and associative and there is a neutral element, that is a word containing all zeros. There are no inverses as we can never obtain zero if there is a zero in the word itself. Therefore, $\left(\alpha_n,\uparrow_B\right)$ is a commutative monoid.
\item {\it Bitwise NAND (Pierce arrow).} Same thing as with Sheffer stroke, except the neutral element is the word containing all digits $1$. There are no inverses, as we can never get one when there is one in the word. Therefore $\left(\alpha_n,\underline{\vee}_B\right)$ is a commutative monoid.
\end{enumerate}

\noindent{\bf Theorem.} If $\left(G,\cdot\right)$ is a group and $a$, $b$ and $c$ are elements of $G$, then:

\begin{itemize}
\item[(i)] $a b=a c$ implies $b=c$ and
\item[(ii)] $b a=c a$ implies $b=c$.
\end{itemize}

\noindent{\bf Proof.} (i) Let us suppose that $ab=ac$. By multiplying that expression with $a^{-1}$ (as $\left(G,\cdot\right)$ is a group there exists $a^{-1}$ for every $a\in G$) on the left we have $a^{-1}(a b)=a^{-1}(a c)$. Now, as associativity also holds, we can rearrange the elements in expression so that $(a^{-1} a) b=(a^{-1} a) c$. As $a^{-1} a=e$, we have $e b=e c$. Now, as $e x=x$ for every $x\in G$, we finally have $b=c$. (ii) Proof is analogous to the (i) with exception of multiplying with $a^{-1}$ on the right.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $\left(G,\cdot\right)$ is a group and $a$, $b$ are elements of $G$, then $a b=e$ implies $a=b^{-1}$ and $b=a^{-1}$.

\noindent\newline{\bf Proof.} Let's multiply first expression by $b^{-1}$ (as $\left(G,\cdot\right)$ is a group there is an inverse for each element in $G$ and so for $b$) on the right. We get $(a b)b^{-1}=e b^{-1}$. Applying associativity, we have $a (b b^{-1})=e b^{-1}$ and by using properties of neutral element $e$, finally, we have $a e=b^{-1}$, i.e. $a=b^{-1}$. For the second part, we only have to multiply first expression with $a^{-1}$ on the left and we will obtain, following the same line of reasoning, $b=a^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $\left(G,\cdot\right)$ is a group and $a$, $b$ are elements of $G$, then

\begin{enumerate}
\item[(i)] $\left(a b\right)^{-1}=b^{-1} a^{-1}$;
\item[(ii)] $\left(a^{-1}\right)^{-1}=a$.
\end{enumerate}

\noindent\newline{\bf Proof.} Ad (i). It is certainly true that $\left(a b\right)^{-1}\left(a b\right)=e$, as there is a neutral element for every element (and we can imagine $a b$ being one element as the operation must yield a unique element if it's a group). Now, we multiply that expression by $b^{-1}$ on the right and get $\left(\left(a b\right)^{-1}\left(a b\right)\right)b^{-1}=e b^{-1}$. Using associativity we have $\left(a b\right)^{-1}\left(\left(a b\right)b^{-1}\right)=e b^{-1}$ and by using properties of $e$ and again associativity, we obtain $\left(a b\right)^{-1}\left(a\left(b b^{-1}\right)\right)=b^{-1}$. Now we use $b b^{-1}=e$ so it is $\left(a b\right)^{-1}\left(a e\right)=b^{-1}$, and $a e$ being $a$, we have $\left(a b\right)^{-1} a=b^{-1}$. By multiplying by $a^{-1}$ on the right, we have $\left(\left(a b\right)^{-1} a\right)a^{-1}=b^{-1}a^{-1}$. By using associativity, we have $\left(a b\right)^{-1}\left(a a^{-1}\right)=b^{-1}a^{-1}$. Now, as $a a^{-1}=e$ we finally obtain $\left(a b\right)^{-1} e=b^{-1}a^{-1}$, i.e. $\left(a b\right)^{-1}=b^{-1}a^{-1}$. Ad (ii). As $\left(G,\cdot\right)$ is a group, every element has its inverse and so does $a^{-1}$ which would be $\left(a^{-1}\right)^{-1}$. Now, it's $a^{-1}\left(a^{-1}\right)^{-1}=e$. By multiplying the expression with $a$ on the left, we have $a\left(a^{-1}\left(a^{-1}\right)^{-1}\right)=a e$. We use properties of $e$, that is $a e=a$, and associativity to obtain $\left(a a^{-1}\right)\left(a^{-1}\right)^{-1}=a$. Finally, as $a a^{-1}=e$ we have $e\left(a^{-1}\right)^{-1}=a$, that is, $\left(a^{-1}\right)^{-1}=a$. That would say, the inverse of the inverse of the original is the original.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} (i) In last three theorems we were very rigorous when explicitly stating what is group and what is a set. But from now on, due to simplicity, we shall denote by $G$ both the group and the set $G$ on which it is defined (if it is unambiguous). (ii) Notice that associativity actually tells us that parentheses are redundant. So, we don't need that kind of rigorous treatment of associativity, if it holds (and in a group, monoid and semigroup it certainly will). If we define $a b c=a (b c)$, then we get that $(a b) c=a (b c)=a b c$. By mathematical induction we can prove it for arbitrarily many elements (in the following lemma).

\noindent\newline{\bf Proposition.} Let $S$ be a set on which we define binary operation $\cdot:S\times S\rightarrow S$ satisfying axioms of totality and at least property of associativity. Parentheses are redundant.

\noindent\newline{\bf Proof.} We have already shown, for $n=3$ (which will be the basis of our induction), that, as $a (b c)=(a b) c$, and because there are no more distributions of parentheses, we can write $a (b c)=(a b) c=a b c$. If we assume the claim is valid for all $k<n$, that is, all distributions of parentheses are equal and can be uniquely designated by $a_1 a_2\cdots a_{n-1}$, then we can show that it will also be valid for $a_1 a_2\cdots a_{n-1} a_n$. First, note that the former expression can be broken into two subproducts so that $\left(a_1\cdots a_i\right)\left(a_{i+1}\cdots a_n\right)$. This can be done, as if it were not the case, in distribution of the parentheses, we would never be allowed to place the parentheses in between to members, changing nothing (i.e. $(a_1\cdots a_n)=a_1\cdot a_n$). Then, as $(i-1)+1=i<n$ and $n-(i+1)+1=n-i<n$ (we can never have $i=0$, as such index does not exist), then by assumption of induction, both those subproduct can be written without parentheses. When moving $i$ from $1$ to $n$, we exhaust all possibilities.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Catalan's problem).} The number of ways in which we can associate a sequence of $n$ factors is equal to Catalan's number, that is,

\begin{equation*}
C_n=\sum_{i=1}^{n-1}{C_{i}C_{n-i}}=\frac{1}{n}\binom{2(n-1)}{(n-1)},
\end{equation*}

\noindent\newline for $n>2$, $n\in\N$, and $C_1=C_2=1$.

\noindent\newline{\bf Proof.} As for $n=1$ we have only one element and for $n=2$ only two elements, we can say that, in both cases we have only one choice to associate them, $(a_1)$ and $(a_1 a_2)$, where the parentheses are actually even sintacticaly redundant. So we can set $C_1=C_2=1$. Now, for $n=3$, we can see that we have two cases, that is $C_3=2$, because we only have $(a_1 a_2) a_3$ and $a_1 (a_2 a_3)$. For $n=4$, we have $\left((a_1 a_2)a_3\right)a_4$, $\left(a_1(a_2 a_3)\right)a_4$, $a_1\left((a_2 a_3) a_4\right)$, $a_1\left(a_2 (a_3 a_4)\right)$ and $(a_1 a_2)(a_3 a_4)$, that is $C_4=5$. Now, we can see the line of reasoning we are going to follow. Let us analyze all the possibilities we can have for $n=4$, than we will generelize it for any $n\in\N$ ($n>2$). First case is when we had $(a_1)(a_2 a_3 a_4)$. Here, $a_1$ can be associated in one way ($C_1=1$) and $a_2 a_3 a_4$ in two ways ($C_3=2$), that is, $C_1\cdot C_3=1\cdot2=2$ ways altogether. Now, we move the parentheses to the right until we reach the end. Now, we have $(a_1 a_2)(a_3 a_4)$. Both of these parentheses we can associate in only one way as $C_2=1$, so we have only one way altogether as $C_2\cdot C_2=1\cdot1=1$. Finally, we have $(a_1 a_2 a_3)(a_4)$. There are two ways to associate the factors inside the former parentheses and only one way to associate the latter parentheses, that is $C_3\cdot C_1=2\cdot 1=2$. That is $C_4=C_3 C_1+C_2 C_2+C_1 C_3=2+1+2=5$. Notice the movement of the indices and notice the useful fact that their sum equals $4$ in every summand. In fact, for $n$ the sum of indeces will move from $1$ to $n-1$ in both directions and their sum will always be $n$. Let's try and justify this fact further. Let's observe the expression of the form $a_1 a_2 a_3\cdots a_n$. Now, we will start associating, as in $n=4$, with $(a_1)(a_2 a_3\cdots a_n)$ and move the right parentheses enclosing $a_1$ and left parentheses enclosing $a_2\cdots a_n$ to the right, one by one step. We can do that for $n-1$ steps. In that way we form a meaningful sequence (top row) with number of associations for the case above (bottom row):

\begin{center}
\begin{tabular}{c|c|c|c|c}
  $(a_1)(a_2\cdots a_n)$ & $(a_1 a_2)(a_3\cdots a_n)$ & $\ldots$ & $(a_1\cdots a_{n-2})(a_{n-1}a_n)$ & $(a_1\cdots a_{n-1})(a_n)$\\
  \hline
  $C_1\cdot C_{n-1}$ & $C_2\cdot C_{n-2}$ & $\ldots$ & $C_{n-2}\cdot C_2$ & $C_{n-1}\cdot C_1$\\
\end{tabular}
\end{center}

\noindent\newline This way, summing all the cases, we have justified that $C_n=\sum_{i=1}^{n-1}{C_{i}C_{n-i}}$ for $n>2$.

Now we want to show that $C_n=\frac{1}{n}\binom{2(n-1)}{(n-1)}$. First, we shall use the generating function defined as 

\begin{equation*}
C(x)=C_1+C_2 x+C_3 x^2+\ldots+C_n x^{n-1}+C_{n+1} x^n+\ldots
\end{equation*}

\noindent\newline to get the recursive formula for $C_n$. One can easily check that

\begin{eqnarray*}
C^2(x)&=&\left(C_1+C_2 x+C_3 x^2+\ldots\right)^2\\
&=&\left(C_1+C_2 x+C_3 x^2+C_4 x^3+\ldots\right)\left(C_1+C_2 x+C_3 x^2+C_4 x^3+\ldots\right)
\end{eqnarray*}

\noindent\newline gives us the recursion we need. For, if we multiply all the summands necessary to get $C_n$, then we need to have $x^i x^j$ such that $i+j=n-1$. As indices follow the exponents, differing only by $1$, we will have the sum of all the combinations of $C_i C_j$ whose indices together add to $n+1$ (that is $(n-1)+2$ as each $C_i$ differs from the corresponding exponent by $1$; adding two indices, the difference becomes $2$). By multiplying first few members, we get the general idea:

\begin{eqnarray*}
C^2(x)&=&C_1 C_1+x\left(C_2 C_1+C_1 C_2\right)+x^2\left(C_1 C_3+C_2 C_2+C_3 C_1\right)+\ldots\\
&+&x^{n-2}\left(\sum_{i=1}^{n-1}{C_i C_{n-i}}\right)+\ldots
\end{eqnarray*}

\noindent\newline Now, by using the recursive formula, we see that where in $C(x)$ we have $C_n x^{n-1}$ there we in $C^2(x)$ have $C_n x^{n-2}$. Therefore, we will multiply $C^2(x)$ by $x$ to get coefficients right; now we have:

\begin{eqnarray*}
x C^2(x)&=&x C_1 C_1+x^2\left(C_2 C_1+C_1 C_2\right)+x^3\left(C_1 C_3+C_2 C_2+C_3 C_1\right)+\ldots\\
&+&x^{n-1}\left(\sum_{i=1}^{n-1}{C_i C_{n-i}}\right)+\ldots
\end{eqnarray*}

\noindent\newline Further progress is made if we subtract $x C^2(x)$ from $C(x)$. Every summand disappears, leaving only $C_1$, that is $1$ (notice that $C_1=C_2=1$ so $x C_2-x C_1 C_1=x-x=0$). So we have $C(x)-x C^2(x)=1$. That is the quadratic equation, $x C^2(x)-C(x)+1=0$ which we are going to solve for $C(x)$. We use the standard formula:

\begin{equation*}
C(x)=\frac{-1\pm\sqrt{1-4x}}{2x}.
\end{equation*}

\noindent\newline All we have to do now is to decide whether to use positive or a negative side in front of square root. That we can easily verify by calculating $C(0)$. Now, we cannot have zero in the denominator so we have to do a little trick. We will substitute $C(x)$ for the formula above:

\begin{equation*}
\frac{-1\pm\sqrt{1-4x}}{2x}=C_1+C_2 x+C_3 x^2+\ldots+C_n x^{n-1}+\ldots
\end{equation*}

\noindent\newline We multiply the whole expression by $2x$, we transfer the $-1$ to the right-hand side and we have:

\begin{equation*}
\pm\sqrt{1-4x}=-1+2 C_1 x+2 C_2 x^2+2 C_3 x^3+\ldots+2 C_n x^n+\ldots
\end{equation*}

\noindent\newline Now if we take $x=0$ (keeping in mind it's actually a limit), on the left only $\pm 1$ remains and on the right only $-1$. For the two sides of equation to be equal we have to take the negative sign, that is, the generating function for our recursion is

\begin{equation*}
C(x)=\frac{-1-\sqrt{1-4x}}{2x},
\end{equation*}

\noindent\newline and by the same logic, we have

\begin{equation*}
-\sqrt{1-4x}=-1+2 C_1 x+2 C_2 x^2+2 C_3 x^3+\ldots+2 C_n x^{n}+\ldots
\end{equation*}

\noindent\newline By using a generalized form of binomial formula, we have

\begin{equation*}
(1-4x)^{\frac{1}{2}}=\sum_{n=0}^{\infty}{(-4)^n \binom{\frac{1}{2}}{n} x^n}.
\end{equation*}

\noindent\newline By equating the latter formula with the former one, we easily see that

\begin{equation*}
-(-4)^n\binom{\frac{1}{2}}{n}=2 C_n.
\end{equation*}

\noindent\newline Now we use the formula

\begin{equation*}
\binom{n}{k}=\frac{\prod_{i=0}^{k-1}{\left(n-i\right)}}{k!}.
\end{equation*}

\noindent\newline Using that, we now have:

\begin{equation*}
-(-4)^n\frac{\prod_{i=0}^{n-1}{\left(\frac{1}{2}-i\right)}}{n!}=2 C_n.
\end{equation*}

\noindent\newline Dividing everything by $2$ and setting $(-4)^n=(-2)^n 2^n$ we get:

\begin{equation*}
C_n=-(-2)^n 2^n\frac{\prod_{i=0}^{n-1}{\left(\frac{1}{2}-i\right)}}{2 n!}.
\end{equation*}

\noindent\newline To further simplify the expression, we will use the fact that $a^n=\prod_{i=0}^{n-1}{a}$, for some $a\in\R$, which gets us to expression

\begin{equation*}
C_n=-2^n\left(\prod_{i=0}^{n-1}{(-2)}\right)\left(\frac{\prod_{i=0}^{n-1}{\left(\frac{1}{2}-i\right)}}{2 n!}\right).
\end{equation*}

\noindent\newline We have the same number of factors in both products so we can distribute them as $\left(-2\right)\left(\frac{1}{2}-i\right)$ to obtain

\begin{equation*}
C_n=-\frac{2^n\prod_{i=0}^{n-1}{\left(2 i-1\right)}}{2 n!}.
\end{equation*}

\noindent\newline Notice that for $i=0$ we will have $(-1)$ in the product. We can extract that member to make the negative sign disappear, leaving the expression explicitly positive:

\begin{equation*}
C_n=\frac{2^n\prod_{i=1}^{n-1}{\left(2 i-1\right)}}{2 n!}.
\end{equation*}

\noindent\newline We can also further simplify $\frac{2^n}{2}$ as $2^{n-1}$. Also, notice that the product actually gives the sequence $1\cdot 3\cdot 5\cdots\left(2n-5\right)\left(2n-3\right)$. If we multiplied that product with even factors such as $2\cdot 4\cdot 6\cdots\left(2n-2\right)=\prod_{i=1}^{n-1}{2i}$, that would nicely equal $\left(2(n-1)\right)!$. So we do it this way:

\begin{equation*}
C_n=\frac{2^{n-1}\prod_{i=1}^{n-1}{\left(2 i-1\right)}}{n!}\cdot\frac{\prod_{i=1}^{n-1}{2i}}{\prod_{i=1}^{n-1}{2i}}=\frac{2^{n-1}\prod_{i=1}^{n-1}{\left(\left(2 i-1\right)2 i\right)}}{n!\prod_{i=1}^{n-1}{2i}}.
\end{equation*}

\noindent\newline It's obvious to notice that $\prod_{i=1}^{n-1}{\left(\left(2 i-1\right)\left(2i\right)\right)}=(2(n-1))!$. Furthermore, we can see that $\prod_{i=1}^{n-1}{2i}=2^{n-1}\prod_{i=1}^{n-1}{i}=2^{n-1}(n-1)!$. Now we have

\begin{equation*}
C_n=\frac{2^{n-1}\left(2(n-1)\right)!}{2^{n-1}n(n-1)!(n-1)!}=\frac{\left(2(n-1)\right)!}{n\left(2(n-1)-(n-1)\right)!\left(n-1\right)!}.
\end{equation*}

\noindent\newline Finally, by using $\binom{n}{k}=\frac{n!}{(n-k)!k!}$, we have the general formula:

\begin{equation*}
C_n=\frac{1}{n}\binom{2(n-1)}{n-1},
\end{equation*}

\noindent\newline for any $n>2$ and $n\in\N$. That is the solution to the Catalan's problem and $C_n$ are called Catalan's numbers. Notice that, in some other literatures, indices might slightly differ, as we took (for good reasons) the starting number as $C_1$, where it's usually taken as $C_0$ and then it's $C_n=\frac{1}{n+1}\binom{2n}{n}$, for $n>1$, $n\in\N$ and $C_0=C_1=1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $S$ be a non-commutative semigroup. The number of different ways in which the expression $a_1 a_2\cdots a_n$, where $a_1,\ldots,a_n\in S$, can be evaluated is $(n-1)!$.

\noindent\newline{\bf Proof.} If $n=2$, the number of ways in which it can be calculated is only one as we only have $a_1 a_2$. Let us suppose that $a_1\cdots a_n$ can be calculated in $(n-1)!$ ways. We need to prove that $a_1\cdots a_n a_{n+1}$ can be calculated in $n!$ different ways. Now, from $n+1$ factors, we can choose consecutive pairs in $n$ ways. Without loss of generality, we can suppose we chose $a_n a_{n+1}$ which yields some element $b$. Now, we have $a_1\cdots a_{n-1} b$, an expression with $n$ factors which can be calculated in $(n-1)!$ ways. That is $n(n-1)!=n!$ different ways for $n+1$ factors.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ be a group and $a_1,\ldots,a_n\in G$. Then:

\begin{equation*}
\left(a_1 a_2\cdots a_n\right)^{-1}=a_n^{-1}a_{n-1}^{-1}\cdots a_1^{-1}.
\end{equation*}

\noindent\newline{\bf Proof.} For $n=1$ we have $a_1^{-1}=a_1^{-1}$. For $n=2$ we have $\left(a_1 a_2\right)^{-1}=a_2^{-1} a_1^{-1}$. That we can prove by the fact that $\left(a_1 a_2\right)^{-1}\left(a_1 a_2\right)=e$. Then, by multiplying the expression on the right-hand side with $a_2^{-1}$ and $a_1^{-1}$, succesively, we get the much desired expression. Let's assume that it's true that $\left(a_1\cdots\right)^{-1}=a_n^{-1}\cdots a_1^{-1}$ for some $n\in\N$. Now, let's prove that $\left(a_1\cdots a_{n+1}\right)^{-1}=a_{n+1}^{-1}\cdots a_1^{-1}$ is also true. We can use the fact that $a_n^{-1}\cdots a_1^{-1}=\left(a_1\cdots a_n\right)^{-1}$. Then we have $a_{n+1}^{-1}\cdots a_1^{-1}=a_{n+1}^{-1}\left(a_1\cdots a_n\right)^{-1}$. We can consider $a_1\cdots a_n$ as a single element, let's say $b$. Now, $a_{n+1}^{-1} b^{-1}=a_{n+1}^{-1}\cdots a_1^{-1}$. From case when $n=2$, we can write the expression as $\left(b a_{n+1}\right)^{-1}=a_{n+1}^{-1}\cdots a_1^{-1}$. Returning substitution for $b$, we have $\left(a_1\cdots a_{n+1}\right)^{-1}=a_{n+1}^{-1}\cdots a_1^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} The former theorem can be more neatly expressed as:

\begin{equation*}
\left(\prod_{i=1}^{n}{a_i}\right)^{-1}=\prod_{i=1}^{n}{a_{n-(i-1)}^{-1}}.
\end{equation*}

\noindent\newline{\bf Problem.} Let $a$, $b$, $c$ and $x$ be elements of a group $G$. Solve for $x$ in terms of $a$, $b$ and $c$ following equations:

\begin{enumerate}
\item $x^2=b$ and $x^5=e$;
\item $a x b=c$;
\item $x^2 b=x a^{-1} c$;
\item $x^2 a=b x c^{-1}$ and $a c x=x a c$;
\item $a x^2=b$ and $x^3=e$;
\item $x^2=a^2$ and $x^5=e$;
\item $(x a x)^3=b x$ and $x^2=\left(x a\right)^{-1}$;
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item $x^2=b$ and $x^5=e$. We can write $x^5$ as $x^2 x^2 x$, then the latter expression becomes $x^2 x^2 x=e$. Substituting $x^2$ for $b$, we have $b b x=e$, that is $b^2 x=e$. Now, multiplying by $\left(b^2\right)^{-1}$ on the left, we have $\left(b^2\right)^{-1}b^2 x=\left(b^2\right)^{-1}$, i.e. $x=\left(b^2\right)^{-1}$.

\item $a x b=c$. Multiplying by $a^{-1}$ on the left and by $b^{-1}$ on the right. That way, it's $a^{-1} a x b b^{-1}=a^{-1} c b^{-1}$, i.e. $x=a^{-1} c b^{-1}$.

\item $x^2 b=x a^{-1} c$. After writing equation as $x x b=x a^{-1} c$, we shall multiply the expression by $x^{-1}$ on the left and by $b^{-1}$ on the right. Now, it's $x^{-1} x x b b^{-1} = x^{-1} x a^{-1} c b^{-1}$ which implies, after using $x^{-1}x=e$ and $b b^{-1}=e$, that $x=a^{-1} c b^{-1}$.

\item $x^2 a=b x c^{-1}$ and $a c x=x a c$. By multiplying the second equation with $x$ on the left, we have $x a c x=x^2 a c$. If we substitute $x^2 a$ from the first equation, we have $x a c x=b x c^{-1} c$, i.e. $x a c x=b x$. Multiplying this equality with $x^{-1}$ on the right, we have $x a c x x^{-1}=b x x^{-1}$, that is, $x a c=b$. Now, we multiply this equality with $c^{-1}$ and then with $a^{-1}$ on the right, and we have $x a c c^{-1} a^{-1}=b c^{-1} a^{-1}$, that is, using successive properties of the neutral element, $x a a^{-1}=b c^{-1} a^{-1}$ and finally $x=b c^{-1} a^{-1}$.

\item $a x^2=b$ and $x^3=e$. If we simply multiply former equality with $x$ on the right, we have $a x^3=b x$. Using $x^3=e$, we have $a=b x$. Multiplying by $b^{-1}$ on the left, the solution is $x=b^{-1} a$.

\item $x^2=a^2$ and $x^5=e$. We can write the latter equality as $x^2 x^2 x=e$. Substituting $x^2=a^2$ we have $a^2 a^2 x=e$, i.e. $a^4 x=e$. We multiply this with $\left(a^4\right)^{-1}$ on the left and get $\left(a^4\right)^{-1} a^4 x=\left(a^4\right)^{-1}$, that is $x=\left(a^{-1}\right)^4$ (notice that, from previous theorem, we have $\left(a^n\right)^{-1}=\left(a^{-1}\right)^n$; actually, using this fact, we could allow ourselves to write this as $a^{-n}$ without any ambiguity).

\item $(x a x)^3=b x$ and $x^2=\left(x a\right)^{-1}$. Let's write the former equation in a different way, that is, as $(x a x)(x a x)(x a x)=b x$, that is $x a x^2 a x^2 a x=b x$. Substituting $x^2$ from the second equation, we have $x a\left(x a\right)^{-1} a\left(x a\right)^{-1}a x=b x$. Now it's $a a^{-1} x^{-1} a x=b x$, i.e. $x^{-1} a x=b x$. If we multiply that equality with $x$ on the left and $x^{-1}$ on the right, we obtain $a=x b$, that is, by multiplying with $b^{-1}$ on the right, $x=a b^{-1}$.
\end{enumerate}

\noindent{\bf Problem.} Prove or disprove that following arguments hold for any group\footnote{For now, consider that $\left(\R\backslash\{0\},\cdot\right)$ is an Abelian group; if something does not hold for an Abelian group then it will hold nor for a group, nor for a semigroup, nor for a monoid, nor for a magma. Take any necessary counterexamples from this Abelian group.}. If the argument generally does not hold, find an example of a group (or an Abelian group) where it would hold.

\begin{enumerate}
\item If $x^2=e$ then $x=e$.
\item If $x^2=a^2$ then $x=a$.
\item $\left(a b\right)^2=a^2 b^2$.
\item If $x^2=x$ then $x=e$.
\item Let $G$ be a group. Then, for all $x\in G$ there exists $y\in G$ such that $x=y^2$.
\item Let $G$ be a group. Then, for all $x,y\in G$ there exists $z\in G$ such that $y=x z$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item It's true in $\R\backslash\{0\}$ (considering $x=-1$ and $e=1$) that $(-1)^2=1$, but $-1\neq 1$ (i.e. $x\neq e$). Therefore, this argument does not generally hold. From $x^2=e$, we can say that $x=x^{-1}$ (by multiplying the equation with $x^{-1}$ from either left or right) and from that that $x^{-1}=e$ (by substituting $x^{-1}$ for $x$). However, considering an Abelian group $\left(\Z,+\right)$ and taking $e=0$ (which is a neutral element for addition), we would have $x+x=0$ and from that $x=-x$ which is true only for $x=0$. So, in this case this argument is valid.

\item It's true in $\R\backslash\{0\}$ (considering $x=-1$ and $a=1$) that $(-1)^2=1^2$, but $-1\neq 1$. Therefore, this argument does not generally hold. But, in Abelian group $\left(\Z,+\right)$, taking $a=1$, we have $x+x=a+a$, that is $x+x=2$, and the only option being $x=1$. So here, it follows that $x=a$ from $x^2=a^2$.

\item I say that if $G$ is a group that is not commutative then it is not true that $\left(a b\right)^2=a^2 b^2$ for all $a,b\in G$. We will prove this by contradiction. Let's assume that $G$ is a non-commutative group (it's not true that for all $a,b\in G$ holds $a b=b a$, i.e. there exist some $x,y\in G$ such that $x y\neq y x$) and that it's true that $\left(a b\right)^2=a^2 b^2$ for all $a,b\in G$. Now, we write this expression down as $a b a b=a a b b$. We multiply it by $b^{-1}$ on the right and by $a^{-1}$ on the left, only to obtain $b a=a b$. Now, as operation is not generally commutative, e.g. for those $x,y\in G$, we have a contradiction, therefore affirming our first argument, that in a non-commutative group it is not necessarily true that $\left(a b\right)^2=a^2 b^2$. However, the argument would be valid if group $G$ were commutative. Then, $\left(a b\right)^2=a b a b=a a b b=a^2 b^2$.

\item Assume that $x^2=x$, i.e. $x x=x$. Then, by multiplying it on the right (we could also multiply it on the left and get the same result) by $x^{-1}$ we get $x x x^{-1}=x x^{-1}$, that is $x=e$.

\item Let's consider again $\R\backslash\{0\}$ with multiplication. Obviously, $-1\in\R\backslash\{0\}$, but there does not exist $y\in\R\backslash\{0\}$ such that $-2=y^2$ ($y^2$ is necessarily non-negative, and $-2$ is negative). Therefore this argument is not generally true.

\item Let $G$ be a group and $x,y\in G$. As $G$ is a group there exists inverse of $x$, $x^{-1}\in G$ (it also holds that $x^{-1}x=x x^{-1}=e$). Now, if we take $x^{-1} y$, as $x^{-1}$ is in $G$, and $y$ is by assumption in $G$, and axioms of totality being satisfied (otherwise it wouldn't be a group), $x^{-1} y$ must yield a unique element in $G$ which we can designate by $z$. That way, $x^{-1} y=z$. Multiplying on the left by $x$, we have $x x^{-1} y=x z$, that is $e y=x z$ and finally $y=x z$. Thus we have proved this argument.

\end{enumerate}

\noindent{\bf Problem.} If $a,b\in G$ and $G$ is an Abelian\footnote{In Abelian groups, commutativity necessarily holds, i.e. for any $a,b\in G$ it's true that $a b=b a$. Use this fact to your own advantage.} group, prove the following:

\begin{enumerate}
\item $a^{-1} b^{-1}=b^{-1} a^{-1}$;
\item $a b^{-1}=b^{-1} a$;
\item $a (a b)=(a b) a$;
\item $a^2 b^2=b^2 a^2$;
\item $\left(x a x^{-1}\right)\left(x b x^{-1}\right)=\left(x b x^{-1}\right)\left(x a x^{-1}\right)$ for any $x\in G$;
\item $a b=b a$ if and only if $a b a^{-1}=b$;
\item $a b=b a$ if and only if $a b a^{-1} b^{-1}=e$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}

\item $a^{-1} b^{-1}=b^{-1} a^{-1}$. As $G$ is commutative, $a b=b a$. Multiplying by $a^{-1}$ on the left, we have $a^{-1} a b=a^{-1} b a$, i.e. $b=a^{-1} b a$. Now, we multiply it again by $a^{-1}$, but on the right and obtain $b a^{-1}=a^{-1} b a a^{-1}$, that is, $b a^{-1}=a^{-1} b$. We do the same thing with $b^{-1}$. We multiply the last equality with $b^{-1}$ on the left and then on the right to get $a^{-1} b^{-1}=b^{-1} a^{-1}$. 

\item $a b^{-1}=b^{-1} a$. We start off with $a b=b a$. We multiply this equality by $b^{-1}$ on the right and get $a b b^{-1}=b a b^{-1}$. Now, it's $a=b a b^{-1}$. We shall multiply that expression with $b^{-1}$ on the left and we have $b^{-1} a=b^{-1} b a b^{-1}$, i.e. $b^{-1} a=a b^{-1}$, or $a b^{-1}=b^{-1} a$, which is really the same as the former expression.

\item $a (a b)=(a b) a$. Again, we use $a b=b a$. Minding associativity more carefully here, we multiply it on the left with $a$ to get $a (a b)=a (b a)$. Now, using associativity, i.e. the fact that $a (b a)=(a b) a$, we get $a (a b)=(a b) a$.

\item $a^2 b^2=b^2 a^2$. Consider multiplying $a b=b a$ with $b$ on the right to get $a b^2=b a b$, and now multiplying with $a$ on the left, we have $a^2 b^2=a b a b$. As $G$ is associative we can more explicitly group elements on the right-hand side so that $a^2 b^2=a (b a) b$. We've done this only to emphasise what we are going to do next; we are going to use commutativity on the expression in the parentheses and get $a^2 b^2=a (a b) b$. Removing parentheses by applying associativity, we have $a^2 b^2=a^2 b^2$.


\item $\left(x a x^{-1}\right)\left(x b x^{-1}\right)=\left(x b x^{-1}\right)\left(x a x^{-1}\right)$ for any $x\in G$. Again, we start with $a b=b a$. Wherever we put neutral element $e$ (which exists as $G$ is an Abelian group), the expression is not changed. We can put it between the first two factors and get $a e b=b a$. We can put it again between the second two factors and now have $a e b=b e a$. As $G$ is an Abelian group, satisfying the condition of the existence of inverse elements, it is true for any $x\in G$ that $x x^{-1}=x^{-1} x=e$. So, we put these expressions into our former equality so that $a x^{-1} x b=b x^{-1} x a$. Now, we multiply it with $x$ on the left and $x^{-1}$ on the right and get $x a x^{-1} x b x^{-1}=x b x^{-1} x a x^{-1}$, and when we group those elements, using associativity, we have $\left(x a x^{-1}\right)\left(x b x^{-1}\right)=\left(x b x^{-1}\right)\left(x a x^{-1}\right)$.

\item $a b=b a$ if and only if $a b a^{-1}=b$. First, we will prove: if $a b=b a$, then $a b a^{-1}=b$. Assume that $a b=b a$, multiply it with $a^{-1}$ on the right and it's $a b a^{-1}=b a a^{-1}$, that is, $a b a^{-1}=b$. Now we prove: if $a b a^{-1}=b$ then $a b=b a$. By multiplying $a b a^{-1}=b$ on the right with $a$, we get $a b a^{-1} a=b a$, i.e. $a b=b a$.

\item $a b=b a$ if and only if $a b a^{-1} b^{-1}=e$. First, let's prove: if $a b=b a$ then $a b a^{-1} b^{-1}=e$. We multiply $a b=b a$ by $a^{-1}$ on the right and get $a b a^{-1}=b a a^{-1}$, i.e. $a b a^{-1}=b$. Now, we multiply this with $b^{-1}$ on the right and get $a b a^{-1} b^{-1}=b b^{-1}$, that is, $a b a^{-1} b^{-1}=e$. Now, let's prove: if $a b a^{-1} b^{-1}=e$ then $a b=b a$. Assume that $a b a^{-1} b^{-1}=e$ and multiply it on the right with $b$. We get $a b a^{-1} b^{-1} b=e b$, which is $a b a^{-1}=b$. Now, multiply it on the right by $a$ to obtain $a b a^{-1} a=b a$, i.e. $a b=b a$.
\end{enumerate}

\noindent{\bf Problem.} Let $G$ be a group. Let $a$, $b$, $c$ denote elements of $G$, and let $e$ be the neutral element of $G$. Prove:

\begin{enumerate}
\item If $a b=e$, then $b a=e$;
\item If $a b c=e$, then $c a b=e$ and $b c a=e$;
\item If $x a y=a^{-1}$, then $y a x=a^{-1}$;
\item Let $a$, $b$ and $c$ each be equal to its own inverse. If $a b=c$, then $b c=a$ and $c a=b$.
\item $a=a^{-1}$ if and only if $a a=e$.
\item If $a b c$ is its own inverse, then $b c a$ is its own inverse, and $c a b$ is its own inverse.
\item Let $a$ and $b$ each be equal to its own inverse. Then $b a$ is the inverse of $a b$.
\item Let $c=c^{-1}$. Then $a b=c$ if and only if $a b c=e$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item If $a b=e$, then $b a=e$. Assume that $a b=e$. Then, we multiply that expression by $a^{-1}$ on the left and get $b=a^{-1}$. Now, we multiply this expression with $a$ on the right and we have $b a=e$.

\item If $a b c=e$, then $c a b=e$ and $b c a=e$. Suppose $a b c=e$. We multiply this expression by $a^{-1}$ on the right and get $a^{-1} a b c=a^{-1}$, i.e. $b c=a^{-1}$. Now we multiply this by $a$ on the right and get $b c a=a^{-1} a$, which is $b c a=e$. In the same spirit, multiplying this expression on the left by $b^{-1}$, and then by $b$ on the right, we get $c a b=e$. We will later formally prove the generalization of this and previous property.

\item If $x a y=a^{-1}$, then $y a x=a^{-1}$. Assume that $x a y=a^{-1}$. We multiply it with $y^{-1}$ on the right and get $x a=a^{-1} y^{-1}$; then, we multiply it by $a^{-1}$ on the right and then it's $x=a^{-1} y^{-1} a^{-1}$. If we multiply it by $a$ on the left, we will then have $a x=y^{-1} a^{-1}$. Finally, multiplying this expression by $y$ on the left gets us $y a x=a^{-1}$.

\item $a=a^{-1}$ if and only if $a a=e$. Assume $a=a^{-1}$. Multiplying that expression by $a$ on the right, we have $a a=a^{-1} a$, that is $a a=e$ or, more neatly, $a^2=e$. Conversely, suppose $a a=e$. Multiplying that expression by $a^{-1}$ on the right yields $a a a^{-1}=e a^{-1}$, that is $a=a^{-1}$.

\item Let $a$, $b$ and $c$ each be equal to its own inverse. If $a b=c$, then $b c=a$ and $c a=b$. Our premise is that $a=a^{-1}$, $b=b^{-1}$ and $c=c^{-1}$; by using previous problem, we actually have $a^2=e$, $b^2=e$ and $c^2=e$. Now if $a b=c$, then, multiplying it with $b$ on the right, we have $a b^2=c b$, which is $a=c b$, as $b^2=e$. Now, we multiply $a=c b$ with $c$ on the left and get $c a=c^2 b$, that is, $c a=b$. Then we multiply this expression with $a$ on the right to get $c a^2=b a$, which is $c=b a$. Now, we multiply this by $b$ on the left and get $b c=b^2 a$, id est $b c=a$.

\item If $a b c$ is its own inverse, then $b c a$ is its own inverse, and $c a b$ is its own inverse. Suppose that $(a b c)(a b c)=e$ or, by applying associative law, $a b c a b c=e$. If we multiply this expression with $a^{-1}$ on the left, we get $b c a b c=a^{-1}$. Now, multiplying it with $a$ on the right gets us $b c a b c a=e$, that is $(b c a)^2=e$. If we multiply former expression by $b^{-1}$ on the left, we have $c a b c a=b^{-1}$. Finally, multiplying this equality on the right by $b$ yields $c a b c a b=e$, that is, $(c a b)^2=e$.

\item Let $a$ and $b$ each be equal to its own inverse. Then $b a$ is the inverse of $a b$. Suppose $a a=e$ and $b b=e$. We multiply the former expression with $b$ on the right and now have $a a b=b$. Multiplying this on the left by $b$ gets us $b a a b=b b$, that is, $(b a)(a b)=e$ (or $a b$ is the inverse of $b a$ and reverse).

\item Let $c=c^{-1}$. Then $a b=c$ if and only if $a b c=e$. Premise is that $c c=e$ (same as $c=c^{-1}$ as proved in a previous problem). Suppose $a b=c$. Then, if we multiply it by $c$ on the right, we have $a b c=c c$, that is, $a b c=e$. Conversely, suppose $a b c=e$; multiply this expression on the right by $c$ to obtain $a b c c=e c$, i.e. $a b=c$.
\end{enumerate}

\noindent{\bf Theorem\footnote{What this theorem actually tells us is that if factors are equal to a neutral element, then some rotation of these factors is also equal to a neutral element.}.} Let $G$ be a group and $a_0, a_1,\ldots,a_n,a_{n+1}\in G$, where $a_0=a_{n+1}=e$ (neutral element\footnote{We defined $a_0=a_{n+1}=e$ only because it's more convinient to manipulate the expression this way. There is no other special reason.} in $G$). If

\begin{equation*}
\prod_{i=n-p+2}^{n+1}{a_i}\prod_{i=0}^{n-p+1}{a_i}=e,
\end{equation*}

\noindent\newline where $n,p\in\N$ such that $n+1\geq p$, then

\begin{equation*}
\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-k+1}{a_i}=e,
\end{equation*}

\noindent\newline where $k\in\N$ such that $p\geq k$. Reverse also holds.

\noindent\newline{\bf Proof.} Before we start with formal proof, consider this case, when $n=10$ (only tells us the number of elements involved), $p=7$ and $k=3$:

\begin{equation*}
\prod_{i=10-7+2}^{10+1}{a_i}\prod_{i=0}^{10-7+1}{a_i}=e,
\end{equation*}

\noindent\newline which is actually:

\begin{equation*}
\prod_{i=5}^{11}{a_i}\prod_{i=0}^{4}{a_i}=e.
\end{equation*}

\noindent\newline Now, writing this down without product symbols gives us:

\begin{equation*}
a_5 a_6 a_7 a_8 a_9 a_{10} a_{11} a_0 a_1 a_2 a_3 a_4=e.
\end{equation*}

\noindent\newline Keep in mind that we defined $a_0=a_{11}=e$, so we can neatly write the expression above, grouping it, as:

\begin{equation*}
\left(a_5 a_6 a_7 a_8 a_9 a_{10}\right)\left(a_1 a_2 a_3 a_4\right)=e.
\end{equation*}

\noindent\newline Now, consider the consequent of the theorem implication, when $k=3$ and compare it with the above. We would have:

\begin{equation*}
\prod_{i=10-3+2}^{10+1}{a_i}\prod_{i=0}^{10-3+1}{a_i}=e,
\end{equation*}

\noindent\newline which is again:

\begin{equation*}
\prod_{i=9}^{11}{a_i}\prod_{i=0}^{8}{a_i}=e.
\end{equation*}

\noindent\newline Considering $a_0=a_{11}=e$ and grouping them meaningfully we have:

\begin{equation*}
\left(a_9 a_{10}\right)\left(a_1 a_2 a_3 a_4 a_5 a_6 a_7 a_8\right)=e.
\end{equation*}

\noindent\newline In order to see the pattern between $p=7$ and $k=3$ rotations, we will make use of the following table, grouping them further in a meaningful way:

\begin{center}
\begin{tabular}{c|c}
  $p=7$ & $\left[a_5 a_6 a_7 a_8\right]\left(a_9 a_{10}\right)\left(a_1 a_2 a_3 a_4\right)=e$\\
  \hline
  $k=3$ & $\left(a_9 a_{10}\right)\left(a_1 a_2 a_3 a_4\right)\left[a_5 a_6 a_7 a_8\right]=e$\\
\end{tabular}
\end{center}

\noindent\newline So, to get from $p=7$ to $k=3$ we would need to multiply the $p=7$ case with inverse of factors in square brackets on the left, and then with the same square brackets on the right. Observe that key factors are around brackets. In $p=7$, first is $a_5$ whose index can be written down as $10-7+2$, that is $n-p+2$. Next is $a_8$ whose index is actually $10-3+1$, i.e. $n-k+1$. So $a_9$ index is $n-k+2$, obviously, $a_{10}$ is $n$ and $a_1$ is $1$. Last is $a_4$, whose index is $10-7+1$, i.e. $n-p+1$. This will help us generalize the product in a more appropriate way, following that line of reasoning:

\begin{equation*}
\prod_{i=n-p+2}^{n-k+1}{a_i}\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-p+1}{a_i}=e.
\end{equation*}

\noindent\newline Notice that again we added $a_0$ and $a_{11}$, not changing actually anything; yet this additional condition will allow the proof to work for $p=n$ and $p=1$ (same for $k=n$ and $k=1$ in reverse). Now we need to multiply this equation by the inverse of the first product on the left. Now, we have:

\begin{equation*}
\left(\prod_{i=n-p+2}^{n-k+1}{a_i}\right)^{-1}\prod_{i=n-p+2}^{n-k+1}{a_i}\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-p+1}{a_i}=\left(\prod_{i=n-p+2}^{n-k+1}{a_i}\right)^{-1}.
\end{equation*}

\noindent\newline Now the first two products form a neutral element, leaving only

\begin{equation*}
\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-p+1}{a_i}=\left(\prod_{i=n-p+2}^{n-k+1}{a_i}\right)^{-1}.
\end{equation*}

\noindent\newline And finally, we multiply, on the right, this expression by the product inside the inverse on the right-hand side and get:

\begin{equation*}
\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-p+1}{a_i}\prod_{i=n-p+2}^{n-k+1}{a_i}=\left(\prod_{i=n-p+2}^{n-k+1}{a_i}\right)^{-1}\prod_{i=n-p+2}^{n-k+1}{a_i}.
\end{equation*}

\noindent\newline Joining products on the left-hand side in respect to their indices and seeing that two products on the right yield a neutral element, we have proved the first part of the theorem:

\begin{equation*}
\prod_{i=n-k+2}^{n+1}{a_i}\prod_{i=0}^{n-k+1}{a_i}=e.
\end{equation*}

\noindent\newline The reverse of the theorem implication is proved in the same way.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} If $G$ is a group and $a_1, a_2, b\in G$, then, if $b$ is inverse of $a_1$ and $a_2$, then $a_1=a_2$.

\noindent\newline{\bf Proof.} Let $a_1,a_2,b$ be elements of $G$ such that $a_1 b=e$ (and $b a_1=e$) and $a_2 b=e$ (and $b a_2=e$). Then it follows that $a_1 b=a_2 b$. Multiplying by $b^{-1}$ on the right, we have $a_1 b b^{-1}=a_2 b b^{-1}$, that is $a_1=a_2$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} The lemma above tells us that every element has it's own unique inverse in a group.

\noindent\newline{\bf Definition.} If $G$ is a finite group, then the {\bf order of group} $G$, denoted by $\left|G\right|$, is equal to the number of elements in $G$.

\noindent\newline{\bf Remark.} We will use the same notation for cardinality of a set. Difference between those two notions is marginal.

\noindent\newline{\bf Problem.} Let $G$ be a finite group and let $S$ be the set of all the elements of $G$ which are not equal to their own inverse, i.e.

\begin{equation*}
S=\left\{x\in G:\ x\neq x^{-1}\right\}.
\end{equation*}

\noindent\newline Prove the following:

\begin{enumerate}

\item In any finite group $G$, the number of elements not equal to their own inverse is an even number, i.e. $\left|S\right|=2k$, $k\in\N_0$ (give an example when $\left|S\right|=0$).

\item The number of elements in $G$ equal to their own inverse ($G\backslash S$) is odd or even, depending on whether the number of elements in $G$ is odd or even.

\item If the order of $G$ is even, there is at least one element $x$ in $G$ such that $x\neq e$ and $x=x^{-1}$.

\item If $G$ is in addition Abelian, then:
\begin{enumerate}
\item $\left(a_1 a_2 \cdots a_n\right)^2=e$;
\item If there is no element $x\neq e$ in $G$ such that $x=x^{-1}$, then $a_1 a_2\cdots a_n=e$.
\item If there is exactly one $x\neq e$ in $G$ such that $x=x^{-1}$, then $a_1 a_2\cdots a_n=x$.
\end{enumerate}

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}

\item {\it In any finite group $G$, the number of elements not equal to their own inverse is an even number, i.e. $\left|S\right|=2k$, $k\in\N_0$ (give an example when $\left|S\right|=0$).} We suppose that all elements $a_i$ in $G$, $i\in\{1,\ldots,n\}$ and $n\in\N$ are denoted so that $a_i\neq a_j$, for every $i\neq j$, $i,j\in\{1,\ldots,n\}$. In a previous lemma we have clearly proved that each $a_i$ needs to have his pair $a_j$ which is its unique inverse. If that were not so, if $a_j$ was inverse to some other element $a_k$, by the previous lemma, $a_i$ would equal $a_k$, contradicting our premise. Therefore, all elements that do not equal their own inverses, elements in $S$, need to come in unique pairs; in conclusion $\left|S\right|$ must be even. And $S$ can be empty, i.e. for a trivial Abelian group $\left(G,\ast\right)$, where $G=\{a\}$ and $\ast:G\times G\rightarrow G$ such that $a\ast a=a$. It's trivial to show that this operation satisfies both axioms of totality and properties of an Abelian group. But, as $a$ is a neutral element and $a\ast a=a$, it is it's own inverse and not contained in $S$. So, $\left|S\right|=0$.

\item {\it The number of elements in $G$ equal to their own inverse ($G\backslash S$) is odd or even, depending on whether the number of elements in $G$ is odd or even.} Suppose that number of elements in $G$ is even, that is $\left|G\right|=2k$, $k\in\N$. Being that $\left|S\right|=2l$, $l\in\N_0$, then, the number of elements equal to their own inverse is $\left|G\backslash S\right|=\left|G\right|-\left|S\right|=2k-2l=2\left(k-l\right)$; in other words, it's even if $\left|G\right|$ is even. Similarly if $G$ has odd number of elements, i.e. $\left|G\right|=2k+1$, $k\in\N_0$, then, being that $\left|G\right|-\left|S\right|=2k+1-2l=2(k+l)+1$, the number of elements equal to their own inverse is odd if number of elements in $G$ is odd.

\item {\it If the order of $G$ is even, there is at least one element $x$ in $G$ such that $x\neq e$ and $x=x^{-1}$.} If $G$ is even, then, by the previous problem, number of elements which equal their own inverse ($\left|G\backslash S\right|$) is even. Being that neutral element is counted inside this set, i.e. $e\in G\backslash S$ (because $e e=e$, that is, neutral element is its own inverse), then removing it makes the number of elements inside that same set odd, leaving room for at least one element that equals its own inverse (of course $G\backslash S$ can never be empty, as it will always contain at least $e$; so if it's order is even, we cannot consider case when it's equal to zero).

\item If $G$ is also Abelian, then:
\begin{enumerate}
\item $\left(a_1 a_2 \cdots a_n\right)^2=e$. We can write this down as $\left(a_1\ldots a_n\right)\left(a_1\ldots a_n\right)=e$. Being that $G$ is a group, associativity holds so we may as well write

\begin{equation*}
a_1 a_2\ldots a_n a_1 a_2 \ldots a_n=e.
\end{equation*}

\noindent\newline Now, without loss of generality, let's suppose that $a_k=a_k^{-1}$ for $k\leq n$. In other words $a_k^2=e$. We can organize the factors in the former equation, using commutativity, to have $a_1 a_1 a_2 a_2 \ldots a_k a_k a_{k+1} a_{k+1} \ldots a_n a_n=e$, that is $a_1^2 a_2^2 \ldots a_k^2 a_{k+1} a_{k+1}\ldots a_n a_n=e$. Because we presumed that $a_k^2=e$, we are left with only $a_{k+1} a_{k+1}\ldots a_n a_n=e$. Now, we got rid off all the elements that equal their own inverse and are left with elements who do not equal their inverses. But, as $G$ is a group, and all the other elements are contained in the former equation, their number is surely equal and we can pair them using commutativity with their inverses. We can presume that $a_{i} a_{i+1}=e$ (and if not, we can always rearrange them in such manner due to commutativity), where $k<i<n$, $i\in\N$. We would have

\begin{equation*}
\prod_{i=\frac{k+1}{2}}^{\frac{n-1}{2}}{\left(a_{2i} a_{2i+1}\right)^2}=\prod_{i=\frac{k+1}{2}}^{\frac{n-1}{2}}{e^2}=e^{n-k-2}=e,
\end{equation*}

\noindent\newline thereby proving the desired inequality (note that we got the exponent on the neutral element by subtracting $\frac{n-1}{2}-\frac{k+1}{2}=\frac{n-1-k-1}{2}=\frac{n-k-2}{2}$, and then multiplying it by $2$, as we had $e^2$).

\item {\it If there is no element $x\neq e$ in $G$ such that $x=x^{-1}$, then $a_1 a_2\cdots a_n=e$.} This problem is similar to the previous problem. As we have no elements which equal their own inverses in the sequence of factors $a_i$ in the previous equation, and, because every element needs to have its own inverse, we can pair them using commutativity as in the previous problem. Therefore, we would get, supposing $a_i a_{i+1}=e$, that $(a_1 a_2)(a_3 a_4)\cdots (a_{n-1} a_n)=e$ (here, commutativity was not necessary in our presumption, altough anyway we chose the indices to behave, we could arrange them in such manner through commutativity). Then, $e e \ldots e=e^{\frac{n}{2}}=e$.

\item {\it If there is exactly one $x\neq e$ in $G$ such that $x=x^{-1}$, then $a_1 a_2\cdots a_n=x$.} As in the previous two problems, suppose that, without loss of generality $a_1=a_1^{-1}$, i.e. $a_1=x$. Then, we can arrange factors, by commutativity, to get $x (a_2 a_3)\cdots (a_{n-1} a_n)=x e^{\frac{n}{2}}=x e=x$.

\end{enumerate}

\end{enumerate}

\noindent{\bf Problem.} Let $G$ be any group. Let $e$ denote the neutral element of $G$.

\begin{enumerate}
\item If $a$, $b$ are any elements of $G$, prove each of the following:

\begin{enumerate}
\item If $a^2=a$, then $a=e$;
\item If $a b=a$, then $b=e$;
\item If $a b=b$, then $a=e$;
\end{enumerate}

\item Explain why every row of a group table must contain each element of the group exactly once.

\item There is exactly one group $G$ on any set of three distinct elements. Use the operation table for this and further two excercises.

\item There is exactly one group $G$ on any set of four distinct elements satisfying condition that $xx=e$ for every $x\in G$.

\item There is exactly one group $G$ on any set of four distinct elements satisfying condition that $xx=e$ for some $x\in G$ and $yy\neq e$ for some $y\in G$.

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item If $a$, $b$ are any elements of $G$, prove each of the following:
\begin{enumerate}

\item If $a^2=a$, then $a=e$. Multiplying this equality on the right (or left) by $a^{-1}$ gets us $a^2 a^{-1}=a a^{-1}$, that is, $a=e$.

\item If $a b=a$, then $b=e$. Multiplying this equality on the left by $a^{-1}$ yields $b=e$.

\item If $a b=b$, then $a=e$. Multiplying this equality on the right by $b^{-1}$ gives us $a=e$.

\end{enumerate}

\noindent{\bf Remark.} Note that we could have used cancellation law on all three equations, i.e. $a b=a e$, and by cancellation law, that is necessarily $b=e$. But, proving cancellation law over and over again implicitly for an excercise is not a bad thing in itself.

\item {\it Explain why every row of a group table must contain each element of the group exactly once.} Suppose that we have a group of $n+1$ elements, $G=\{a_1,\ldots,a_n,e\}$, where $e$ is a neutral element and that some element $a_i$ (we can take $a_1$ without loss of generality) occurs $k$ times in a row $j$ (we can take $j=n-1$ also for illustrative purposes), where $k\in\N_0\backslash\{1\}$. First we shall analyize it for a non-zero number in that set. That table would look something like this:

\begin{center}
\begin{tabular}{c|ccccccc}
  %\hline
  $\cdot$ & $a_1$ & $a_2$ & $a_3$ & \ldots & $a_{n-1}$ & $a_n$ & $e$\\
  \hline
  $a_1$ & & & & \ldots & & & $a_1$ \\
  $a_2$ & & & & \ldots & & & $a_2$ \\
	$a_3$ & & & & \ldots & & & $a_3$ \\
  $\vdots $ & & & & & & & \\
	$a_{n-1}$ & & $a_1 $ & $a_1$ & \ldots & & $a_1$ & $a_{n-1}$\\
  $a_{n}$ & & & & \ldots & & & $a_n$\\
	$e$ & $a_1$ & $a_2$ & $a_3$ & \ldots & $a_{n-1}$ & $a_n$ & $e$\\
  %\hline
\end{tabular}
\end{center}

\noindent\newline Now, it must be true, e.g. $a_{n-1} a_2=a_1$ and $a_{n-1} a_3=a_1$ and so on... These two first equations are enough to conclude that $a_{n-1} a_2=a_{n-1} a_3$, therefore, by cancellation law (or by multiplying with $a_{n-1}^{-1}$ on the left), it must be $a_2=a_3$. We would get a chain of equalities for $k$ elements (as we assumed some element occurs $k$ times in a row), but that is a contradiction that we have $n+1$ elements as, if we consider set $R$ which contains all equal elements,

\begin{equation*}
R=\{a_i,a_j\in G:\ a_i=a_j,\ i,j\in S_k\subseteq\N\backslash\{1\},\ \left|S_k\right|=k\},
\end{equation*}

\noindent\newline we can easily substitute all $a_i\in R$ for one element, say, some $a_j$, and say that $G$ actually has $n-k+1$ elements (of course, when we said in the beggining that $G$ had $n$ elements, we implicitly assumed they were distinct, which would have been enough for a proof by contradiction). And, as for $k=0$, that means that our $a_1$ does not appear at all in a row. Now, as we have $n+1$ places in that row (for $a_1, a_2,\ldots,a_n$ and $e$) and only $n$ elements on disposal (that is $a_2,\ldots,a_n$ and $e$), it would mean that some of these forementioned elements would have to appear twice; that is not advisable as we saw what happened earlier in this problem.

\item {\it There is exactly one group on any set of three distinct elements.} We can assume we have a set $G=\{e,a,b\}$. Now, one would think that we actually have room for $27$ permutations (in each "box" of the table we have three possible elements to distribute) to complete that table (we will write a number of possibilities in each "box"):

\begin{center}
\begin{tabular}{c|ccc}
$\cdot$ & $e$ & $a$ & $b$\\
\hline
$e$ & $3$ & $3$ & $3$\\
$a$ & $3$ & $3$ & $3$\\
$b$ & $3$ & $3$ & $3$\\
\end{tabular}
\end{center}

\noindent\newline Keep in mind that every element has to appear exactly once in every row and that every element multiplied by a neutral element yields that same element. That limits our possibilities to $27-15=8$ ($15$ counting all the possibilities in rows and columns for $e$) cases because of the latter property (notice that we can distribute only two remaining elements in order to remain consistent with the former property):

\begin{center}
\begin{tabular}{c|ccc}
$\cdot$ & $e$ & $a$ & $b$\\
\hline
$e$ & $e$ & $a$ & $b$\\
$a$ & $a$ & $2$ & $2$\\
$b$ & $b$ & $2$ & $2$\\
\end{tabular}
\end{center}

\noindent\newline We must also mind not to have $b a=b$ or $a b=a$, as it would mean, by cancellation law, that $a=e$ or $b=e$, respectively. Now, that means that we must fill this places with the only remaining element and that is $e$, leaving us only two possibilities:

\begin{center}
\begin{tabular}{c|ccc}
$\cdot$ & $e$ & $a$ & $b$\\
\hline
$e$ & $e$ & $a$ & $b$\\
$a$ & $a$ & $1$ & $e$\\
$b$ & $b$ & $e$ & $1$\\
\end{tabular}
\end{center}

\noindent\newline But, these two are easily filled, as we must also not allow ourselves to put $a$ on the diagonal in the same row where we have $a$ (same goes for $b$), as we would get, e.g. $a^2=a$ and by cancellation property (being that we can write it as $a a=a e$) that would yield $a=e$. Therefore, it must be that $a^2=b$ and $b^2=a$, leaving us with the following table:

\begin{center}
\begin{tabular}{c|ccc}
$\cdot$ & $e$ & $a$ & $b$\\
\hline
$e$ & $e$ & $a$ & $b$\\
$a$ & $a$ & $b$ & $e$\\
$b$ & $b$ & $e$ & $a$\\
\end{tabular}
\end{center}

\item {\it There is exactly one group $G$ on any set of four distinct elements satisfying condition that $xx=e$ for every $x\in G$.} We shall only present the finished table, presuming that $G=\{e,a,b,c\}$:

\begin{center}
\begin{tabular}{c|cccc}
$\cdot$ & $e$ & $a$ & $b$ & $c$\\
\hline
$e$ & $e$ & $a$ & $b$ & $c$\\
$a$ & $a$ & $e$ & $c$ & $b$\\
$b$ & $b$ & $c$ & $e$ & $a$\\
$c$ & $c$ & $b$ & $a$ & $e$\\
\end{tabular}
\end{center}

\item {\it There is exactly one group $G$ on any set of four distinct elements satisfying condition that $xx=e$ for some $x\in G$ and $yy\neq e$ for some $y\in G$.} We will follow the same example, as in a previous excercise, making only slight modifications to the operation table (suppose that $aa=e$ and $bb\neq e$). Notice that it must be $b a\neq b$ and $b a\neq a$, leaving $b a=c$. Furthermore, if $b^2\neq e$, that means that either $b a=e$ or $b c=e$. But we have $a^2=e$ and that would mean $b a^2=a$, that is $b=a$. So it must be $b c=e$ (and the only remaining place in the row is for $a$, that is $b^2=a$), but then it must not be $c^2=e$ (we would get $b c^2=c$, that is, $b=c$). So, we can then take $c b=e$ or $c a=e$. If we took $c a=e$, we would have $c a=a^2$ and then $c=a$. So, the only remaining option is to have $c b=e$ and $c^2=a$. The operation table is such:

\begin{center}
\begin{tabular}{c|cccc}
$\cdot$ & $e$ & $a$ & $b$ & $c$\\
\hline
$e$ & $e$ & $a$ & $b$ & $c$\\
$a$ & $a$ & $e$ & $c$ & $b$\\
$b$ & $b$ & $c$ & $a$ & $e$\\
$c$ & $c$ & $b$ & $e$ & $a$\\
\end{tabular}
\end{center}

\end{enumerate}

\noindent{\bf Definition.} If $G$ and $H$ are any two groups, their {\bf direct product} is denoted by $G\times H$ and consists of all ordered pairs $(x,y)$ where $x\in G$ and $y\in H$, that is:

\begin{equation*}
G\times H=\left\{\left(x,y\right):\ x\in G,\ y\in H\right\}.
\end{equation*}

\noindent\newline{\bf Remark.} The definition above considers that a consistent operation is also defined on both groups, and is transfered to their direct product. Customary, if operation is multiplication-like, we will write:

\begin{equation*}
(x,y)(x',y')=(x x',y y').
\end{equation*}

\noindent\newline Similarly, if operation is addition-like, we will write:

\begin{equation*}
(x,y)+(x',y')=(x+x',y+y').
\end{equation*}

\noindent\newline{\bf Theorem.} If $G$ and $H$ are groups, then $G\times H$ is a group. Furthermore, if $G$ and $H$ are Abelian groups, then $G\times H$ is also Abelian group.

\noindent\newline{\bf Proof.} We will do the proof by checking all the group axioms. If we manage to prove associativity and existence of neutral element and inverses, we will have proved that $G\times H$ is a group. In addition, if we prove commutativity, we will have proved that $G\times H$ is an Abelian group. We will prove commutativity before we find neutral elements and inverses, as we cut our work in half.

\begin{enumerate}
\item[A.] On the left-hand side we have:

\begin{equation*}
(x_1,y_1)\left[(x_2,y_2)(x_3,y_3)\right]=(x_1,y_1)(x_2 x_3,y_2 y_3)=(x_1 x_2 x_3, y_1 y_2 y_3).
\end{equation*}

\noindent\newline On the right-hand side,

\begin{equation*}
\left[(x_1,y_1)(x_2,y_2)\right](x_3,y_3)=(x_1 x_2,y_1 y_2)(x_3, y_3)=(x_1 x_2 x_3, y_1 y_2 y_3).
\end{equation*}

\noindent\newline Thus we have proved associativity.

\item[C.] It's obvious that $(x,y)(x',y')=(x x',y y')$. Now, as $G$ and $H$ are Abelian, we can write the previous expression as $(x,y)(x',y')=(x' x, y' y)=(x',y')(x,y)$, thus proving commutativity.

\item[N.] We need $(e_1,e_2)\in G\times H$ such that, for every $(x,y)\in G\times H$, holds $(x,y)(e_1,e_2)=(e_1,e_2)(x,y)=(x,y)$. From this we have $(e_1 x, e_2 y)=(x,y)$. We need $e_1$ such that $e_1 x=x$ and $e_2$ such that $e_2 y=y$. As $x\in G$ and $y\in H$, we will need neutral elements from $G$ and $H$. Let $e_G$ be neutral element in $G$ and $e_H$ neutral element in $H$. Then, obviously $e_G x=x$ and $e_H y=y$. Therefore, our neutral element is $(e_G,e_H)\in G\times H$.

\item[I.] Let's find $(x^{-1},y^{-1})$ such that $(x^{-1},y^{-1})(x,y)=(x,y)(x^{-1},y^{-1})=(e_G,e_H)$. From this expression we can derive that $x x^{-1}=e_G$ and $y y^{-1}=e_H$. As $G$ and $H$ all have inverses, we can take $x_G^{-1}\in G$ such that $x x_G^{-1}=x_G^{-1} x=e_G$ and $y_H^{-1}\in H$ such that $y y_H^{-1}=y_H^{-1} y=e_H$. So, our inverse elements do exist and they are $(x_G^{-1},y_H^{-1})\in G\times H$.

\end{enumerate}

\noindent By this, we have proved that $G\times H$ is (Abelian group) if $G$ and $H$ are both (Abelian) groups.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} List the elements of $\Z_2\times\Z_3$, and write its operation table (with additive notation).

\noindent\newline{\bf Solution.} As $\Z_2=\{0,1\}$ and $\Z_3=\{0,1,2\}$, their direct product will be the set

\begin{equation*}
\Z_2\times\Z_3=\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)\}.
\end{equation*}

\noindent\newline Operation table is as follows:

\begin{center}
\begin{tabular}{c|cccccc}
$+$ & $(0,0)$ & $(0,1)$ & $(0,2)$ & $(1,0)$ & $(1,1)$ & $(1,2)$ \\
\hline
$(0,0)$ & $(0,0)$ & $(0,1)$ & $(0,2)$ & $(1,0)$ & $(1,1)$ & $(1,2)$ \\
$(0,1)$ & $(0,1)$ & $(0,2)$ & $(0,0)$ & $(1,1)$ & $(1,2)$ & $(1,0)$ \\
$(0,2)$ & $(0,2)$ & $(0,0)$ & $(0,1)$ & $(1,2)$ & $(1,0)$ & $(1,1)$ \\
$(1,0)$ & $(1,0)$ & $(1,1)$ & $(1,2)$ & $(0,0)$ & $(0,1)$ & $(0,2)$ \\
$(1,1)$ & $(1,1)$ & $(1,2)$ & $(1,0)$ & $(0,1)$ & $(0,2)$ & $(0,0)$ \\
$(1,2)$ & $(1,2)$ & $(1,0)$ & $(1,1)$ & $(0,2)$ & $(0,0)$ & $(0,1)$ \\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} Suppose the groups $G$ and $H$ both have the property that every element is its own inverse. Prove that $G\times H$ also has this property.

\noindent\newline{\bf Solution.} We can write the fact that every element is its own inverse in this fashion: $x^2=e_G$ for every $x\in G$ and $y^2=e_H$ for every $y\in H$, where $e_G$ is a neutral element in $G$ and $e_H$ neutral element in $H$. Now, let us prove that $(x,y)(x,y)=(e_G,e_H)$, that is, that every element in $G\times H$ is its own inverse. We know that $(x,y)=(x,y)$ for every $(x,y)\in G\times H$. Multiplying that expression on the right (or left) with $(x,y)$ gets us $(x,y)(x,y)=(x,y)(x,y)$. We will calculate the right-hand side. We have $(x,y)(x,y)=(x x, y y)$, which is actually $(x,y)(x,y)=(x^2 y^2)$. Now, that is, by properties of $G$ and $H$, $(x,y)(x,y)=(e_G,e_H)$.

\noindent\newline{\bf Problem.} Let $G$ be a group, and $a,b\in G$. For any positive integer $n$ we define $a^n$ by $a^n=\prod_{i=1}^{n}{a}$. If there is an element $x\in G$ such that $x^2=a$, we say that $a$ has a square root in $G$. Similarly, if $a=y^3$ for some $y\in G$, we say $a$ has a cube root in $G$. In general, $a$ has an $n$th root in $G$ if $a=z^n$ for some $z\in G$. Prove the following:

\begin{enumerate}

\item $(b a b^{-1})^n=b a^n b^{-1}$;

\item If $a b=b a$, then $(a b)^n=a^n b^n$ for every positive integer $n$.

\item If $x a x=e$, then $(x a)^{2n}=a^n$.

\item If $a^3=e$, then $a$ has a square root.

\item If $a^2=e$, then $a$ has a cube root.

\item If $a^{-1}$ has a cube root, so does $a$.

\item If $x^2 a x=a^{-1}$, then $a$ has a cube root.

\item If $x a x=b$, then $a b$ has square root.

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}

\item $(b a b^{-1})^n=b a^n b^{-1}$. Proof by induction. For $n=1$ it's easy to see that $(b a b^{-1})^1=b a^1 b^{-1}$, that is, $b a b^{-1}=b a b^{-1}$. Assume that the argument is valid for some $n=k$, i.e. $(b a b^{-1})^k=b a^k b^{-1}$ is true. Let's prove that it's true for $n=k+1$. Then we have $(b a b^{-1})^{k+1}=b a^{k+1} b^{-1}$. From the left-hand side we get $(b a b^{-1})^k(b a b^{-1})$. By using assumption of mathematical induction that is $(b a b^{-1})^k(b a b^{-1})=(b a^k b^{-1})(b a b^{-1})$. Due to associativity we can disregard parentheses on the right to get $(b a b^{-1})^k(b a b^{-1})=b a^k b^{-1} b a b^{-1}$. Now, as $b^{-1} b=e$, we have $(b a b^{-1})^k(b a b^{-1})=b a^k a b^{-1}$; as $a^k a=a^{k+1}$, we have proved the argument, as we now have $(b a b^{-1})^{k+1}=b a^{k+1} b^{-1}$.

\item {\it If $a b=b a$, then $(a b)^n=a^n b^n$ for every positive integer $n$.} Proof by induction. For $n=1$ we have $(a b)^1=a^1 b^1$, that is $a b=a b$, and thus a proved basis of induction. Now, assume that $(a b)^k=a^k b^k$ is true for some $k\in\N$. Let's prove that it's true for $k+1$. We have, on the right-hand side, $(a b)^{k+1}=(a b)^k(a b)$. Now, by our hypothesis, it's $(a b)^{k+1}=a^k b^k a b$. As our premise is commutativity, we are able to rearrange the elements on the right in such manner that $(a b)^{k+1}=a^k a b^k b$ (we switched places of $b^k$ and $a$). Now we have $(a b)^{k+1}=a^{k+1} b^{k+1}$. This proves our argument. {\it Remark.} Notice that this property does not hold for commutativity, as we can only say that $(a b)^n=\underbrace{(a b)(a b)\cdots(a b)}_{n\textnormal{ times}}$.

\item {\it If $x a x=e$, then $(x a)^{2n}=a^n$.} Proof by induction. First we have $n=1$. That is, $(x a)^{2\cdot 1}=a^1$. Now, $x a x a=a$, and by using premise, we have $e a=a$, i.e. $a=a$. Basis of induction is proved. Suppose the argument $(x a)^{2k}=a^k$ is valid for some $k\in\N$. Let's prove that $(x a)^{2k+2}=a^{k+1}$ is also true, that is, $(x a)^{2k}(x a)^2=a^{k+1}$. From our assumption for $k$ it follows that $(x a)^{2k}(x a)^2=a^k (x a)^2$. But, from basis of induction we had $(x a)^2=a$. So, we have $(x a)^{2k+2}=a^k a$ and then $(x a)^{2k+2}=a^{k+1}$, quod erat demonstrandum. {\it Example.} If we take group $\R\backslash\{0\}$ with multiplication, we can take $x=\frac{1}{5}$ and $a=25$. Then, $\frac{1}{5}\cdot 25\cdot\frac{1}{5}=1$, obviously; note that this yields $1$, which is the neutral element for multiplication in $\R\backslash\{0\}$. Now, $\left(\frac{1}{5}\cdot 25\right)^{2n}=5^{2n}$, which is exactly $25^n$, that is $a^n$.

\item {\it If $a^3=e$, then $a$ has a square root.} We need to find $x\in G$ such that $x^2=a$. Multiplying $a^3=e$ by $(a^2)^{-1}$ on the right (or on the left) we have $a a^2 (a^2)^{-1}=(a^2)^{-1}$. Now, all that remains is $a=(a^2)^{-1}$. We can write this a bit different, as $a=(a^{-1})^2$. We found $x\in G$ such that $x^2=a$, and that is $x=a^{-1}$. It's inverse is it's square root! {\it Example.} One fine example is in group of real numbers (without zero). It's obvious that $(1)^3=1$ (and $1$ is neutral element for multiplication in $\R\backslash\{0\}$) and also that there exist such $x$ that $x^2=1$, and that is, not only $1$ (which is the multiplicative inverse of $1$, that is, of itself), but also $-1$. In group $\left(\Z,+\right)$ we have $3\cdot 0=0$ (notice that, when addition or addition-like operations are in question, we do not write $a^n$, but $n a$, or $n\cdot a$), and then there exists such $x$ that $2x=0$, and that is $x=0$ (which is the additive inverse of $0$, i.e. itself).

\item {\it If $a^2=e$, then $a$ has a cube root.} We need to find $x\in G$ such that $x^3=a$. If we multiply the equation in our implication's antecedent by $a$ (on the left or right) we have $a^3=a$. Therefore, $x=a$ is the square root of $a$. In other words, if $a^2=e$, cube root of $a$ is itself. {\it Example.} Take, for instance, $\R\backslash\{0\}$, with multiplication. Obviously $1^2=1$, which is a neutral element for multiplication in $\R\backslash\{0\}$. By this theorem, $1$ has a cube root, and that is itself, as $1^3=1$. Similarly, $(-1)^2=1$ and $(-1)^3=-1$ (cube root of $-1$ is itself).

\item {\it If $a^{-1}$ has a cube root, so does $a$.} As $a^{-1}$ has a cube root, then there exists such $x\in G$ that $x^3=a^{-1}$. If we multiply this by $a$ on the right and then by $(x^3)^{-1}$ on the left, we have $a x^3 (x^3)^{-1}=a a^{-1} (x^3)^{-1}$. Now, from this we get $(x^{-1})^3=a$. That is, the cube root of $a$ is the inverse of the cube root of $a^{-1}$. {\it Example.} In $\R\backslash\{0\}$ with multiplication we have, e.g. $(\frac{1}{2})^3=\frac{1}{8}$. Obviously, inverse of $\frac{1}{8}$ is $8$, and inverse of $\frac{1}{2}$ (which is cube root of $\frac{1}{8}$) is $2$. So we have $2^3=8$. In $\left(\Z,+\right)$ we have, e.g. $3\cdot 5=15$. Inverse of $5$ is $-5$ and inverse of $15$ is $-15$. So, it's $3\cdot(-5)=-15$, which is, again, true.

\item {\it If $x^2 a x=a^{-1}$, then $a$ has a cube root.} We need $y\in G$ such that $y^3=a$. Suppose $x^2 a x=a^{-1}$. Multiplying this by $(a x)(x a x)$ on the left gives us $x^2 (a x) (a x) (x a x)=a^{-1} a x^2 a x$, i.e. $(x a x)^3=x^2 a x$ and that is $(x a x)^3=a^{-1}$. Thus, it follows that $a^{-1}$ has a cube root, that is $x a x$. And, by the previous problem, if $a^{-1}$ has a cube root, so does $a$, and it's the inverse of the cube root of $a^{-1}$, which would, in this case, be $y=(x a x)^{-1}$, i.e., $y=x^{-1} a^{-1} x^{-1}$.

\item {\it If $x a x=b$, then $a b$ has square root.} Multiplying this on the left by $a$ gives us $a x a x=a b$, that is, $(a x)^2=a b$. Therefore, $a b$ has a square root and it is $a x$.

\end{enumerate}

\noindent{\bf Problem.} Let $M(n)\in\R^{n\times n}$ denote the set of all $n\times n$ square matrices. Is $M(n)$ with matrix multiplication\footnote{Let $A=[a_{i j}]_{n\times n}$ and $B=[b_{i j}]_{n\times n}$ be matrices. Then their product is a matrix $A B=[c_{i j}]_{n\times n}$ with $c_{i j}=\sum_{k=1}^{n}{a_{i k} b_{k j}}$.} a group?

\noindent\newline{\bf Solution.} We will check all the properties. It's obvious that closure is satisfied as, by definition, we get a $n\times n$ matrix when multiplying two $n\times n$ matrices. As for the rest:

\begin{itemize}
\item[A.] Let $A,B,C\in M(n)$ such that $A=[a_{i j}]_{n\times n}$, $B=[b_{i j}]_{n\times n}$ and $C=[c_{i j}]_{n\times n}$. We will check whether $A (B C)=(A B) C$. We see that:

\begin{equation*}
A B=\left[\left(\sum_{k=1}^{n}{a_{i k} b_{k j}}\right)_{i j}\right]_{n\times n}.
\end{equation*}

\noindent\newline Then, $(A B) C$ is defined as:

\begin{equation*}
(A B) C=\left[\left(\sum_{l=1}^{n}{\left(\sum_{k=1}^{n}{a_{i k} b_{k j}}\right)_{i l} c_{l j}}\right)_{i j}\right]_{n\times n}.
\end{equation*}

\noindent\newline But that can be written as:

\begin{equation*}
(A B) C=\left[\left(\sum_{l=1}^{n}{c_{l j}\sum_{k=1}^{n}{a_{i k} b_{k l}}}\right)_{i j}\right]_{n\times n}=\left[\left(\sum_{l=1}^{n}{\sum_{k=1}^{n}{a_{i k} b_{k l} c_{l j}}}\right)_{i j}\right]_{n\times n}.
\end{equation*}

\noindent\newline Rearranging sums, we can see that associativity holds:

\begin{eqnarray*}
(A B) C&=&\left[\left(\sum_{k=1}^{n}{a_{i k}\sum_{l=1}^{n}{b_{k l} c_{l j}}}\right)_{i j}\right]_{n\times n}\\\\
&=&\left[\left(\sum_{k=1}^{n}{a_{i k}\left(\sum_{l=1}^{n}{b_{k l} c_{l j}}\right)_{k j}}\right)_{i j}\right]_{n\times n}=A (B C).
\end{eqnarray*}

\item[N.] Neutral element is $I=[p_{i j}]_{n\times n}$ such that $p_{i i}=1$ for all $i\in\{1,\ldots,n\}$ and $p_{i j}=0$ for all $i\neq j$, where $i,j\in\{1,\ldots,n\}$. Take $A\in M(n)$. Then:

\begin{equation*}
A I=\left[\left(\sum_{k=1}^{n}{a_{i k} p_{k j}}\right)_{i j}\right]_{n\times n}.
\end{equation*}

\noindent\newline Obviously $a_{i k} p_{k j}=0$ when $k\neq j$, so all that remains is $a_{i j} p_{j j}=a_{i j}$ corresponding to $i j$-th element of the $A I$ matrix. Same thing goes for $I A$. Therefore $A I=I A=A$, where $I\in M(n)$ is the identity matrix.

\item[I.] There are no inverses, in general, e.g. take $A\in M(n)$ such that $a_{i j}=0$ for all $i,j\in\{1,\ldots,n\}$. Then we would need $B$ such that $A B=I$, but:

\begin{equation*}
\left[\left(\sum_{k=1}^{n}{a_{i k} b_{k j}}\right)_{i j}\right]_{n\times n}=\left[0_{i j}\right]_{n\times n},
\end{equation*}

\noindent\newline which can never be equal to $I$.

\item[C.] Matrix multiplication is not commutative in general as

\begin{equation*}
\left[\left(\sum_{k=1}^{n}{a_{i k} b_{k j}}\right)_{i j}\right]_{n\times n}=\left[\left(\sum_{k=1}^{n}{b_{i k} a_{k j}}\right)_{i j}\right]_{n\times n}
\end{equation*}

\noindent\newline will not hold due to different arrangements of matrix members.

\end{itemize}

\noindent Set $M(n)$ with matrix multiplication is a monoid.

\noindent\newline{\bf Problem.} Let $O(n)\in\R^{n\times n}$ be the set of all $n\times n$ orthogonal matrices\footnote{A matrix $A$ is orthogonal if $A A^T=A^T A=I$.}. Is $O(n)$ with matrix multiplication a group?

\noindent\newline{\bf Solution.} We have already checked associativity and found a neutral element (identity matrix) in the previous problem for $M(n)\supset O(n)$. Now, by definition of orthogonal matrices it's true that $A A^T=A^T A=I$ for all $A\in O(n)$. Therefore there are inverses for all orthogonal matrices and it's their respective transpose matrix. Commutativity, of course, does not still hold in general. Set $O(n)$ with matrix multiplication is a group.

\newpage

\begin{center}
{\bf Category Theory Basics}
\end{center}

\vskip 0.5cm

Before we venture further into the stunning world of abstract algebra by defining subgroups in the next chapter, I would like, as I have mentioned in introduction to say something about the basics of categories in mathematics. The reason why I'm doing this, is because I feel that gentle comparison with abstract algebra, and gentle introduction of modern concepts might accelerate the infusion of the latter into the reader's mind. However, I do not feel that there ever will be a reader, and even if it were the case, I see no point in it. The beauty was to lie only for me, the beauty was to affirm my power over the academic circles which have ignored me for years and which have, and perhaps forever will, consider me a complete failure. Let it be so. For may I be then considered Ed Wood of mathematics and derogatively called Euler for my ventures in philosophy. There is no point at all.

\noindent\newline {\bf Definition.} Sometimes it is easier to say what something consists of than what it is. The difference in quality is infinite, yet it's so far the best I can do. So, a {\bf category} (denoted by $\mathcal{C}$; or, in case when we're dealing with more categories, $\mathcal{A}$, $\mathcal{B}$, et cetera) consists of:

\begin{enumerate}
\item {\bf Objects.} Usually denoted by $A$, $B$, $C$, etc. The collection of objects in category $\mathcal{C}$ is then $ob(\mathcal{C})=\{A,B,C,\ldots\}$ (a bit naively).
\item {\bf Morphisms.} A morphism can be considered as an {\it arrow} going from one object to the other; usually denoted by $f$, $g$, $h$, and so on. The collection of morphisms in category $\mathcal{C}$ is then $ar(\mathcal{C})=\{f,g,h,\ldots\}$.
\item {\bf Typings.} Typing on a morphism is the relation that actually relates (forgive me for a bit some Hegelian dialectic) objects by the use of morphisms. Therefore, they can be thought of as functions whose domain is one object and codomain another. one example of a typing would be $f:A\rightarrow B$. In this case, $A\rightarrow B$ is the {\it type} of $f$ and $f$ is a {\it morphism from} $A$ to $B$. If we're dealing with more categories, we will use, exempli gratia, $f:A\rightarrow_{\mathcal{C}}B$ for a morphism from $A$ to $B$ in category $\mathcal{C}$. Now, if we want all the morphisms (say we have $f$, $g$ and $h$) of the type $A\rightarrow B$, we get them all in one collection denoted by, e.g. $Hom(A,B)=\{f,g,h\}$. Sometimes it's $Mor(A,B)$, $Hom_{\mathcal{C}}(A,B)$ (especially when dealing with more then one category) and rarely $\mathcal{C}(A,B)$.
\item {\bf Compositions.} We have already discussed the notion of a partial binary relation (it has to be closed and give unique elements, but does not have to be defined for each ordered pair). Now, in category theory, a partial binary relation is a sort of composition (denoted in many ways, but for now, we shall use either $\circ$ or juxtaposition) between morphisms (that we can intuitively know from function composition).
\item {\bf Identities.} For each object, there is a identity morphism, i.e. for some object $A$, there is $id_A:A\rightarrow A$.
\end{enumerate}

\noindent\newline {\bf Remark.} Now, we have defined, all sorts of "things": objects, morphisms, compositions, etc. But what do they really represent? I would say, nothing special, but that would be, sort of, the same as to say, anything at all! One may fall into a beginner's trap, which they sure nailed in category-theoretic alphabet, and consider morphisms as functions when describing certain categories, and objects as elements of a set, et similis. But, of course, this is only half-true. Sometimes, as we will see, morphisms can themselves represent elements of some set we're defining as a category, but in another, objects might do the same trick. This will be explained later through some examples.

Now, every category is subject to the following three typing axioms and two composition axioms.

\noindent\newline {\bf Typing axioms.}

\begin{enumerate}
\item {\it Unique type\footnote{There are no duplicates of the same morphisms.}.} If $f:A\rightarrow B$ and $f:A'\rightarrow B'$, then $A=A'$ and $B=B'$.

\item {\it Composition type.} For every $f:A\rightarrow B$ and $g:B\rightarrow C$, there is $g\circ f:A\rightarrow C$ (i.e. $f g:A\rightarrow C$).

\item {\it Identity type.} For every $A$, there is $id_A:A\rightarrow A$.
\end{enumerate}

\noindent\newline {\bf Composition axioms.}

\begin{enumerate}
\item {\it Associativity.} For every $f:A\rightarrow B$, $g:B\rightarrow C$ and $h:C\rightarrow D$ it's $h\circ (g\circ f)=(h\circ g)\circ f$, i.e. $(f g) h=f (g h)$.

\item {\it Identity.} For every $f:A\rightarrow B$, it's $id_B\circ f=f=f\circ id_A$ (or, $f id_B=f=id_A f$).
\end{enumerate}

\noindent\newline If only unique type axiom is not satisfied, then we're dealing with a {\it pre-category}.

\noindent\newline {\bf Set.} Set can be defined as a category whose objects are viewed as elements of a set, and we have only identity arrows, no other morphisms. It's trivial to see that all axioms are satisfied.

\noindent\newline {\bf Monoid.} Monoid is a one-object category whose (identity) morphisms are elements of the underlying monoid set, with composition between morphisms serving as an operation defined on that underlying monoid set. If we have a monoid $(S,\ast)$, where $S=\{e,a_1,\ldots,a_n\}$, then define category $\mathcal{C}$ such that $ob(\mathcal{C})=\{A\}$ and $ar(\mathcal{C})=\{a_1,\ldots,a_n\}$, such that $a_i:A\rightarrow A$, for $i=1,\ldots,n$. Obviously $Mor(A\rightarrow A)=S$. It's easy to see that associativity is satisfied, there is a neutral element for composition, $id_A:A\rightarrow A$, which can be denoted by $e$. Then $e\circ a_i=a_i\circ e=a_i$ for all $i=1,\ldots,n$. So, it's trivial to see that this is a monoid structure satisfying category axioms.

\noindent\newline {\bf Group.} Monoid category can be made into a category resembling a group by further defining $a_i^{-1}:A\rightarrow A$ for every $a_i:A\rightarrow A$, for all $i=1,\ldots,n$. Same thing can be done with Abelian group, by defining $a_i a_j=a_j a_i$ for all $i,j=1,\ldots,n$.

\noindent\newline Of course, these were some rather naive examples that are useful mostly to make us see the way we can view structures in algebra from categorical point of view.

\newpage

\begin{center}
{\bf Subgroups}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group and $S\subseteq G$, $S\neq\emptyset$. If $x y\in S$ for every $x,y\in S$, and if $a^{-1}\in S$, for every $a\in S$, then $S$ is called a {\bf subgroup} of $G$ (we are assuming, of course, that we're dealing with the same operation that is defined on $G$).

\noindent\newline{\bf Theorem.} Let $G$ be a group. Every subgroup of $G$ is a group.

\noindent\newline{\bf Proof.} Let $G$ be a group and $S$ a subgroup of $G$. We need only check associativity and existence of a neutral element, as, by definition, $S$ is closed with respect to the same operation defined on $G$, and every element in $S$ has its inverse in $S$. As for associativity, take any $a,b,c\in S$. As $S\subseteq G$, then $a,b,c\in G$. Being that $G$ is a group, it is necessarily associative, and then it must be that $a(bc)=(ab)c$. Therefore, $S$ is also associative. Now, as $a^{-1}\in S$ for every $a\in S$, and as $x y\in S$ for every $x,y\in S$, then also, necessarily, $a a^{-1}\in S$. But, $a a^{-1}=e$, which proves that $e\in S$. Thus, $S$ is a group.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $C$ and $D$ be sets, with $C\subseteq D$. Then $\mathcal{P}_C$ is a subgroup of $\mathcal{P}_D$ (operation is symmetric difference).

\noindent\newline{\bf Solution.} By definition, $\mathcal{P}_C=\{S: S\subseteq C\}$ and $\mathcal{P}_D=\{S: S\subseteq D\}$. If we take some $S\subseteq D$ then it must be that $S\in\mathcal{P}_D$. In this problem, it's $C\subseteq D$, so $C\in\mathcal{P}_D$. Now, if we take some $S\in\mathcal{P}_C$, then, by definition, $S\subseteq C$, which is, in turn, subset of $D$, therefore $S\subseteq D$ and it must be that $S\in\mathcal{P}_C$. By definition of set inclusion\footnote{$(\forall x)(x\in X\rightarrow x\in Y)$ iff $X\subseteq Y$}, it must be that $\mathcal{P}_C\subseteq\mathcal{P}_D$. We have shown previously that $\left(\mathcal{P}_D,\Delta\right)$ is an Abelian group, but so is $\left(\mathcal{P}_C,\Delta\right)$! Therefore, it must be that $\mathcal{P}_C$ is closed under operation of symmetric difference $\Delta$ and with respect to inverses, by definition of a group. Then it must be, as, in addition, $\mathcal{P}_C\subseteq\mathcal{P}_D$ that $\mathcal{P}_C$ is a subgroup of $\mathcal{P}_D$.

\noindent\newline{\bf Remark.} In further excercises, we will denote $\R\backslash\{0\}$ as $\R^{\ast}$. Also, we will use $\mathcal{F}(\R)$ to represent the set of all functions going from $\R$ to $\R$. Similarly, with $\mathcal{C}(\R)$ the set of all continuous functions going from $\R$ to $\R$, and with $\mathcal{D}(\R)$ the set of all differentiable functions going from $\R$ to $\R$.

\noindent\newline{\bf Theorem.} Let $\mathcal{F}(\R)=\{f:\ f:\R\rightarrow\R\}$. Then $\left(\mathcal{F}(\R),+\right)$ is a group, and $\left(\mathcal{C}(\R),+\right)$ and $\left(\mathcal{D}(\R),+\right)$ are subgroups of $\left(\mathcal{F}(\R),+\right)$, and therefore groups by themselves.

\noindent\newline{\bf Proof.} Note that we define function addition\footnote{Note that most people get confused with $(f+g)(x)=f(x)+g(x)$, thinking that there is some underlying mathematical construction of function addition. But, we cannot {\it add} functions themselves. And, thus, to avoid confusion, we will use $[f+g](x):=(f+g)(x)$, to suggest to reader that the plus sign is just a part of notation, not some operation between the two mapping rules. We could as well write $h(x)=f(x)+g(x)$; the difference between the latter and the former is only in notation, nothing more, nothing less.} as $[f+g](x)=f(x)+g(x)$ for all $x\in\R$. Take $f,g,h\in\mathcal{F}(\R)$. We will check whether associativity holds. Now, take $[f+[g+h]](x)=f(x)+[g+h](x)=f(x)+g(x)+h(x)=[f+g](x)+h(x)=[[f+g]+h](x)$, for all $x\in\R$; in conclusion, it is associative. Commutativity holds as $[f+g](x)=f(x)+g(x)=g(x)+f(x)=[g+f](x)$, for all $x\in\R$. Now, neutral element is $\mathcal{O}(x)=0$, as $[f+\mathcal{O}](x)=f(x)+\mathcal{O}(x)=f(x)+0=f(x)$ (remember that we need not show that it's a left neutral element because addition is commutative). It's obvious that $[-f](x)=-f(x)$ is the inverse function, as $[f+[-f]](x)=f(x)+(-f(x))=0=\mathcal{O}(x)$, for all $x\in\R$. Thus we have proved that $\left(\mathcal{F}(\R),+\right)$ is an Abelian group. Now, it's obvious\footnote{We're using the fact that all differentiable functions are continuous.} that $\mathcal{D}{\R}\subset\mathcal{C}(\R)\subset\mathcal{F}(\R)$. Now, as the sum of two continuous functions is a continuous function and the sum of two differentiable functions is a differentiable functions, both are closed under addition. If $f$ is continuous then so is $-f$ which is it's additive inverse. Also, if $f$ is differentiable, then so is $-f$, which is it's additive inverse. Thus, $\left(\mathcal{C}(\R),+\right)$ and $\left(\mathcal{D}(\R),+\right)$ are subgroups of $\left(\mathcal{F}(\R),+\right)$, and then groups by themselves.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Determine whether or not $H$ is a subgroup of $G$ (again, assume that the operation on $H$ is the same as on $G$; and of course, from previous excercises we should {\it know} that $G$ are groups in all problems below):

\begin{enumerate}

\item $G=\left(\R^{\ast},\cdot\right)$, $H=\left\{2^n:\ n\in\Z\right\}$;

\item $G=\left(\R,+\right)$, $H=\left\{\log{a}:\ a\in\Q,\ a>0\right\}$;

\item $G=\left(\R,+\right)$, $H=\left\{\log{n}:\ n\in\Z,\ n>0\right\}$;

\item $G=\left(\R,+\right)$, $H=\left\{x\in\R:\ \tan{x}\in\Q\right\}$;

\item $G=\left(\R^{\ast},\cdot\right)$, $H=\left\{2^n 3^m:\ m,n\in\Z\right\}$;

\item $G=\left(\R\times\R,+\right)$, $H=\left\{(x,y):\ y=2x\right\}$;

\item $G=\left(\R\times\R,+\right)$, $H=\left\{(x,y):\ x^2+y^2>0\right\}$;

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(0)=0\right\}$;

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x)=0,\ \forall x\in[0,1]\right\}$;

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(-x)=-f(x)\right\}$;

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x+n\pi)=f(x),\ \forall n\in\Z,\ \forall x\in\R\right\}$;

\item $G=\left(\mathcal{C}(\R),+\right)$, $H=\left\{f\in\mathcal{C}(\R):\ \int_{0}^{1}{f(x)dx}=0\right\}$;

\item $G=\left(\mathcal{D}(\R),+\right)$, $H=\left\{f\in\mathcal{D}(\R):\ \frac{df}{dx}=c,\ c\in\R\right\}$;

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x)\in\Z,\ \forall x\in\R\right\}$;
\end{enumerate}

\noindent{\bf Solution.} Now, we only need to check whether $H\subseteq G$ (if it's not obvious), check whether the operation is closed on $H$ and if it's closed with respect to inverses.

\begin{enumerate}

\item $G=\left(\R^{\ast},\cdot\right)$, $H=\left\{2^n:\ n\in\Z\right\}$. Obviously if $n\in\Z$, then $2^n\in\Q\subset\R$, so $H\subset G$. Now, take $a,b\in H$, we need to show that $a b\in H$. So, let $a=2^n$, $n\in\Z$ and $b=2^m$, $m\in\Z$. If we take $a b=2^n\cdot 2^m$, obviously $a b\in H$, as $a b=2^{m+n}$, which is in $H$. Let's find inverse of $a\in H$; take, the same $a=2^n$ and try to find $2^n\cdot x=1$. That is of course $2^{-n}$, because $2^n\cdot 2^{-n}=2^0=1$. Of course, $a^{-1}=2^{-n}$ is in $H$ as $-n\in\Z$. Therefore $H$ is a subgroup of $G$.

\item $G=\left(\R,+\right)$, $H=\left\{\log{a}:\ a\in\Q,\ a>0\right\}$. If $a\in\Q^{+}$, then $\log{a}\in\R$, so $H\subset G$. Take $x,y\in H$, such that $x=\log{a}$, $y=\log{b}$, where $a,b\in\Q^{+}$. Now, $x+y=\log{a}+\log{b}=\log{a b}\in H$. Now, to find inverse, we need $x^{-1}\in H$, take $x^{-1}=\log{z}$, $z\in\Q$, such that $x+x^{-1}=0$. Now $\log{a z}=0$ iff $z=\frac{1}{a}$ as $\log{a\frac{1}{a}}=\log{1}=0$. So there is an inverse, now we can denote it by $-x=-\log{a}$, and it is in $H$, as $\frac{1}{a}\in\Q^{+}$ ($a\in\Q^{+}$ also, so we are secure of it being zero). $H$ is a subgroup of $G$.

\item $G=\left(\R,+\right)$, $H=\left\{\log{n}:\ n\in\Z,\ n>0\right\}$. Obviously $H$ will not be a subgroup of $G$ as it's not closed with respect to inverses. There is no $-x=-\log{n}$ for every $x=\log{n}$ because $-x=\log{\frac{1}{n}}$ which is in $H$ if and only if $n=1$.

\item $G=\left(\R,+\right)$, $H=\left\{x\in\R:\ \tan{x}\in\Q\right\}$. It's true that $H\subset G$ as $x$ defined to be in $\R$. Now, take $x,y\in H$ such that $\tan{x}\in\Q$ and $\tan{y}\in\Q$. Let $\tan{x}=\frac{a}{b}$, $a\in\Z$, $b\in\N$ and $\tan{y}=\frac{c}{d}$, $c\in\Z$, $d\in\N$. Then, we need to show that $x+y$ is such that $\tan{(x+y)}\in\Q$. An addition formula tells us:

\begin{equation*}
\tan{(x+y)}=\frac{\tan{x}+\tan{y}}{1-\tan{x}\tan{y}}=\frac{\frac{a}{b}+\frac{c}{d}}{1-\frac{a b}{c d}}=\frac{\frac{a d+b c}{b d}}{\frac{c d-a b}{c d}}=\frac{(a d+b c)(c d)}{(b d)(c d-a b)}.
\end{equation*}

\noindent\newline So, as both $\tan{x}$ and $\tan{y}$ are rational, their sum and product will be rational, and we will have that $\tan{(x+y)}$ is a rational number. But what if $\tan{x}\tan{y}=1$, i.e. $c d=a b$? Then we have division by zero! Well, as $\tan{\left(\frac{\pi}{4}+k\frac{\pi}{2}\right)}=1$, then $\tan{x}\tan{y}=1$ for $x=y=\frac{\pi}{4}+k\frac{\pi}{2}$, $k\in\Z$ and both $\tan{x}$ and $\tan{y}$ will be rational (they will each equal $1$). So, this operation is not closed on $H$ and $H$ is not a subgroup of $G$.

\item $G=\left(\R^{\ast},\cdot\right)$, $H=\left\{2^n 3^m:\ m,n\in\Z\right\}$. It's obvious that $2^n 3^m\in H\subset\R^{\ast}$. Now, if we take $2^n 3^m, 2^p 3^q\in H$, then $2^n 3^m 2^p 3^q=2^{n+p} 3^{m+q}$. As $n,m,p,q\in\Z$, then also $(n+p),(m+q)\in\Z$; therefore $H$ is closed in respect to multiplication. As for inverses, obviously it's $2^{-n} 3^{-m}\in H$ because $2^{n} 3^{m} 2^{-n} 3^{-m}=2^{n-n} 3^{m-m}=2^0 3^0=1$, which is neutral element for multiplication. Therefore, $H$ is a subgroup of $G$.

\item $G=\left(\R\times\R,+\right)$, $H=\left\{(x,y):\ y=2x\right\}$. Obviously $(x,2x)\in H\subset\R^2$. Now, if we take $(x,y),(p,q)\in H$ such that $y=2x$ and $q=2p$, if we add them, $(x,y)+(p,q)=(x+p,y+q)$ is $y+q=2(x+p)$? Well, considering that $y=2x$ and $q=2p$, then $y+q=2x+2p=2(x+p)$, therefore $H$ is closed with respect to addition. Neutral element for addition is $0$, so neutral element in $G$ is $(0,0)$. And that we shall get if we take $(x,y)+(x^{-1},y^{-1})=0$. Therefore, it must be that $x+x^{-1}=0$ and $y+y^{-1}=0$. So, obviosly $x^{-1}=-x$, and if $y=2x$ and $y^{-1}=-y$ then $y^{-1}=-2x=2 x^{-1}$. So, inverse element for $(x,y)\in H$ is $(-x,-2x)\in H$. In conclusion, $H$ is a subgroup of $G$.

\item $G=\left(\R\times\R,+\right)$, $H=\left\{(x,y):\ x^2+y^2>0\right\}$. As all elements in $H$, except $(0,0)$, satisfy $x^2+y^2>0$, obviously $H=\left(\R\times\R^2\right)\backslash\{(0,0)\}\subset\left(\R\times\R\right)$. But, take $(-4,4),(4,-4)\in H$. Obviously $(-4)^2+4^2=16+16=32>0$, and same goes for $(4,-4)$, so they really are in $H$, but $(-4,4)+(4,-4)=(-4+4,4-4)=(0,0)\notin H$, as $0^2+0^2=0+0=0$ and it's not the case that $0>0$. So $H$ cannot be a subgroup of $G$.

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(0)=0\right\}$. Obviously $H\subset\mathcal{F}(\R)$. Now, take $f,g\in H$ such that $f(0)=g(0)=0$. We need to show that for $f+g$ it's $[f+g](0)=0$. Now, it's true that $f(x)+g(x)=[f+g](x)$, so if $f(0)=0$ and $g(0)=0$, then $0=f(0)+g(0)=[f+g](0)$. So, $[f+g]\in H$. Now, obviously, $f(x)+\left(-f(x)\right)=0=\mathcal{O}(x)$ which is a neutral element for addition in $G$, so $H$ is a subgroup of $G$, as $-f(x)\in H$ if $f(x)\in H$; because $f(0)=0$ and then $-f(0)=0$.

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x)=0,\ \forall x\in[0,1]\right\}$. Obviously, again, $H\subset\mathcal{F}(\R)$. But, take $f,g\in H$ and then it's $f(x)=0$ and $g(x)=0$ for every $x$ in $[0,1]$. If we add $f$ and $g$, we get $f(x)+g(x)=[f+g](x)=0$ for all $x$ in $[0,1]$ (as $f(x)=0$ and $g(x)=0$ in $[0,1]$, their sum on this segment is also zero). Now, as for the inverse $f\in H$ and then $f(x)=0$ for all $x\in[0,1]$. But then, also $-f(x)=-0=0$, for all $x\in[0,1]$. And, $f(x)+(-f(x))=\mathcal{O}(x)$, which is a neutral element for addition of functions in $G$. in conclusion, $H$ is a subgroup of $G$.

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(-x)=-f(x)\right\}$. Again, it's obvious that $H\subset G$ (through definition: $H$ is only strengthening of $G$ by adding more conditions). Let $f,g$ be in $H$. Then, $f(-x)=-f(x)$ and $g(-x)=-g(x)$. Now, if we take $[f+g](-x)=f(-x)+g(-x)=-f(x)-g(x)=-(f(x)+g(x))=-[f+g](x)$, so if $f,g\in H$ then $[f+g]\in H$, therefore, $H$ is closed under addition. Take $f\in H$. It's inverse should be some $g\in H$ such that $f(x)+g(x)=0$, i.e. $g(x)=-f(x)$, but $-f(x)=f(-x)$, so for every $f\in H$ there is $(-f)\in H$ and $H$ is closed with respect to inverses. $H$ is a subgroup of $G$.

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x+n\pi)=f(x),\ \forall n\in\Z,\ \forall x\in\R\right\}$. Again, $H\subset\mathcal{F}(\R)$. Take $f,g\in H$ and we will show that $[f+g]\in H$. Indeed, $[f+g](x)=f(x)+g(x)=f(x+n\pi)+g(x+n\pi)=[f+g](x+n\pi)$, $\forall n\in\Z$. $H$ is closed under function addition, but is it closed with respect to inverses? Obviously, $f(x)+(-f(x))=\mathcal{O}(x)$. Now, if $f\in H$ then $-f\in H$, as $f(x)=f(x+n\pi)$, and multiplying it by $(-1)$ yields $-f(x)=-f(x+n\pi)$, for all $n\in\Z$. Therefore, $H$ is a subgroup of $G$.

\item $G=\left(\mathcal{C}(\R),+\right)$, $H=\left\{f\in\mathcal{C}(\R):\ \int_{0}^{1}{f(x)dx}=0\right\}$. Obviously $H\subset\mathcal{C}(\R)$. Now, let $f,g\in H$ and observe $[f+g](x)=f(x)+g(x)$. We will integrate both sides of equation to get $\int_{0}^{1}{[f+g](x)dx}=\int_{0}^{1}{(f(x)+g(x))dx}=\int_{0}^{1}{f(x)dx}+\int_{0}^{1}{g(x)dx}=0+0=0$. So, $[f+g]\in H$. Now, as for the inverse, for every $f\in H$ there must be $-f\in H$. But, $-f(x)=\int_{0}^{1}{(-f(x))dx}=-\int_{0}^{1}{f(x)dx}=-0=0$, so $-f\in H$ and $H$ is closed under addition and in respect to inverses: $H$ is a subgroup of $G$.

\item $G=\left(\mathcal{D}(\R),+\right)$, $H=\left\{f\in\mathcal{D}(\R):\ \frac{df}{dx}=c,\ c\in\R\right\}$. It's easy to see that $H\subset\mathcal{D}(\R)$. Take $f,g\in H$. Then, $[f+g](x)=f(x)+d(x)$. Applying differential operator on this equation, we get $\frac{d[f+g]}{dx}(x)=\frac{d}{dx}\left(f(x)+g(x)\right)=\frac{df}{dx}(x)+\frac{dg}{dx}(x)=c_f+c_g=c_{f+g}$, $c_{f+g}\in\R$. Now, $-f(x)=\frac{d}{dx}(-f(x))=-\frac{df}{dx}(x)=-c=c_{-f}$, $c_{-f}\in\R$. Therefore, $H$ being closed under function addition and with respect to inverses, $H$ is a subgroup of $G$.

\item $G=\left(\mathcal{F}(\R),+\right)$, $H=\left\{f\in\mathcal{F}(\R):\ f(x)\in\Z,\ \forall x\in\R\right\}$. Again, $H\subset\mathcal{F}(\R)$. Let $f,g$ be in $H$ and let us consider $(f+x)(x)=f(x)+g(x)$. But, $f(x)\in\Z$ and $g(x)\in\Z$ for all $x\in\R$. Therefore their sum is also in $\Z$ and $(f+x)(x)\in\Z$, for every $x\in\R$, and so $[f+g]\in H$. Now, $f(x)$ being in $\Z$, then $-f(x)$ is also obviously in $\Z$, for every $x\in\R$, and therefore in $H$. $H$ is a subgroup of $G$.
\end{enumerate}

\noindent{\bf Problem.} Let $G$ be an Abelian group.

\begin{enumerate}
\item If $H=\left\{x\in G:\ x=x^{-1}\right\}$, that is, $H$ consists of all the elements of $G$ which are their own inverses, prove that $H$ is a subgroup of $G$.
\item Let $n$ be a fixed integer, and let $H=\left\{x\in G:\ x^n=e\right\}$. Prove that $H$ is a subgroup of $G$.
\item Let $H=\left\{x\in G:\ x=y^2,\ y\in G\right\}$; that is, let $H$ be the set of all the elements of $G$ which have a square root. Prove that $H$ is a subgroup of $G$.
\item Let $H$ be a subgroup of $G$, and let $K=\left\{x\in G: x^2\in H\right\}$. Prove that $K$ is a subgroup of $G$.
\item Let $H$ be a subgroup of $G$, and let $K$ consist of all the elements $x$ in $G$ such that some power of $x$ is in $H$. That is, $K=\left\{x\in G: x^n\in H,\ n>0,\ n\in\Z\right\}$. Prove that $K$ is a subgroup of $G$.
\item Suppose $H$ and $K$ are subgroups of $G$, and define $H K$ as follows:

\begin{equation*}
H K=\left\{x y:\ x\in H\ \textnormal{and}\ y\in K\right\}.
\end{equation*}

\noindent\newline Prove that $H K$ is a subgroup of $G$.

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}

\item {\it If $H=\left\{x\in G:\ x=x^{-1}\right\}$, that is, $H$ consists of all the elements of $G$ which are their own inverses, prove that $H$ is a subgroup of $G$.} Obviously $H\subseteq G$. Now if we take $x,y\in H$, then, $x^2=e$ and multiplying it by $y^2$ on the left gives us $x^2 y^2=y^2$. But, $y^2=e$, so $x^2 y^2=e$. Rearranging the elements using commutativity, we get $(x y)(x y)=e$, i.e. their product is the inverse of itself, so it must be in $H$. This of course, would not work, generally, if $G$ were not Abelian; we could not rearrange the elements in such manner. And, $H$ is, naturally, closed in respect to inverses by definition. Therefore, $H$ is a subgroup of $G$.

\item {\it Let $n$ be a fixed integer, and let $H=\left\{x\in G:\ x^n=e\right\}$. Prove that $H$ is a subgroup of $G$.} We can easily see that $H\subset G$. Now, let $x,y\in H$. Obviously $x^n=e$, but if we multiply it on the right (or left, doesn't matter, it's Abelian) by $y^n$, we get $x^n y^n=y^n$. But, $y^n=e$, so we have $x^n y^n=e$. By previously proven property, we have $(x y)^n=x^n y^n$ and so $(x y)^n=e$. So their product $x y$ is such that $(x y)^n=e$ and it must be in $H$. Again, take $x\in H$, and we have $x^n=e$. Now, we have $x x^{n-1}=e$, therefore, the inverse of $x$ is $x^{n-1}$. Is it in $H$? Obviously $e^n=(x^n)^n=e$. Now, $(x x^{n-1})^n=e$. Then, $x^n (x^{n-1})^n=e$ and, as $x^n=e$, it's $(x^{n-1})^n=e$, so $x^{n-1}\in H$ and $H$ is a subgroup of $G$.

\item {\it Let $H=\left\{x\in G:\ x=y^2,\ y\in G\right\}$; that is, let $H$ be the set of all the elements of $G$ which have a square root. Prove that $H$ is a subgroup of $G$.} Again, $H\subset G$ by definition. If we take $x,y\in H$, then $x=z^2$ and $y=w^2$, for some $w,z\in G$. Their product is then $x y=z^2 w^2$, and by using $(z w)^2=z^2 w^2$ (which would not hold if the group was not Abelian in nature), we have $x y=(z w)^2$. In conclusion product $x y$ has a square root $z w\in G$ so it must be in $H$. If $x\in H$ then $x=y^2$ for some $y\in G$. Now, $y$ has an inverse in $G$ and that is $y^{-1}$. So, multiplying this by $(y^{-1})^2$ (which must be also in $G$, as product $y^{-1} y^{-1}\in G$) it has to be $x (y^{-1})^2=e$. So, the inverse of $x$ is $(y^{-1})^2$ (the squared inverse of it's square root). But is $(y^{-1})^2$ in $H$? Obviously it is, as $H$ contains all elements of $G$ that have a square root in $G$ and, square root of $(y^{-1})^2$ is obviously $y^{-1}$. And, again, $H$ is a subgroup of $G$.

\item {\it Let $H$ be a subgroup of $G$, and let $K=\left\{x\in G: x^2\in H\right\}$. Prove that $K$ is a subgroup of $G$.} Notice that $K\subset G$. If $x,y\in K$, then $x^2\in H$ and $y^2\in H$. As, $H$ is a group it's closed under multiplication, therefore product $x^2 y^2$ is in $H$. But, that product is $x^2 y^2=(x y)^2\in H$ (for this commutativity is necessary). As $x,y\in K$, also $x,y\in G$. So $(x y)\in G$, as $G$ is closed under multiplication. And, as $(x y)\in G$ and $(x y)^2\in H$, by definition of $K$, then $(x y)\in K$. Now, is inverse of $x\in K$ in $K$? Obviously, $x^2\in H$ and, due to $H$ being a group, there it has an inverse and it's $(x^2)^{-1}\in H$. That is, $(x^{-1})^2\in H$. Of course, $x^{-1}$ is in $G$, as $x^{-1}\in H\subset G$, so, because it's also $(x^{-1})^2\in H$, it's also $x^{-1}\in K$. Therefore, $K$ is closed with respect to inverses and it's true that $K$ is a subgroup of $G$.

\item {\it Let $H$ be a subgroup of $G$, and let $K$ consist of all the elements $x$ in $G$ such that some power of $x$ is in $H$. That is, $K=\left\{x\in G: x^n\in H,\ n>0,\ n\in\Z\right\}$. Prove that $K$ is a subgroup of $G$.} Obviously, $K\subset G$. Take $x,y\in H$ and then it's $x^n,y^n\in H$. Yet, $H$ is a group, so $x^n y^n\in H$, i.e. $(x y)^n\in H$ (we cannot claim this without commutativity). As $x,y\in K\subset G$, and $G$ is a group, therefore closed under multiplication, it's $(x y)\in G$. Combining that fact with $(x y)^n\in H$, obviously $(x y)\in K$. Therefore, $K$ is closed under multiplication. Now, if $x\in K$, then $x^n\in H$. As $H$ is a group, it's closed with respect to inverses, so it's also that $(x^n)^{-1}\in H$. That is, $(x^{-1})^n\in H$. As $x^{-1}\in H\subset G$, it's also $x^{-1}\in G$. Now it's obvious that $x^{-1}\in K$ and $K$ is closed with respect to inverses, thus proving that $K$ is a subgroup of $G$.

\item {\it Suppose $H$ and $K$ are subgroups of $G$, and define $H K$ as follows:}

\begin{equation*}
H K=\left\{x y:\ x\in H\ \textnormal{and}\ y\in K\right\}.
\end{equation*}

\noindent\newline {\it Prove that $H K$ is a subgroup of $G$.} If $(x y)\in H K$, then, due to $x\in H\subset G$, it's $x\in G$ and, by the same logic, as $y\in K\subset G$, it's also $y\in G$. $G$ is closed under multiplication, so $(x y)\in G$. That proves that $H K\subseteq G$. Now, take some $(a b)\in H K$ and $(c d)\in H K$. Obviously, $a,c\in H$ and $b,d\in K$. Now, as $H K\subseteq G$, $(a b),(c d)\in G$. As $G$ is a group it's closed under multiplication, so $(a b)(c d)\in G$. By using associativity and commutativity (necessary for this one) we can rearrange elements to get $(a b)(c d)=(a c)(b d)\in G$. Now, as $a,c\in H$, their product is in $H$, i.e. $(a c)\in H$. Also, $(b d)\in K$. Now, as $(a c)(b d)\in G$, $(a c)\in H$ and $(b d)\in K$, obviously $(a c)(b d)\in H K$. But as $(a c)(b d)\in G$, then it's $(a c)(b d)=(a b)(c d)\in G$, and those product being equal, obviosly also $(a b)(c d)\in H K$. Now, if we take $x y\in H K$, then $x^{-1}\in H$ and $y^{-1}\in K$. As both of these groups are subgroups of $G$, and $G$ is closed under multiplication, we have $x^{-1} y^{-1}\in G$ such that $x^{-1}\in H$ and $y^{-1}\in K$. So, $x^{-1} y^{-1}\in H K\subset G$. In $G$, it's $x^{-1} y^{-1}=(x y)^{-1}\in G$. Those expressions being equal, i.e. it's the one and the same element, it's $(x y)^{-1}\in H K$, which is inverse of $(x y)\in H K$. Therefore, $H K$ is a subgroup of $G$.

\end{enumerate}

\noindent{\bf Problem.} Let $G$ be a group.

\begin{enumerate}
\item If $H$ and $K$ are subgroups of $G$, prove that $H\cap K$ is a subgroup of $G$.

\item Let $H$ and $K$ be subgroups of $G$. Prove that if $H\subseteq K$, then $H$ is a subgroup of $K$.

\item By the {\it center} of a group $G$ we mean the set of all the elements of $G$ which commute with every element of $G$, that is,

\begin{equation*}
C=\{a\in G:\ a x=x a,\ \forall x\in G\}.
\end{equation*}

\noindent\newline Prove that $C$ is a subgroup of $G$.

\item Let $C'=\{a\in G:\ (a x)^2=(x a)^2,\ \forall x\in G\}$. Prove that $C'$ is a subgroup of $G$.

\item Let $G$ be a {\it finite} group, and let $S$ be a nonempty subset of $G$. Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$.

\item Let $G$ be a group and $f:G\rightarrow G$ a function. A {\it period} of $f$ is any element $a$ in $G$ such that $f(x)=f(a x)$ for every $x\in G$. Prove that the set of all the periods of $f$ is a subgroup of $G$.

\item Let $H$ be a subgroup of $G$, and let $K=\{x\in G:\ x a x^{-1}\in H\ \textnormal{iff}\ a\in H\}$. Prove:

\begin{enumerate}
\item $K$ is a subgroup of $G$.
\item $H$ is a subgroup of $K$.
\end{enumerate}

\item Let $G$ and $H$ be groups, and $G\times H$ their direct product.
\begin{enumerate}
\item Prove that $\{(x,e):\ x\in G\}$ is a subgroup of $G\times H$.
\item Prove that $\{(x,x):\ x\in G\}$ is a subgroup of $G\times G$.
\end{enumerate}
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $H$ and $K$ are subgroups of $G$, prove that $H\cap K$ is a subgroup of $G$.} Obviously $(H\cap K)\subseteq G$. If we take $x,y\in H\cap K$, then $x$ and $y$ are both in $H$ and $K$. Now, as $H$ is a subgroup of $G$ and $x,y\in H$, then $x y\in H$. Same thing goes for $K$; it's also a subgroup of $G$, therefore it's closed with respect to multiplication and $x y\in K$. So, the following conjuction is true: $x y\in H$ and $x y\in K$, which, by definition of set intersection tells us that $x y\in H\cap K$. So $H\cap K$ is closed under multiplication. Take $x\in H\cap K$. Then, as $x\in H$ and $x\in K$, it has an inverse in $x^{-1}\in H$ and $x^{-1}\in K$, that is, $x^{-1}\in H\cap K$. So, $H\cap K$ is a subgroup of $G$.

\item {\it Let $H$ and $K$ be subgroups of $G$. Prove that if $H\subseteq K$, then $H$ is a subgroup of $K$.} Let $x,y\in H$. As $H$ is a subgroup of $G$, then $x y\in H$ and $x y\in K$. Now, if $x\in H$, it has an inverse $x^{-1}\in H$. But, also $x^{-1}\in K$ as $H$ is a subset of $K$. So, $H$ is a subset of $K$, it's closed with respect to multiplication and inverses, therefore $H$ is a subgroup of $K$.

\item {\it By the {\it center} of a group $G$ we mean the set of all the elements of $G$ which commute with every element of $G$, that is,}

\begin{equation*}
C=\{a\in G:\ a x=x a,\ \forall x\in G\}.
\end{equation*}

\noindent\newline {\it Prove that $C$ is a subgroup of $G$.} Notice that $C\subseteq G$. If we take some $a,b\in C$, then $a x=x a$ and $b x=x b$, for all $x\in G$. Now, as $x\in G$, and $G$ is a group, then $x^{-1}\in G$. We can say that $a=x a x^{-1}$ and $b=x a x^{-1}$, for all $x\in G$. Let's take their product, $a b=x a x^{-1} x b x^{-1}$, that is, $a b=x a b x^{-1}$. Multiplying that by $x^{-1}$ gives us $(a b) x=x (a b)$. As $(a b)$ obviously commute with all $x\in G$, then $(a b)\in C$. Therefore, $C$ is closed under multiplication. Take $a\in C$. Then, $a x=x a$ for all $x\in G$. Now, as $a\in G$, it has an inverse in $G$, and that is $a^{-1}\in G$. So we can multiply the previos equation with $a^{-1}$ on the left and get $x=a^{-1} x a$. Multiplying it by $(x a)^{-1}$ on the right gives us $x (x a)^{-1}=a^{-1}$. We multiply it again by $x^{-1}$ on the left and get $(x a)^{-1}=x^{-1} a^{-1}$. Obviously, that is $a^{-1} x^{-1}=x^{-1} a^{-1}$. We can multiply this again by $x$ on the left and on the right to get $x a^{-1} x^{-1} x=x x^{-1} a^{-1} x$, i.e. $x a^{-1}=a^{-1} x$, for all $x\in G$. So $a^{-1}\in G$ commutes with all $x\in G$ and therefore must be in $C$. Thus, we have proved that $C$ is a subgroup of $G$.

\item {\it Let $C'=\{a\in G:\ (a x)^2=(x a)^2,\ \forall x\in G\}$. Prove that $C'$ is a subgroup of $G$.} Obviously, $C'\subseteq G$. If $a\in C'$ then $(a x)^2=(x a)^2$. Multiplying this by $x$ on the left gives us $x (a x)^2=x (x a)^2$. Rearranging elements, gives us $x a x a x=x (x a)^2$, and that is $(x a)^2 x=x (x a)^2$, for every $x\in G$, which means that $C'\subseteq C$. If we take $a,b\in C'$, then, $a, b$ are also in $C$ and $a b\in C$. So it must be that $(a b)x=x(a b)$, for all $x\in G$. Multiplying that by $(a b)x$ gives us $(a b x)^2=(x a b)(a b x)$. But, $(a b x)=(x a b)$, so we have $(a b x)^2=(x a b)^2$, for all $x\in G$, and therefore such product must be in $C'$. So $C'$ is closed with respect to multiplication. Now, for the inverses, it must be that if $a\in C'$, then $a\in C$. And, as $C$ is a subgroup of $G$, it must be that $a^{-1}\in C$, and $a^{-1} x=x a^{-1}$. Multiplying that on the right by $a^{-1} x$ gives us $(a^{-1} x)^2=(x a^{-1})(a^{-1} x)$. But, again, $(a^{-1}x)=(x a^{-1})$, which yields $(a^{-1} x)^2=(x a^{-1})^2$, for all $x\in G$, and such expression tells us that $a^{-1}$ must be in $C'$. So, $C'$ is a subgroup of $C$ and $G$.

\item {\it Let $G$ be a finite group, and let $S$ be a nonempty subset of $G$. Suppose $S$ is closed with respect to multiplication. Prove that $S$ is a subgroup of $G$.} Suppose $S=\{a_1,\ldots,a_n\}$ and $\left|S\right|=n$. Also, $S\subseteq G$, so for every $a,b,c\in S$ they are in $G$. We have the same operation on $S$ as on $G$. And, as $G$ is a group, for every $a,b,c\in S\subseteq G$ it holds associativity: $a (b c)=(a b) c$. Now, we will take some $a_i\in S$. As $S$ is closed under multiplication, it must be that every product $a_i a_1, a_i a_2,\ldots, a_i a_n$ is defined and is in $S$. There are $n$ such products, and every one of them would need to be some $a_j\in S$. Now, we cannot have, by a previos problem, that two such products give the same element, i.e. take $a_i a_1=a_j$ and $a_i a_2=a_j$, then $a_i a1=a_i a_2$ and we would get $a_1=a_2$ which is a contradiction to our premise that all $a_i\in S$ are distinct (and that order of $S$ is $n$). Now, if $a_i a_i=a_i$ then, as $a_i a_i$ in $S$ and by that in $G$, and then $a_i a_i=a_i$ can be multiplied by $a_i^{-1}\in G$, giving $a_i=e$. Now, if $a_i a_i=a_k$ for some $k$, then $a_i$ must appear as a result of some other product, say $a_j$, and then $a_i a_j=a_i$ for some $j$, and, as $a_i a_j\in S\subseteq G$, then it must be that $a_i a_j=a_i\in G$. Then $a_i^{-1}\in G$ and it must be that $a_j=e$. Now, that only shows that $a_j$ is a right neutral element in $S$. But, if it was that $a_j a_i$ equals some $a_k\in S$, taking $a_j^{-1}$ in $G$, it would be, as $a_j=e$ in $G$, that $a_j^{-1}=e^{-1}=e$. That would mean that $a_i=a_j^{-1} a_k$ is actually $a_i=a_k$, which is a contradiction. Therefore, there is a neutral element in $S$. As
there is a neutral element say $a_j=e$. Then, for every product $a_i a_k\in S$, for a fixed $a_i$, we must have use all the elements and use them only once as a result of a product. And it will happen that $a_i a_k=e$ for some $a_k$, making $a_k$ the left inverse of $a_i$. But, as $a_k\in G$, it is also $a_i a_k=e$ in $G$. If it were that $a_k a_j=e$, then it would mean that $a_i a_k=a_k a_j$. Multiplying this by $a_j$ would give $a_i a_k a_j=a_k a_j a_j$, that is, $a_i=a_k a_j a_j$. But, as $a_k a_j=e$, then $a_i=a_j$, so, $a_k$ must be also both the left and right inverse of $a_i$, and therefore the inverse of $a_i$. Same reasoning goes for every $a_i$. Thus, $S$ is a subgroup of $G$.

{\it Comment.} Notice that this does not necessarily hold if $G$ is infinite, as there might be no breach of double use in multiplication table (if it can even exist for am infinite group). For example take $\left(\Q\backslash\{0\},\cdot\right)$ which is a (commutative) group. Now, $\N\subset\Q$ and $(\N,\cdot)$ is closed with respect to multiplication, but there are no multiplicative inverses for any $x\in\N$ except when $x=1$.

\item {\it Let $G$ be a group and $f:G\rightarrow G$ a function. A period of $f$ is any element $a$ in $G$ such that $f(x)=f(a x)$ for every $x\in G$. Prove that the set of all the periods of $f$ is a subgroup of $G$.} Let $P=\{a\in G:\ f(x)=f(a x),\ \forall x\in G\}$. Obviously $P\subseteq G$. Now, if $a,b\in P$, then $f(x)=f(a x)$ and $f(x)=f(b x)$ for all $x\in G$. Now, obviously $f(x)=f(a x)$. If we take $p=(a x)$, then $f(p)=f(b p)$, for all $x\in G$, that is $f(a x)=f(b a x)$, which is in turn equal to $f(x)$, as $f(a x)=f(x)$. That is, $f(x)=f(b a x)$, for all $x\in G$ which means that $b a\in G$. Now if we take $a\in P$, we'll check if $a^{-1}\in G$ satisfies conditions of set $P$. Take $f(a^{-1} x)$. If we substitute $p=(a^{-1} x)$, as $a^{-1} x\in G$ we have $f(p)=f(a p)$, and that is $f(a^{-1} x)=f(a a^{-1} x)=f(x)$, for all $x\in G$. Therefore, $a^{-1}\in P$ and $P$ is a subgroup of $G$.

\item {\it Let $H$ be a subgroup of $G$, and let $K=\{x\in G:\ x a x^{-1}\in H\ \textnormal{iff}\ a\in H\}$. Prove:}

\begin{enumerate}
\item {\it $K$ is a subgroup of $G$.} Obviously $K\subseteq G$. Now, we take $x,y\in K$ and check whether their product is in $K$. Also, we will need some $a\in H$. Because $x\in K$, it must be true that $x a x^{-1}\in H$. Now, we see that $x a x^{-1}\in H$ and if we take some $y\in K$, then $y x a x^{-1} y^{-1}\in H$, i.e. $y x a (y x)^{-1}\in H$. Also, $x,y\in G$ (by definition of $K$) and so it's $y x\in G$, too. And all that means that $y x\in K$. That is, the product of $x,y\in K$ is again in $K$. Now, for some $a\in H$ and $x\in K$ it's $x a x^{-1}\in H$. As $H$ is a group it means that $x^{-1} a^{-1} x\in H$. That can be written as $x^{-1} a^{-1} (x^{-1})^{-1}$. Note that $x^{-1}\in G$ as $H$ is a subgroup of $G$. And, as $a^{-1}\in H$ (it's a subgroup, so it's closed with respect to inverses), it means that $x^{-1}\in K$. Therefore $K$ is a subgroup of $G$.

\item {\it $H$ is a subgroup of $K$.} We need to show that if $a\in H$ then $a\in K$. Now, for some $x\in K$ it's $x a x^{-1}\in H$. As $H$ is a group, and it's closed under multiplication and inverses, it's also $a x a x^{-1} a^{-1}\in H$. And, as $x a x^{-1}\in H$, and $a$ and $a^{-1}$ are also in $G$ (as $H$ is a subgroup), it means that $a\in K$. Therefore $H\subseteq K$. Now, the other two properties have already been shown in a previous problem, they are independent of the now-known fact that $H$ is a subset of $K$. Therefore $H$ is a subgroup of $K$.
\end{enumerate}

\item {\it Let $G$ and $H$ be groups, and $G\times H$ their direct product.}
\begin{enumerate}
\item {\it Prove that $\{(x,e):\ x\in G\}$ is a subgroup of $G\times H$.} Let $S=\{(x,e):\ x\in G\}$ and, as $x\in G$ and $e\in H$, then $S\subseteq G\times H$. Now, take $(x,e),(y,e)\in S$, where $x,y\in G$. Their product is $(x y, e)$. As $x, y\in G$, then also $x y\in G$. In conclusion, $(x y, e)\in S$ and $S$ is closed under multiplication. If we take $(x,e)\in S$, for $x\in G$, then, $(x^{-1},e)\in S$ because $x^{-1}\in G$. We didn't check $e\in H$ specially as, $e^{-1}=e$ and $e e=e$. Therefore, $S$ is closed with respect to inverses. $S$ is a subgroup of $G\times H$.

\item {\it Prove that $\{(x,x):\ x\in G\}$ is a subgroup of $G\times G$.} Again, let $S=\{(x,x):\ x\in G\}$. Obviously $S\subseteq G\times G$. If we take $(x,x),(y,y)\in S$, then $(x y,x y)\in S$, as, because $x,y\in G$, also $x y\in G$. Therefore, $S$ is closed under multiplication. Now, take $(x,x)\in S$. Obviously $(x^{-1},x^{-1})\in S$, as $x^{-1}\in G$ for every $x\in G$. $S$ is a subgroup of $G\times G$.
\end{enumerate}
\end{enumerate}

\noindent\newline{\bf Problem.} Let $G$ be a group, $A$ it's subgroup and $B\subset G$ such that $A\cap B=\emptyset$. Prove that $B$ cannot be a subgroup of $G$ (in any case).

\noindent\newline{\bf Solution.} As $A$ is a subgroup of $G$, it is closed with respect to inverses and multiplication. Therefore for every $a\in A$ it follows that $a^{-1}\in A$, and so $a a^{-1}\in A$, which is $a a^{-1}=e\in A\subset G$. As $A\cap B=\emptyset$ it implies that, as $e\in A$, it cannot be that $e\in B$. Set $B$ is missing, therefore, a neutral element, and cannot be closed with respect to multiplication, even if every element in it contained it's inverse. Thus, $B$ cannot be a subgroup.

\noindent\newline{\bf Remark.} A result of a previous problem tells us that, considering family of sets $\{S_i\}_{i\in\lambda}$, where $\lambda\neq\emptyset$, and $S_i\subseteq G$, where $G$ is a group, a necessary condition for all $S_i$ to be subgroups of $G$ is that

\begin{equation*}
\bigcap_{i\in\lambda}{S_i}\neq\emptyset.
\end{equation*}

\noindent\newline{\bf Problem.} Prove that $SO(n)\in\R^{n\times n}$, where $SO(n)$ (special orthogonal group) is the set of all orthogonal matrices with additional property that $\det{A}=1$, is a subgroup of $O(n)$.

\noindent\newline{\bf Solution.} Obviously $SO(n)\subset O(n)$, by definition. Using the fact that $\det{A B}=\det{A}\det{B}$, we can see that for all $A,B\in SO(n)$ it's $\det{A}=1$ and $\det{B}=1$, but also $\det{A B}=\det{A}\det{B}=1\cdot 1=1$. Therefore $A B\in SO(n)$ so it's closed with respect to matrix multiplication. Now, for $A\in SO(n)$ we need to check that $A^T\in SO(n)$. We know that $\det{A}=1$ and $\det{I}=1$, so taking $\det{A A^T}=\det{I}$, i.e. $\det{A}\det{A^T}=\det{I}$, it must be that $\det{A^T}=1$. Therefore $A^T\in SO(n)$, which makes special orthogonal group closed with respect to inverses and by that a subgroup of $O(n)$.

\newpage

\begin{center}
{\bf Generators of Groups}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group. Now, for some finite $X\subseteq G$, $X\neq\emptyset$, we define a set $S\subseteq G$ which contains all the possible products of $x\in X$ and $x^{-1}\in G$. We say that $S$ is a {\bf subgroup generated by} elements $x\in X\subseteq G$.

\noindent\newline{\bf Remark.} We make a mention-use distinction for the definition above in the analytic way of thought. We said that $S$ is a {\it subgroup} of $G$ in the {\it mention-sense} ({\it subgroup} is just part of a name for $S$ now), but in the following theorem we are going to prove it to be an actual subgroup in the {\it use-sense} ($S$ is actually a subgroup now).

\noindent\newline{\bf Theorem.} Set $S$ defined as in previous definition, with operation inherited from group $G$, is a subgroup of $G$.

\noindent\newline{\bf Proof.} By definition, for finite $X\subseteq G$, $S$ contains all the possible products of elements in $X$. Now, is $S\subseteq G$? Whatever elements $a_1,\ldots,a_n,a_1^{-1},\ldots,a_n^{-1}\in X$ we take, their arbitrary product is in $S$. But, as $X\subseteq G$, then all these elements are in $G$, and, as $G$ is a group, their arbitrary product is also in $G$. Then, $S$ is by definition closed under multiplication and inverses. Whatever product we take, we can use the fact that

\begin{equation*}
(a_1 a_2 \cdots a_n)^{-1}=a_n^{-1}\cdots a_2^{-1} a_1^{-1}.
\end{equation*}

\noindent\newline So, as $S\subseteq G$, and $S$ being closed under multiplication and with respect to inverses, $S$ is a subgroup of $G$, and by itself, a group.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} We can also do the reverse thing, and for some finite $X\subseteq G$ define subgroup $S$ generated by elements from $X\subseteq G$.

\noindent\newline{\bf Definition.} If $S$ is a subgroup generated by a single element $a\in G$, we say that $S$ is a {\bf cyclic subgroup} of $G$ and we say that $a$ is its {\bf generator}. We denote $S$ by $\left\langle a\right\rangle$.

\noindent\newline{\bf Definition.} A set of equations, involving only generators and their inverses, are called {\bf defining equations} for $G$ if if these equations completely determine the multiplication table of $G$.

\noindent\newline{\bf Problem.} List all the cyclic subgroups of $\Z_{10}$.

\noindent\newline{\bf Solution.} Cyclic subgroup must be a subgroup generated by a single $a\in\Z_{10}$. We have $10$ cases (for $0,1,\ldots,9$), which we will list. We need not check whether they are really subgroups, the previos theorem guarantees us that they will be subgroups; every subgroup generated by some arbitrary elements in it's overlying group is a subgroup. So, a cyclic subgroup is also a subgroup\footnote{Note the {\it use-mention} distinction of the word subgroup again.}. The list is (the sets are "sorted" in the way the operation is applied):

\begin{itemize}
\item $\left\langle0\right\rangle=\{0\}$;
\item $\left\langle1\right\rangle=\{1,2,3,4,5,6,7,8,9,0\}$;
\item $\left\langle2\right\rangle=\{2,4,6,8,0\}$;
\item $\left\langle3\right\rangle=\{3,6,9,2,5,8,1,4,7,0\}$;
\item $\left\langle4\right\rangle=\{4,8,2,6,0\}$;
\item $\left\langle5\right\rangle=\{5,0\}$;
\item $\left\langle6\right\rangle=\{6,2,8,4,0\}$;
\item $\left\langle7\right\rangle=\{7,4,1,8,5,2,9,6,3,0\}$;
\item $\left\langle8\right\rangle=\{8,6,4,2,0\}$;
\item $\left\langle9\right\rangle=\{9,8,7,6,5,4,3,2,1,0\}$.
\end{itemize}

\noindent{\bf Problem.} Show that $\Z_{10}$ is generated by $2$ and $5$.

\noindent\newline{\bf Solution.} We need to show that every element in $\Z_{10}$ can be obtained by adding $2$'s and $5$'s. Now, obviously $5+5=0$. Then, $2+2+2+5=1$. Now, let's denote $a=2+2+2+5$. Then, every other element can be obtained by adding $a$ with itself. $2=a+a$, $3=a+a+a$ and so on. Therefore, $\Z_{10}$ is generated by $2$ and $5$.

\noindent\newline{\bf Problem.} Describe the subgroup of $\Z_{12}$ generated by $6$ and $9$.

\noindent\newline{\bf Solution.} First, we have two sets, $\left\langle6\right\rangle=\{6,0\}$ and $\left\langle9\right\rangle=\{9,6,3,0\}$. Combining these elements together, we see that the sum of any two numbers from both sets is already in both sets. Therefore, we have the subgroup with underlying set $\{0,3,6,9\}$.

\noindent\newline{\bf Problem.} Describe the subgroup of $\Z$ generated by $10$ and $15$.

\noindent\newline{\bf Solution.} All the elements of that subgroup will be of the form $10k+15l$, where $k,l\in\Z$. That would mean that $5(2k+3l)$ is the form of all elements; they will all be multiples of $5$. Also note that $2k+3l$ ranges through all elements of $\Z$, as, we could take $a=-2+3=1$ and then generate them all as $a+a=2$, $a+a+a=3$, and so on. Same holds for negative numbers, we can take $-a$ and get them all. So, multiplying all elements of $\Z$ by $5$ gives us all elements that are divisible by $5$. That is the set $S=\{0,5,-5,10,-10,\ldots\}$, i.e. $S=\{n\in\Z:\ n=5k,\ \forall k\in\Z\}$.

\noindent\newline{\bf Problem.} Show that $\Z$ is generated by $5$ and $7$.

\noindent\newline{\bf Solution.} All the combinations of $5$'s and $7$'s will be of the form $5k+7l$, for all $k,l\in\Z$. Now, taking $a=3\cdot 7+4\cdot(-5)=1$ (best to view multiplication here not as multiplication qua multiplication, but as shortened addition), we can see, again that we can get all the elements by adding $a$ and $-a$ with themselves. Note that $0=a-a$, or more simply $5-5$, $7-7$, etc.

\noindent\newline{\bf Problem.} Show that $\Z_2\times\Z_3$ is a cyclic group.

\noindent\newline{\bf Solution.} All it's elements can be generated by $(1,2)\in\Z_2\times\Z_3$. That is, $\left\langle(1,2)\right\rangle=\{(1,2),(0,1),(1,0),(0,2),(1,1),(0,0)\}$. We can see that this set is actually equal to the observed set. This group is the subset of $\Z\times\Z$, and as it is cyclic, it's a subgroup of the former and, therefore, a group.

\noindent\newline{\bf Problem.} Show that $\Z_3\times\Z_4$ is a cyclic group.

\noindent\newline{\bf Solution.} This time, we can take $(1,1)$ to be generator. We could have also taken $(1,2)$ and $(2,3)$. Now, 

\begin{equation*}
\left\langle(1,1)\right\rangle=\{(1,1),(2,2),(0,3),(1,0),(2,1),(0,2),(1,3),(2,0),(0,1),(1,2),(2,3),(0,0)\}.
\end{equation*}

\noindent\newline Notice that we ran through all $3\cdot 4=12$ elements. As $\Z_3\times\Z_4\subset\Z\times\Z$, and as $\Z_3\times\Z_4$ is cyclic, it's a subgroup of $\Z^2$, and then a group by itself.

\noindent\newline{\bf Problem.} Show that $\Z_2\times\Z_4$ is not a cyclic group, but is generated by $(1,1)$ and $(1,2)$.

\noindent\newline{\bf Solution.} We see that $\Z_2\times\Z_4=\{(0,0),(1,0),(0,1),(1,1),(0,2),(1,2),(0,3),(1,3)\}$. Now, it cannot be generated by a single element, as, for potential generators of the form $(0,y)$ we never get a different ordinate (always $0+0=0$). Now, following the same logic, we cannot also take elements of the form $(x,0)$, we never get a different abscise. The only ones left are $(1,1)$, $(1,2)$ and $(1,3)$. But, it cannot be generated by $(1,1)$ only, as we would be getting $(0,2)$, $(1,3)$, $(0,0)$ and $(1,1)$ over and over. Same thing goes for $(1,2)$, as we would have $(0,0)$ and $(1,2)$; for $(1,3)$ we have $(0,2)$, $(1,1)$, $(0,0)$ and then $(1,3)$ again. Therefore $\Z_3\times\Z_4$ is not a cyclic group (it has to be generated by a {\it single} element in order to be cyclic). But, we can see that combining $(1,1)$ and $(1,2)$, so that $a=(1,1)+(1,1)+(1,2)=(1,0)$ and $b=(1,2)+(1,1)+(1,1)+(1,1)=(0,1)$, would give us all the elements of $\Z_2\times\Z_4$ (as we run through all ordinates by applying $a$'s and through all absices by applying $b$'s).

\noindent\newline{\bf Problem.} Suppose a group $G$ is generated by two elements $a$ and $b$. If $a b=b a$, prove that $G$ is Abelian.

\noindent\newline{\bf Solution.} Now, let's take some $c,d\in G$ such that $c$ and $d$ are combinations of products and inverses of $a$ and $b$. If $c$ were of the form $c=a^n b^m a^p b^q$, we could rearrange elements using $a b=b a$ to get $c$ of the form $c=a^u b^v$. The same can be done with $d=a^t b^s$. Now, the product can also be rearranged so that $c d=a^u b^v a^t b^s=a^t b^s a^u b^v=d c$. Therefore, $G$ is Abelian.

\noindent\newline{\bf Problem.} Let $G$ be the group $\{e,a,b,b^2,a b,a b^2\}$ whose generators satisfy $a^2=e$, $b^3=e$, $b a=a b^2$. Write the table of $G$.

\noindent\newline{\bf Solution.} We will only present the finished table (be reminded that we can use the helpful fact that every element must appear only once in a row):

\begin{center}
\begin{tabular}{c|cccccc}
$\cdot$ & $e$ & $a$ & $b$ & $b^2$ & $a b$ & $a b^2$ \\
\hline
$e$ & $e$ & $a$ & $b$ & $b^2$ & $a b$ & $a b^2$ \\
$a$ & $a$ & $e$ & $a b$ & $a b^2$ & $b$ & $b^2$ \\
$b$ & $b$ & $a b^2$ & $b^2$ & $e$ & $a$ & $a b$ \\
$b^2$ & $b^2$ & $a b$ & $e$ & $b$ & $a b^2$ & $a$ \\
$a b$ & $a b$ & $b^2$ & $b a$ & $a$ & $e$ & $b$ \\
$a b^2$ & $a b^2$ & $b$ & $a$ & $a b$ & $b^2$ & $e$ \\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} Let $G$ be the group $\{e,a,b,b^2,b^3,a b,a b^2,a b^3\}$ whose generators satisfy $a^2=e$, $b^4=e$, $b a=a b^3$. Write the table of $G$\footnote{This group is called {\it dihedral group $D_4$}.}.

\noindent\newline{\bf Solution.} The finished table is:

\begin{center}
\begin{tabular}{c|cccccccc}
$\cdot$ & $e$ & $a$ & $b$ & $b^2$ & $b^3$ & $a b$ & $a b^2$ & $a b^3$ \\
\hline
$e$ & $e$ & $a$ & $b$ & $b^2$ & $b^3$ & $a b$ & $a b^2$ & $a b^3$\\
$a$ & $a$ & $e$ & $a b$ & $a b^2$ & $a b^3$ & $b$ & $b^2$ & $b^3$\\
$b$ & $b$ & $a b^3$ & $b^2$ & $b^3$ & $e$ & $a$ & $a b$ & $a b^2$\\
$b^2$ & $b^2$ & $a b^2$ & $b^3$ & $e$ & $b$ & $a b^3$ & $a$ & $a b$\\
$b^3$ & $b^3$ & $a b$ & $e$ & $b$ & $b^2$ & $a b^2$ & $a b^3$ & $a$\\
$a b$ & $a b$ & $b^3$ & $a b^2$ & $a b^3$ & $a$ & $e$ & $b$ & $b^2$\\
$a b^2$ & $a b^2$ & $b^2$ & $a b^3$ & $a$ & $a b$ & $b^3$ & $e$ & $b$\\
$a b^3$ & $a b^3$ & $b$ & $a$ & $a b$ & $a b^2$ & $b^2$ & $b^3$ & $e$\\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} Let $G$ be the group $\{e,a,b,b^2,b^3,a b,a b^2,a b^3\}$ whose generators satisfy $a^4=e$, $a^2=b^2$, $b a=a b^3$. Write the table of group\footnote{This group is called the {\it quaternion group}.} $G$.

\noindent\newline{\bf Solution.} The table is:

\begin{center}
\begin{tabular}{c|cccccccc}
$\cdot$ & $e$ & $a$ & $b$ & $b^2$ & $b^3$ & $a b$ & $a b^2$ & $a b^3$ \\
\hline
$e$ & $e$ & $a$ & $b$ & $b^2$ & $b^3$ & $a b$ & $a b^2$ & $a b^3$\\
$a$ & $a$ & $b^2$ & $a b$ & $a b^2$ & $a b^3$ & $b^3$ & $e$ & $b$\\
$b$ & $b$ & $a b^3$ & $b^2$ & $b^3$ & $e$ & $a$ & $a b$ & $a b^2$\\
$b^2$ & $b^2$ & $a b^2$ & $b^3$ & $e$ & $b$ & $a b^3$ & $a$ & $a b$\\
$b^3$ & $b^3$ & $a b$ & $e$ & $b$ & $b^2$ & $a b^2$ & $a b^3$ & $a$\\
$a b$ & $a b$ & $b$ & $a b^2$ & $a b^3$ & $a$ & $b^2$ & $b^3$ & $e$\\
$a b^2$ & $a b^2$ & $e$ & $a b^3$ & $a$ & $a b$ & $b$ & $b^2$ & $b^3$\\
$a b^3$ & $a b^3$ & $b^3$ & $a$ & $a b$ & $a b^2$ & $e$ & $b$ & $b^2$\\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} Let $G$ be the {\it commutative} group $\{e,a,b,c,a b,b c,a c,a b c\}$ whose generators satisfy $a^2=b^2=c^2=e$. Write the table of $G$.

\noindent\newline{\bf Solution.} The table is as follows:

\begin{center}
\begin{tabular}{c|cccccccc}
$\cdot$ & $e$ & $a$ & $b$ & $c$ & $a b$ & $b c$ & $a c$ & $a b c$ \\
\hline
$e$ & $e$ & $a$ & $b$ & $c$ & $a b$ & $b c$ & $a c$ & $a b c$\\
$a$ & $a$ & $e$ & $a b$ & $a c$ & $b$ & $a b c$ & $c$ & $b c$\\
$b$ & $b$ & $a b$ & $e$ & $b c$ & $a$ & $c$ & $a b c$ & $a c$\\
$c$ & $c$ & $a c$ & $b c$ & $e$ & $a b c$ & $b$ & $a$ & $a b$\\
$a b$ & $a b$ & $b$ & $a$ & $a b c$ & $e$ & $a c$ & $b c$ & $c$\\
$b c$ & $b c$ & $a b c$ & $c$ & $b$ & $a c$ & $e$ & $a b$ & $a$\\
$a c$ & $a c$ & $c$ & $a b c$ & $a$ & $b c$ & $a b$ & $e$ & $b$\\
$a b c$ & $a b c$ & $b c$ & $a c$ & $a b$ & $c$ & $a$ & $b$ & $e$\\
\end{tabular}
\end{center}

\newpage

\begin{center}
{\bf Functions}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ and $B$ be sets. A {\bf function}\footnote{In FOL, these two rules can be written as: \begin{enumerate}
\item $(\forall x\in A)(\exists y\in B)(y=f(x))$;
\item $(\forall x_1,x_2\in A)(x_1=x_2\rightarrow f(x_1)=f(x_2))$.
\end{enumerate}} from set $A$ to set $B$ is an ordered triple $(f,A,B)$, where $f$ is a rule which {\it for each} element of $A$ assigns {\it a unique} (one and only one) element in $B$. We denote the function by $f:A\rightarrow B$. We call set $A$ {\bf domain} and $B$ {\bf codomain}.

\noindent\newline{\bf Definition.} We say that a function $f:A\rightarrow B$ is {\bf injective}\footnote{That is, for each element in $B$ there is exactly one element in $A$:

\begin{equation*}
(\forall x_1,x_2\in A)(f(x_1)=f(x_2)\rightarrow x_1=x_2).
\end{equation*}} if each element of $B$ is the image of exactly one element in $A$.

\noindent\newline{\bf Definition.} Let $f:A\rightarrow B$ be a function. The set

\begin{equation*}
\ran{f}=\{y\in B:\ \left(\exists x\in A\right)\left(y=f(x)\right)\}
\end{equation*}

\noindent\newline is called the {\bf image} (or range) of the function $f:A\rightarrow B$. We say that the function $f:A\rightarrow B$ is {\bf surjective}\footnote{I.e. the codomain of function $f:A\rightarrow B$ is equal to it's image; for every element $y\in B$ there is an element in $A$ such that $f(x)=y$; in FOL:

\begin{equation*}
(\forall y\in B)(\exists x\in A)(f(x)=y).
\end{equation*}} if $\ran{f}=B$.

\noindent\newline{\bf Lemma.} Let $f:A\rightarrow B$ be a function. Then, $\ran{f}\subseteq B$.

\noindent\newline{\bf Proof.} By definition, $\ran{f}=\{y\in B:\ \left(\exists x\in A\right)\left(y=f(x)\right)\}$. So, if we take $y\in\ran{f}$, then $y\in B$ and from that $\ran{f}\subseteq B$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $f:A\rightarrow B$ be a function. Then,

\begin{equation*}
\{f(x):\ x\in A\}=\{y\in B:\ \left(\exists x\in A\right)\left(y=f(x)\right)\}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $R=\{f(x):\ x\in A\}$ and $R'=\{y\in B:\ \left(\exists x\in A\right)\left(y=f(x)\right)\}$. If we take $y\in R$, then $y=f(x)$, for some $x\in A$, i.e. there exists $x\in A$ such that $y=f(x)$. That implies $y\in R'$ and $R\subseteq R'$. Take $y\in R'$. Then there exists $x\in A$ such that $y=f(x)$. As $y$ is of the form $y=f(x)$, for some $x\in A$, then $y\in R$ and $R'\subseteq R$. That implies $R=R'$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $f:A\rightarrow B$ be a function. Then, $\ran{f}=B$ if and only if for all $y\in B$ there exists $x\in A$ such that $y=f(x)$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $\ran{f}=B$. As $\ran{f}=\{f(x):\ x\in A\}$, then if $y\in B=\ran{f}$, there must exist $x\in A$ such that $f(x)=y$. {\it Sufficiency.} Let for all $y\in B$ exist $x\in A$ such that $y=f(x)$. By a previous proposition we have $\ran{f}\subseteq B$. If we take $y\in B$, then there exists $x\in A$ such that $f(x)=y$, so $y\in\ran{f}$ and $B\subseteq\ran{f}$. That implies $\ran{f}=B$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} We say that a function is {\bf bijective} if it's both surjective and injective.

\noindent\newline{\bf Remark.} From now on, we will write only $f$ when we're refering to a function $f:A\rightarrow B$ (but make sure to get the distinction between the rule $f$ and the function $(f,A,B)$ which is an ordered triple containing the rule with both domain and codomain).

\noindent\newline{\bf Definition.} Let $f:A\rightarrow B$ and $g:B\rightarrow C$ be functions. The {\bf composite function} denoted by $g\circ f$ is a function from $A$ to $C$ defined as follows:

\begin{equation*}
[f\circ g](x)=f(g(x)),\ \forall x\in A.
\end{equation*}

\noindent\newline{\bf Theorem.} Let $f:A\rightarrow B$, $g:B\rightarrow C$ and $h:C\rightarrow D$ be functions. Then:

\begin{equation*}
[h\circ[g\circ f]](x)=[[h\circ g]\circ f](x),\ \forall x\in A,
\end{equation*}

\noindent\newline that is, function composition is associative.

\noindent\newline{\bf Proof.} The proof follows directly from definition. We will show that both expressions equal $h(g(f(x)))$. We have:

\begin{equation*}
[h\circ[g\circ f]](x)=h([g\circ f](x))=h(g(f(x))).
\end{equation*}

\noindent\newline On the other side, we have:

\begin{equation*}
[[h\circ g]\circ f](x)=[h\circ g](f(x))=h(g(f(x))).
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $f:A\rightarrow B$ and $g:B\rightarrow C$ be functions. Then the following statements are true:

\begin{enumerate}
\item If $f$ and $g$ are injective, then $g\circ f$ is injective.
\item If $f$ and $g$ are surjective, then $g\circ f$ is surjective.
\item If $f$ and $g$ are bijective, then $g\circ f$ is bijective.
\end{enumerate}

\noindent{\bf Proof.}

\begin{enumerate}
\item {\it If $f$ and $g$ are injective, then $g\circ f$ is injective.} Taking some elements $x_1,x_2\in A$, such that, as $g\circ f$ is a function, holds $g(f(x_1))=g(f(x_2))$, then from injectivity of $g$ we have $f(x_1)=f(x_2)$. Then, from injectivity of $f$ we have $x_1=x_2$. Summing up, for arbitrary $x_1,x_2\in A$ we had that from $g(f(x_1))=g(f(x_2))$ follows $x_1=x_2$. Thus, $g\circ f$ is injective.

\item {\it If $f$ and $g$ are surjective, then $g\circ f$ is surjective.} As $f$ is surjective, for all $x\in A$ we have $z\in B$ such that $f(x)=z$. And, as $g$ is surjective, then for all $f(x)=z\in B$ we have $y\in C$ such that $g(z)=y$. But, $z=f(x)$, so $g(z)=g(f(x))$ and therefore we have $y=g(f(x))$. Summing up, for all $x\in A$ we have $y\in C$ such that $g(f(x))=y$. Concluding, $g\circ f$ is surjective.

\item {\it If $f$ and $g$ are bijective, then $g\circ f$ is bijective.} Trivially, if $f$ and $g$ are injective, then so is $g\circ f$; if $f$ and $g$ are surjective, then so is $g\circ f$. As $g\circ f$ is, then, surjective and injective, it follows that $g\circ f$ is bijective.

\begin{flushright}
$\square$\\
\end{flushright}

\end{enumerate}

\noindent{\bf Theorem.} Let $f:A\rightarrow B$ and $g:B\rightarrow C$ be functions. Then:

\begin{enumerate}
\item If $g\circ f$ is injective, then $g$ is injective.
\item If $g\circ f$ is injective, then $f$ is not necessarily injective.
\item If $g\circ f$ is surjective, then $f$ is surjective.
\item If $g\circ f$ is surjective, then $g$ is not neccesarily surjective.
\end{enumerate}

\noindent{\bf Proof.}

\begin{enumerate}
\item {\it If $g\circ f$ is injective, then $g$ is injective.} For all $x_1,x_2\in A$, as $g\circ f$ injective, $g(f(x_1))=g(f(x_2))$ implies $x_1=x_2$. Now, suppose that $g$ is not injective and that would mean that for some $y_1,y_2\in B$ (where $y_1=f(x_1)$ and $y_2=f(x_2)$, because $f$ is a function) it's true that $g(y_1)=g(y_2)$ and $y_1\neq y_2$. But, as $y_1=f(x_1)$ and $y_2=f(x_2)$, that would mean that $g(f(x_1))=g(f(x_2))$ and $f(x_1)\neq f(x_2)$. As $f$ is a function, then from $f(x_1)\neq f(x_2)$ follows that $x_1\neq x_2$. That is a contradiction with our assumption that $g\circ f$ is injective.

\item {\it If $g\circ f$ is injective, then $f$ is not necessarily injective.} Proof by counterexample. Take $g:\R^{+}\rightarrow\R$, and $g(x)=\ln{x}$. Now, take $f:\R\{0\}\rightarrow\R^{+}$ as $f(x)=x^2$. Obviously $f$ is not injective, while $g$ is. If we take $g(f(x))=\ln{(x^2)}=2\ln{x}$, it's injective, as, taking $g(f(x_1))=g(f(x_2))$ implies $2\ln{x_1}=2\ln{x_2}$. Now, dividing by two gets us $\ln{x_1}=\ln{x_2}$. Taking\footnote{We will now make the assumption that the reader is familiar with the concept of inverse function at least at an intuitive level.} the $g^{-1}:\R\rightarrow\R^{+}$, where $g^{-1}(x)=e^x$ gives us $\exp{\ln{x_1}}=\exp{\ln{x_2}}$, i.e. $x_1=x_2$. Therefore, $g\circ f$ is injective, while $f$ is not.

\item {\it If $g\circ f$ is surjective, then $g$ is surjective.} As $g\circ f$ is surjective, then $(\forall x\in A)(\exists z\in C)(z=g(f(x)))$. Also, as $f$ is function, then $(\forall x\in A)(\exists y\in B)(y=f(x))$. If $g$ were not surjective then it would mean that for some $y\in B$ there does not exist $z\in C$ such that $g(y)=z$. But, as $y=f(x)$ for all $x\in A$, then it would mean that for some $f(x)=y\in B$ does not exist $z\in C$ such that $g(f(x))=z$. That is a contradiction to our assumption.

\item {\it If $g\circ f$ is surjective, then $f$ is not neccesarily surjective.} Proof by counterexample. Take $A=\{a_1\}$, $B=\{b_1, b_2\}$ and $C=\{c_1\}$. Now, define $f(a_1)=b_1$, $g(b_1)=g(b_2)=c_1$. Then, obviously $g(f(a_1))=g(b_1)=c_1$ (that is for all elements in $A$) completely defines composition $g\circ f$. Now, $g\circ f$ is surjective, but $f$ is not.

\begin{flushright}
$\square$\\
\end{flushright}

\end{enumerate}

\noindent{\bf Remark.} In order to complete, rigorously, the proof of the following theorem, we will need to define functions in the set-theoretic sense, and for that we will need some short notes on relations.

\noindent\newline{\bf Definition.} Let $A$ and $B$ be sets. Then any $\mathcal{R}\subseteq A\times B$ is called a {\bf relation} of $A$ to $B$. The fact that $x\in A$ and $y\in B$ are in relation $\mathcal{R}$, we denote by $x\mathcal{R}y$. We call $A$ domain of $\mathcal{R}$ and $B$ codomain of $\mathcal{R}$.

\noindent\newline{\bf Definition.} We say that a relation $\mathcal{R}\subseteq A\times B$ is a {\bf partial function} if it satisfies the following condition:

\begin{equation*}
(\forall x\in A)(\forall y_1, y_2\in B)((x,y_1),(x,y_2)\in\mathcal{R}\rightarrow y_1=y_2).
\end{equation*}

\noindent\newline{\bf Remark.} In this way we can define function $f:A\rightarrow B$ as a partial function which further satisfies (that is, it's {\bf left-total})

\begin{equation*}
(\forall x\in A)(\exists y\in B)((x,y)\in f).
\end{equation*}

\noindent\newline{\bf Definition.} If, for $y\in B$, there exists $x\in A$ such that $f(x)=y$, we write $f^{-1}(y)=x$ and call it {\it preimage} of $y$. The set, denoted ambiguously, $f^{-1}(y)=\{x\in A:\ y=f(x)\}$, for $y\in B$, containing all preimages of $y$ is called the {\it fiber} of $y$.

\noindent\newline{\bf Remark.} Note that, if $x$ is unique, we will write $f^{-1}(y)=x$, as we will mean that exact element; on the other hand, if $S=\{x\in A: y=f(x)\}$ and $|S|>1$, then we will write $f^{-1}(y)=S$, meaning $S$ as the fiber of $y$.

\noindent\newline{\bf Definition.} Function $i_S:S\rightarrow S$ defined with $i_S(x)=x$, for all $x\in S$ is called {\it identity} on $S$.

\noindent\newline{\bf Remark.} Note that $i_S$ is indeed well defined and is a function. It is defined for all $x\in S$ and returns a unique $x\in S$, for each $x\in S$.

\noindent\newline{\bf Definition.} Let $f:A\rightarrow B$ be a function. We say that $f$ has a {\bf left inverse} if there exists $g:B\rightarrow A$ such that $[g\circ f]=i_A$. We say that $f$ has a {\bf right inverse} if there exists $g:B\rightarrow A$ such that $[f\circ g]=i_B$.

\noindent\newline{\bf Definition.} We say that $f:A\rightarrow B$ has an {\bf inverse} if there is a function $f^{-1}:B\rightarrow A$ such that $y=f(x)$ if and only if $f^{-1}(y)=x$, for every $x\in A$ and $y\in B$.

\noindent\newline{\bf Remark.} In the set theoretic sense, this can be written as $f^{-1}=\{(y,x):(x,y)\in f\}$, with of course, the two forementioned properties of functions.

\noindent\newline{\bf Proposition.} Identity is bijective and its own inverse.

\noindent\newline{\bf Proof.} Let $i_S:S\rightarrow S$ with $i_S(x)=x$, for all $x\in S$. {\it Injectivity.} If we take $i_S(x_1)=i_S(x_2)$, by definition we get $x_1=x_2$. {\it Surjectivity.} If we take $y\in S$, then there exists $x\in S$, such that $x=y$. As $i_S(x)=x$, then $i_S(x)=y$. Thus, $i_S$ is a bijection. It is obvious that for all $x,y\in S$, $i_S(x)=y$ if and only if $i_S(y)=x$ (if $i_S(x)=y$, then $x=y$ and $i_S(y)=y=x$; conversely, if $i_S(y)=x$, then $y=x$ and $i_S(x)=x=y$) and $i_S$ is the inverse of $i_S$

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} A function $f:A\rightarrow B$ has an inverse $f^{-1}:B\rightarrow A$ if and only if it has a left inverse $f_L:B\rightarrow A$ and a right inverse $f_R:B\rightarrow A$ such that $f_L=f_R$.

\noindent\newline{\bf Proof.} Let $f:A\rightarrow B$ be a function. {\it Necessity.} Let $f$ have an inverse $f^{-1}:B\rightarrow A$. Then, for all $x\in A$ and $y\in B$, $f(x)=y$ if and only if $x=f^{-1}(y)$, where $f^{-1}:B\rightarrow A$. Then, $[f^{-1}\circ f](x)=f^{-1}(f(x))$. But, $f(x)=y$ and $f^{-1}(y)=x$, so $f^{-1}(f(x))=f^{-1}(y)=x$. As $[f^{-1}\circ f]:A\rightarrow A$, with $[f^{-1}\circ f](x)=x$, we have $[f^{-1}\circ f]=i_A$ and $f$ has a left inverse. Similarly, $[f\circ f^{-1}](y)=f(f^{-1}(y))=f(x)=y$, for all $x\in A$ and $y\in B$, so $[f\circ f^{-1}]=i_B$ and $f$ has a right inverse. {\it Sufficiency.} Let $f$ have a left inverse and a right inverse. Then there exist $f_L:B\rightarrow A$ and $f_R:B\rightarrow A$, such that $[f_L\circ f]=i_A$ and $[f\circ f_R]=i_B$. Let $x\in A$ and $y\in B$. As $f$ has a left inverse we have $f_L(f(x))=x$, for all $x\in A$. But, as $f$ is a function, for all $x\in A$ there exists $y\in B$ such that $f(x)=y$, so we have $f_L(y)=x$. Then, as $f$ has a right inverse, we have $f(f_R(y))=y$, for all $y\in B$. As $f_R$ is a function, for all $y\in B$, there exists $x\in A$ such that $f_R(y)=x$. Then, $f(x)=y$. As $f_L=f_R$, we have that for all $x\in A$ there exists $y\in B$ such that $f(x)=y$ and that for all $y\in B$ there exists $x\in B$ such that $f_L(y)=x$. That is equivalent to $f(x)=y$ if and only if $f_L(y)=x$, for all $x\in A$ and $y\in B$, which means that, by definition $f_L$ is an inverse of $f$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Inverse of injection is a partial function.

\noindent\newline{\bf Proof.} Suppose $f:A\rightarrow B$ is an injection. We define a relation

\begin{equation*}
f^{-1}=\{(y,x)\in B\times A: (x,y)\in f\}.
\end{equation*}

\noindent\newline Now, as $f$ is injective, it follows that

\begin{equation*}
(\forall x_1,x_2\in A)(f(x_1)=f(x_2)\rightarrow x_1=x_2).
\end{equation*}

\noindent\newline Now, we'll write down the fact that $f(x_1)=y$ as $(x_1,y)\in f$. That means that

\begin{equation*}
(\forall y\in B)(\forall x_1,x_2\in A)((x_1,y),(x_2,y)\in f\rightarrow x_1=x_2).
\end{equation*}

\noindent\newline But, if $(x_1,y)\in f$, that means that $(y,x_1)\in f^{-1}$. So, we can say:

\begin{equation*}
(\forall y\in B)(\forall x_1,x_2\in A)((y,x_1),(y,x_2)\in f^{-1}\rightarrow x_1=x_2).
\end{equation*}

\noindent\newline That means that $f^{-1}$ is a partial function.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} A function $f:A\rightarrow B$ has an inverse if and only if it is bijective. In that case, the inverse $f^{-1}$ is a bijective function from $B$ to $A$.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $f:A\rightarrow B$ has an inverse $f^{-1}:B\rightarrow A$. That means that $y=f(x)$ if and only if $f^{-1}(y)=x$, for all $x\in A$ and $y\in B$. If $f$ were not injective that would mean that there exists some $x_1,x_2\in A$ such that $f(x_1)=f(x_2)=y$ and $x_1\neq x_2$. That would mean that $f^{-1}(y)=x_1$ and $f^{-1}(y)=x_2$, where $x_1\neq x_2$ i.e. for some $y\in B$ (domain of $f^{-1}$) we have two different values in $A$ (codomain of $f^{-1}$), which is a contradiction to the assumption that $f^{-1}$ is a function. Therefore, $f$ must be injective. Now, suppose that $f$ is not surjective. That would mean that for some $y\in B$ there does not exist $x\in A$ such that $y=f(x)$. But, we have, for all $x\in A$ and $y\in B$ that $f^{-1}(y)=x$; that would be again contradict the assumption that $f^{-1}$ is a function, as we would have some $x$ in domain of $f^{-1}$ that does not have it's image, $y$, in codomain of $f^{-1}$. Therefore, $f$ must also be surjective. As $f$ is surjective and injective, it is bijective.

{\it Sufficiency.} We define first the relation $f^{-1}=\{(y,x):(x,y)\in f\}$. Now, as $f$ is injective, from the following lemma, it follows that $f^{-1}$ is a partial function. But, as $f$ is also surjective it holds that:

\begin{equation*}
(\forall y\in B)(\exists x\in A)(y=f(x)),
\end{equation*}

\noindent\newline that is, in set-theoretic view:

\begin{equation*}
(\forall y\in B)(\exists x\in A)((x,y)\in f),
\end{equation*}

\noindent\newline Now, as we have that if $(x,y)\in f$ implies $(y,x)\in f^{-1}$, then

\begin{equation*}
(\forall y\in B)(\exists x\in A)((y,x)\in f^{-1}).
\end{equation*}

\noindent\newline By definition that means that $f^{-1}$ is also left-total. That, combined with the fact that it is a partial function, tells us that $f^{-1}$ is a function for which holds $f^{-1}=\{(y,x):\ (x,y)\in f\}$, which can be written as $(y,x)\in f^{-1}$ if and only if $(x,y)\in f$, i.e. $x=f^{-1}(y)$ if and only if $y=f(x)$, for all $x\in A$ and $y\in B$, which means that it's inverse of $f$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $A$ be a finite set and $f:A\rightarrow B$ a function. Then, $\ran{f}$ is finite and $\left|\ran{f}\right|\leq\left|\dom{f}\right|$.

\noindent\newline{\bf Proof.} Let $A=\{a_1,a_2,\ldots,a_m\}$ with $m\in\Z^{+}$. Then, $\dom{f}=A$. By definition $\ran{f}=\{f(x):\ x\in A\}=\{f(a_1),f(a_2),\ldots,f(a_m)\}$, so $\ran{f}$ is finite. As $f$ is a function, each $f(a_i)$ will equal only one $y\in B$, so we can denote $f(a_i)=b_i$, for $i\in\{1,\ldots,m\}$. Yet, we have no guarantee that $b_i\neq b_j$, i.e. $f(a_i)\neq f(a_j)$ for all $i,j\in\{1,\ldots,m\}$, $i\neq j$, and it must be that $|\ran{f}|\leq m=|A|$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $S$ and $T$ be finite sets such that $S\subseteq T$ and $|S|=|T|$. Then, $S=T$.

\noindent\newline{\bf Proof.} If $|S|=|T|=1$, then, $S=\{s\}$ and $T=\{t\}$. But, as $S\subseteq T$, for all $x\in S$, there exists some $y\in T$ such that $x=y$. But that is only $s=t$ and it must be $S=T$. Suppose the statement is true for some $k\in\Z^{+}$, i.e. $S\subseteq T$, $|S|=|T|=k$ implies $S=T$. Let us prove it is true for $k+1\in\Z$. So, if $S=\{s_1,\ldots,s_k,s_{k+1}\}$ and $T=\{t_1,\ldots,t_k,t_{k+1}\}$, with $S\subseteq T$, then we need to show that $S=T$. Notice that we can write $S=S'\cup\{s_{k+1}\}$. As $s_{k+1}\in S$ and $S\subseteq T$, then also $s_{k+1}\in T$ (without loss of generality assume that $t_{k+1}=s_{k+1}$). Then, we can write $T=T'\cup\{t_{k+1}\}=T'\cup\{s_{k+1}\}$. Now, notice that $|S'|=(k+1)-1$ and $|T'|=(k+1)-1=k$. So, $|S'|=|T'|=k$. If we take $s_i\in S'$, then $S'\subseteq S\subseteq T$ implies $s_i\in T$. But, $s_{k+1}\neq t_{k+1}$, then there exists $t_j\in T-\{t_{k+1}\}$ such that $s_i=t_j$. Therefore, $S'\subseteq T-\{t_{k+1}\}$, i.e. $S'\subseteq T'$. With $|S'|=|T'|=k$, that implies $S'=T'$ and we get $T=T'\cup\{s_{k+1}\}=S'\cup\{s_{k+1}\}=S$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $f:A\rightarrow B$ a function. Then,

\begin{enumerate}
\item Function $f$ has a right inverse if and only if $f$ is surjective.
\item Function $f$ has a left inverse if and only if $f$ is injective.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Necessity.} Suppose $f$ has a right inverse. Then there exists $f_R:B\rightarrow A$ such that $f\circ f_R=i_B$. That means that $f(f_R(y))=y$, for all $y\in B$. Then, $f_R:B\rightarrow A$ is a well-defined function and for all $y\in B$ there exists $x\in A$ such that $f_R(y)=x$. But, if $f_R(y)=x$ and $x\in A$, then $f_R(y)\in A$, and we have $f(f_R(y))=y$, i.e. $f(x)=y$. Therefore, $f$ is a surjection. {\it Sufficiency.} Suppose that $f$ is surjective. Then, for all $y\in B$ there exists $x\in A$ such that $f(x)=y$. Let us define $g:B\rightarrow A$ so that $g(y)=f^{-1}(y)$, for all $y\in B$ for which exists $x\in A$ such that $f(x)=y$. But, as $f$ is surjective, that condition is satisfied for all $y\in B$ and $g$ is well-defined. We can see that, if we take $y\in B$ we have $f(g(y))=f(f^{-1}(y))$. As for $y\in B$ there exists $a\in A$ such that $f(a)=y$, we have $f^{-1}(y)=a$. Then, $f(g(y))=f(a)$. But, $f(a)=y$ and $f(g(y))=y$. Thus, $f$ has a right inverse and it is $g$.

{\it Ad $2$.} {\it Necessity.} Suppose $f$ has a left inverse. Then, there exists $f_L:B\rightarrow A$ such that $[f_L\circ f](x)=x$, for all $x\in A$. Let $x_1,x_2\in A$. Then, there exist $y_1,y_2\in B$ such that $f(x_1)=y_1$ and $f(x_2)=y_2$. As $f_L$ is a well defined function, then for all $y_1,y_2\in B$, if $y_1=y_2$, it must be $f_L(y_1)=f_L(y_2)$ (uniqueness). In other words, for all $f(x_1),f(x_2)\in B$, if $f(x_1)=f(x_2)$ it must be $f_L(f(x_1))=f_L(f(x_2))$. But, $f_L(f(x))=x$, for all $x\in A$, so we have $x_1=x_2$. As our choice of $x_1$ and $x_2$ was arbitrary the statement is true for all $x_1,x_2\in A$ and $f$ is, by that, injective. {\it Sufficiency.} If $f$ is injective, then for all $x_1,x_2\in A$, if $f(x_1)=f(x_2)$ then $x_1=x_2$. Let $g:B\rightarrow A$ be a function such that $g(y)=f^{-1}(y)$ for all $y\in B$ for which exists $x\in A$ such that $f(x)=y$. Also, for $y\in B$ for which does not exist $x\in A$ such that $f(x)=y$, let $g(y)=a$, for some $a\in A$. So, $g$ is defined for all $y\in B$. Take $y_1,y_2\in B$ and assume $y_1=y_2$. We have that there exist $x_1,x_2\in A$ such that $y_1=f(x_1)$ and $y_2=f(x_2)$. Therefore, we have $f(x_1)=f(x_2)$. By injectivity of $f$, we have $x_1=x_2$. As $f$ is defined for all $x\in A$, i.e. for all $x\in A$ there exists $y\in B$ such that $f(x)=y$, then it must be that for all $x\in A$, there exists $y\in B$ such that $x=f^{-1}(y)$, i.e. $x=g(y)$. So, for $x_1,x_2\in A$, there exists $z_1,z_2\in B$ such that $g(z_1)=x_1$ and $g(z_2)=x_2$. But, $g(z_1)=f^{-1}(z_1)=x_1$ and $f^{-1}(z_1)=x_1$ implies $z_1=f(x_1$, i.e. $z_1=y_1$. Similarly, $z_2=y_2$. From that we have $g(y_1)=x_1=x_2=g(y_2)$, i.e. $g(x_1)=g(x_2)$. Thus, $g$ is a well defined function. If we take $x\in A$, there exists $y\in B$ such that $f(x)=y$. Now, for $y\in B$ such that there exists $x\in A$ and $f(x)=y$, then $g(y)=x$. But, that means that $g(f(x))=x$, i.e. $g$ is a left inverse of $f$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ and $B$ be finite sets and $f:A\rightarrow B$ a function. Then the following holds:

\begin{enumerate}

\item If $|\dom{f}|<|\cod{f}|$, then $f$ cannot be a surjection.

\item If $|\dom{f}|>|\cod{f}|$, then $f$ cannot be an injection\footnote{Actually another form of the {\bf pigeonhole principle}.}.

\item If $|\dom{f}|=|\cod{f}|$, then $f$ is an injection iff it is a surjection.

\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $A=\{a_1,\ldots,a_m\}$ and $B=\{b_1,\ldots,b_n\}$ such that $m<n$. Assume $\ran{f}=B$, i.e. $f$ is surjective. From that follows $|\ran{f}|=|B|$, that is $|\ran{f}|=n$. But, by a previous lemma, as $A$ and $B$ are finite we have $|\ran{f}|\leq m$. In other words, $n\leq m$. But, by assumption also $m<n$, so we have $n\leq m<n$ and from that $n<n$, which is impossible. Therefore, $f$ cannot be surjective.

{\it Ad $2$.} Let $A=\{a_1,\ldots,a_m\}$ and $B=\{b_1,\ldots,b_n\}$ such that $m>n$. Let us denote $A_n=\{a_{n+1},\ldots,a_m\}$. Notice that $A-A_n\neq\emptyset$, as $m=n+k$, where $k\in\Z^{+}$. If there exist $a_i,a_j\in A-A_n$, $i\neq j$ and $i,j\in\{1,\ldots,n\}$, such that $f(a_i)=f(a_j)$, then $f$ is not an injection and we are done. Assume that for all $a_i,a_j\in A-A_n$, we have that $a_i\neq a_j$ (sufficient to say $i\neq j$) implies $f(a_i)\neq f(a_j)$. If we denote $f(a_i)=b_i$, for all $i\in\{1,\ldots,n\}$ then, $\ran{f}=\{b_1,\ldots,b_n\}$. All elements are different by assumption, so $\left|\ran{f}\right|=n$. From $\ran{f}\subseteq B$ and latter result, we have $\ran{f}=B$, i.e. $f$ is a surjection. Also, $\ran{f}=\{b_1,\ldots,b_n\}$. As $A_n\neq\emptyset$, there exists $a_{n+r}\in A_n$, for some $r\in\Z^{+}$, such that $n<n+r\leq m$. As $f$ is a function, then there exists $b_i\in B=\ran{f}$ such that $f(a_{n+r})=b_p$, where $p\in\{1,\ldots,n\}$. But, also, $f(a_p)=b_p$, impliying $f(a_p)=f(a_{n+r})$. Now, clearly $a_{n+r}\neq a_p$ (contradicting this would imply $n+r=p$, but $p\leq n$, so it must be $n+r\neq p$), that implies that it is not the case that $f(a_p)=f(a_{n+r})$ implies $a_{n+r}=a_p$, i.e. $f$ is not an injection.

{\it Ad $3$.} Let $A=\{a_1,\ldots,a_m\}$ and $B=\{b_1,\ldots,b_m\}$. {\it Necessity.} Let $f$ be an injection. Then, by a previous proposition $\left|\ran{f}\right|=|A|=m$. As $|B|=m$, we have $|\ran{f}|=|B|$. Furthermore, as $\ran{f}\subseteq B$ it follows that $\ran{f}=B$, i.e. $f$ is a surjection. {\it Sufficiency.} Let $f$ be a surjection. Then, $\ran{f}=B$ and from that $|\ran{f}|=|B|=m$. We can say $\ran{f}=\{f(a_1),\ldots,f(a_m)\}$ and all $f(a_i)\neq f(a_j)$, for $i\neq j$, i.e. $a_i\neq a_j$, so $f$ is an injection. $b_p=b_r=b$, 

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} The third case in theorem above actually tells us that if $f:A\rightarrow B$, where $A$ and $B$ are finite and with equal number of elements, is injection or a surjection then it is necessarily a bijection. A problem below tells us, that, if sets are not finite, the analogues to the cases above don't have to be true.

\noindent\newline{\bf Problem.} Let $A,B\subseteq\R$ be sets such that $B\subset A$. Show that there exists a bijection $f:A\rightarrow B$.

\noindent\newline{\bf Solution.} Let $A=[a,b]$, $B=[c,d]$ with $c\neq d$. Take two points $X(a,c)$ and $Y(b,d)$ in $\R^2$. If we take a line defined by those points, we get the equation:

\begin{equation*}
y-c=\frac{d-c}{b-a}(x-a).
\end{equation*}

\noindent\newline Simplifying that expression gives us:

\begin{equation*}
y=\frac{d-c}{b-a}x+\frac{-a(d-c)+c(b-a)}{b-a}.
\end{equation*}

\noindent\newline So, we will take that to be:

\begin{equation*}
f(x)=\frac{d-c}{b-a}x-\frac{a(d-c)-c(b-a)}{b-a}.
\end{equation*}

\noindent\newline Linear function is obviously injective, and so is the above instantiation. We can see that from the coefficient next to $x$, $f'(x)=\frac{d-c}{b-a}$ is strictly greater than zero (as $d>c$ and $b>a$), making it a monotonously rising function and therefore injective. Let's take $f(a)$. That is obviously:

\begin{equation*}
f(a)=\frac{a(d-c)}{b-a}-\frac{a(d-c)-c(b-a)}{b-a}=\frac{c(b-a)}{b-a}=c.
\end{equation*}

\noindent\newline Now, for $f(b)$ we have:

\begin{equation*}
f(b)=\frac{b(d-c)}{b-a}-\frac{a(d-c)-c(b-a)}{b-a}=\frac{b d-a d}{b-a}=d.
\end{equation*}

\noindent\newline So, $f([a,b])=[f(a),f(b)]=[c,d]$, that is $\textnormal{Im}(f)=B$, making $f$ surjective and by that bijective.

\noindent\newline{\bf Problem.} Let $\mathcal{F}_{b}(\R)$ be the set of all bijective functions of the form $f:\R\rightarrow\R$. Prove that $\left(\mathcal{F}_{b}(\R),\circ\right)$ (i.e. the set of all bijective functions of the formerly described form with function composition as respective operation) is a group.

\noindent\newline{\bf Solution.} By previously proven theorem, function composition is associative (for functions of any type). There is a neutral element $\textnormal{id}:\R\rightarrow\R$ defined as $id(x)=x$ (because $[f\circ\textnormal{id}](x)=f(\textnormal{id}(x))=f(x)$ and $[\textnormal{id}\circ f](x)=\textnormal{id}(f(x))=f(x)$ for every $x\in\R$). By previosly proven theorem, every bijective function has an inverse, and in this case that would be $f^{-1}:\R\rightarrow\R$. Composition is, however, not commutative. We could take, e.g. $f(x)=x+1$ and $g(x)=2x$. Obviously, $[f\circ g](x)=f(g(x))=2x+1$, while $[g\circ f](x)=g(f(x))=2(x+1)$. Therefore $\left(\mathcal{F}_{b}(\R),\circ\right)$ is a group.

\noindent\newline{\bf Problem.} Let $A$ and $B$ be finite non-empty sets where $|A|=n$ and $|B|=m$. What is the number of:

\begin{enumerate}
\item functions $f:A\rightarrow B$;
\item injections $f:A\rightarrow B$;
\item bijections $f:A\rightarrow A$?
\end{enumerate}

\noindent\newline{\bf Solution.} We will determine the number of:

\begin{enumerate}
\item {\it functions $f:A\rightarrow B$}. Now, $f$ will be a set of all ordered pairs $(x,y)\in A\times B$ with conditions that each $x\in A$ must appear only once (so it will be many-to-one) and every $x\in A$ must be contained in some ordered pair (so it will be left-total). For every $y\in B$ we can either choose many $x\in A$ or none at all. One example, for $f\subseteq\{x_1,x_2,x_3\}\times\{y_1,y_2,y_3\}$, would be:

\begin{equation*}
f=\{(x_1,y_1),(x_2,y_1),(x_3,y_2)\}.
\end{equation*}

\noindent\newline In general, for $A=\{x_1,\ldots,x_n\}$ we have:

\begin{equation*}
f=\{(x_1,\cdot),(x_2,\cdot),\ldots,(x_n,\cdot)\}.
\end{equation*}

\noindent\newline That way, every $x\in A$ appears and appears only once. For each $x\in A$ we can choose $m$ different $y\in B=\{y_1,\ldots,y_m\}$. So, for every ordered pair we have $m$ choices, as we can take multiple $y\in B$ for one $x\in A$. For $n$ ordered pairs the number of different choices is $\underbrace{m\cdot m\cdots m}_{n\ \textnormal{times}}=m^n$. The number of different functions from $A$ to $B$ is $|B|^{|A|}$; more general:

\begin{equation*}
|\textnormal{cod}f|^{|\textnormal{dom}f|}.
\end{equation*}

\item {\it injections $f:A\rightarrow B$}. We take all the conditions from the previous problem, adding that every $y\in B$ must appear only once. Therefore, for first ordered pair we have $m$ choices, for second $m-1$, for third $m-2$, etc. As we have $n$ ordered pairs, that is $m(m-1)(m-2)\cdots(m-(n-1))=\frac{m!}{(m-n)!}$. The number of different injections from $A$ to $B$ is then $\frac{|B|!}{(|B|-|A|)!}$; more general:

\begin{equation*}
\frac{|\textnormal{cod}f|!}{(|\textnormal{cod}f|-|\textnormal{dom}f|)!}.
\end{equation*}

\item {\it bijections $f:A\rightarrow A$}. Taking all conditions from the previous example, we add the condition that $B=A$ and therefore $|B|=|A|$. Using the fact that $0!=1$ we have that the number of bijections from $A$ to $A$ is $|A|!$, or:

\begin{equation*}
|\textnormal{dom}f|!.
\end{equation*}

\end{enumerate}

\noindent\newline{\bf Definition.} We define a function (in the mention-sense) called {\it Iverson brackets} of the form $[\cdot]:\Phi\rightarrow\{1,0\}$, where $\Phi$ is the set of all propositions of the form $P(x)$. Then, Iverson brackets is defined by a rule: $[P(x)]=1$ if and only if $P(x)$ is true, and $[P(x)]=0$ otherwise.

\noindent\newline{\bf Problem.} We have said that Iverson brackets is a function, but only in the mention-sense. Prove that it really {\it is} a function.

\noindent\newline{\bf Solution.} By definition of a proposition, it is any judgment to which we can assign a truth value (true or false). So, Iverson brackets is defined for every proposition, satisfying the first axiom; and as every proposition can be, according to law of non-contradiction, true or false, no proposition can take more than one value in $\{1,0\}$. Therefore, Iverson brackets is a function.

\noindent\newline{\bf Remark.}  As a special case of Iverson brackets we can consider indicator function $I_{S}:S\rightarrow\{1,0\}$, where $S$ is a non-empty set, defined as:

\begin{equation*}
I_{S}(x)=[x\in S].
\end{equation*}

\noindent\newline{\bf Remark.} By using Iverson brackets, we can construct functions using logical connectives in a more analytic fashion. For example, disjunction can be then defined using the operation of addition modulo\footnote{Notice that, if the propositions are taken as sets, as it is in the second-order logic, if the sets are disjunct we can take only simple addition: they are never going to be both true.} $2$ (denoted by $+_{2}$):

\begin{equation*}
I(P(x)\vee Q(x))=\mathcal{B}\left([P(x)]+_{2}[Q(x)]\right),
\end{equation*}

\noindent\newline where $\mathcal{B}:\{0,1\}\rightarrow\{\bot,\top\}$ such that $\mathcal{B}(0)=\bot$ and $\mathcal{B}(1)=\top$ (it's obvious that this is a bijection; its inverse is trivial). Also, $I:\Phi\rightarrow\{\bot,\top\}$ is an interpretation. Conjunction can be defined in the similar sense using multiplication:

\begin{equation*}
I(P(x)\wedge Q(x))=\mathcal{B}\left([P(x)]\cdot[Q(x)]\right).
\end{equation*}

\noindent\newline{\bf Problem.} Determine whether each of the following functions $f\in\mathcal{F}(\R)$ is or is not injective and is or is not surjective:

\begin{enumerate}
\item $f(x)=x I_{\Q}(x)+2x I_{\mathbb{I}}(x)$;
\item $f(x)=2 x I_{\Z}(x)+x I_{\R\backslash\Z}(x)$;
\item $f:A\times B\rightarrow A$, defined by $f(x,y)=x$;
\item $f:A\times B\rightarrow B\times A$, defined by $f(x,y)=(y,x)$;
\item $f:A\rightarrow A\times B$, defined by $f(x)=(x,b)$, $b\in B$;
\item $G$ is a group, $a\in G$, and $f:G\rightarrow G$ is defined by $f(x)=a x$;
\item $G$ is a group and $f:G\rightarrow G$ is defined by $f(x)=x^{-1}$;
\item $G$ is a group and $f:G\rightarrow G$ is defined by $f(x)=x^2$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $f(x)=x I_{\Q}(x)+2x I_{\mathbb{I}}(x)$.} Take $f(a)=f(b)$, that means that $a I_{\Q}(a)+2a I_{\mathbb{I}}(a)=b I_{\Q}(b)+2b I_{\mathbb{I}}(b)$, i.e. that $a$ and $b$ are both rational or both irrational. If they are both rational, then the second members on every side of equation go to zero, and we only have $a=b$. If they are both irrational, then it's the reverse case and we have $2a=2b$, that is, $a=b$. Therefore, $f(x)$ is injective. Now, obviously if we take some $y\in\Q$, then, there exists such $x\in\Q$ that $f(x)=y$ and that is $x$. If we take $y\in\mathbb{I}$, then there exists $x\in\mathbb{I}$ such that $f(x)=y$ and that is $\frac{1}{2}x$, which is again irrational. And, as $\Q\cup\mathbb{I}=\R$, we have exhausted all elements in codomain of $f$, so $f$ is surjective.

Now, as $\Q\cup\mathbb{I}=\R$, $f$ is defined for all $x\in\R$, thus $f$ is surjective. Also, $f$ is bijective.

\item {\it $f(x)=2 x I_{\Z}(x)+x I_{\R\backslash\Z}(x)$}. If we take $f(a)=f(b)$, that is, $2 a I_{\Z}(a)+a I_{\R\backslash\Z}(a)=2 b I_{\Z}(b)+b I_{\R\backslash\Z}(b)$, that means that $a$ and $b$ are either both integers or are both in $\R\backslash\Z$. So, if they are both integers, the second members of addition on each side of equation go to zero and we have $2a=2b$. In the otherwise-case, we have $a=b$, so $f$ is injective. Can $f(x)=1$? If that is so, then $1=2x I_{\Z}(x)+x I_{\R\backslash\Z}(x)$. If $x\in\Z$ then we can only have even numbers, as $2 x$ is even for all $x\in\Z$. It cannot be $1$. If $x\notin\Z$, then $f(x)=x$, so $x$ must be $1$, but $1\in\Z$, and that cannot be. So we don't have at least one element, that is $1\in\R$ that has a preimage in $\R$. This function is not surjective.

\item {\it $f:A\times B\rightarrow A$, defined by $f(x,y)=x$.} Take $(a,b),(c,d)\in(A\times B)$. Then, $f(a,b)=f(c,d)$ means that $a=c$, but not necessarily $(a,b)=(c,d)$. E.g. $(a_1,b_1)\neq(a_1,b_2)$, such that $b_1\neq b_2$. Then, $f(a_1,b_1)=a_1$ and $f(a_1,b_2)=a_1$ (i.e. two different elements in domain have the same element $a_1$ in codomain). Therefore, $f$ is not injective. Now, $f$ is surjective as, if we take $z\in A$, there is $(z,y)\in(A\times B)$ such that $f(z,y)=z$.

\item {\it $f:A\times B\rightarrow B\times A$, defined by $f(x,y)=(y,x)$.} If $f(a,b)=f(c,d)$ that means that $(b,a)=(d,c)$ and that $b=d$ and $a=c$, which is the same as saying $(a,b)=(c,d)$, therefore $f$ is injective. Now, take some $(y,x)\in B\times A$. There exists $(x,y)\in A\times B$ such that $f(x,y)=(y,x)$. Obviously $f$ is surjective and by that bijective.

\item {\it $f:A\rightarrow A\times B$, defined by $f(x)=(x,b)$, $b\in B$.} If $f(z)=f(w)$ then $(z,b)=(w,b)$, which implies that $z=w$. So, $f$ is injective. Yet, $f$ is not surjective as $\textnormal{Im(f)}=(A\times\{b\})\subset(A\times B)$.

\item {\it $G$ is a group, $a\in G$, and $f:G\rightarrow G$ is defined by $f(x)=a x$.} Take $x,y\in G$. Then $f(x)=f(y)$ implies $a x=a y$. By cancellation law for groups (in this case, multiplying by $a^{-1}$ on the left), we have $x=y$. Thus, $f$ is injective. Now, if we take some $y\in G$, does there exist such $x\in G$ so that $f(x)=y$, i.e. $a x=y$?. Well, obviously, if $a\in G$, then $a^{-1}\in G$, so we have $x=a^{-1} y$; function $f$ is surjective and therefore bijective.

\item {\it $G$ is a group and $f:G\rightarrow G$ is defined by $f(x)=x^{-1}$.} If $f(a)=f(b)$, then $a^{-1}=b^{-1}$. Multipliying by $a$ on the left and $b$ on the right, we have $b=a$, therefore $f$ is injective. Now, taking $y\in G$, does there exist $x\in G$ such that $f(x)=y$, i.e. $x^{-1}=y$? Multiplying by $x$ on the right we have $e=y x$, and then, multiplying by $y^{-1}$ ($y$ has an inverse in $G$), we have $x=y^{-1}$. So, $f$ is surjective, and then bijective.

\item {\it $G$ is a group and $f:G\rightarrow G$ is defined by $f(x)=x^2$.} Now, if $f(a)=f(b)$, then $a^2=b^2$. But, as we have previously shown, from this does not necessarily follow that $a=b$, e.g. $\R\backslash\{0\}$ with multiplication as respective operation is a group and we have $(-2)^2=2^2$, but $-2\neq 2$. So, $f$ is not necessarily injective. But $f$ is also not necessarily surjective as for arbitrary $y\in G$ there does not have to exist $x\in G$ such that $f(x)=y$, e.g. $\R\backslash\{0\}$ with multiplication is a group and for $-1\in\R\backslash\{0\}$ there does not exist such $x\in\R\backslash\{0\}$ that $x^2=-1$. Thus, $f$ is neither necessarily injective nor necessarily injective.
\end{enumerate}

\noindent{\bf Theorem\footnote{Generalization of the first two problems in previous excercises. For advanced readers!}.} Let $(G,\oplus,\odot)$ be a ring and $A,B\subseteq G$, such that $A\cap B=\emptyset$ and $A\cup B=G$. Let $u:A\rightarrow A$ and $v:B\rightarrow B$ be bijective functions. We define function of the form $f:G\rightarrow G$ defined as

\begin{equation*}
f(x)=u(x)\odot I_{A}(x)\oplus v(x)\odot I_{B}(x),
\end{equation*}

\noindent\newline where indicator function $I_{S}$ is defined by Iverson brackets, such that:

\begin{equation*}
I_{S}(x)=[x\in S],%\oplus e_{\oplus}\odot[x\notin S],
\end{equation*}

\noindent\newline and $[\cdot]:\Phi\rightarrow\{e_{\oplus},e_{\odot}\}$, defined such that $[P(x)]=e_{\oplus}$ if and only if $P(x)$ is true; $[P(x)]=e_{\odot}$ otherwise; $e_{\odot}$ is neutral element for $\odot$ operation and $e_{\oplus}$ a neutral element for $\oplus$ operation. Then, $f$ is bijective.

\noindent\newline {\bf Proof.} Now, as $u,v$ are bijective and each being defined with domain and codomain on $A$ and $B$, respectively, it cannot be that $u(x)=v(x)$, as $A\cap B=\emptyset$. If it were that $u(x)=v(x)$, that would mean that $u(x)\in A$ (as $u$ is a function) and $v(x)\in B$ (as $v$ is a function). But then, it would mean that $u(x)=v(x)\in B$, i.e. $u(x)\in A$ and $u(x)\in B$, which would be a contradiction to our assumtion that $A\cap B=\emptyset$.
So, if we take $f(x)=f(y)$, that means that $u(x)\odot I_{A}(x)\oplus v(x)\odot I_{B}(x)=u(y)\odot I_{A}(y)\oplus v(y)\odot I_{B}(y)$. And if it were that $x$ were in $A$ and $y$ were in $B$, it would mean that $u(x)=v(x)$, which could not be (same thing for $x\in B$ and $y\in A$). Therefore either $x,y\in A$ or $x,y\in B$. So, it would mean that $u(x)=u(y)$ or $v(x)=v(y)$, but $u,v$ are bijective, so $x=y$ follows from both. Thus, $f$ is also injective. If we take $y\in G$ does there exist $x\in G$ such that $f(x)=y$? Obviously, $A\cup B=G$, and they are disjunct, so, suppose $y\in A$. That would mean that it was $y=f(x)=u(x)$. If it were that $f(x)=v(x)$, then $y=f(x)=v(x)\in B$, which would be a contradiction. Therefore it was that $y=u(x)$. But, $u(x)$ is surjective, so there exists $x\in B$ such that $y=u(x)$. Same proof goes for $y\in B$. Thus, $f$ is surjective, and by that also bijective. We will only make a comment that, as we're dealing with a commutative ring, neutral element $e_{\oplus}$ acts as a zero and $e_{\odot}$ as one, so we can view the operations in function $f$ as addition and multiplication in $\R$ without, for now, entouring into more deeper analysis.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} If $f:A\rightarrow B$ is injective and $g:B\rightarrow C$ surjective, is then $g\circ f:A\rightarrow C$ necessarily bijective?

\noindent\newline {\bf Solution.} We can take $f:\R\rightarrow\R$ to be $f(x)=x$, which is obviously injective, and $g:\R\rightarrow[-1,1]$ to be $g(x)=\sin{x}$, which is surjective. But, $g\circ f:\R\rightarrow[-1,1]$ with $g(f(x))=\sin{x}$ is not injective and therefore not bijective.

\noindent\newline{\bf Problem.} Each of the following functions is bijective. Describe its inverse:

\begin{enumerate}
\item $f:\R\rightarrow\R$, defined by $f(x)=2x I_{\Q}(x)+3x I_{\mathbb{I}(x)}$;
\item $A=\{a,b,c,d\}$, $B=\{1,2,3,4\}$ and $f:A\rightarrow B$ is given by:

\begin{equation*}
f=\left(\begin{array}{cccc}
a & b & c & d\\
3 & 1 & 2 & 4\\
\end{array}\right);
\end{equation*}

\item $G$ is a group, $a\in G$, and $f:G\rightarrow G$ is defined by $f(x)=a x$.

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $f:\R\rightarrow\R$, defined by $f(x)=2x I_{\Q}(x)+3x I_{\mathbb{I}(x)}$.} Now, if we have $f(x)$ of the form $3x$, then we know that $x$ is irrational. If, however, we have $f(x)$ of the form $2 x$, we know that $x$ is rational. Therefore $f^{-1}(x)$ is a rational number if $x$ is divisible by $2$ and irrational if it's divisible by $3$, $(2|x\rightarrow f^{-1}(x)\in\Q)\wedge(3|x\rightarrow f^{-1}(x)\in\mathbb{I})$, $f^{-1}:\R\rightarrow\R$.

\item {\it $A=\{a,b,c,d\}$, $B=\{1,2,3,4\}$ and $f:A\rightarrow B$ is given by:

\begin{equation*}
f=\left(\begin{array}{cccc}
a & b & c & d\\
3 & 1 & 2 & 4\\
\end{array}\right).
\end{equation*}}

\noindent\newline Obviously, $f^{-1}:B\rightarrow A$ and:

\begin{equation*}
f^{-1}=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
b & c & a & d\\
\end{array}\right).
\end{equation*}

\item {\it $G$ is a group, $a\in G$, and $f:G\rightarrow G$ is defined by $f(x)=a x$.} If we take $x\in G$, then we want to find the original $y\in G$, such that $x=a y$. As $G$ is a group, and $a\in G$, we can take the inverse of $a^{-1}\in G$ and multiply the equation on the left with the former to get $a^{-1} x=y$. Taking $f^{-1}(x)=y$, we have $f^{-1}:G\rightarrow G$ with $f^{-1}(x)=a^{-1} x$.

\end{enumerate}

\newpage

\begin{center}
{\bf Permutations}
\end{center}

\vskip 0.5cm

\noindent\newline{\bf Definition.} Let $A$ be a non-empty set. Then, any bijection of the form $f:A\rightarrow A$ is called a {\bf permutation}.

\noindent\newline{\bf Remark.} (i) Note that permutations are a subset of bijective functions, by definition. As for $f:A\rightarrow A$ and $g:A\rightarrow A$ it's $f\circ g:A\rightarrow A$ and $g\circ f:A\rightarrow A$ and as $f^{-1}:A\rightarrow A$, obviously permutations form a group by being a subgroup of bijective functions with function composition as respective operation (we have also shown previously that function composition is associative). (ii) Note that the number of permutations from $A$ to $A$, with $|A|=n$, as previously shown for bijective functions, is $n!$. (iii) Also, we will denote the neutral element for group of permutations with $\epsilon$ and define it as $\epsilon:A\rightarrow A$ with rule $\epsilon(x)=x$. The other permutations are denoted with small greek letters and defined through tables, exempli gratia (one permutation of $A$, where $|A|=4$):

\begin{equation*}
\alpha=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
3 & 4 & 1 & 2
\end{array}\right).
\end{equation*}

\noindent\newline{\bf Definition.} For any set $A$, the group\footnote{Actually shown in the previous remark.} of all permutations of $A$ is called the {\bf symmetric group} on $A$ and is represented by symbol $S_A$.

\noindent\newline{\bf Remark.} For $A=\{1,\ldots,n\}$, the group of all permutations of $A$ is called symmetric group on $n$ elements and is denoted by $S_n$.

\noindent\newline{\bf Definition.} For every $n\in\N\backslash\{1,2\}$, the regular polygon with $n$ sides has a group of symmetries symbolized by $D_n$. These groups are called {\bf dihedral groups.}

\noindent\newline{\bf Problem.} Consider the following permutations $f,g,h\in S_6$:

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
f=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
6 & 1 & 3 & 5 & 4 & 2\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
2 & 3 & 1 & 6 & 5 & 4\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\end{center}

\begin{equation*}
h=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 1 & 6 & 4 & 5 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline Compute the following:

\begin{enumerate}
\item $f^{-1}$, $g^{-1}$, $h^{-1}$, $f\circ g$, $g\circ f$;
\item $f\circ(g\circ h)$;
\item $g\circ h^{-1}$;
\item $h\circ g^{-1}\circ f^{-1}$;
\item $g\circ g\circ g$.
\end{enumerate}

\noindent{\bf Solution.} {\it Ad $1$:}

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
f^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
2 & 6 & 3 & 5 & 4 & 1\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 1 & 2 & 6 & 5 & 4\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\end{center}

\begin{equation*}
h^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
2 & 6 & 1 & 4 & 5 & 3\\
\end{array}\right),
\end{equation*}

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
f\circ g=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
1 & 3 & 6 & 2 & 4 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g\circ f=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
4 & 2 & 1 & 5 & 6 & 3\\
\end{array}\right).
\end{equation*}}
\end{parbox}
\end{center}

\noindent\newline {\it Ad $2$:}

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g\circ h=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
1 & 2 & 4 & 6 & 5 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
f\circ(g\circ h)=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
6 & 1 & 5 & 2 & 4 & 3\\
\end{array}\right).
\end{equation*}}
\end{parbox}
\end{center}

\noindent\newline {\it Ad $3$:}

\begin{equation*}
g\circ h^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 4 & 2 & 6 & 5 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline {\it Ad $4$:}

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g^{-1}\circ f^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
1 & 4 & 2 & 5 & 6 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
h\circ g^{-1}\circ f^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 1 & 4 & 2 & 5 & 6\\
\end{array}\right).
\end{equation*}}
\end{parbox}
\end{center}

\noindent\newline {\it Ad $5$:}

\begin{center}
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g\circ g=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 1 & 2 & 4 & 5 & 6\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g\circ g\circ g=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
1 & 2 & 3 & 6 & 5 & 4\\
\end{array}\right).
\end{equation*}}
\end{parbox}
\end{center}

\noindent\newline{\bf Problem.} List the elements of the cyclic subgroup of $S_6$ generated by

\begin{equation*}
f=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
2 & 3 & 4 & 1 & 6 & 5\\
\end{array}\right).
\end{equation*}

\noindent\newline{\bf Solution.} We also need to find $f^{-1}$ and then all the possible compositions of $f$ with or without $f^{-1}$ and reverse. Now:

\begin{equation*}
f^{-1}=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
4 & 1 & 2 & 3 & 6 & 5\\
\end{array}\right).
\end{equation*}

\noindent\newline We venture further by finding:

\begin{equation*}
f\circ f=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
3 & 4 & 1 & 2 & 5 & 6\\
\end{array}\right).
\end{equation*}

\noindent\newline Following the same line of reasoning:

\begin{equation*}
f\circ f\circ f=\left(\begin{array}{cccccc}
1 & 2 & 3 & 4 & 5 & 6\\
4 & 1 & 2 & 3 & 6 & 5\\
\end{array}\right).
\end{equation*}

\noindent\newline Notice that $f\circ f\circ f=f^{-1}$ and therefore $f\circ f\circ f\circ f=f\circ f^{-1}=\epsilon$. Then we would be geting $\epsilon\circ f=f$ and the same sequence all over again. Therefore $\langle f\rangle=\{\epsilon,f,f^{-1},f\circ f\}$.

\noindent\newline{\bf Problem.} Let $G$ be the subset of $S_4$ consisting of the permutations:

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\epsilon=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
1 & 2 & 3 & 4\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
f=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
2 & 1 & 4 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}\end{center}

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
3 & 4 & 1 & 2\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
4 & 3 & 2 & 1\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline Show that $G$ is a group of permutations, and write its table.

\noindent\newline{\bf Solution.} In order to show that $G=\{\epsilon,f,g,h\}$ is a group of permutations, we need to show that each $x\in G$ is a permutation and that $G$ is a group. We can show that $G$ is a group by using the fact that $G\subseteq S_4$ and that $S_4$ is a symmetric group on $4$ elements (there are $4!=24$ permutations). We have already shown in a previous remark that a set endowed with composition and containing all permutations of some set will form a group. Therefore, it is sufficient to show that $G$ is a subgroup of $S_4$. We need to find compositions $f\circ f$, $g\circ g$, $h\circ h$, $f\circ g$, $g\circ f$, $f\circ h$, $h\circ f$, $g\circ h$ and $h\circ g$. The compositions with $\epsilon$ are trivial as $\epsilon\circ x=x\circ\epsilon=x$, for all $x\in G$ (and $x\in S_4$ also!).

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
f\circ g=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
4 & 3 & 2 & 1\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g\circ f=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
4 & 3 & 2 & 1\\
\end{array}\right),
\end{equation*}}
\end{parbox}\end{center}

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
f\circ h=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
3 & 4 & 1 & 2\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h\circ f=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
3 & 4 & 1 & 2\\
\end{array}\right),
\end{equation*}}
\end{parbox}\end{center}


\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g\circ h=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
2 & 1 & 4 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h\circ g=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
2 & 1 & 4 & 3\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline We can see that $f\circ g=g\circ f=h$, then, $f\circ h=h\circ f=g$ and $g\circ h=h\circ g=f$. Now, it can easily be verified that $f\circ f=g\circ g=h\circ h=\epsilon$. The table is:

\begin{center}
\begin{tabular}{c|cccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$\\
$f$ & $f$ & $\epsilon$ & $h$ & $g$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$\\
$h$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\noindent\newline Notice that $G$ is also an Abelian group.

\noindent\newline{\bf Theorem.} Denote $\sigma_{S}$ to represent a permutation of a set $S$. Let $A=\{a_1,\ldots,a_n\}$, $n\in\N$. Then, let $S_A$ be group of all permutations of the set $A$ and $G$ an Abelian subgroup of $S_A$. Now, let $B=\{b_1,\ldots,b_m\}$, $m\in\N$, be a set disjoint from $A$ and $S_{A\cup B}$ group of all permutations of $A\cup B$. Now, for every $\sigma_{A}\in G$ define $\sigma_{A\cup B}\in S_{A\cup B}$ such that\footnote{Take the plus sign to be just a delimiter as this operation will never be executed; for mathematical rigour we should define this in the context of a ring.}

\begin{equation*}
(\forall x\in A\cup B)(\sigma_{A\cup B}(x)=\sigma_{A}(x)\mathcal{I}_{A}(x)+x\mathcal{I}_{B}(x)).
\end{equation*}

\noindent\newline Then the set $H$, containing such defined $\sigma_{A\cup B}$, is an Abelian subgroup of $S_{A\cup B}$.

\noindent\newline{\bf Proof.} Let $A=\{a_1,\ldots,a_n\}$ be a set, $S_A$ group of permutations of $A$ and $G$ an Abelian subgroup of $G$. Take $\sigma_A,\rho_A\in G$. That means that $[\sigma_A\circ\rho_A](x)=[\rho_A\circ\sigma_A](x)$, for all $x\in A$. Now take $B=\{b_1,\ldots,b_m\}$ and define $A\cup B=\{a_1,\ldots,a_n,b_1,\ldots,b_m\}$ and $H$ as a group of all permutations of $A\cup B$. For every $\sigma_A\in G$ define $\sigma_{A\cup B}\in S_{A\cup B}$ as described in the theorem. We need to show that $S_{A\cup B}$ is an Abelian subgroup of $H$. Obviously, $H\subseteq S_{A\cup B}$, as $\sigma_{A\cup B}\in S_{A\cup B}$ is defined as $\sigma_{A\cup B}:A\cup B\rightarrow A\cup B$, i.e. it's a permutation of $A\cup B$ and therefore contained in $H$. Take $\sigma_{A\cup B},\rho_{A\cup B}\in H$. Their composition $\sigma_{A\cup B}\circ\rho_{A\cup B}$ is also in $H$: as $\sigma_{A\cup B}(x)=\sigma_{A}(x)$, for some $\sigma_A\in G$, for all $x\in A$ and as $\rho_{A\cup B}(x)=\rho{A}(x)$, for some $\rho_A\in G$, for all $x\in A$, then, as $G$ is a subgroup, it's also true that $\sigma_{A}\circ\rho_{A}\in G$; therefore $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=[\sigma_{A}\circ\rho_{A}](x)$, for all $x\in A$. Obviosly all the other $x\in B$ take values $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=\sigma_{A\cup B}(\rho_{A\cup B}(x))=\sigma_{A\cup B}(x)=x$. Now, if $\sigma_{A\cup B}\in H$ is then $\sigma_{A\cup B}^{-1}\in H$? Well, if $\sigma_{A\cup B}(x)=\sigma_{A}(x)$, for all $x\in A$ and for some $\sigma_A\in G$, then, also $\sigma_A^{-1}\in G$. So define $\sigma_{A\cup B}^{-1}(x)=\sigma_A^{-1}(x)$, for all $x\in A$ and $\sigma_{A\cup B}^{-1}(x)=x$, for all $x\in B$ (identity part is its own inverse here). So, $H$ is closed with respect to inverses. Now, take $\sigma_{A\cup B},\rho_{A\cup B}\in S_{A\cup B}$. We have $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=[\sigma_A\circ\rho_A](x)$, for all $x\in A$ and for some $\sigma_A,\rho_A\in G$. But, $G$ is an Abelian subgroup so, $\sigma_A\circ\rho_A=\rho_A\circ\sigma_A$. We have that $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=[\rho_A\circ\sigma_A](x)=[\rho_{A\cup B}\circ\sigma_{A\cup B}](x)$, for all $x\in A$. Now, for all $x\in B$, $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=x$, but also $[\rho_{A\cup B}\circ\sigma_{A\cup B}](x)=x$. So, $[\sigma_{A\cup B}\circ\rho_{A\cup B}](x)=[\rho_{A\cup B}\circ\sigma_{A\cup B}](x)$, for all $x\in B$. Therefore $H$ is an Abelian subgroup of $S_{A\cup B}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Find a four-element Abelian subgroup of $S_5$. Write its table.

\noindent\newline{\bf Solution.} The construction of such group can be taken from previous problem by using previous theorem. We can take:

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\epsilon=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 2 & 3 & 4 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
f=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 4 & 3 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}\end{center}

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
3 & 4 & 1 & 2 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
4 & 3 & 2 & 1 & 5\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline It's easy to see that the following table is then valid for these permutations:

\begin{center}
\begin{tabular}{c|cccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$\\
$f$ & $f$ & $\epsilon$ & $h$ & $g$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$\\
$h$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\noindent\newline With that in mind, it's easy to see that group $\{\epsilon,f,g,h\}$ is an Abelian subgroup of $S_5$.

\noindent\newline{\bf Problem.} The subgroup of $S_5$ generated by

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 3 & 4 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 2 & 4 & 5 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline has six elements. List them, then write the table of this group.

\noindent\newline{\bf Solution.} First, we will find $g^{-1}$ and $h^{-1}$. That is:

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
g^{-1}=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 3 & 4 & 5\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
h^{-1}=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 2 & 5 & 3 & 4\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline Notice that $g=g^{-1}$, that is, $g$ is its own inverse. Therefore, $g\circ g=\epsilon$. Now, to find the rest, we have:

\begin{center}\begin{parbox}{0.4\linewidth}{
\begin{equation*}
g\circ h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 4 & 5 & 3\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.4\linewidth}{
\begin{equation*}
h\circ g\circ h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 5 & 3 & 4\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\begin{equation*}
g\circ h\circ g\circ h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 2 & 5 & 3 & 4\\
\end{array}\right),
\end{equation*}

\noindent\newline Notice that $g\circ h\circ g\circ h=h^{-1}$, we can try and see what happens when we compose this further with $g$. But notice also that we did not verify $g\circ g\circ h$, as due to associativity, and the fact that $g$ is its own inverse it's true that $g\circ(g\circ h)=(g\circ g)\circ h=\epsilon\circ h=h$. Further:

\begin{equation*}
g\circ h^{-1}=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 5 & 3 & 4\\
\end{array}\right).
\end{equation*}

\noindent\newline This is $g\circ h^{-1}=h\circ g\circ h$. Now, finally (we need not chech $h^{-1}\circ g^{-1}$ or the other way round as $g=g^{-1}$):

\begin{equation*}
h^{-1}\circ g=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 5 & 3 & 4\\
\end{array}\right),
\end{equation*}

\noindent\newline which is $h^{-1}\circ g=h\circ g\circ h$. We will denote $g=g^{-1}$, then $g\circ h=f$, then $h\circ g\circ h=i$ and $h^{-1}=j$. Also:

\begin{equation*}
h\circ g=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
2 & 1 & 4 & 5 & 3\\
\end{array}\right),
\end{equation*}

\noindent\newline that is, $h\circ g=g\circ h=f$. Now, we need $h\circ h$:

\begin{equation*}
h\circ h=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 2 & 5 & 3 & 4\\
\end{array}\right),
\end{equation*}

\noindent\newline which is really $h^{-1}=j$. The multiplication table is:

\begin{center}
\begin{tabular}{c|cccccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$ & $i$ & $j$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$ & $i$ & $j$\\
$f$ & $f$ & $j$ & $h$ & $i$ & $\epsilon$ & $g$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$ & $j$ & $i$\\
$h$ & $h$ & $i$ & $f$ & $j$ & $g$ & $\epsilon$\\
$i$ & $i$ & $\epsilon$ & $j$ & $g$ & $h$ & $f$\\
$j$ & $j$ & $g$ & $i$ & $\epsilon$ & $f$ & $h$\\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} In each of the following, $A$ is the subset of $\R$ and $G$ is a set of permutations of $A$. Show that $G$ is a subgroup of $S_A$, and write the table of $G$.

\begin{enumerate}
\item $A$ is the set of all $x\in\R$ such that $x\neq0,1$. $G=\{\epsilon,f,g\}$, where $f(x)=\frac{1}{1-x}$ and $g(x)=\frac{x-1}{x}$;

\item $A$ is the set of all the nonzero real numbers. $G=\{\epsilon,f,g,h\}$, where $f(x)=\frac{1}{x}$, $g(x)=-x$ and $h(x)=-\frac{1}{x}$;

\item $A$ is the set of all the real numbers $x\neq 0,1$. $G=\{\epsilon,f,g,h,j,k\}$, where $f(x)=1-x$, $g(x)=\frac{1}{x}$, $h(x)=\frac{1}{1-x}$, $j(x)=\frac{x-1}{x}$ and $k(x)=\frac{x}{x-1}$;

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $A$ is the set of all $x\in\R$ such that $x\neq0,1$. $G=\{\epsilon,f,g\}$, where $f(x)=\frac{1}{1-x}$ and $g(x)=\frac{x-1}{x}$.} It's obvious that $f:A\rightarrow A$ and $g:A\rightarrow A$, so they are permutations of $A$ and therefore in $S_A$. So, $G\subset S_A$. Now, to find the compositions:

\begin{equation*}
[g\circ f](x)=\frac{\frac{1}{1-x}-1}{\frac{1}{1-x}}=\frac{\frac{x}{1-x}}{\frac{1}{1-x}}=x.
\end{equation*}

\noindent\newline Obviously $f\circ g=\epsilon$. Further:

\begin{equation*}
[f\circ g](x)=\frac{1}{1-\frac{x-1}{x}}=\frac{1}{\frac{1}{x}}=x.
\end{equation*}

\noindent\newline So, $f\circ g=g\circ f=\epsilon$. This means that $G$ is closed with respect to inverses. Last,

\begin{equation*}
[f\circ f](x)=\frac{1}{1-\frac{1}{1-x}}=\frac{1}{\frac{-x}{1-x}}=\frac{x-1}{x}=g(x),
\end{equation*}

\begin{equation*}
[g\circ g](x)=\frac{\frac{x-1}{x}-1}{\frac{x-1}{x}}=\frac{\frac{-1}{x}}{\frac{x-1}{x}}=\frac{1}{1-x}=f(x).
\end{equation*}

\noindent\newline So, $G$ is closed under composition and, in conclusion, $G$ is a subgroup of $S_A$. Table:

\begin{center}
\begin{tabular}{c|ccc}
$\circ$ & $\epsilon$ & $f$ & $g$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$\\
$f$ & $f$ & $g$ & $\epsilon$\\
$g$ & $g$ & $\epsilon$ & $f$\\
\end{tabular}
\end{center}

\item {\it $A$ is the set of all the nonzero real numbers. $G=\{\epsilon,f,g,h\}$, where $f(x)=\frac{1}{x}$, $g(x)=-x$ and $h(x)=-\frac{1}{x}$.} As in previous example it's obvious that $f$, $g$ and $h$ are permutations of $A$. Now, $f(g(x))=g(f(x))=-\frac{1}{x}=h(x)$. Also, $h(f(x))=f(h(x))=-x=g(x)$. Then, $g(h(x))=h(g(x))=\frac{1}{x}=f(x)$. So, $G$ is closed under composition. Now, $g(g(x))=x$, $f(f(x))=x$ and $h(h(x))=x$, i.e. every element in $G$ is its own inverse. Thus, $G$ is a subgroup of $S_A$. Table:

\begin{center}
\begin{tabular}{c|cccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$\\
$f$ & $f$ & $\epsilon$ & $h$ & $g$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$\\
$h$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\item {\it $A$ is the set of all the real numbers $x\neq 0,1$. $G=\{\epsilon,f,g,h,j,k\}$, where $f(x)=1-x$, $g(x)=\frac{1}{x}$, $h(x)=\frac{1}{1-x}$, $j(x)=\frac{x-1}{x}$ and $k(x)=\frac{x}{x-1}$.} Obviously, $f$, $g$, $h$ and $j$ are permutations of $A$. The following calculations will explicitly give us the table and show that closure under composition is valid, as well as that every element has an inverse:

\begin{eqnarray*}
\left[g\circ f\right](x)&=&g(f(x))=g(1-x)=\frac{1}{1-x}=h(x),\\
\left[f\circ g\right](x)&=&1-\frac{1}{x}=\frac{x-1}{x}=j(x),\\
\left[h\circ f\right](x)&=&\frac{1}{1-1+x}=\frac{1}{x}=g(x),\\
\left[f\circ h\right](x)&=&1-\frac{1}{1-x}=\frac{x}{x-1}=k(x),\\
\end{eqnarray*}

\begin{eqnarray*}
\left[h\circ g\right](x)&=&\frac{1}{1-\frac{1}{x}}=\frac{1}{\frac{x-1}{x}}=\frac{x}{x-1}=k(x),\\
\left[g\circ h\right](x)&=&\frac{1}{\frac{1}{1-x}}=1-x=f(x),\\
\left[j\circ f\right](x)&=&\frac{1-x-1}{1-x}=\frac{x}{x-1}=k(x),\\
\left[f\circ j\right](x)&=&1-\frac{x-1}{x}=\frac{x-x+1}{x}=\frac{1}{x}=g(x),\\
\end{eqnarray*}

\begin{eqnarray*}
\left[g\circ j\right](x)&=&\frac{x}{x-1}=k(x),\\
\left[j\circ g\right](x)&=&\frac{\frac{1}{x}-1}{\frac{1}{x}}=\frac{\frac{1-x}{x}}{\frac{1}{x}}=1-x=f(x),\\
\left[h\circ j\right](x)&=&\frac{1}{1-\frac{x-1}{x}}=\frac{1}{\frac{1}{x}}=x=\epsilon(x),\\
\left[j\circ h\right](x)&=&\frac{\frac{1}{1-x}-1}{\frac{1}{1-x}}=\frac{\frac{x}{1-x}}{\frac{1}{1-x}}=x=\epsilon(x),\\
\end{eqnarray*}

\begin{eqnarray*}
\left[f\circ k\right](x)&=&1-\frac{x}{x-1}=\frac{1}{1-x}=h(x),\\
\left[k\circ f\right](x)&=&\frac{x-1}{x}=j(x),\\
\left[g\circ k\right](x)&=&\frac{x-1}{x}=j(x),\\
\left[k\circ g\right](x)&=&\frac{\frac{1}{x}}{\frac{1}{x}-1}=\frac{\frac{1}{x}}{\frac{1-x}{x}}=\frac{1}{1-x}=h(x),\\
\end{eqnarray*}

\begin{eqnarray*}
\left[h\circ k\right](x)&=&\frac{1}{1-\frac{x}{x-1}}=\frac{1}{\frac{1}{1-x}}=1-x=f(x),\\
\left[k\circ h\right](x)&=&\frac{\frac{1}{1-x}}{\frac{1}{1-x}-1}=\frac{\frac{1}{1-x}}{\frac{x}{1-x}}=\frac{1}{x}=g(x),\\
\left[j\circ k\right](x)&=&\frac{\frac{x}{x-1}-1}{\frac{x}{x-1}}=\frac{\frac{1}{x-1}}{\frac{x}{x-1}}=\frac{1}{x}=g(x),\\
\left[k\circ j\right](x)&=&\frac{\frac{x-1}{x}}{\frac{x-1}{x}-1}=\frac{\frac{1-x}{x}}{\frac{1}{x}}=1-x=f(x),\\
\end{eqnarray*}

\begin{eqnarray*}
\left[f\circ f\right](x)&=&1-(1-x)=x=\epsilon(x),\\
\left[g\circ g\right](x)&=&\frac{1}{\frac{1}{x}}=x=\epsilon(x),\\
\left[h\circ h\right](x)&=&\frac{1}{1-\frac{1}{1-x}}=\frac{1}{\frac{x}{x-1}}=\frac{x-1}{x}=j(x),\\
\left[j\circ j\right](x)&=&\frac{\frac{x-1}{x}-1}{\frac{x-1}{x}}=\frac{\frac{1}{x}}{\frac{1-x}{x}}=\frac{1}{1-x}=h(x),\\
\left[k\circ k\right](x)&=&\frac{\frac{x}{x-1}}{\frac{x}{x-1}-1}=\frac{\frac{x}{x-1}}{\frac{1}{x-1}}=x=\epsilon(x).
\end{eqnarray*}

\noindent\newline It is evident now that $G$ is a subgroup of $S_A$ with the following table:


\begin{center}
\begin{tabular}{c|cccccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$ & $j$ & $k$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$ & $j$ & $k$\\
$f$ & $f$ & $\epsilon$ & $j$ & $k$ & $g$ & $h$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$ & $k$ & $j$\\
$h$ & $h$ & $g$ & $k$ & $j$ & $\epsilon$ & $f$\\
$j$ & $j$ & $k$ & $f$ & $\epsilon$ & $h$ & $g$\\
$k$ & $k$ & $j$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\end{enumerate}

\noindent{\bf Problem.} For each integer $n$, define $f_n$ by $f_n(x)=x+n$.

\begin{enumerate}
\item Prove that for each integer $n$, $f_n$ is a permutation of $\R$, that is, $f_n\in S_{\R}$;

\item Prove that $f_n\circ f_m=f_{n+m}$ and $f^{-1}=f_{-n}$;

\item Let $G=\{f_n:\ n\in\Z\}$. Prove that $G$ is a subgroup of $S_{\R}$;

\item Prove that $G$ is cyclic. (Indicate a generator of $G$.)
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Prove that for each integer $n$, $f_n(x)=x+n$ is a permutation of $\R$, that is, $f_n\in S_{\R}$.} We need to show that $f_n$ is a bijection of the form $f_n:\R\rightarrow\R$. Obviously, $\textnormal{Im}f=\R$ as addition is closed in the set of real numbers, i.e. if we take $y\in R$ then there is $x\in\R$, $x=y-n$ so that $y=x+n$, makin $f_n$ surjective. If we take $f_n(a)=f_n(b)$, then $a+n=b+n$, which implies $a=b$. Thus, $f_n$ is injective and therefore, because it is also surjective, it is bijective. As it's domain is the same as its codomain, that is $\R$, it is a permutation of $\R$, by definition.

\item {\it Prove that $f_n\circ f_m=f_{n+m}$ and $f^{-1}=f_{-n}$.} See that $[f_n\circ f_m](x)=(x+m)+n=x+(m+n)=f_{m+n}(x)$. Then, as in previous problem we had $x=y-n$, then $f^{-1}(x)=x-n$; we can chech that by $[f\circ f^{-1}](x)=(x-n)+n=x$ and $[f^{-1}\circ f](x)=(x+n)-n=x$. Thus, $f^{-1}(x)=x-n=x+(-n)=f_{-n}(x)$.

\item {\it Let $G=\{f_n:\ n\in\Z\}$. Prove that $G$ is a subgroup of $S_{\R}$.} We have already proved that $f_n$ is a permutation for every $n\in\Z$, so $G\subset S_{\R}$. Then, if we take $f_n,f_m\in G$, from the second problem, it follows that its composition $f_{n+m}$ is in $G$, as $n+m\in\Z$, for every $n,m\in\Z$. From the third problem we have that there is an inverse for every $f_n\in G$ and that is $f_{-n}$ and if $n\in\Z$, then $-n\in\Z$. In conclusion, $G$ is a subgroup of $S_{\R}$.

\item {\it Prove that $G$ is cyclic.} We can take $\left\langle f_{1}\right\rangle$ as the generator of $G$. Then, it must also contain it's inverse $f_{-1}$. Obviously we can get a neutral element by $f_{1}\circ f_{-1}$ and $f_{-1}\circ f_{1}$. Now, for every $n\in\N$, we can have $f_{-n}$ by taking $\underbrace{f_{-1}\circ\ldots\circ f_{-1}}_{n\textnormal{ times}}$ which can be shown to be valid by mathematical induction. We take $n=1$ for a base and then we have just $f_{-1}$. Suppose that for some $n$ we have $f_{-n}$ by the forementioned line of reasoning. Then we need to prove that for $(n+1)$ we have $f_{-(n+1)}$. See that $f_{-1}\circ f_{-n}=(x+(-n))+(-1)=x+(-(n+1))=f_{-(n+1)}$. As, by assumption of induction we got $f_{-n}$ by applying composition of $f_{-1}$ $n$ times, we got $f_{-(n+1)}$ by applying $f_{-1}$ one more time and that is $(n+1)$ times. By the same reasoning we prove that we have $f_n$ by composing $f_{1}$ with itself $n$ times (actually $n-1$ times, if we want to be rigorous about the linguistic part of what the author is trying to say, but the point is clear). Therefore, $G$ is cyclic and $G=\left\langle f_{1}\right\rangle$.
\end{enumerate}

\noindent{\bf Problem.} For any pair of real numbers $a\neq0$ and $b$, define a function $f_{a,b}$ as follows: $f_{a,b}(x)=a x+b$.

\begin{enumerate}
\item Prove that $f_{a,b}$ is a permutation of $\R$, that is, $f_{a,b}\in S_{\R}$;

\item Prove that $f_{a,b}\circ f_{c,d}=f_{a c,a d+b}$;

\item Prove that $f^{-1}_{a,b}=f_{\frac{1}{a},-\frac{b}{a}}$;

\item Let $G=\{f_{a,b}:\ a,b\in\R,\ a\neq0\}$. Show that $G$ is a subgroup of $S_{\R}$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Prove that $f_{a,b}=a x+b$ is a permutation of $\R$, that is, $f_{a,b}\in S_{\R}$.} We're dealing with a linear function here, we know it to be of the form $f_{a,b}:\R\rightarrow\R$ and that it is bijective. Therefore $f_{a,b}\in S{\R}$ (the reader can easily check it the same way as in previous problem).

\item {\it Prove that $f_{a,b}\circ f_{c,d}=f_{a c,a d+b}$.} See that $[f_{a,b}\circ f_{c,d}](x)=a (c x+d)+b=(a c)x+(a d+b)=f_{a c,a d+b}(x)$.

\item {\it Prove that $f^{-1}_{a,b}=f_{\frac{1}{a},-\frac{b}{a}}$.} Take $x\in\textnormal{cod}f=\R$. We need to find such $y\in\textnormal{dom}f=\R$ so that $x=f(y)$. We have $x=a y+b$ and that is $x-b=a y$, i.e. $y=\frac{1}{a}x-\frac{b}{a}$. We can take $f^{-1}_{a,b}(x)=y$ and then it is $f^{-1}_{a,b}=\frac{1}{a}x-\frac{b}{a}=f_{\frac{1}{a},-\frac{b}{a}}$. Obviously, by previous problem $f_{a,b}\circ f_{\frac{1}{a},-\frac{b}{a}}=f_{a\circ\frac{1}{a},-a\frac{b}{a}+b}=f_{1,0}$, which is a neutral element, as $f_{1,0}(x)=x$, obviously an identity function.

\item {\it Let $G=\{f_{a,b}:\ a,b\in\R,\ a\neq0\}$. Show that $G$ is a subgroup of $S_{\R}$.} From the first problem, we proved that if $f_{a,b}\in G$, then $f_{a,b}\in S_{\R}$, so $G\subset S_{\R}$. Then, the second problem tells us that $G$ is closed with respect to composition and the third that it's closed with respect to inverses. Therefore, $G$ is a subgroup of $S_{\R}$.
\end{enumerate}

\noindent\newline{\bf Definition.} The {\bf symmetries of a polynomial $p$} are all the permutations of the subscripts which leave $p$ unchanged. They form a group of permutations.

\noindent\newline{\bf Problem.} List the symmetries of each of the following polynomials, and write their group table.

\begin{enumerate}
\item $p=x_1 x_2+x_2 x_3$;
\item $p=(x_1-x_2)(x_2-x_3)(x_1-x_3)$;
\item $p=x_1 x_2+x_2 x_3+x_1 x_3$;
\item $p=(x_1-x_2)(x_3-x_4)$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item $p=x_1 x_2+x_2 x_3$. The polynomial remains unchanged if we switch $x_1$ with $x_3$:

\begin{equation*}
f=\left(\begin{array}{ccc}
1 & 2 & 3\\
3 & 2 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline Obviously we have the identity $\epsilon$ and the inverse of $f$ which is, easy to see, $f$ itself. Therefore the table is:

\begin{center}
\begin{tabular}{c|cc}
$\circ$ & $\epsilon$ & $f$\\
\hline
$\epsilon$ & $\epsilon$ & $f$\\
$f$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\item $p=(x_1-x_2)(x_2-x_3)(x_1-x_3)$. One can easily see that the polynomial remains equal for even commutations of members in the brackets. Notice that, for odd commutations, it will be of opposite sign, e.g. $p\neq -(x_2-x_1)(x_2-x_3)(x_1-x_3)$. But, to analyze the first case, we change the order of subtraction in the first and third brackets. We have $p=(x_2-x_1)(x_2-x_3)(x_3-x_1)$. Notice that we can put these into correspondence with the first polynomial by permuting the order of bracket multiplication (notice that $x_1$ appears in first and last brackets twice as first member, and in the second polynomial it's the same with $x_2$), i.e. $p=(x_2-x_1)(x_3-x_1)(x_2-x_3)$. Now, we notice that it still differs from the first polynomial (considering notation) as in the original, $x_3$ appears twice in second and third brackets as the second member. So we permute again, but only the first and third in our new polynomial to obtain $p=(x_2-x_3)(x_3-x_1)(x_2-x_1)$. It's easy to see that now we can put indices in correspondence by:

\begin{equation*}
f=\left(\begin{array}{ccc}
1 & 2 & 3\\
2 & 3 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline Now, by the same reasoning we can obtain $p=(x_1-x_2)(x_3-x_2)(x_3-x_1)$ (by switching members in second and third parentheses). We permute the brackets to correspond with original and get $p=(x_3-x_1)(x_1-x_2)(x_3-x_2)$. Now we see that indices can be put into correspondence by

\begin{equation*}
g=\left(\begin{array}{ccc}
1 & 2 & 3\\
3 & 1 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline By careful examination, one can see that, by interchanging members in first and second parentheses, there is no way that indices of $p=(x_2-x_1)(x_3-x_2)(x_1-x_3)$ can be put into correspondence (by means of permutation, at least) with the indices of the original. Therefore, we are only left with $f$ and $g$. It's now easy to see that $fg=gf=\epsilon$, $ff=g$ and $gg=f$. So, we have a table:

\begin{center}
\begin{tabular}{c|ccc}
$\circ$ & $\epsilon$ & $f$ & $g$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$\\
$f$ & $f$ & $g$ & $\epsilon$\\
$g$ & $g$ & $\epsilon$ & $f$\\
\end{tabular}
\end{center}

\item $p=x_1 x_2+x_2 x_3+x_1 x_3$. Polynomial $p=x_3 x_1+x_1 x_2+x_3 x_2$ is equal to the forementioned one and can be obtained by permutation:

\begin{equation*}
f=\left(\begin{array}{ccc}
1 & 2 & 3\\
3 & 1 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline Furthermore, polynomial $p=x_2 x_1+x_1 x_3+x_2 x_3$ can be obtained by:

\begin{equation*}
g=\left(\begin{array}{ccc}
1 & 2 & 3\\
2 & 1 & 3\\
\end{array}\right).
\end{equation*}

\noindent\newline Polynomial $p=x_2 x_3+x_3 x_1+x_2 x_1$ is obtained from the first one by permutation:

\begin{equation*}
h=\left(\begin{array}{ccc}
1 & 2 & 3\\
2 & 3 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline Now, we have $p=x_1 x_3+x_3 x_2+x_1 x_2$ by:

\begin{equation*}
j=\left(\begin{array}{ccc}
1 & 2 & 3\\
1 & 3 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline Finally, exempting a trivial neutral element we have $p=x_3 x_2+x_2 x_1+x_3 x_1$ obtained by permutation:

\begin{equation*}
k=\left(\begin{array}{ccc}
1 & 2 & 3\\
3 & 2 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline Multiplication table is as follows:

\begin{center}
\begin{tabular}{c|cccccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$ & $j$ & $k$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$ & $j$ & $k$\\
$f$ & $f$ & $h$ & $j$ & $\epsilon$ & $k$ & $g$\\
$g$ & $g$ & $k$ & $\epsilon$ & $j$ & $h$ & $f$\\
$h$ & $h$ & $\epsilon$ & $k$ & $f$ & $g$ & $j$\\
$j$ & $j$ & $g$ & $f$ & $k$ & $\epsilon$ & $h$\\
$k$ & $k$ & $j$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\item $p=(x_1-x_2)(x_3-x_4)$. If we switch $x_1$ and $x_4$, we also have to switch $x_2$ and $x_3$ so we have $p=(x_4-x_3)(x_2-x_1)$ (obviously equal to our initial polynomial as we did an even number of permutations on operation of subtraction). That corresponds to:

\begin{equation*}
f=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
4 & 3 & 2 & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline Also, we can exchange $x_1$ with $x_2$ and $x_3$ with $x_4$ to get $p=(x_2-x_1)(x_4-x_3)$ (it's equal to the initial polynomial for the same reason as in previous permutation). So we have:

\begin{equation*}
g=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
2 & 1 & 4 & 3\\
\end{array}\right).
\end{equation*}

\noindent\newline Furthermore, we can permute indices so that we have $p=(x_3-x_4)(x_1-x_2)$ which is represented by:

\begin{equation*}
h=\left(\begin{array}{cccc}
1 & 2 & 3 & 4\\
3 & 4 & 1 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline Note that $f^{-1}=f$, $g^{-1}=g$ and $h^{-1}=h$. Also, $f g=g f=h$, $f h=h f=g$ and $g h=h g=f$, so we have the following table of multiplication:

\begin{center}
\begin{tabular}{c|cccc}
$\circ$ & $\epsilon$ & $f$ & $g$ & $h$\\
\hline
$\epsilon$ & $\epsilon$ & $f$ & $g$ & $h$\\
$f$ & $f$ & $\epsilon$ & $h$ & $g$\\
$g$ & $g$ & $h$ & $\epsilon$ & $f$\\
$h$ & $h$ & $g$ & $f$ & $\epsilon$\\
\end{tabular}
\end{center}

\end{enumerate}

\noindent{\bf Problem.} Let $A$ be a set and $a\in A$. Let $G$ be the subset of $S_A$ consisting of all the permutations $f$ of $A$ such that $f(a)=a$. Prove that $G$ is a subgroup of $S_A$.

\noindent\newline{\bf Solution.} It's true that $G\subseteq S_A$, so we first have to prove that for some $f,g\in G$ it's true that $f g\in G$. Now, as $f,g\in G$, then $f(a)=a$ and $g(a)=a$ for some $a\in A$. But, $f(g(a))=f(a)=a$, and as $G$ contains all permutations in $S_A$ with this property, it must contain $f g$, i.e. $f g\in G$. If $f\in G$, then $f(a)=a$. As $S_A$ is a group there is $f^{-1}\in S_A$, such that $f f^{-1}=f^{-1} f=\epsilon$, i.e. $f(f^{-1}(x))=f^{-1}(f(x))=x$, for all $x\in A$. Therefore, from $f(a)=a$ we have $f^{-1}(f(a))=f^{-1}(a)$, that is $a=f^{-1}(a)$. As this is true, then $f^{-1}\in G$ and by that it follows that $G$ is a subgroup of $S_A$.

\noindent\newline{\bf Problem.} If $f$ is a permutation of $A$ and $a\in A$, we say that {\it $f$ moves $a$} if $f(a)\neq a$. Let $A$ be an infinite set, and let $G$ be the subset of $S_A$ which consists of all the permutations $f$ of $A$ which move only a finite number of elements of $A$. Prove that $G$ is a subgroup of $S_A$.

\noindent\newline{\bf Solution.} Obviously, $G\subset S_A$. Let $P_n$ denote the set $\{p_1,\ldots,p_n\}$, for some $n\in\N$. Take $f,g\in G$. We need to show that $f g\in G$. It's true that $f(p_i)\neq p_i$ and $g(p_i)\neq p_i$, for all $i=\{1,\ldots,n\}$. That can be written as $f(p_i),g(p_i)\in A\backslash P_n$. We need to show that $f(g(p_i))\neq p_i$. As $g(p_i)\neq p_i$, we need to check what happens with $f(a)$ where $a=g(p_i)\in A\backslash P_n$. If $f(a)=p_i$ then it would mean that $f$ moves $a$ and it would be in $P_n$, leading to a contradiction. Therefore, it must be that $f(a)$ is $a$, i.e. $f(g(p_i))=g(p_i)\neq p_i$, for all $i$. Therefore, $f g\in G$. Now, if we take $f\in G$, we need to check whether $f^{-1}\in G$. As $f(p_i)\neq p_i$, suppose that $f^{-1}(p_i)=p_i$. Then it would mean that $f(f^{-1}(p_i))=f(p_i)$, i.e. $p_i=f(p_i)$, which is a contradiction to the assumption that $f$ moves all $p_i$. Therefore, $f^{-1}\in G$ and, by that, $G$ is a subgroup of $S_A$.

\noindent\newline{\bf Problem.} Let $A$ be a finite set, and $B$ a subset of $A$. Let $G$ be the subset of $S_A$ consisting of all the permutations $f$ of $A$ such that $f(x)\in B$ for every $x\in B$. Prove that $G$ is a subgroup of $S_A$. Give an example to show that the conclusion is not necessarily true if $A$ is an infinite set.

\noindent\newline{\bf Solution.} As $G\subseteq S_A$ by definition, we need only check composition. A careful reader should remember that if $G\subset S_A$ and $G$ is closed with respect to composition and finite, then $G$ is a subgroup of $S_A$. Take $f,g\in G$. Then, $f(x)\in B$ and $g(x)\in B$ for every $x\in B$. It's easy to see that $f(g(x))\in B$ as $a=g(x)\in B$ for all $x\in B$ and $f(a)\in B$ because $a=g(x)\in B$, for all $x\in B$. Now, if we were to take in consideration inverses, we would be left with vague and unclear expressions. If we were to consider $f^{-1}(f(x))=x$, we would have that $f(x)\in B$ for all $x\in B$. Now, all information that we have is that always $f^{-1}(f(x))\in B$ for all $x\in B$. Actually, $f^{-1}(y)\in B$ whenever $y=f(x)$ and $f(x)\in B$, but that leaves a lot of empty spaces for some $y$ that are in $B$ but are not equal to $f(x)$; in other words we don't know how $f(x)$ behaves. So we have to call upon the previous problem, that $A$ is finite (and by that $S_A$ also). Then, without checking inverses, we know that $G$ is a subgroup of $S_A$ only by the fact that it's closed with respect to composition and that $G\subseteq S_A$. {\it Comment.} Now, we will give a counterexpamle, why $G$ is not a subgroup of $S_A$ if $A$ is infinite. We can take $A=\Q$. Now $S_A$ will contain permutations $f_a:A\rightarrow A$ with $f_a=a x$, where $a\in\Q$. Now if we take $B=\N$, presumed group $G$ will contain $f_b:A\rightarrow A$ with $f_b=b x$ where $b\in\N$ (it will leave elements in $\N$ fixed and will therefore be in $G$). It will be closed with respect to multiplication as $[f_a\circ f_b](x)=a b x$ and $a b\in\N$, but the inverses will be a problem as $f^{-1}_b(x)=\frac{1}{b x}=\frac{1}{b}x$ and $\frac{1}{b}\notin\N$ unless $b=1$.

\newpage

\begin{center}
{\bf Finite groups of permutations}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Every permutation $f:A\rightarrow A$, where $A$ is finite, for which exists some $a\in A$ such that $[f\circ f\circ\cdots\circ f](a)=a$ and $[f\circ f](a)\neq a$ and $[f\circ f](x)=x$, for all $x\in A\backslash\{a\}$, is called a {\bf cycle}.

\noindent\newline{\bf Example.} Following permutation is a cycle:

\begin{equation*}
f=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
1 & 4 & 3 & 5 & 2
\end{array}\right).
\end{equation*}

\noindent Such permutation can be also written as $f=(2\ 4\ 5)$ to signify that $f$ moves $2$ to $4$ then $4$ to $5$ and finally $5$ to $2$. We can write that fact down also as $2\rightarrow 4\rightarrow 5\rightarrow 2$.

\noindent\newline{\bf Example.} In previous example, $f$ was a cycle. Let's observe a permutation that is not a cycle:

\begin{equation*}
g=\left(\begin{array}{ccccc}
1 & 2 & 3 & 4 & 5\\
3 & 4 & 1 & 5 & 2
\end{array}\right).
\end{equation*}

\noindent\newline We can see that $2\rightarrow 4\rightarrow 5\rightarrow 2$, but the other elements do not remain fixed as $1\rightarrow 3\rightarrow 1$. But now, $g$ can be written down as a composition of two disjoint (they share no elements in common while completing a cycle) cycles $g_1=(2\ 4\ 5)$ and $g_2=(1\ 3)$ such that $g=g_1\circ g_2$. We can easily see that it will really produce $g$ (we can write that fact down as $g=(2\ 4\ 5)(1\ 3)$). Now we have a sufficient reason to believe that the following theorem holds and can be proved.

\noindent\newline{\bf Theorem.} Every permutation that is not identity is either a cycle or a composition of disjoint cycles.

\noindent\newline{\bf Proof.} Assume that $f:A\rightarrow A$ is not identity, i.e. there are some $x\in A$ for which $f(x)\neq x$. Now we take that $x$ (we will denote it by $x_1$) and take $f(x_1)=x_2$. If $f(x_2)=x_1$ we have a first cycle. If not, we follow $f(x_2)=x_3$ and so on. We either (as we're dealing with a finite group) exhaust all the elements and thus the permutation is a (single) cycle, or we reach a repetition for $x_k$ in the chain $x_1\rightarrow x_2\rightarrow\cdots\rightarrow x_k$, for some $k<|A|$. It's easy to see that it must be $f(x_k)=x_1$, as if it were some other $x_i$ it would follow that $f(x_{i-1})=x_i$ and $f(x_k)=x_i$ and that would indicate a breach of definition of permutation as an injection and by that as a bijection. Next, we choose another element $y_1\in A$ for which $f(y_1)\neq y_1$ and do the same line of reasoning. As there is a finite number of elements in $A$, we have to reach the end sooner or later. Taking composition of those cycles gives the permutation we started with, as they are all disjoint (if they were not it would mean that some $f(x_i)=y_j$ for some $i,j\leq|A|$, again implicating a breach in definition of permutation as also $f(x_i)=x_{i+1}$).

\begin{flushright}
$\square$\\
\end{flushright} 

\noindent{\bf Lemma.} Disjoint cycles commute.

\noindent\newline{\bf Proof.} Follows directly from the fact that they are disjoint, e.g. if we take $f=f_1 f_2$ such that $f_1=(a_1\ldots a_k)$ and $f_2=(a_{k+1}\ldots a_n)$, then, $f_2$ will permute elements $a_{k+1}\ldots a_n$ leaving others unchanged (the ones for $f_1$). Similarly, if we take $f_1$ first it will leave elements which $f_2$ permutes unchanged.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If $f=(a_1\ldots a_n)$ is a cycle, then $n$ is called the {\bf length of the cycle} $f$.

\noindent\newline{\bf Theorem.} Decomposition of a permutation into cycles is unique up to the order of the cycles.

\noindent\newline{\bf Proof.} Suppose that $f=f_1 f_2\cdots f_n$ and $f'=g_1 g_2\cdots g_m$, where $f_i,g_j$ are cycles for all $i\in\{1,\ldots,n\}$ and $j\in\{1,\ldots,m\}$, where $n<m$. Then $f_1 f_2\cdots f_n=g_1 g_2\cdots g_m$. But, as all $f_i$ and $g_j$ are permutations, they are also bijections and have inverse elements. Therefore, we can take $g^{-1}_1 f_1 f_2 \cdots f_n=g_2\cdots g_m$. Now, there must be some $f_i$ that shares some element with $g^{-1}_1$. If that were not so, that would mean that for $f$ some element remains fixed, while for $f'$ it does not and $f\neq f'$, leading to contradiction. So, suppose that $f_1$, without loss of generality shares some element in common with $g^{-1}_1$ (by previous lemma they commute). Because cycles are disjoint, no other $f_i$ can have that same element, and due to the fact that permutations are functions and bijections, it must be that $g^{-1}_1 f_1=\epsilon$. Using the same process we arrive at $\epsilon=g_{n+1}\cdots g_m$, meaning that we can disregard $g_{n+1}\cdots g_m$. So it must be that $f=f'$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Compute each of the following products in $S_9$:

\begin{enumerate}
\item $(1\ 4\ 5)(3\ 7)(6\ 8\ 2)$;
\item $(1\ 7)(6\ 2\ 8)(9\ 3\ 5\ 4)$;
\item $(7\ 1\ 8\ 2\ 5)(3\ 6)(4\ 9)$;
\item $(1\ 2)(3\ 4\ 7)$;
\item $(1\ 4\ 7)(1\ 6\ 7\ 8)(7\ 4\ 1\ 3\ 2)$;
\item $(6\ 1\ 4\ 8)(2\ 3\ 4\ 5)(1\ 2\ 4\ 9\ 3)$.
\end{enumerate}

\noindent{\bf Solution.} We have to remind ourselves that first operations performed are those on the right and observe what happens for every $i\in\{1,2,\ldots,9\}$.

\begin{enumerate}
\item $(1\ 4\ 5)(3\ 7)(6\ 8\ 2)$. First cycle leaves $1$ fixed, second one also, but the third one carries it to $4$. First cycle carries $2$ to $6$ and all the others leave $6$ fixed. First cycle leaves $3$ fixed, second carries it to $7$ and third one leaves $7$ fixed. Thus reasoning we arrive at:

\begin{equation*}
(1\ 4\ 5)(3\ 7)(6\ 8\ 2)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
4 & 6 & 7 & 5 & 1 & 8 & 3 & 2 & 9\\
\end{array}\right).
\end{equation*}

\item $(1\ 7)(6\ 2\ 8)(9\ 3\ 5\ 4)$. We have that:

\begin{equation*}
(1\ 7)(6\ 2\ 8)(9\ 3\ 5\ 4)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
7 & 8 & 5 & 9 & 4 & 2 & 1 & 6 & 3\\
\end{array}\right).
\end{equation*}

\item $(7\ 1\ 8\ 2\ 5)(3\ 6)(4\ 9)$. After composition it's:

\begin{equation*}
(7\ 1\ 8\ 2\ 5)(3\ 6)(4\ 9)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
8 & 5 & 6 & 9 & 7 & 3 & 1 & 2 & 4\\
\end{array}\right).
\end{equation*}

\item $(1\ 2)(3\ 4\ 7)$. The result is simply:

\begin{equation*}
(1\ 2)(3\ 4\ 7)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
2 & 1 & 4 & 7 & 5 & 6 & 3 & 8 & 9\\
\end{array}\right).
\end{equation*}

\item $(1\ 4\ 7)(1\ 6\ 7\ 8)(7\ 4\ 1\ 3\ 2)$. We have:

\begin{equation*}
(1\ 4\ 7)(1\ 6\ 7\ 8)(7\ 4\ 1\ 3\ 2)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
3 & 8 & 2 & 6 & 5 & 1 & 7 & 4 & 9\\
\end{array}\right).
\end{equation*}

\item $(6\ 1\ 4\ 8)(2\ 3\ 4\ 5)(1\ 2\ 4\ 9\ 3)$. After carefully performing compositions, as now they are not disjunct (same as in previos example, actually):

\begin{equation*}
(6\ 1\ 4\ 8)(2\ 3\ 4\ 5)(1\ 2\ 4\ 9\ 3)=\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
3 & 5 & 4 & 9 & 2 & 1 & 7 & 6 & 8\\
\end{array}\right).
\end{equation*}

\end{enumerate}

\noindent{\bf Problem.} Write each of the following permutations in $S_9$ as a product of disjoint cycles:

\begin{equation*}
\textnormal{(a) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
4 & 9 & 2 & 5 & 1 & 7 & 6 & 8 & 3\\
\end{array}\right);
\end{equation*}

\begin{equation*}
\textnormal{(b) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
7 & 4 & 9 & 2 & 3 & 8 & 1 & 6 & 5\\
\end{array}\right);
\end{equation*}

\begin{equation*}
\textnormal{(c) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
7 & 9 & 5 & 3 & 1 & 2 & 4 & 8 & 6\\
\end{array}\right);
\end{equation*}

\begin{equation*}
\textnormal{(d) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
9 & 8 & 7 & 4 & 3 & 6 & 5 & 1 & 2\\
\end{array}\right).
\end{equation*}

\noindent\newline{\bf Solution.}

\begin{equation*}
\textnormal{(a) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
4 & 9 & 2 & 5 & 1 & 7 & 6 & 8 & 3\\
\end{array}\right)=(1\ 4\ 5)(2\ 9\ 3)(6\ 7).
\end{equation*}

\begin{equation*}
\textnormal{(b) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
7 & 4 & 9 & 2 & 3 & 8 & 1 & 6 & 5\\
\end{array}\right)=(1\ 7)(2\ 4)(3\ 9\ 5)(6\ 8).
\end{equation*}

\begin{equation*}
\textnormal{(c) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
7 & 9 & 5 & 3 & 1 & 2 & 4 & 8 & 6\\
\end{array}\right)=(1\ 7\ 4\ 3\ 5)(2\ 9\ 6).
\end{equation*}

\begin{equation*}
\textnormal{(d) }\left(\begin{array}{ccccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
9 & 8 & 7 & 4 & 3 & 6 & 5 & 1 & 2\\
\end{array}\right)=(1\ 9\ 2\ 8)(3\ 7\ 5).
\end{equation*}

\noindent\newline{\bf Definition.} Cycles of length $2$ are called {\bf transpositions}.

\noindent\newline{\bf Theorem.} Every cycle can be written down as a composition of transpositions.

\noindent\newline{\bf Proof.} Let $a_1\rightarrow a_2\rightarrow a_3\rightarrow\cdots\rightarrow a_{n}\rightarrow a_1$ be a cycle of length $n$ denoted as $(a_1 a_2\ldots a_n)$. We have $f(a_i)=a_{i+1}$ for all $i\in\{1,\ldots,n-1\}$ and $f(a_n)=a_1$. If we take composition $(a_1 a_3)(a_1 a_2)$ we have:

\begin{equation*}
(a_1 a_3)(a_1 a_2)=\left(\begin{array}{ccc}
a_1 & a_2 & a_3\\
a_2 & a_3 & a_1\\
\end{array}\right)=(a_1 a_2 a_3).
\end{equation*}

\noindent\newline By the same line of reasoning we see that we will have

\begin{equation*}
(a_1 a_2\ldots a_n)=(a_1 a_n)(a_1 a_{n-1})\cdots(a_1 a_3)(a_1 a_2),
\end{equation*}

\noindent\newline as $a_1$ will be carried to $a_2$ and $a_2$ doesn't appear again in the list. But, $a_2$ is carried to $a_1$ in the first transposition and to $a_3$ in the second which does not appear again; $a_3$ will be carried to $a_1$ in the second transposition and $a_1$ to $a_4$ in the third transposition. Meaning that $a_i$ will be carried to $a_{i+1}$ in $i$-th transposition, where $i\in\{1,\ldots,n-1\}$. Special case, $a_n$ will be carried to $a_1$ as $a_n$ does not appear anywhere before.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Comment.} Note that decomposition into transpositions need not be unique (as it is with disjoint cycles). We give a counterexample, and the reader can easily check that $(a_1 a_4)(a_1 a_3)(a_1 a_2)=(a_4 a_3)(a_4 a_2)(a_4 a_1)=(a_1 a_2 a_3 a_4)$.

\noindent\newline{\bf Problem.} Express each of the following as a product of transpositions in $S_8$:

\begin{enumerate}
\item $\left(1\ 3\ 7\ 4\ 2\ 8\right)$,
\item $\left(4\ 1\ 6\right)\left(8\ 2\ 3\ 5\right)$,
\item $\left(1\ 2\ 3\right)\left(4\ 5\ 6\right)\left(1\ 5\ 7\ 4\right)$,
\item $\pi=\left(\begin{array}{cccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
3 & 1 & 4 & 2 & 8 & 7 & 6 & 5\\
\end{array}\right)$
\end{enumerate}

\noindent{\bf Solution.} For each of these problems we will fix one element from each cycle and from it develop all the transpositions. Notice that we are always going from right to left, as the rightmost composition gets executed first.

\begin{enumerate}
\item $\left(1\ 3\ 7\ 4\ 2\ 8\right)=\left(1\ 8\right)\left(1\ 2\right)\left(1\ 4\right)\left(1\ 7\right)\left(1\ 3\right)$.
\item $\left(4\ 1\ 6\right)\left(8\ 2\ 3\ 5\right)=\left(4\ 6\right)\left(4\ 1\right)\left(8\ 5\right)\left(8\ 3\right)\left(8\ 2\right)$.
\item $\left(1\ 2\ 3\right)\left(4\ 5\ 6\right)\left(1\ 5\ 7\ 4\right)=\left(1\ 3\right)\left(1\ 2\right)\left(4\ 6\right)\left(4\ 5\right)\left(1\ 4\right)\left(1\ 7\right)\left(1\ 5\right)$.
\item $\pi=\left(\begin{array}{cccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
3 & 1 & 4 & 2 & 8 & 7 & 6 & 5\\
\end{array}\right)=\left(6\ 7\right)\left(5\ 8\right)\left(1\ 2\right)\left(1\ 4\right)\left(1\ 3\right).$
\end{enumerate}

\noindent{\bf Problem.} In $S_5$, write $(1\ 2\ 3\ 4\ 5)$ in five different ways as a cycle, and in five different ways as a product of transpositions.

\noindent\newline{\bf Solution.} It's easy to see that the cycle $(1\ 2\ 3\ 4\ 5)$ is invariant to rotations of elements, so we have $(2\ 3\ 4\ 5\ 1)$, $(3\ 4\ 5\ 1\ 2)$, $(4\ 5\ 1\ 2\ 3)$ and $(5\ 1\ 2\ 3\ 4)$ (five if we count the starting cycle). Now, as for transpositions, we have five elements to choose for a fixed element. So we have $(1\ 5)(1\ 4)(1\ 3)(1\ 2)$, $(2\ 1)(2\ 5)(2\ 4)(2\ 3)$, $(3\ 2)(3\ 1)(3\ 5)(3\ 4)$, $(4\ 3)(4\ 2)(4\ 1)(4\ 5)$ and $(5\ 4)(5\ 3)(5\ 2)(5\ 1)$.

\noindent\newline{\bf Problem.} In $S_5$, express each of the following as the square of a cycle: (a) $(1\ 3\ 2)$, (b) $(1\ 2\ 3\ 4\ 5)$, (c) $(1\ 3)(2\ 4)$.

\noindent\newline{\bf Solution.} (a) We have $(1\ 2\ 3)(1\ 2\ 3)=(1\ 3\ 2)$, so $(1\ 3\ 2)=(1\ 2\ 3)^2$. (b) By wild guess we can take $(1\ 4\ 2\ 5\ 3)(1\ 4\ 2\ 5\ 3)=(1\ 2\ 3\ 4\ 5)$, so $(1\ 2\ 3\ 4\ 5)=(1\ 4\ 2\ 5\ 3)^2$. (c) From the second proposition below it will be easy to see that $(1\ 2\ 3\ 4)^2=(1\ 3)(2\ 4)$.

\noindent\newline{\bf Proposition.} Let $f$ be a permutation of a finite set. If $f=\left(a_1 a_2 \ldots a_{n-1} a_n\right)$, then $f^{-1}=\left(a_n a_{n-1}\ldots a_2 a_1\right)$.

\noindent\newline{\bf Proof.} It is sufficient to show that $f f^{-1}=\epsilon$. Now, applying $f^{-1}$ first, we have $a_i\rightarrow a_{i-1}$, for all $i\in\left\{2,3,\ldots,n\right\}$. From $f$ we have $a_{i-1}\rightarrow a_i$, for all $i\in\left\{2,3,\ldots,n\right\}$. Whenever we take $a_i$ from $f^{-1}$ it goes to $a_{i-1}$; then, applying $f$ it goes back to $a_i$. Therefore, $f\left(f^{-1}(a_i)\right)=a_i$, meaning $f f^{-1}=\epsilon$ (of course, we need to mention that $a_1\rightarrow a_n$ in $f^{-1}$ and $a_n\rightarrow a_1$ in $f$, which obviously satisfies $f f^{-1}=\epsilon$ while composing). Also, if $f f^{-1}=\epsilon$, then it must be that $f^{-1} f=\epsilon$. Suppose we have that $g f=\epsilon$. It would mean that $g f f^{-1}=f^{-1}$, giving $g=f^{-1}$ again. From this follows that $f$ is indeed inverse of $f^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $f$ be a cycle $f=\left(a_1 a_2\ldots a_n\right)$ where $n\in\N$. Then $f^m$, where $m\in\N$ and $n=k m$, for some $k\in\N$, will be a product of $m$ disjoint cycles $f_i=\left(a_i a_{i+m} a_{i+2m}\ldots a_{i+(k-1)m)}\right)$, where $i\in\{1,2,\ldots,m\}$. Each $f_i$ is of length $k$.

\noindent\newline{\bf Proof.} First we may consider following example as a motivation for our proof. Take $f=\left(1\ 2\ 3\ 4\ 5\ 6\right)$ and $m=3$. Taking for $m=2$ first and then for $m=3$ we have:

\begin{eqnarray*}
f^2&=&\left(1\ 2\ 3\ 4\ 5\ 6\right)\left(1\ 2\ 3\ 4\ 5\ 6\right)=\left(1\ 3\ 5\right)\left(2\ 4\ 6\right),\\
f^3=f f^2&=&\left(1\ 2\ 3\ 4\ 5\ 6\right)\left(1\ 3\ 5\right)\left(2\ 4\ 6\right)=\left(1\ 4\right)\left(2\ 5\right)\left(3\ 6\right).
\end{eqnarray*}

\noindent\newline To proceed with the more general proof, we have that $n=k m$, where $m,n,k\in\N$. So $f$ can be written down as:

\begin{equation*}
f=\left(a_1 a_2\ldots a_{n-1} a_n\right)=\left(a_1 a_2 a_3\ldots a_{k m-1} a_{k m}\right).
\end{equation*}

\noindent\newline Before we continue, notice that $a_{i+n}=a_{i}$, for all $i\in\{1,2,\ldots,n\}$. Observe what happens with $a_1$ when we take $f^m$. For $m=2$, $a_1$ goes to $a_{2}$; for $m=3$, $a_1$ goes to $a_{2}$ and then to $a_{3}$. Continuing in this way, we may conclude that for some $m$, $a_1$ will go to $a_{1+m}$ when taking $f^m$. In other words, we skip $m$ elements for every step. When we reach $a_{1+(k-1)m}$, we have reached the end. Due to the fact that $n=m k$, we have that $a_{1+(k-1)m}=a_{1+k m-m}=a_{1+n-m}$ will go to $a_{1+n}$ and that is $a_1$. That way, we complete one cycle and can do the rest for $a_{2}$, $a_{3}$ and all up to $a_{m}$ (notice that there will be $m$ of them) and they will be disjoint. Naturally, for $a_{m}$ we'll be having $a_{m+(k-1)m}$ as the last element and that is $a_n$. As for each cycle $i$ we had elements $a_{i}$, $a_{i+m}$ ranging to $a_{i+(k-1)m}$, we have $\frac{i+(k-1)m-i}{m}+1$ of them (e.g. from $2,4,6,8,10$ we have $\frac{10-2}{2}+1=5$), and that is $\frac{(k-1)m}{m}+1=(k-1)+1=k$; in other words, length of each cycle is $k$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $f$ be a cycle $f=\left(a_1 a_2\ldots a_p\right)$, where $p$ is a prime number. Then every power of $f$ is a cycle.

\noindent\newline{\bf Proof.} We will observe the case when $f=\left(a_1\ a_2\ a_3\right)$ and take $n=2$. We will construct a sequence by repeating elements from $f$ (preserving their order) $n$ times. That way we obtain $a_1, a_2, a_3, a_1, a_2, a_3, a_1$. From that we construct a new sequence $A_1=a_1$, $A_2=a_2$, $A_3=a_3$, $A_4=a_1$ (notice that $A_4=A_{1+3}=a_1$), $A_5=a_2$ (see that $A_5=A_{1+2\cdot 2}$, $A_6=a_3$, $A_7=a_1$; also, notice that $A_7=A_{2\cdot 3+1}$). Now, as we take steps while composing permutations with itself, we always skip one member (when taking a power of $2$), that is from $A_1$ we get to $A_3$ then to $A_5$ and then to $A_7$ which is $a_1$. This way, we enclosed a cycle using all elements from $f$.

Now we will prove this for $f^n$ when $n\in\N$ and $p$ (length of $f$) is prime. We construct a sequence such that $A_{i+k p}=a_i$, for $i\in\{1,2,\ldots, p\}$ and $k\in\{0,1,2,\ldots,n-1\}$, and $A_{n p+1}=a_1$. Now, when taking a power of $n$, we skip every $n$ members in the sequence, that is, we start from $A_1$, get to $A_{1+n}$ then to $A_{1+2n}$ and so on (always of the form $A_{1+m n}$, where $m\in\N$). We can see that we will arrive at $A_{n p+1-n}$ as it's $A_{n p+1-n}=A_{1+(p-1)n}$ (here $m=p-1$). In the next step we will complete the cycle as we will have $A_{1+(p-1)n+n}=A_{1+pn-n+n}=A_{1+p n}=a_1$. The whole sequence is of length $n p+1$. And for skipping $n$ steps (and disregarding the last $A_{n p+1}=a_1$), obviously, length of the subsequence (disregarding, again, the last one) is $\frac{n p}{n}=p$. The new sequence is $A_1, A_{1+n}, A_{1+2n},\ldots, A_{1+(p-1)n}$. Suppose two of those elements are equal, i.e. $A_{1+m_1 n}=A_{1+m_2 n}$, but $m_1\neq m_2$. Take $A_{1+m_1 n}=A_{i+k_1 p}$ and $A_{1+m_2 n}=A_{i+k_2 p}$. Now, from $m_1 n=i-1+k_1 p$ (and $1\leq i\leq p$, i.e. $0\leq i-1\leq p-1<p$) we have that $m_1 n$ divided by $p$ yields $i-1$ as a remainder. Same thing goes for $m_2 n$. That would mean that $m_1 n=m_2 n+p q$ for some $q\in\Z$, i.e. $m_1\equiv m_2\pmod p$. That would be possible for $m_2=m_1+p t$, where $t\in\Z$, but $m_1$ and $m_2$ are both positive and less than $p$, so there does not exist such $t$. We have a contradiction to our statement that there exist two equal $a_i$ in our subsequence of length $p$. So it must be that all $a_i$ are used and create one cycle.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} If $\alpha=\left(3\ 7\ 1\ 4\right)$, $\beta=\left(1\ 2\ 3\right)$, and $\gamma=\left(2\ 4\ 1\ 3\ 5\right)$ in $S_7$, express each of the following as a product of disjoint cycles: (a) $\alpha^{-1}\beta$, (b) $\gamma^{-1}\alpha$, (c) $\alpha^2\beta$, (d) $\beta^2\alpha\gamma$, (e) $\gamma^4$, (f) $\gamma^3\alpha^{-1}$, (g) $\beta^{-1}\gamma$, (h) $\alpha^{-1}\gamma^2\alpha$.

\noindent\newline{\bf Solution.} Using previous propositions it will be easier to solve these problems.

\begin{eqnarray*}
\textnormal{(a)}&&\ \alpha^{-1}\beta=\left(4\ 1\ 7\ 3\right)\left(1\ 2\ 3\right)=\left(3\ 7\right)\left(1\ 2\ 4\right).\\
\textnormal{(b)}&&\ \gamma^{-1}\alpha=\left(5\ 3\ 1\ 4\ 2\right)\left(3\ 7\ 1\ 4\right)=\left(1\ 2\ 5\ 3\ 7\ 4\right).\\
\textnormal{(c)}&&\ \alpha^2\beta=\left(3\ 1\right)\left(7\ 4\right)\left(1\ 2\ 3\right)=\left(1\ 2\ 3\right)\left(7\ 4\right).\\
\textnormal{(d)}&&\ \beta^2\alpha\gamma=\left(1\ 3\ 2\right)\left(3\ 7\ 1\ 4\right)\left(2\ 4\ 1\ 3\ 5\right)=\left(1\ 7\ 3\ 5\right).\\
\textnormal{(e)}&&\ \gamma^4=(\gamma^2)^2=\left(1\ 5\ 4\ 3\ 2\right)^2=\left(1\ 4\ 2\ 5\ 3\right).\\
\textnormal{(f)}&&\ \gamma^3\alpha^{-1}=\gamma^2\gamma\alpha^{-1}=\left(1\ 2\ 3\ 4\ 5\right)\left(4\ 1\ 7\ 3\right)=\left(1\ 7\ 4\ 2\ 3\ 5\right).\\
\textnormal{(g)}&&\ \beta^{-1}\gamma=\left(3\ 2\ 1\right)\left(2\ 4\ 1\ 3\ 5\right)=\left(1\ 2\ 4\ 3\ 5\right).\\
\textnormal{(h)}&&\ \alpha^{-1}\gamma^2\alpha=\left(4\ 1\ 7\ 3\right)\left(1\ 5\ 4\ 3\ 2\right)\left(3\ 7\ 1\ 4\right)=\left(1\ 4\ 2\ 7\ 5\right).
\end{eqnarray*}

\noindent\newline{\bf Problem.} Compute $\alpha^{-1}$, $\alpha^2$, $\alpha^3$, $\alpha^4$ and $\alpha^5$ where: (a) $\alpha=\left(1\ 2\ 3\right)$, (b) $\alpha=\left(1\ 2\ 3\ 4\right)$, (c) $\alpha=\left(1\ 2\ 3\ 4\ 5\ 6\right)$.

\noindent\newline{\bf Solution.} (a) $\alpha^{-1}=\left(3\ 2\ 1\right)$, $\alpha^2=\left(1\ 3\ 2\right)=\left(3\ 2\ 1\right)=\alpha^{-1}$, $\alpha^3=\alpha^2\alpha=\alpha^{-1}\alpha=\epsilon$, $\alpha^4=\alpha^3\alpha=\epsilon\alpha=\alpha$, $\alpha^5=\alpha^4\alpha=\alpha\alpha=\alpha^2=\alpha^{-1}$. (b) $\alpha^{1}=\left(4\ 3\ 2\ 1\right)$, $\alpha^2=\left(1\ 3\right)\left(2\ 4\right)$, $\alpha^3=\alpha^2\alpha=\left(1\ 3\right)\left(2\ 4\right)\left(1\ 2\ 3\ 4\right)=\left(1\ 4\ 3\ 2\right)=\left(4\ 3\ 2\ 1\right)=\alpha^{-1}$, $\alpha^4=\alpha^3\alpha=\alpha^{-1}\alpha=\epsilon$, $\alpha^5=\alpha^4\alpha=\epsilon\alpha=\alpha$. (c) $\alpha^{-1}=\left(6\ 5\ 4\ 3\ 2\ 1\right)$, $\alpha^2=\left(1\ 3\ 5\right)\left(2\ 4\ 6\right)$, $\alpha^3=\left(1\ 4\right)\left(2\ 5\right)\left(3\ 6\right)$, $\alpha^4=\alpha^3\alpha=\left(1\ 4\right)\left(2\ 5\right)\left(3\ 6\right)\left(1\ 2\ 3\ 4\ 5\ 6\right)=\left(1\ 5\ 3\right)\left(2\ 6\ 4\right)$, $\alpha^5=\alpha^4\alpha=\left(1\ 5\ 3\right)\left(2\ 6\ 4\right)\left(1\ 2\ 3\ 4\ 5\ 6\right)=\left(1\ 6\ 5\ 4\ 3\ 2\ 1\right)=\alpha^{-1}$.

\noindent\newline{\bf Problem.} Let $\alpha$ be a cycle of length $s$, say $\alpha=\left(a_1 a_2\ldots a_s\right)$. Prove each of the following:

\begin{enumerate}
\item There are $s$ distinct powers of $\alpha$.
\item $\alpha^{s-1}=\alpha^{-1}$.
\item $\alpha^2$ is a cycle if and only if $s$ is odd.
\item $\alpha^{s+1}=\alpha$.
\item If $s$ is odd, $\alpha$ is the square of some cycle of length $s$.
\item If $s$ is even, say $s=2t$, then $\alpha^2$ is the product of two cycles of length $t$. (Find them.)
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it There are $s$ distinct powers of $\alpha$.} First we have $\alpha^0=\epsilon$ then $\alpha$ itself. Taking $\alpha^2$ will create a cycle that will carry $a_1$ to $a_2$ and then to $a_3$, i.e. we always skip $2$ elements. Now, we can do that for all $\alpha^i$, when $1<i<s$ and it will be that, for each $i$, $a_1$ goes to $a_{1+i}$ so they will be distinct. We can do that until we reach $\alpha^{s}$ as $\alpha^{s-1}$ will carry $a_1$ to $a_s$. So, $\alpha^s$ will carry $a_1$ to next one after $a_s$ and that is $a_1$, i.e. $\alpha^s=\epsilon=\alpha^0$. From there, the process repeats itself. So, counting $\epsilon$, $\alpha$ and all $i$ between $1$ and $s$ it will be $(s-1)-1+2=s$ distinct powers of $\alpha$.

\item {\it $\alpha^{s-1}=\alpha^{-1}$}. We have $\alpha^{s-1}=\alpha^s\alpha^{-1}$. Now, $\alpha^s=\epsilon$, as for each $i$ we have that $a_{i}$ goes to $a_{i+s}$ and that is again $a_i$. Therefore, $\alpha^{s-1}=\epsilon\alpha^{-1}=\alpha^{-1}$.

\item {\it $\alpha^2$ is a cycle if and only if $s$ is odd. Necessity.} Let $\alpha^2$ be a cycle. We have to prove that the length of $\alpha$ is odd. Suppose the length $s$ was even. By a previous theorem, as then $2|s$ we have that $\alpha^2$ is a product of two disjoint cycles of length $\frac{s}{2}$. That is a contradiction to our assumption that $\alpha^2$ is a cycle, so $s$ has to be odd. {\it Sufficiency.} Let $s$, i.e. length of $\alpha$ be odd. We have to show that $\alpha^2$ is a cycle. If we start from $a_1$ it will carry to $a_3$ and all the odd-numbered indices (up to $a_s$). But, then it will skip $a_1$ and carry to $a_2$ and then to $a_4$ until we exhaust all even-numbered indices (up to $a_{s-1}$). Finally, $a_{s-1}$ will skip $a_s$ and carry to $a_1$, thus finishing the cycle and using all elements (as we had all odd and all even indices).

\item {\it $\alpha^{s+1}=\alpha$.} Follows from the fact that $\alpha^s=\epsilon$. We have $\alpha^{s+1}=\alpha^s\alpha=\epsilon\alpha=\alpha$.

\item {\it If $s$ is odd, $\alpha$ is the square of some cycle of length $s$.} Let $s$ be odd, i.e. length of $\alpha$ is odd. If we take:

\begin{equation*}
\beta=\left(a_1 a_{\frac{s+1}{2}} a_2 a_{\frac{s+3}{2}} \ldots a_{s-1} a_{\frac{s-3}{2}} a_s a_{\frac{s-1}{2}}\right).
\end{equation*}

\noindent\newline If we take $\beta^2$ then $a_1$ goes to $a_2$ which will go to $a_3$ and so on up to $\frac{s-1}{2}$. Obviously this sequence will have $\frac{s-1}{2}$ elements. Then, as we have reached the end, we continue to $\frac{s+1}{2}$, then to $\frac{s+3}{2}$ to $a_{s-1}$ to $a_s$. In this second sequence we have $s-\frac{s+1}{2}+1=\frac{2s-s-1+2}{2}=\frac{s+1}{2}$ elements, totaling $\frac{s-1}{2}+\frac{s+1}{2}=\frac{2s}{2}=s$ elements. This can be done only if $s$ is odd as we would not get natural numbers for e.g. $\frac{s+1}{2}$, $\frac{s-1}{2}$, et cetera. It's easy to see that, by this reasoning $\beta^2=\alpha$.

\item {\it If $s$ is even, say $s=2t$, then $\alpha^2$ is the product of two cycles of length $t$. (Find them.)} As $2|s$, from previous proposition we have that $\alpha$ is a product of two disjoint cycles of length $\frac{s}{2}=t$. It's easy to verify that

\begin{equation*}
\left(a_1 a_3\ldots a_{s-3} a_{s-1}\right)\left(a_2 a_4\ldots a_{s-2} a_s\right)=\left(a_1 a_2\ldots a_{s-1} a_s\right)^2.
\end{equation*}

\noindent\newline Length of first cycle is $\frac{s-1-1}{2}+1=\frac{s-2+2}{2}=t$ and of the second one $\frac{s-2}{2}+1=\frac{s-2+2}{2}=t$.

\end{enumerate}

\noindent{\bf Lemma.} No matter how $\epsilon$ is written as a composition of transpositions, the number of transpositions is always even.

\noindent\newline{\bf Proof.} Suppose that $\epsilon$ can be written as a product of $m$ transpositions such that $\epsilon=t_1 t_2\cdots t_m$. Let $x$ be an element whose last appearence is in $k$-th transposition, $t_k=(x a)$ where $a$ is some other element. We observe $t_{k-1}$. Now, $t_{k-1}$ can contain $x$ and $a$ and then it would be $t_{k-1}t_k=(x a)(x a)$ which is identity and can be removed - now we have $m-2$ transpositions. If $t_{k-1}$ contains $x$ but not $a$, i.e. some element $b$ which differs from $x$ and $a$ then $t_{k-1}t_k=(x b)(x a)=(x a)(a b)$, therefore $x$ appears last in one place further to the left. If $t_{k-1}$ contains $a$ but not $x$ (some $b$ instead) we have $t_{k-1}=(a b)(x a)=(x b)(a b)$ and again $x$ appears last one place further to the left. If $t_{k-1}$ contains neither $x$ nor $a$, but some $b$ and $c$ different from $x$ and $a$ we have $t_{k-1}=(b c)(x a)=(x a)(b c)$ and here $x$ appears last one place further to the left. Now, either we have always $m-2$ elements or $x$ gets pushed one place to the left. Therefore, if we reach the end we cannot have only $t_1=(x a)$ as then $\epsilon(x)=a$ which is a contradiction (it must be that $\epsilon(x)=x$). Therefore, $m$ must be even. And for those elements after $k$, we can fix some other element, see when it appears last and do the same thing again.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If a permutation can be written in an even number of transpositions then it cannot be written in an odd number of transpositions.

\noindent\newline{\bf Proof.} Let $f$ be a permutation and $f^{-1}$ its inverse. If $f$ can be written in an even and an odd number of transpositions, the same goes for $f^{-1}$. Also, we have that $f f^{-1}=\epsilon$. From previous lemma we have that $\epsilon$ can be written only in an even number of transpositions. Therefore, if $f$ can be written in an even number of transpositions and $f^{-1}$ in odd number of transpositions, then $\epsilon$ would have to be odd. Same thing if $f$ is odd and $f^{-1}$ even, $\epsilon$ would have to be odd. Therefore, it can only be that $f$ is either even or odd.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Determine which of the following permutations is even, and which is odd: (a) $(7\ 1\ 8\ 6\ 4)$, (b) $(1\ 2)(7\ 6)(3\ 4\ 5)$, (c) $(1\ 2\ 7\ 6)(3\ 2\ 4\ 1)(7\ 8\ 1\ 2)$, (d) $(1\ 2\ 3)(2\ 3\ 4\ 5)(1\ 3\ 5\ 7)$,

\begin{equation*}
\textnormal{(e) }\pi=\left(\begin{array}{cccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
7 & 4 & 1 & 5 & 6 & 2 & 3 & 9\\
\end{array}\right)
\end{equation*}

\noindent\newline{\bf Solution.} (a) $(7\ 1\ 8\ 6\ 4)$, (b) $(1\ 2)(7\ 6)(3\ 4\ 5)$, (c) $(1\ 2\ 7\ 6)(3\ 2\ 4\ 1)(7\ 8\ 1\ 2)$, (d) $(1\ 2\ 3)(2\ 3\ 4\ 5)(1\ 3\ 5\ 7)$,

\begin{equation*}
\textnormal{(e) }\pi=\left(\begin{array}{cccccccc}
1 & 2 & 3 & 4 & 5 & 6 & 7 & 8\\
7 & 4 & 1 & 5 & 6 & 2 & 3 & 9\\
\end{array}\right)
\end{equation*}

\noindent\newline{\bf Problem.} Prove each of the following:

\begin{enumerate}
\item The product of two even permutations is even.
\item The product of two odd permutations is even.
\item The product of an even permutation and an odd permutation is odd.
\item A cycle of length $l$ is even if $l$ is odd.
\item A cycle of length $l$ is odd if $l$ is even.
\item If $\alpha$ and $\beta$ are cycles of length $l$ and $m$, respectively, then $\alpha\beta$ is even or odd depending on whether $l+m-2$ is even or odd.
\item If $\pi=\beta_1\cdots\beta_r$ where each $\beta_i$ is a cycle of length $l_i$, then $\pi$ is even or odd depending on whether $l_1+l_2+\cdots+l_r-r$ is even or odd.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it The product of two even permutations is even.} Let $\alpha$ and $\beta$ be even permutations, i.e. $\alpha$ can be written as a product of $2n$ and $\beta$ as a product of $2m$ transpositions, where $m,n\in\N$. Then $\alpha\beta$ can be written as product of $2n+2m=2(n+m)$ transpositions which is an even number again and it is therefore an even permutation.
\item {\it The product of two odd permutations is even.} Let $\alpha$ and $\beta$ be odd permutations, that is $\alpha$ can be written as a product of $2n+1$ and $\beta$ as a product of $2m+1$ transpositions, where $m,n\in\N$. Then $\alpha\beta$ can be written as product of $2n+1+2m+1=2(n+m+1)$ transpositions which is an even number and it is therefore an even permutation.
\item {\it The product of an even permutation and an odd permutation is odd.} Let $\alpha$ be an even permutation and $\beta$ an odd permutations. Then $\alpha$ can be written as a product of $2n$ and $\beta$ as a product of $2m+1$ transpositions, where $m,n\in\N$. Then $\alpha\beta$ can be written as product of $2n+2m+1=2(n+m)+1$ transpositions which is an odd number and it is therefore an odd permutation.
\item {\it A cycle of length $l$ is even if $l$ is odd.} Let $\alpha=(a_1 a_2\ldots a_l)$ be a cycle of length $l$. It can be written as a product of transpositions as

\begin{equation*}
\alpha=(a_1 a_l)(a_1 a_{l-1})\cdots(a_1 a_4)(a_1 a_3)(a_1 a_2).
\end{equation*}

\noindent\newline Notice that we have a transposition $(a_1 a_i)$ for every $i\in\{2,\ldots,l\}$. That's $l-1$ transpositions which is even if $l$ is odd. Therefore, cycle of length $l$ is even if $l$ is odd.

\item {\it A cycle of length $l$ is odd if $l$ is even.} Follows from the same point of reasoning as the previous statement. If $l$ is even then $l-1$ is odd, that is $\alpha$ can be written as $l-1$ transpositions, so it is also odd.

\item {\it If $\alpha$ and $\beta$ are cycles of length $l$ and $m$, respectively, then $\alpha\beta$ is even or odd depending on whether $l+m-2$ is even or odd.} If $\alpha$ is a cycle of length $l$, then it can be written as a product of $l-1$ transpositions. Same thing goes for $\beta$ as it can be written as a product of $m-1$ transpositions. Then $\alpha\beta$ can be written as a product of $(l-1)+(m-1)=l+m-2$ transpositions. Therefore, whether $\alpha\beta$ is even or odd will depend on $l+m-2$.

\item {\it If $\pi=\beta_1\cdots\beta_r$ where each $\beta_i$ is a cycle of length $l_i$, then $\pi$ is even or odd depending on whether $l_1+l_2+\cdots+l_r-r$ is even or odd.} Each cycle $\beta_i$ can be written as a product of $l_i-1$ transpositions. Therefore, $\pi$ can be written as a product of $\sum_{i=1}^{r}{l_i-1}=\sum_{i=1}^{r}{l_1}-r$ transpositions. Whether $\pi$ is even or odd will depend on that sum.

\end{enumerate}

\noindent{\bf Problem.} In each of the following, let $\alpha$ and $\beta$ be disjoint cycles, say $\alpha=(a_1 a_2\ldots a_s)$ and $\beta=(b_1 b_2\ldots b_r)$.

\begin{enumerate}
\item For every positive integer $n$, $(\alpha\beta)^n=\alpha^n\beta^n$.
\item If $\alpha\beta=\epsilon$ then $\alpha=\epsilon$ and $\beta=\epsilon$.
\item If $(\alpha\beta)^t=\epsilon$, then $\alpha^t=\epsilon$ and $\beta^t=\epsilon$.
\item Find a transposition $\gamma$ such that $\alpha\beta\gamma$ is a cycle.
\item Let $\gamma$ be the same transposition as in the preceding exercise. Show that $\alpha\gamma\beta$ and $\gamma\alpha\beta$ are cycles.
\item Let $\alpha$ and $\beta$ be cycles of odd length (not disjoint). Prove that if $\alpha^2=\beta^2$, then $\alpha=\beta$.
\end{enumerate}


\noindent{\bf Solution.}

\begin{enumerate}
\item {\it For every positive integer $n$, $(\alpha\beta)^n=\alpha^n\beta^n$.} Disjoint cycles commute (and are associative), as proven previously, so we have $(\alpha\beta)^n=\alpha^n\beta^n$.

\item {\it If $\alpha\beta=\epsilon$ then $\alpha=\epsilon$ and $\beta=\epsilon$.} We have $(a_1 a_2\ldots a_s)(b_1 b_2\ldots b_r)=\epsilon$, but $\alpha$ does not influence nor modify $\beta$ (and reverse) in any way. Therefore, it must be that both $\alpha$ and $\beta$ are identity.

\item {\it If $(\alpha\beta)^t=\epsilon$, then $\alpha^t=\epsilon$ and $\beta^t=\epsilon$.} From two previous problems we have that $(\alpha\beta)^t=\alpha^t\beta^t$, so it must be that $\alpha^t=\epsilon$ and $\beta^t=\epsilon$.

\item {\it Find a transposition $\gamma$ such that $\alpha\beta\gamma$ is a cycle.} Let $(x_1 x_2)=\gamma$. We have $(a_1 a_2\ldots a_s)(b_1 b_2\ldots b_r)(x_1 x_2)$ and we want to find $x_1$ and $x_2$ which will "connect" $\alpha$ and $\beta$. Say we finish $\beta$ and want to continue on $\alpha$. We can take $x_1=a_s$ and $x_2=b_r$. Then we have:

\begin{equation*}
(a_1 a_2\ldots a_s)(b_1 b_2\ldots b_r)(a_s b_r)=(a_s b_1 b_2\ldots b_r a_1 a_2\ldots a_{s-1})=(b_1 b_2\ldots b_r a_1 a_2 \ldots a_s).
\end{equation*}

\item {\it Let $\gamma$ be the same transposition as in the preceding exercise. Show that $\alpha\gamma\beta$ and $\gamma\alpha\beta$ are cycles.} We have $\gamma=(a_s b_r)$. First we shall consider:

\begin{equation*}
(a_1 a_2\ldots a_s)(a_s b_r)(b_1 b_2\ldots b_{r-1} b_r)=(b_1 b_2\ldots b_{r-1} a_1 a_2\ldots a_{s} b_r).
\end{equation*}

\noindent\newline We also have:

\begin{equation*}
(a_s b_r)(a_1 a_2\ldots a_s)(b_1 b_2\ldots b_{r-1} b_r)=(b_1 b_2\ldots b_{r-1} a_s a_1 a_2\ldots a_{s-1} b_r).
\end{equation*}

\item {\it Let $\alpha$ and $\beta$ be cycles of odd length (not disjoint). Prove that if $\alpha^2=\beta^2$, then $\alpha=\beta$.} As $\alpha^2$ and $\beta^2$ are equal cycles (due to length of $\alpha$ and $\beta$ being odd), they are of the same length $n$ (which is odd) and it must be that $\alpha^2(x_i)=x_{i+2}=\beta^2(x_i)$ for all $i\in\{1,\ldots,n-2\}$. Also $\alpha^2(x_{n-1})=x_{2}=\beta^2(x_{n-1})$ and $\alpha^2(x_{n})=x_{1}=\beta^2(x_{n})$. Then, $\alpha{x_i}=x_{i+1}$, but also $\beta{x_i}=x_{i+1}$. So, $\alpha{x_i}=\beta{x_i}$ for all $i\in\{1,\ldots,n-1\}$ and $\alpha{x_n}=x_1=\beta{x_n}$.

\end{enumerate}

\noindent{\bf Problem.} If $\alpha$ is any cycle and $\pi$ any permutation, $\pi\alpha\pi^{-1}$ is called a {\it conjugate} of $\alpha$. In the following parts, let $\pi$ denote any permutation in $S_n$. Prove each of the following in $S_n$:

\begin{enumerate}
\item Let $\alpha=(a_1 a_2\ldots a_s)$ be a cycle. Then $\pi\alpha\pi^{-1}$ is the cycle $(\pi(a_1),\ldots,\pi(a_s))$.
\item Any two cycles of the same length are conjugates of each other.
\item If $\alpha$ and $\beta$ are disjoint cycles, then $\pi\alpha\pi^{-1}$ and $\pi\beta\pi^{-1}$ are disjoint cycles.
\item Let $\sigma$ be a product $\alpha_1\cdots\alpha_t$ of $t$ disjoint cycles of lengths $l_1,\ldots,l_t$, respectively. Then $\pi\sigma\pi^{-1}$ is also a product of $t$ disjoint cycles of lengths $l_1,\ldots,l_t$.
\item Let $\alpha_1$ and $\alpha_2$ be cycles of the same length. Let $\beta_1$ and $\beta_2$ be cycles of the same length. Let $\alpha_1$ and $\beta_1$ be disjoint, and let $\alpha_2$ and $\beta_2$ be disjoint. There is a permutation $\pi\in S_n$ such that $\alpha_1\beta_1=\pi\alpha_2\beta_2\pi^{-1}$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Let $\alpha=(a_1 a_2\ldots a_s)$ be a cycle. Then $\pi\alpha\pi^{-1}$ is the cycle $(\pi(a_1),\ldots,\pi(a_s))$.} Let us denote $\pi'=(\pi(a_1),\ldots,\pi(a_s))$. If we take $\pi'(\pi(a_i))$ it has to go to $\pi'(\pi(a_{i+1}))$ by definition of $\pi'$. Also, note that $\alpha(a_i)=a_{i+1}$ (except for $\alpha(a_n)=a_1$, which won't influence our examinations for now). Now we have $\pi'(\pi(a_i))=\pi(\alpha(\pi^{-1}(\pi(a_i))))=\pi(\alpha(a_i))=\pi(a_{i+1})$.

\item {\it Any two cycles of the same length are conjugates of each other.} Let $N=\{1,2,\ldots,n\}$. If $\beta$ is conjugate of $\alpha$, then $\beta$ can be written as $\pi\beta\pi^{-1}$, where $\pi\in S_n$ is some permutation. Conversely, if $\alpha$ is conjugate od $\beta$, then $\alpha$ can be written as $\rho\beta\rho^{-1}$, where $\rho\in S_n$ is some other permutation. If $\alpha=(a_1 a_2 \ldots a_s)$, i.e. $\alpha(a_i)=a_{i+1}$ (with $\alpha(a_s)=a_1$) and $\beta=(b_1 b_2\ldots b_s)$, id est $\beta(b_i)=b_{i+1}$ (with $\beta(b_s)=b_1$), then we can take such $\pi$ that $\pi(b_i)=a_i$, for all $i\in\{1,\ldots,s\}$. Obviously, $\pi:N\rightarrow N$ is bijection (and a permutation in $S_n$). Note that $\pi^{1}(a_i)=b_i$. Then we define:

\begin{equation*}
\pi'(a_i)=\pi(\beta(\pi^{-1}(a_i)))=\pi(\beta(b_i))=\pi(b_{i+1})=a_{i+1}.
\end{equation*}

\noindent\newline Same thing goes for $a_s$ (it goes to $a_1$; the reader can check for himself). Obviously $\pi':N\rightarrow N$ (as $\pi$, $\pi^{-1}$ and $\alpha$ all stay on $N$) and $\alpha:N\rightarrow N$. Furthermore $\pi'(a_i)=a_{i+1}=\alpha(a_i)$, therefore $\pi'=\alpha$. In conclusion, $\alpha$ is conjugate of $\beta$. Same thing goes to prove the other side; we take $\rho(a_i)=b_i$, for all $i\in\{1,\ldots,s\}$. Then:

\begin{equation*}
\rho'(b_i)=\rho(\alpha(\rho^{-1}(b_i)))=\rho(\alpha(a_i))=\rho(a_{i+1})=b_{i+1}.
\end{equation*}

\noindent\newline So, by the same reasoning $\rho'=\beta$ and $\beta$ is conjugate of $\alpha$. To make a further point we could have taken $\rho=\pi^{-1}$.

\item {\it If $\alpha$ and $\beta$ are disjoint cycles, then $\pi\alpha\pi^{-1}$ and $\pi\beta\pi^{-1}$ are disjoint cycles.} If $\alpha$ and $\beta$ are disjoint, it means that $\alpha(x)\neq\beta(x)$, for all $x\in\N$. Now we have $\pi\alpha\pi^{-1}=(\pi(a_1)\pi(a_2)\ldots\pi(a_s))$ and $\pi\beta\pi^{-1}=(\pi(b_1)\pi(b_2)\ldots\pi(b_r))$. Suppose for some $i$ it's true that $\pi(a_i)=\pi(b_i)$. But, as $\pi$ is a permutation and by that a bijection and also an injection, from $\pi(a_i)=\pi(b_i)$ follows that $a_i=b_i$, which is a contradiction to our assumption that $\alpha$ and $\beta$ are disjoint. Therefore, their conjugates (using the same permutation) must be disjoint also.

\item {\it Let $\sigma$ be a product $\alpha_1\cdots\alpha_t$ of $t$ disjoint cycles of lengths $l_1,\ldots,l_t$, respectively. Then $\pi\sigma\pi^{-1}$ is also a product of $t$ disjoint cycles of lengths $l_1,\ldots,l_t$.} From previous problem we have that all $\pi\alpha_i\pi$ are disjoint cycles, and their lengths are, from the first problem, $l_i$. If we take the product of all conjugates of $\alpha_i$ we have:

\begin{equation*}
\pi\alpha_1\pi^{-1}\pi\alpha_2\pi^{-1}\cdots\pi\alpha_{t-1}\pi^{-1}\pi\alpha_t\pi^{-1}.
\end{equation*}

\noindent\newline It's easy to see that all $\pi^{-1}\pi$ cancel each other out and all that remains is:

\begin{equation*}
\pi\alpha_1\alpha_2\cdots\alpha_{t-1}\alpha_t\pi^{-1}=\pi\sigma\pi^{-1}.
\end{equation*}

\noindent\newline Therefore, $\pi\sigma\pi^{-1}$ was obtained as product of all $\pi\alpha_i\pi^{-1}$, which are disjoint and of the same lengths as $\alpha_i$. This proves our hypothesis.

\item {\it Let $\alpha_1$ and $\alpha_2$ be cycles of the same length. Let $\beta_1$ and $\beta_2$ be cycles of the same length. Let $\alpha_1$ and $\beta_1$ be disjoint, and let $\alpha_2$ and $\beta_2$ be disjoint. There is a permutation $\pi\in S_n$ such that $\alpha_1\beta_1=\pi\alpha_2\beta_2\pi^{-1}$.} If we take some $\pi\in S_n$ (e.g. such that $\pi(a'_i)=a_i$ and $\pi(b'_i)=b_i$, where $a_i$ is from $\alpha_1$ and $a'_i$ is from $\alpha_2$; same thing for $\beta_1$ and $\beta_2$) it's true that, because they are cycles of the same length, $\alpha_1$ and $\alpha_2$ will be conjugates of each other, i.e. $\alpha_1=\pi\alpha_2\pi^{-1}$ (follows from the second problem). Same thing goes for $\beta_1=\pi\beta_2\pi^{-1}$. Such permutation will work as $a_i$ and $b_i$ won't mix. We take the product $\alpha_1\beta_1=\pi\alpha_2\pi^{-1}\pi\beta_2\pi^{-1}=\pi\alpha_2\beta_2\pi^{-1}$ and that concludes this problem.
\end{enumerate}

\noindent{\bf Problem.} If $\alpha$ is any permutation, the least positive integer $n$ such that $\alpha^n=\epsilon$ is called the {\it order} of $\alpha$. Prove in $S_n$:

\begin{enumerate}
\item If $\alpha=(a_1\ldots a_s)$ is a cycle of length $s$, then $\alpha^s=\epsilon$, $\alpha^{2 s}=\epsilon$, and $\alpha^{3 s}=\epsilon$. Is $\alpha^k=\epsilon$ for any positive integer $k<s$?

\item If $\alpha=(a_1\ldots a_s)$ is a cycle of length $s$, the order of $\alpha$ is $s$.
\end{enumerate}

\noindent{\bf Solution.} 

\begin{enumerate}
\item {\it If $\alpha=(a_1\ldots a_s)$ is a cycle of length $s$, then $\alpha^s=\epsilon$, $\alpha^{2 s}=\epsilon$, and $\alpha^{3 s}=\epsilon$. Is $\alpha^k=\epsilon$ for any positive integer $k<s$?} Each time we raise $\alpha$ to the power of $k$, we get $a_{i+k}$ from $a_i$. Therefore, if we take $a_1$ and take $\alpha$ to the power of $s$, we will have $a_{1+s}$ which will go to $a_1$ (as $a_s$ is the last in the cycle and the next is $a_1$ again). This will, of course work for all powers of the form $k s$, as we will have $a_{1+k s}$ from $a_1$. Better said, suppose that it's true, and we will show that $a_{1+(k+1)s}$ will take us to $a_1$. We have $a_{(1+k s)+s}$. By assumption $a_{1+k s}$ will take us to $a_1$ and adding $s$ will, by something we have also shown take us to $a_1$. Thus, by reasoning through mathematical induction, the assumption is valid. Also, suppose $k<s$ and $\alpha^k=\epsilon$. That would mean that $a_1$ goes to $a_{1+k}$ and that $a_1$ comes after $a_k$, but that is contradiction to the fact that $a_{k+1}$ goes after $a_{k}$ (when $k<s$, of course) and that the cycle is of length $k$ and not $s$. Therefore, there cannot be any positive integer $k<s$ such that $\alpha^k=\epsilon$.

\item {\it If $\alpha=(a_1\ldots a_s)$ is a cycle of length $s$, the order of $\alpha$ is $s$.} Follows from the reasoning in previous problem. The least number that will take us from $a_1$ to $a_{1+s}$ is of course $s$.

\end{enumerate}

\noindent{\bf Problem.} Find the order of each of the following permutations: (a) $(1\ 2)(3\ 4\ 5)$, (b) $(1\ 2)(3\ 4\ 5\ 6)$, (c) $(1\ 2\ 3\ 4)(5\ 6\ 7\ 8\ 9)$.

\noindent\newline{\bf Solution.} (a) The permutations is composed of two disjoint cycles of lengths $2$ and $3$. The cycle of length $2$ will be identity for all even powers, and the cycle of length $3$ will be identity for $3$, $6$, etc. If we take the least common multiple of $2$ and $3$ it's going to be $6$. And indeed, when we reach $6$ we will have an even permutation, thus putting first cycle to identity. We will also have $6$ which will put second cycle to identity. (b) Following the similar reasoning as in previous problem, the cycles will both be identity when taken to the power of $4$. (c) As lengths of cycles are $4$ and $5$, the first time they meet and form identity is at power of $20$.

\noindent\newline{\bf Problem.} What is the order of $\alpha\beta$, if $\alpha$ and $\beta$ are disjoint cycles of lengths $4$ and $6$, respectively? Explain, what is the order of $\alpha\beta$, if $\alpha$ and $\beta$ are disjoint cycles of lengths $r$ and $s$?

\noindent\newline{\bf Solution.} As disjoint cycles commute, we have that $(\alpha\beta)^n=\alpha^n\beta^n$. Thus, we seek $k_1$ and $k_2$ such that $\alpha^{4k_1}\beta^{6k_2}=\epsilon$ and that $4k_1=6k_2$. The least such (positive natural) numbers will be $k_1=3$ and $k_2=2$. So:

\begin{equation*}
(\alpha\beta)^12=\alpha^12\beta^12=\alpha^{4\cdot 3}\beta^{6\cdot 2}=\epsilon\epsilon=\epsilon.
\end{equation*}

\noindent\newline We can assume form this reasoning that for two disjoint cycles of lengths $r$ and $s$, order of their product will be least common multiple of $r$ and $s$.

\noindent\newline{\bf Definition.} The set of all the even permutations in $S_n$ is denoted by $A_n$ and is called the {\bf alternating group}\footnote{For now in the mention-sense.} on $n$ symbols.

\noindent\newline{\bf Problem.} Prove each of the following in $S_n$:

\begin{enumerate}
\item Let $\alpha_1,\ldots,\alpha_r$ be distinct even permutations, and $\beta$ an odd permutation. Then $\alpha_1\beta,\ldots,\alpha_r\beta$ are $r$ {\it distinct} odd permutations.

\item If $\beta_1,\ldots,\beta_r$ are distinct odd permutations, then $\beta_1\beta_1,\beta_1\beta_2,\ldots,\beta_1\beta_r$ are $r$ {\it distinct} even permutations.

\item In $S_n$, there are the same number of odd permutations as even permutations.

\item The set of all the even permutations (alternating group) is a subgroup of $S_n$.

\item Let $H$ be any subgroup of $S_n$. $H$ either contains only even permutations, or $H$ contains the same number of odd as even permutations.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Let $\alpha_1,\ldots,\alpha_r$ be distinct even permutations, and $\beta$ an odd permutation. Then $\alpha_1\beta,\ldots,\alpha_r\beta$ are $r$ distinct odd permutations.} Suppose $\alpha_i\beta=\alpha_j\beta$ for some $i\neq j$ and $i,j\in\{1,\ldots,r\}$. That would mean that, after multiplying with $\beta^{-1}$ on the right. Then we would have that $\alpha_i=\alpha_j$ which is a contradiction to assumption that all $\alpha_i$ are distinct permutations. Furthermore, as $\alpha_i$ are even and $\beta$ odd, then by previous problem, $\alpha_i\beta$ are odd.

\item {\it If $\beta_1,\ldots,\beta_r$ are distinct odd permutations, then $\beta_1\beta_1,\beta_1\beta_2,\ldots,\beta_1\beta_r$ are $r$ distinct even permutations.} Suppose $\beta_1\beta_i=\beta_1\beta_j$ for some $i\neq j$ where $i,j\in\{1,\ldots,r\}$. Then, taking left inverse, $\beta_1^{-1}$ and multiplying equality on the left yields $\beta_i=\beta_j$, which is contradiction to assumption that all $\beta_i$ are distinct. Furthermore, as $\beta_1$ is odd and $\beta_i$ are odd then $\beta_1\beta_i$ are even permutations.

\item {\it In $S_n$, there are the same number of odd permutations as even permutations.} Suppose there are $d$ (distinct) odd permutations and $v$ (distinct) even permutations. First we will show that there are more odd permutations then even permutations. We have that for all even permutations $\alpha_1,\ldots,\alpha_v$ there are $v$ distinct odd permutations $\alpha_i\beta$ for one odd permutation $\beta$. Now, as there are $d$ odd permutations we have $\alpha_i\beta_j$, where $i\in\{1,\ldots,v\}$ and $j\in\{1,\ldots,d\}$, distinct odd permutations. That is $dv$ distinct odd permutations. Obviously $dv\geq v$, so there are more odd permutations than even permutations. Furthermore, as we have $d$ distinct odd permutations, we have distinct even permutations $\alpha_1\alpha_i$ (for $i\in\{1,\ldots,d\}$) and there are $d$ of them. But we can do that not only for $\alpha_1$, but for all $\alpha_j$, that is $\alpha_i\alpha_j$ (for $i,j\in\{1,\ldots,d\}$). So there are $d^2$ even permutations. And, as $d^2\geq d$ we have more even permutations than odd permutations. If we have both more (or equal) number of odd permutations than even permutations and, conversely, more (or equal) number of even permutations than odd permutations, our conclusion is that there is the same number of odd permutations as even permutation.

\item {\it The set of all the even permutations (alternating group) is a subgroup of $S_n$.} Let us denote $A_n$ as alternating group on $n$ symbols. Any even permutation is a permutation and it belongs in $S_n$, therefore, obviously, $A_n\subset S_n$ (we can use a stronger statement as we have proved that there is the same number of odd as even permutations, so there must be some other elements in $S_n$ that are not in $A_n$). If we take two even permutations $f,g\in A_n$, their product is an even permutation again and $f g,g f\in A_n$. Also, if $f\in A_n$ is an even permutation, it's inverse is an even permutation and $f^{-1}\in A_n$ (follows from $f f^{-1}=\epsilon$ and $\epsilon$ is an even permutation). Therefore $A_n$ is subgroup of $S_n$.

\item {\it Let $H$ be any subgroup of $S_n$. $H$ either contains only even permutations, or $H$ contains the same number of odd as even permutations.} If $H$ is subgroup of $S_n$, then it's true that $H\subseteq S_n$, it's closed under composition and with respect to inverses. For one thing, $H$ must contain $\epsilon$ or else it could not be closed under multiplication and with respect to inverses (as $f f^{-1}=\epsilon$). But, $\epsilon$ is an even permutation. Therefore $H$ cannot contain only odd permutations. As $\{\epsilon\}$ is a trivial group (with respect to composition, of course) then it is possible that $H$ contains only even permutations (even if that is only one). But, if we add one even permutation, it's inverse would be even and composition with other even permutations would be even. Now, suppose we had more odd than even permutations. That is an impossibility as for each two odd permutations their product is an even permutation, i.e. if there are $d$ (distinct) odd then, there must be $d^2$ (distinct) even permutations. Suppose there are more even than odd permutations. For one odd permutation and all even permutation their products are all odd, therefore, for each odd permutation there are as many odd as there are even permutations so that is an imposibility. Therefore their numbers must equal.

\end{enumerate}

\noindent{\bf Problem.} Remember that in any group $G$, a set $S$ of elements of $G$ is said to {\it generate} $G$ if every element of $G$ can be expressed as a product of elements in $S$ and inverses of elements in $S$. Prove:

\begin{enumerate}
\item The set $T$ of all the transpositions in $S_n$ generates $S_n$.
\item The set $T_1=\{(1\ 2),(1\ 3),\ldots,(1\ n)\}$ generates $S_n$.
\item Every even permutation is a product of one or more cycles of length $3$. Conclude that the set $U$ of all cycles of length $3$ generates $A_n$.
\item The set $U_1=\{(1\ 2\ 3),(1\ 2\ 4),\ldots,(1\ 2\ n)\}$ generates $A_n$.
\item The pair of cycles $(1\ 2)$ and $(1\ 2\ \cdots\ n)$ generates $S_n$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it The set $T$ of all the transpositions in $S_n$ generates $S_n$.} Set $T$ contains all transpositions, i.e. all cycles of the form $(i\ j)$ where $i,j\{1,\ldots,n\}$ and $i\neq j$. Every permutation can be written down as a product of disjoint cycles. Suppose we had $\pi\in S_n$ such that:

\begin{equation*}
\pi=\left(x^{(1)}_1 x^{(1)}_2 \ldots x^{(1)}_{p_1}\right)\left(x^{(2)}_1 x^{(2)}_2 \ldots x^{(2)}_{p_2}\right)\cdots\left(x^{(m)}_1 x^{(m)}_2\ldots x^{(m)}_{p_m}\right).
\end{equation*}

\noindent\newline It can be shown as a product of transpositions so that:

\begin{equation*}
\pi=\left(x^{(1)}_1 x^{(1)}_{p_1}\right)\cdots\left(x^{(1)}_1 x^{(1)}_3\right)\left(x^{(1)}_1 x^{(1)}_2\right)\cdots\left(x^{(m)}_1 x^{(m)}_{p_m}\right)\cdots\left(x^{(m)}_1 x^{(m)}_2\right).
\end{equation*}

\noindent\newline Also, let us just note that identity can be expressed as $\epsilon=(1\ 2)(2\ 1)$ (and in many other ways). Therefore, as every permutation can be written as a product of transpositions, set $T$ generates $S_n$.

\item {\it The set $T_1=\{(1\ 2),(1\ 3),\ldots,(1\ n)\}$ generates $S_n$.} In previous problem we have shown {\it how} a permutation can be written as a product of transpositions. Now, any transposition of the form $(i\ j)$ where $i,j\in\{1,\ldots,n\}$ and $i\neq j$ can be written as $(i\ j)=(j\ 1)(1\ i)(1\ j)$. Therefore, $T_1$ generates $S_n$.

\item {\it Every even permutation is a product of one or more cycles of length $3$. Conclude that the set $U$ of all cycles of length $3$ generates $A_n$.} Let $(i\ j)$ and $(k\ l)$ (where $i,j,k,l\in\{1,\ldots,n\}$ and $i\neq j$ and $k\neq l$) be two transpositions. Their product can be written as $(i\ j)(k\ l)=(l\ k\ j)(i\ k\ j)$. If we have $(i\ j)(i\ k)=(i\ k\ j)$ (additional condition that $i\neq k$). Therefore, every even permutation is a product of one or more cycles of length $3$. The product of even permutations is even, therefore the set $U$ generates $A_n$ (as every even permutation can be written as a product of elements in $U$).

\item {\it The set $U_1=\{(1\ 2\ 3),(1\ 2\ 4),\ldots,(1\ 2\ n)\}$ generates $A_n$.} Every cycle of length $3$ can be written as:

\begin{equation*}
(i\ j)(k\ l)=(1\ 2\ k)(1\ 2\ i)(1\ 2\ l)(1\ 2\ j)(1\ 2\ k)(1\ 2\ i).
\end{equation*}

\noindent\newline Also, we have:

\begin{equation*}
(i\ j)(i\ k)=(i\ k\ j)=(1\ 2\ j)(1\ 2\ i)(1\ 2\ k)(1\ 2\ j)(1\ 2\ i).
\end{equation*}

\noindent\newline The last condition is sufficient, and therefore, every even permutation can be written as product of elements in $U_1$; in other words, $U_1$ generates $A_n$.

\item {\it The pair of cycles $(1\ 2)$ and $(1\ 2\ \cdots\ n)$ generates $S_n$.} Every transposition of the form $(1\ i)$ (where $i\in\{2,\ldots,n\}$) can be written as:

\begin{equation*}
(n-(i-2)\ n-(i-1))=(1\ 2\ \cdots\ n)^{-i}(1\ 2)(1\ 2\ \cdots\ n)^{i}.
\end{equation*}

\noindent\newline This way we can obtain all $(k\ k+1)$, where $k\in\{1,\ldots,n-1\}$, by taking $i=n-k+2$. We can see that the only elements not left to identity will be the ones leading to $2$ and $1$. And which ones will lead to $1$ and $2$? Well, as $n\rightarrow 1$ for $i=1$ we have $n-1\rightarrow 1$ for $i=2$ and (following the same reasoning) we have $n-(i-1)\rightarrow 1$. Also, $n\rightarrow 2$ for $i=2$, $n-1\rightarrow 2$ for $i=3$ and we may conclude that $n-(i-2)\rightarrow 2$ in general. Now, what happens in the second cycle is that $1\rightarrow 2$ and $2\rightarrow 1$. Finally, when taking inverse, we are asking ourselves, where will $1$ and $2$ go? Obviously, $1\rightarrow n$ for $i=-1$, $1\rightarrow n-1$ for $i=-2$ and generally $1\rightarrow n-(i-1)$. Similarly, $2\rightarrow n$ for $i=-2$, $2\rightarrow n-1$ for $i=-3$ and generally $2\rightarrow n-(i-2)$. Therefore we have that $n-(i-1)\rightarrow n-(i-2)$ and $n-(i-2)\rightarrow n-(i-1)$. We also have that:

\begin{equation*}
(1\ i)=(1\ 2)(2\ 3)\cdots(i-1\ i-2)(i-1\ i)(i-1\ i-2)\cdots(3\ 2)(2\ 1).
\end{equation*}

\noindent\newline It is obvious that all elements $1<k<i$ will go to $k+1$ and then to $k$ again. But, $1$ will go to $2$, then to $3$, all up to $i$ and then stop (as there is no more $i$ on the left). Same thing for $i$, as it will go to $i-1$ then to $i-2$ all up to $1$. And, all such transpositions are those containted in $T_1$ and $T_1$ generates $S_n$, then the pair of cycles $(1\ 2)$ and $(1\ 2\ \cdots\ n)$ also generates $S_n$.

\end{enumerate}

\newpage

\begin{center}
{\bf Isomorphism}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G_1$ and $G_2$ be groups. A bijective function $f:G_1\rightarrow G_2$ with the property that for any two elements $a$ and $b$ in $G_1$

\begin{equation*}
f(a b)=f(a)f(b)
\end{equation*}

\noindent\newline is called an {\bf isomorphism} from $G_1$ to $G_2$. If there exists an isomorphism from $G_1$ to $G_2$, we say that $G_1$ is {\bf isomorphic} to $G_2$ and symbolize this fact by writing:

\begin{equation*}
G_1\cong G_2.
\end{equation*}

\noindent\newline{\bf Proposition.} Isomorphism is an equivalence relation.

\noindent\newline{\bf Proof.} (a) {\it Reflexivity.} Every group is isomorphic to itself as we can take $\epsilon: G\rightarrow G$ such that $\epsilon(x)=x$, for all $x\in G$. It's obviously a bijection and $\epsilon(a b)=a b=\epsilon(a)\epsilon(b)$, for all $a,b\in G$. Therefore, $G\cong G$. (b) {\it Symmetry.} If we have $G_1\cong G_2$ then there exists isomorphism $f:G_1\rightarrow G_2$. As isomorphism is a bijection it's inverse $f^{-1}:G_2\rightarrow G_1$ is also a bijection. Furthermore, as we have $f(a b)=f(a)f(b)$ for $a,b\in G_1$, then $f^{-1}(f(a b))=f^{-1}(f(a) f(b))$. That is, $f^{-1}(f(a) f(b))=a b$, for $f(a),f(b)\in G_2$. But, as $f^{-1}(f(a))=a$ and $f^{-1}(f(b))=b$, we have $f^{-1}(f(a) f(b))=f^{-1}(f(a))f^{-1}(f(b))$, where $f(a),f(b)\in G_2$. We could rename $a':=f(a)$ and $b':=f(b)$ so that we have $f^{-1}(a' b')=f^{-1}(a')f^{-1}(b')$ for all $a',b'\in G_2$. Therefore $f^{-1}$ is also an isomoprhism and we have that $G_1\cong G_2$ implies $G_2\cong G_1$. (c) {\it Transitivity.} Suppose we have $G_1\cong G_2$ and $G_2\cong G_3$. We have to show that $G_1\cong G_3$. As $G_1\cong G_2$, there exists isomorphism $f:G_1\rightarrow G_2$ such that $f(a b)=f(a)f(b)$, for all $a,b\in G_1$. Also, as $G_2\cong G_3$, there exists an isomorphism $g:G_2\rightarrow G_3$ such that $g(a b)=g(a)g(b)$, for all $a,b\in G_2$. As isomorphism is bijection by definition then composition $g\circ f:G_1\rightarrow G_3$ is also a bijection. Furthermore we have that $g(f(a b))=g(f(a)f(b))$, for all $a,b\in G_1$. As $f(a b)\in G_2$ and $f(a),f(b)\in G_2$, then $g(f(a)f(b))=g(f(a))g(f(b))$. So we have $[g\circ f](a b)=[g\circ f](a)[g\circ f](b)$, for all $a,b\in G_1$. Therefore, $g\circ f$ is an isomorphism from $G_1$ to $G_3$, so $G_1\cong G_2$ and $G_2\cong G_3$ implies $G_1\cong G_3$.

\begin{flushright}
$\square$\\
\end{flushright}

We will first exemplify our motivation for the following theorem. Indeed, proof of a theorem by itself is worth nothing, if it does not shed more light on the theorem itself. It has not only to reveal something extraordinary, but it has to show why the theorem works and will always work. Proving it in a simple way is by no means an advantage. Proving it in a way that will reveal (or at least allow us a glimpse at) all the machinery behind it. Let us first define group $V:=(V,\cdot)$ where $V=\{1,i,-1,-i\}$. When we're dealing with "small" finite groups, showing isomorphism between them will be just a matter of table-checking. Now, let us take another group $\Z_4:=(\Z_4,+_4)$. We will show that $V\cong \Z_4$. If we construct tables for $V$ and $\Z_4$, respectively, we will have:

\begin{center}
\parbox{.4\linewidth}{
%\quad
\begin{tabular}{c|cccc}
  %\hline
  $\cdot$ & $1$ & $-1$ & $i$ & $-i$\\
  \hline
  $1$ & $1$ & $-1$ & $i$ & $-i$\\
  $-1$ & $-1$ & $1$ & $-i$ & $i$\\
	$i$ & ${\it i}$ & ${\it -i}$ & ${\bf -1}$ & ${\bf 1}$\\
	$-i$ & ${\it -i}$ & ${\it i}$ & ${\bf 1}$ & ${\bf -1}$\\
  %\hline
\end{tabular}}
\hspace{1.5cm}
\parbox{.4\linewidth}{
%\quad
\begin{tabular}{c|cccc}
  %\hline
  $+_4$ & $0$ & $1$ & $2$ & $3$\\
  \hline
  $0$ & $0$ & $1$ & $2$ & $3$\\
  $1$ & ${\it 1}$ & ${\bf 2}$ & ${\it 3}$ & ${\bf 0}$\\
	$2$ & $2$ & $3$ & $0$ & $1$\\
	$3$ & ${\it 3}$ & ${\bf 0}$ & ${\it 1}$ & ${\bf 2}$\\
  %\hline
\end{tabular}}
\end{center}

\noindent\newline Now, an obvious thing is to look for the identity and then seek out patterns. Obviously, identity for $V$ is $1$ (as we're dealing with multiplication of complex numbers; actually $(V,\cdot)$ is a subgroup of $(\C\backslash\{0\},\cdot)$). Identity in $\Z_4$ is $0$ (we're dealing with addition). We will start constructing isomorphism $f:V\rightarrow\Z_4$. We have $f(1)=0$. Good thing is to start looking for blocks of subtables (such as one marked in bold) with few tips: rows have to match, i.e. we cannot have a subtable containing elements that don't correspond to ones that define rows in one, and in other have such elements appear in the subtable. Such is the logical choice for the subtable in $\Z_4$ that corresponds to the subtable in $V$. They both contain neutral elements on the diagonal and they both have other element that is different from the ones defining rows (i.e. in $V$ we have $-1$ appearing in $i$ and $-i$ rows and in $\Z_4$ we have $2$ appearing in $1$ and $3$ rows). So, we may assume that $f(-1)=2$. Also, $2+_4 2=0$ and $-1\cdot -1=1$ (they are inverses of themselves which is by itself a good guiding point). We may further guess that $f(i)=1$ (as actually $i$ means "rotate by $\pi$ and logical order is $1,i,-1,-i$, therefore $i$ acts as an increment, as does $1$ in $\Z_4$). Then, $f(-i)=3$ as $-i\cdot i=1$. So it leaves us with:

\begin{equation*}
f=\left(\begin{array}{cccc}
1 & i & -1 & -i\\
0 & 1 & 2 & 3\\
\end{array}\right)
\end{equation*}

\noindent\newline We have to check if $f(a b)=f(a)f(b)$ for all $a,b\in V$. E.g. we have $f(i\cdot -1)=f(-i)=3=1+_4 2$. Both groups are commutative so we need not check all possibilities. By pure reason we shall assume that this is indeed correct (and the reader can check himself by going through the table one more time). But, now, we want to define functions $\pi_a:V\rightarrow V$ of the type $\pi_a(x)=a x$, for all $a\in V$. Permutation $\pi_1$ is identity as $\pi_1(x)=1\cdot x=x$. Therefore $\pi_1=\epsilon$. To go further, we have $\pi_i(x)=i x$ which will have the following table:

\begin{equation*}
\pi_i=\left(\begin{array}{cccc}
1 & i & -1 & -i\\
i & -1 & -i & 1\\
\end{array}\right).
\end{equation*}

\noindent\newline In the same fashion we construct $\pi_{-1}=-x$ and $\pi_{-i}=-i x$:

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{-1}=\left(\begin{array}{cccc}
1 & i & -1 & -i\\
-1 & -i & 1 & i\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{-i}=\left(\begin{array}{cccc}
1 & i & -1 & -i\\
-i & 1 & i & -1\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline We can see that we have here the following table of multiplication for these permutations (a helpful fact is that $\pi_i=\left(1\ i\ -1\ -i\right)$, $\pi_{-1}=\left(-1\ 1\right)\left(-i\ i\right)$ and $\pi_{-i}=\left(-i\ -1\ i\ 1\right)$):

\begin{center}
\begin{tabular}{c|cccc}
  %\hline
  $\circ$ & $\pi_1$ & $\pi_i$ & $\pi_{-1}$ & $\pi_{-i}$\\
  \hline
  $\pi_1$ & $\pi_1$ & $\pi_i$ & $\pi_{-1}$ & $\pi_{-i}$\\
  $\pi_i$ & $\pi_i$ & $\pi_{-1}$ & $\pi_{-i}$ & $\pi_{1}$\\
	$\pi_{-1}$ & $\pi_{-1}$ & $\pi_{-i}$ & $\pi_{1}$ & $\pi_{i}$\\
	$\pi_{-i}$ & $\pi_{-i}$ & $\pi_{1}$ & $\pi_{i}$ & $\pi_{-1}$\\
\end{tabular}\end{center}

\noindent\newline Set of permutations $\{\pi_1,\pi_i,\pi_{-1},\pi_{-i}\}$ (with composition) is obviously isomorphic to group $V$. The following theorem will be proved generaly, by constructing such permutations over some group and proving the existence of isomorphism. Yet, we have forgotten something that was implicitly understood here: we had to prove that $\pi_a:V\rightarrow V$ is actually a permutation, for all $a\in V$. As forementioned, here the case was trivial, but when we're dealing with something larger and unknown we have to go step by step and prove that $\pi_a$ is a function and such function for which $\textnormal{dom}(f)=\textnormal{cod}(f)$ and such function that is bijective. Also, we have to prove that the set of such permutations is also a group (and that will be done easier by proving it to be a subgroup of $S_n$).

\noindent\newline{\bf Theorem (Cayley).} Every group is isomorphic to a group of permutations.

\noindent\newline{\bf Proof.} Let $G$ be a group. We define a function $\pi_a: G\rightarrow G$, as in previous (motivational) example, such that $\pi_a(x)=a x$, where $a\in G$. First we will show that this is indeed a permutation. A function it surely is, taking $a x$, where $a,x\in G$ will, due to group $G$ being closed with respect to multiplication, yield $a x\in G$. Such $a x$ will also be unique as group $G$ satisfies axioms of totality. As its domain and codomain are the same, it is a permutation (providing we prove it's also bijective).

Furthermore we have to prove that it is injective and surjective. If we take $\pi_a(x)=\pi_b(y)$ we have $a x=a y$. As $a\in G$ and $G$ is group, i.e. closed with respect to inverses among other things, we also have $a^{-1}\in G$ such that $a^{-1} a=e$, where $e\in G$ is identity (neutral element). Therefore, multiplying $a x=a y$ on the left with $a^{-1}$ gives us $a^{-1} a x=a^{-1} a y$ which actually implies that $e x=e y$, and by that $x=y$. Thus, $\pi_a$ is injective for all $a\in G$. Now, for surjectivity, we take some $y\in G$. There has to exist original $x\in G$ such that $y=\pi_a(x)$ and that is $y=a x$. Taking the inverse and multiplying on the left gives us $a^{-1} y=x$. As inverse exists, there exists such $x\in G$. Thus, $\pi_a$ is a bijection and, taking former conditions into consideration, a permutation.

We now take the set $G^{\ast}=\{\pi_a:\ a\in G\}$, which will contain one such permutation for each $a\in G$. Let $S_G$ denote the group\footnote{$S_G$ is the subgroup of the group of all bijections from $G$ to $G$. If we take two $f,g\in S_G$, then $f g$ is also a bijection from $G$ to $G$, and if we take inverse $f^{-1}$ of $f\in S_G$ it is also in $S_G$ as inverse of bijection is a bijection and, again, it goes from $G$ to $G$.} of all permutations on $G$. We will show that $G^{\ast}$ is a subgroup of $S_G$. First, $G^{\ast}$ is a subset of $S_G$, as $\pi_a$ is a permutation and contained in $S_G$ (which contains all permutations on $G$). If we take $\pi_a,\pi_b\in G^{\ast}$, then $\pi_a(\pi_b(x))=\pi_a(b x)=a (b x)=(a b) x$. As $a b\in G$ ($G$ is closed under multiplication), then $\pi_{a b}=(a b) x$ is in $G^{\ast}$ ($a b$ is in $G$ so by definition of $G^{\ast}$ permutation $\pi_{a b}$ is contained in $G^{\ast}$). If we take $\pi_a\in G^{\ast}$, then we have to show that $\pi_a^{-1}\in G^{\ast}$. We have $\pi_a(x)=a x$ and it's inverse\footnote{As $\pi_a(\pi_a^{-1}(x))=a(a^{-1})x=x$ and $\pi_a^{-1}(\pi_a(x))=a^{-1}(a x)=x$.} is $\pi_a^{-1}(x)=a^{-1} x$. Group $G$ is closed with respect to inverses so $a^{-1}$ is in $G$, and by that $\pi_{a^{-1}}$ is in $G^{\ast}$. Therefore, $G^{\ast}$ is a subgroup of $S_G$, and by that, a group by itself.

Now we take function $f:G\rightarrow G^{\ast}$ such that $f(a)=\pi_a$. Such function is defined for all $a$, and all $\pi_a$ are unique for each $a$. Also, $f$ is injective as $f(a)=f(b)$ implies $\pi_a=\pi_b$. That implies $a x=b x$, so taking the inverse of $x$ (we can do that as $x\in G$ and $x^{-1}\in G$) and multiplying with it the equation on the right gives us $a=b$. Thus, $f$ is injective. If we take $\pi_a\in G^{\ast}$ there exists $a\in G$ such that $\pi_a=f(a)$ and $f$ is surjective. By that, it is also bijective. Now, we have to show that $f(a b)=f(a)f(b)$. It's easy to see\footnote{Note that $\pi_{a b}=\pi_a\pi_b$ as $\pi_{a b}(x)=(a b)x=a (b x)=\pi_a(\pi_b(x))$.} that $f(a b)=\pi_{a b}=\pi_a\pi_b=f(a)f(b)$. Thus, $f$ is an isomorphism from $G$ to $G^{\ast}$ and we have just proved that $G\cong G^{\ast}$. In other words, a weaker statement, $G$ is isomorphic to some group of permutations.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a group and $G^{\ast}=\{\pi_a:\ a\in G\}$, where $\pi_a:G\rightarrow G$ is a permutation on $G$. We say that $G^{\ast}$ is:

\begin{itemize}
\item {\bf left regular representation} of $G$ if $\pi_a(x)=a x$;
\item {\bf right regular representation} of $G$ if $\pi_a(x)=x a$;
\item {\bf regular representation} of $G$ if $G$ is commutative.
\end{itemize}

\noindent\newline{\bf Problem.} Find the right and left regular representation of each of the following groups, and compute their tables. (If the group is Abelian, find its regular representation.) (a) $\Z_3$; (b) $P_2$, the group of subsets of a two-element set; (c) $\Z_4$.

\noindent\newline{\bf Solution.} (a) The multiplication table for $\Z_3$ is as follows:

\begin{center}\begin{tabular}{c|ccc}
  %\hline
  $+_3$ & $0$ & $1$ & $2$\\
  \hline
  $0$ & $0$ & $1$ & $2$\\
  $1$ & $1$ & $2$ & $0$\\
	$2$ & $2$ & $0$ & $1$\\
  %\hline
\end{tabular}\end{center}

\noindent\newline The table for $\Z_3$ is symmetric so the group is commutative. It will have a regular representation. We define $G^{\ast}=\{\pi_0,\pi_1,\pi_2\}$. Then, taking into consideration that $\pi_0=\epsilon$, we have:

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{1}=\left(\begin{array}{ccc}
0 & 1 & 2\\
1 & 2 & 0\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{2}=\left(\begin{array}{ccc}
0 & 1 & 2\\
2 & 0 & 1\\
\end{array}\right).
\end{equation*}}
\end{parbox}\end{center}

\noindent\newline Note that $\pi_1=(0\ 1\ 2)$ and $\pi_2=(0\ 2\ 1)$.

\noindent\newline (b) Let $\{a,b\}$ be a two-element set. Then $P_2=\{\emptyset,\{a\},\{b\},\{a,b\}\}$. The table of $P_2$ (taking symmetric difference as operation on $P_2$) is:

\begin{center}\begin{tabular}{c|cccc}
  %\hline
  $\Delta$ & $\emptyset$ & $\{a\}$ & $\{b\}$ & $\{a,b\}$\\
  \hline
  $\emptyset$ & $\emptyset$ & $\{a\}$ & $\{b\}$ & $\{a,b\}$\\
  $\{a\}$ & $\{a\}$ & $\emptyset$ & $\{a,b\}$ & $\{b\}$\\
	$\{b\}$ & $\{b\}$ & $\{a,b\}$ & $\emptyset$ & $\{a\}$\\
	$\{a,b\}$ & $\{a,b\}$ & $\{b\}$ & $\{a\}$ & $\emptyset$\\
  %\hline
\end{tabular}\end{center}

\noindent\newline Again, $P_2$ is commutative (as set union is commutative) so it has a regular representation. We have $P_2^{\ast}=\left\{\pi_{\emptyset},\pi_{\{a\}},\pi_{\{b\}},\pi_{\{a,b\}}\right\}$, where (taking that $\pi_{\emptyset}=\epsilon$):

\begin{equation*}
\pi_{\{a\}}=\left(\begin{array}{cccc}
\emptyset & \{a\} & \{b\} & \{a,b\}\\
\{a\} & \emptyset & \{a,b\} & \{b\}\\
\end{array}\right)=(\emptyset\ \{a\})(\{b\}\ \{a,b\}),
\end{equation*}

\begin{equation*}
\pi_{\{b\}}=\left(\begin{array}{cccc}
\emptyset & \{a\} & \{b\} & \{a,b\}\\
\{b\} & \{a,b\} & \emptyset & \{a\}\\
\end{array}\right)=(\emptyset\ \{b\})(\{a\}\ \{a,b\}),
\end{equation*}

\begin{equation*}
\pi_{\{a,b\}}=\left(\begin{array}{cccc}
\emptyset & \{a\} & \{b\} & \{a,b\}\\
\{a,b\} & \{b\} & \{a\} & \emptyset\\
\end{array}\right)=(\emptyset\ \{a,b\})(\{a\}\ \{b\}).
\end{equation*}

%\noindent\newline Note that $\pi_{\{a\}}$, $\pi_{\{b\}}$ and $\pi_{\{a,b\}}$.

\noindent\newline (c) We already have multiplication table for $\Z_4$ (look above). The group is also Abelian, therefore it will have a regular representation. We have $G^{\ast}=\{\pi_0,\pi_1,\pi_2,\pi_3\}$, where (notice that $\pi_0=\epsilon$):

\begin{center}\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{1}=\left(\begin{array}{cccc}
0 & 1 & 2 & 3\\
1 & 2 & 3 & 0\\
\end{array}\right),
\end{equation*}}
\end{parbox}
\hskip 1cm
\begin{parbox}{0.3\linewidth}{
\begin{equation*}
\pi_{2}=\left(\begin{array}{cccc}
0 & 1 & 2 & 3\\
2 & 3 & 0 & 1\\
\end{array}\right),
\end{equation*}}
\end{parbox}

\begin{equation*}
\pi_{3}=\left(\begin{array}{cccc}
0 & 1 & 2 & 3\\
3 & 0 & 1 & 2\\
\end{array}\right).
\end{equation*}
\end{center}

\noindent\newline Notice that $\pi_1=(0\ 1\ 2\ 3)$, $\pi_2=(0\ 2)(1\ 3)$ and $\pi_3=(3\ 2\ 1\ 0)$.

\noindent\newline{\bf Problem.} Let $G_1$ and $G_2$ be groups, and let $f:G_1\rightarrow G_2$ be an isomorphism. Prove the following:

\begin{enumerate}
\item If $e_1$ denotes the neutral element of $G_1$ and $e_2$ denotes the neutral element of $G_2$, prove that $f(e_1)=e_2$.
\item Prove that for each element in $a$ in $G_1$, $f(a^{-1})=[f(a)]^{-1}$.
\item If $G_1$ is a cyclic group with generator $a$, prove that $G_2$ is also a cyclic group, with generator $f(a)$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $e_1$ denotes the neutral element of $G_1$ and $e_2$ denotes the neutral element of $G_2$, prove that $f(e_1)=e_2$.} Let $f(e_1)=e'_2$, where $e'_2\neq e_2$, i.e. we have that $e'_2(e'_2)^{-1}=e_2$. We will show that $e'_2=e_2$. If we take $f^{-1}(e_2)=f^{-1}(e'_2(e'_2)^{-1})$, as $f^{-1}:G_1\rightarrow G_2$ is also an isomorphism (symmetry), it follows that $f^{-1}(e_2)=f^{-1}(e'_2)f^{-1}((e'_2)^{-1})$. But, $f(e_1)=e'_2$, so $f^{-1}(f(e_1))=f^{-1}(e'_2)$, i.e. $e_1=e'_2$. From that we have $f^{-1}(e_2)=e_1 f^{-1}((e'_2)^{-1})$. As $e_1$ is a neutral element in $G_1$ (and notice that $f^{-1}((e'_2)^{-1})\in G_1$) we have that $f^{-1}(e_2)=f^{-1}(e'_2)^{-1}$. As $f^{-1}$ is a bijection, we have that $e_2=(e'_2)^{-1}$. That means that $e'_2 e_2=e_2$ and from that follows $e'_2=e_2$ (as $e'_2 e_2=e'_2$).

\item {\it Prove that for each element in $a$ in $G_1$, $f(a^{-1})=[f(a)]^{-1}$.} As $G_1$ is a group, there exists $a^{-1}\in G_1$ such that $a a^{-1}=e_1$, where $e_1$ is a neutral element in $G_1$. As $f:G_1\rightarrow G_2$ is an isomorphism, we have that $f(e_1)=f(a a^{-1})=f(a)f(a^{-1})$. By the previous problem $f(e_1)=e_2$, where $e_2$ is a neutral element in $G_2$. Therefore, we have $e_2=f(a)f(a^{-1})$. Notice that $f(a)\in G_2$ and, as $G_2$ is a group, it has an inverse\footnote{Note that this is not an inverse function, but an inverse element in $G_2$; in other words $f^{-1}(a)\neq[f(a)]^{-1}$.} $[f(a)]^{-1}\in G_2$. By multiplying $e_2=f(a)f(a^{-1})$ with $[f(a)]^{-1}$ on the left, we have $[f(a)]^{-1} e_2=[f(a)]^{-1}f(a)f(a^{-1})$, which is $[f(a)]^{-1}=f(a^{-1})$.

\item {\it If $G_1$ is a cyclic group with generator $a$, prove that $G_2$ is also a cyclic group, with generator $f(a)$.} As $G_1$ is a cyclic group with generator $a$ then every element in $G_1$ is of the form $a^n$, or $a^{-n}$, where $n\in\N$. Taking $f(a^n)=f(a a^{n-1})=f(a)f(a^{n-1})$ and continuing the process, we finally get $f(a^n)=\underbrace{f(a)f(a)\cdots f(a)}_{n\textnormal{ times}}=[f(a)]^n$. Same thing goes for $f(a^{-n})=\underbrace{f(a^{-1})f(a^{-1})\cdots f(a^{-1})}_{n\textnormal{ times}}=[f(a)]^{-n}$. As $f$ is bijective, every element in $G_2$ will be of this form (or a neutral element). Therefore $f(a)$ generates $G_2$.

\end{enumerate}

\noindent{\bf Problem.} Let $E$ designate the group of all the even integers, with respect to addition. Prove that $\Z\cong E$.

\noindent\newline{\bf Solution.} We can take the function $f:\Z\rightarrow E$ such that $f(x)=2x$. As this is a linear function, it is a bijection and

\begin{equation*}
f(x+y)=2(x+y)=2x+2y=f(x)+f(y).
\end{equation*}

\noindent\newline Therefore, $f$ is an isomorphism from $\Z$ to $E$ and $\Z\cong E$.

\noindent\newline{\bf Problem.} Let $G$ be the group $\{10^n:\ n\in\Z\}$ with respect to multiplication. Prove\footnote{Notice that when we're talking groups, we're always thinking of $\Z$ with addition; if we took multiplication, $(\Z,\cdot)$ would not be a group (monoid at best).} that $G\cong\Z$.

\noindent\newline{\bf Solution.} Let $f:G\rightarrow\Z$ be a function defined with $f(x)=\log{x}$. Logarithmic function is a bijection (also, notice that $x>0$ for all $x\in G$). We have:

\begin{equation*}
f(x y)=\log{x y}=\log{x}+\log{y}=f(x)+f(y).
\end{equation*}

\noindent\newline To conclude, $f$ is an isomorphism from $G$ to $\Z$ and $G\cong\Z$.

\noindent\newline{\bf Problem.} Prove\footnote{We're also thinking addition here, as multiplication for complex numbers is defined a bit different than that of the direct product of two groups.} that $\C\cong\R\times\R$.

\noindent\newline{\bf Solution.} If we took $f:\C\rightarrow\R\times\R$ such that $f(a+b i)=(a,b)$ we would be on our way of proving this isomorphism. We have to prove that this is a bijection. First, injectivity. If $(a,b)=(c,d)$ then $a=c$ and $b=d$ (as two ordered pairs are equal if and only if values at their respective places are the same). Therefore, $a+b i=c+d i$ (as two complex numbers are equal if and only if their real and imaginary values are the same). Surjectivity is trivial. If we took $(a,b)\in\R\times\R$, we can always find $(a+b i)\in\C$ such that $f(a+b i)=(a,b)$. In conclusion, $f$ is a bijection. Now (remember the definition of the direct product) we have:

\begin{eqnarray*}
f((a+b i)+(c+d i))&=&f((a+c)+(b+d)i)=(a+c,b+d)\\
&=&(a,b)+(c,d)=f(a+b i)+f(c+d i).
\end{eqnarray*}

\noindent\newline Thus, $f$ is an isomorphism from $\C$ to $\R\times\R$ and it follows that $\C\cong\R\times\R$.

\noindent\newline{\bf Problem.} Prove\footnote{$\R$ is a group of real numbers under addition, as multiplication requires excluding the zero. $\R^{+}$ is a group of real numbers under multiplication, as we don't have negative numbers to fulfill condition of existence of additive inverses; also, zero is excluded.} that $\R\cong\R^{+}$. Prove that $\R\ncong\R^{\ast}$ (remember that $\R^{\ast}$ is the group with $\R\backslash\{0\}$ under multiplication).

\noindent\newline{\bf Solution.} One of the functions of the form $f:\R\rightarrow\R^{+}$ is $f(x)=e^x$. Exponential function is a bijection. Furthermore,

\begin{equation*}
f(x+y)=e^{x+y}=e^x e^y=f(x)f(y).
\end{equation*}

\noindent\newline By that, $f$ is an isomorphism from $\R$ to $\R^{+}$ and $\R\cong\R^{+}$. Now, we want to show that $\R\ncong\R^{\ast}$. We will prove that by demonstrating that certain pairings are impossible. Suppose we have an isomorphism $f:\R\rightarrow\R^{\ast}$. As neutral elements must correspond, we have that $f(0)=1$. Also, elements which are their own inverses must correspond, and as $-1\cdot(-1)=1$, we have that $-1$ is its own inverse in $\R^{\ast}$. But, if such element were to exist in $\R$, it would have to be $x+x=0$. But, then it must be that $x=0$ and that is the only element that is it's own inverse in $\R$. So, we would have that $f(0)=-1$ and would have a contradiction to necessary condition that $f(0)=1$. So there does not exist an isomorphism $f$ from $\R$ to $\R^{\ast}$ (and reverse) and $\R\ncong\R^{\ast}$.

\noindent\newline{\bf Problem.} If we know generators and defining equations for two groups, $G$ and $G'$, and if we are able to match the generators of $G$ with those of $G'$ so that the defining equations are the same, we may conclude that $G\cong G'$. Prove that the following pairs of groups $G$, $G'$ are isomorphic:

\begin{enumerate}
\item $G$ is the subgroup of $S_4$ generated by $(2\ 4)$ and $(1\ 2\ 3\ 4)$;\\
$G'=\{e, a, b, b^2, b^3, a b, a b^2, a b^3\}$ where $a^2=e$, $b^4=e$ and $b a=a b^3$;
\item $G=S_3$; $G'=\{e, a, b, a b, a b a, a b a b\}$ where $a^2=e$, $b^2=e$, and $b a b=a b a$;
\item $G=D_4$; $G'=\{e, a, b, a b, a b a, (a b)^2, b a, b a b\}$ where $a^2=b^2=e$ and $(a b)^4=e$;
\item $G=\Z_2\times\Z_2\times\Z_2$; $G'=\{e, a, b, c, a b, a c, b c, a b c\}$ where $a^2=b^2=c^2=e$ and $(a b)^2=(b c)^2=(a c)^2=e$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $G$ is the subgroup of $S_4$ generated by $(2\ 4)$ and $(1\ 2\ 3\ 4)$;\\
$G'=\{e, a, b, b^2, b^3, a b, a b^2, a b^3\}$ where $a^2=e$, $b^4=e$ and $b a=a b^3$.}\\
As $(2\ 4)(2\ 4)=\epsilon$ and $(1\ 2\ 3\ 4)^4=\epsilon$ obviously $f((2\ 4))=a$ and $f((1\ 2\ 3\ 4))=b$. Now we have to check whether $(1\ 2\ 3\ 4)(2\ 4)=(2\ 4)(1\ 2\ 3\ 4)^3$. On the left-hand side we have $(2\ 1)(3\ 4)$ and on the right-hand side $(2\ 4)(1\ 4\ 3\ 2)=(1\ 2)(3\ 4)$. Therefore, as we were able to match generators and defining equations, we have that $G\cong G'$.

\item {\it $G=S_3$; $G'=\{e, a, b, a b, a b a, a b a b\}$ where $a^2=e$, $b^2=e$, and $b a b=a b a$.} $S_3$ is generated (as proved above) by $(1\ 2)$ and $(1\ 3)$. Obviously $(1\ 2)^2=\epsilon$ and $(1\ 3)^2=\epsilon$. We make a wild guess and take $f((1\ 2))=a$ and $f((1\ 3))=b$ (actually makes no difference as the defining equation is symmetrical). So we have $(1\ 3)(1\ 2)(1\ 3)=(3\ 2)$ and $(1\ 2)(1\ 3)(1\ 2)=(2\ 3)$. Obviously $S_3\cong G'$.

\item {\it $G=D_4$; $G'=\{e, a, b, a b, a b a, (a b)^2, b a, b a b\}$ where $a^2=b^2=e$ and $(a b)^4=e$.} Remember that generators of $D_4$ satisfy $c^2=e'$, $d^4=e'$ and $d c=c d^3$. In $G'$ we have $(a b)^4=e$, so we can confirm our suspicions that $f(d)=(a b)$. Also, if we multiply $d c=c d^3$ on the right with $d c$, we have $(d c)^2=e$. We can assume that $f(d c)=a$ and $f(c)=b$. And we can see that then $f(c^2)=f(c)f(c)=b b=b^2=e=f(e')$. Also, $f((d c)^2)=f(d c)f(d c)=a a=a^2=e=f(e')$ and $f(d^4)=f(d^2)f(d^2)=[f(d)]^4=(a b)^4=e=f(e')$. So $D_4\cong G'$.

\item {\it $G=\Z_2\times\Z_2\times\Z_2$; $G'=\{e, a, b, c, a b, a c, b c, a b c\}$ where $a^2=b^2=c^2=e$ and $(a b)^2=(b c)^2=(a c)^2=e$.} In $G$ we have ordered triples. Neutral element is $(0,0,0)$. For one thing, we have that $(0,0,1)+(0,0,1)=(0,0,0)$ and we can try $f(a)=(0,0,1)$. Similarly, we can take $f(b)=(0,1,0)$ and $f(c)=(1,0,0)$. Supposing $f$ is an isomorphism we would have $f((a b)^2)=f(a b)+f(a b)=(0,1,1)+(0,1,1)=(0,0,0)=f(e)$. We would also have $f((b c)^2)=(1,1,0)+(1,1,0)=(0,0,0)$ and $f((a c)^2)=(1,0,1)+(1,0,1)=(0,0,0)$. Thus, $G\cong G'$.
\end{enumerate}

\noindent{\bf Problem.} $G=\{x\in\R:\ x\neq-1\}$ with the operation $x\ast y=x+y+x y$. Show that $f(x)=x-1$ is an isomorphism from $\R^{\ast}$ to $G$. Thus, $\R^{\ast}\cong G$.

\noindent\newline{\bf Solution.} We have that $f$ is a linear function and is therefore an injection. To prove that it is a surjection, we only need to check what happens when $x=0$ (as it might copy it somewhere in $G$ and we excluded it). Obviously $f(0)=0-1=-1$ and $-1\notin G$. All other elements have their originals. Thus, $f$ is a bijection. Next, we have:

\begin{eqnarray*}
f(x)\ast f(y)&=&(x-1)\ast(y-1)=x-1+y-1+(x-1)(y-1)\\
&=&x-1+y-1+x y-x-y+1=x y-1=f(x y).
\end{eqnarray*}

\noindent\newline From that follows $f(x y)=f(x)\ast f(y)$ and from that $\R^{\ast}\cong G$.

\noindent\newline{\bf Problem.} $G$ is the set of the real numbers with the operation $x\ast y=x+y+1$. Find an isomorphism $f:\R\rightarrow G$ and show that it is an isomorphism.

\noindent\newline{\bf Solution.} We can try (after some thinking) $f(x)=x+2$. Then we have $f(x+y)=x+y+2=x+1+y+1=f(x)\ast f(y)$. Also, $f$ is a linear function from $\R$ to $\R$ (underlying set of $G$ is $\R$) so it is bijective and $\R\cong G$.

\noindent\newline{\bf Problem.} $G$ is the set of the nonzero real numbers with the operation $x\ast y=\frac{x y}{2}$. Find an isomorphism from $\R^{\ast}$ to $G$.

\noindent\newline{\bf Solution.} Similarly, after some trying out, we get that the best option is $f:\R^{\ast}\rightarrow G$ with $f(x)=2 x$ (also a linear function and a bijection; the only thing we need to worry is about zero, but $f(0)=0$ and zero is neither in $\R$ nor in $G$). We have $f(x y)=2 x y=\frac{(2 x)\cdot(2 y)}{2}=f(x)\ast f(y)$. Thus, $\R^{\ast}\cong G$.

\noindent\newline{\bf Problem.} Show that $f(x,y)=(-1)^y x$ is an isomorphism from $\R^{+}\times\Z_2$ to $\R^{\ast}$. Conclude that $\R^{\ast}\cong\R^{+}\times\Z_2$.

\noindent\newline{\bf Solution.} First we will show that $f$ is an injection. If $f(x,y)=f(z,w)$ then we have $(-1)^y x=(-1)^z w$. Dividing equality by $x$ (we can do that as $x\in\R^{+}$) and by $(-1)^z$ (obviously $(-1)^z\neq 0$, for all $z\in\Z_2$) we have $\frac{(-1)^y}{(-1)^z}=\frac{w}{x}$. Notice that the left side will always be either $1$ or $-1$. That can be only if $w=x$. But what if it were $y\neq z$, e.g., without loss of generality, $y=1$ and $z=0$? Then, from the starting equality, we would have $-x=w$. But, as $x=w$, it would follow that $-x=x$, meaning $x$ can only be zero. That cannot be as $x\in\R^{+}$. Therefore, it must be that $y=z$. As we deduced from $f(x,y)=f(z,w)$ that $(x,y)=(z,w)$, $f$ is an injection. As for the surjectivity, we take a non zero number of the form $z=(-1)^y x$. If $z<0$ then $y=1$ and if $z>0$ then $y=0$ (as $x$ is always positive). So it only remains the problem of picking $x$. For $z>0$ we have $z=x$ and for $z<0$ we have $z=-x$, i.e. $x=-z$. To sum it all up, $x=|z|$. Thus for every $z\in\R^{\ast}$ there is an ordered pair $(x,y)\in\R^{+}\times\Z_2$ such that $z=(-1)^y x$, and because of that, $f$ is a surjection. Now we only check second condition for isomorphism:

\begin{equation*}
f((x,y)\cdot(z,w))=f(x z,y w)=(-1)^{y w} x z=(-1)^y x (-1)^w z=f(x,y)f(z,w).
\end{equation*}

\noindent\newline Therefore, $f$ is an isomorphism from $\R^{+}\times\Z_2$ to $\R^{\ast}$. From that it follows that $\R^{+}\times\Z_2\cong\R^{\ast}$, and because isomorphism is an equivalence relation, we have that $\R^{\ast}\cong\R^{+}\times\Z_2$ (symmetry).

\noindent\newline{\bf Problem.} Let $G$ and $H$ be groups. Prove that $G\times H\cong H\times G$.

\noindent\newline{\bf Solution.} If we take $f:G\times H\rightarrow H\times G$ such that $f(x,y)=(y,x)$ it will be a bijection. If $f(x,y)=f(z,w)$ then $(y,x)=(w,z)$ and from that follows $y=w$ and $x=z$ which is the same as $(x,y)=(z,w)$, thus it is an injection. If we take $(y,x)\in H\times G$, then there exists $(z,w)\in G\times H$ such that $f(z,w)=(y,x)$ and that is $(z,w)=(x,y)$. Furthermore, we have $f((x,y)(z,w))=f(x z,y w)=(y w, x z)=(y,x)(w,z)=f(x,y)f(z,w)$. In conclusion, $f$ is an isomorphism and $G\times H\cong H\times G$.

\noindent\newline{\bf Problem.} Let $G$ be any group. Prove that $G$ is Abelian if and only if the function\footnote{Not to be confused with the more specific $f(x)=\frac{1}{x}$ function on real numbers (without zero).} $f(x)=x^{-1}$ is an isomorphism from $G$ to $G$.

\noindent\newline{\bf Solution.} {\it Necessity.} Let $G$ be Abelian, i.e. $x y=y x$ for all $x,y\in G$. We need to prove that $f(x)=x^{-1}$ is an isomorphism from $G$ to $G$. We have that $f$ is an injection as from $f(x)=f(y)$, i.e. $x^{-1}=y^{-1}$ follows that, after multiplying equality with $x$ on the left and $y$ on the right, we have $x x^{-1} y=x y^{-1} y$ and that is $y=x$. If $y\in G$ then we need to find $x\in G$ such that $f(x)=y$. But, as $G$ is a group, every element has an inverse, so we take $x=y^{-1}$ and have\footnote{Proved at the beginning of the script.} $f(y^{-1})=(y^{-1})^{-1}=y$. Thus, $f$ is a surjection. Then we have $f(x y)=(x y)^{-1}=y^{-1} x^{-1}$. But, as $G$ is Abelian, we have $y^{-1} x^{-1}=x^{-1} y^{-1}=f(x)f(y)$. Therefore, $f$ is an isomorphism from $G$ to $G$. {\it Sufficiency.} Let $f$ be an isomorphism from $G$ to $G$. We need to prove that $G$ is Abelian. We have that $f$ is a bijection and that $f(x y)=f(x)f(y)$ for all $x,y\in G$. That means that $f(x,y)=(x y)^{-1}=x^{-1} y^{-1}=f(x)f(y)$. That is, $y^{-1} x^{-1}=x^{-1} y^{-1}$. Multiplying this equality on the left and right by $y$, we have $x^{-1} y=y x^{-1}$. Then, multiplying with $x$ on left and right, we have $y x=x y$ (which is valid for all $x,y\in G$, of course). Therefore, $G$ is Abelian.

\noindent\newline{\bf Problem.} Let $G$ be any group, with it operation denoted multiplicatively. Let $H$ be a group with the same set as $G$ and let its operation be defined by $x\ast y=y\cdot x$ (where $\cdot$ is the operation of $G$). Prove that $G\cong H$.

\noindent\newline{\bf Solution.} Let $f$ be a mapping from $H$ to $G$ such that $f(x)=x$. As $f$ is identity from the set $H$ (or $G$) to itself (underlying sets of $G$ and $H$ are the same), it is a bijection. We have: 

\begin{equation*}
f(x\ast y)=x\ast y=y\cdot x=f(y)\cdot f(x)=f(x)\ast f(y).
\end{equation*}

\noindent\newline Therefore, $f$ is an isomorphism from $H$ to $G$ and from that fact we have $G\cong H$.

\noindent\newline{\bf Problem.} Let $c$ be a fixed element of $G$. Let $H$ be a group with the same set as $G$, and with the operation $x\ast y=x c y$. Prove that the function $f(x)=c^{-1} x$ is an isomorphism from $G$ to $H$.

\noindent\newline{\bf Solution.} First we will show that $f$ is a bijection. Let $f(x)=f(y)$. Then $c^{-1} x=c^{-1} y$. As $c\in G$ and $G$ is a group, we can multiply this equation with $c$ on the left to get $x=y$. From this follows that $f$ is an injection. Next, if we take $y\in H$ we need to find $x\in G$ such that $c^{-1} x=y$. And that is $x=c y$. As $y\in H$, it is also in $G$ (underlying sets are same), and the product $c y$ is in $G$ (as $G$ is a group). Such element exists in $G$ and $f$ is a surjection, and by that a bijection. Furthermore, consider:

\begin{equation*}
f(x y)=c^{-1} x y=c^{-1} x c c^{-1} y=f(x) c f(y)=f(x)\ast f(y).
\end{equation*}

\noindent\newline Then, $f$ is an isomoprhism from $G$ to $H$ and $G\cong H$.

\noindent\newline{\bf Definition.} If $G$ is a group, an {\bf automorphism} of $G$ is an isomorphism from $G$ to $G$.

\noindent\newline{\bf Comment.} Since an automorphism is a bijection from $G$ to $G$ it is a permutation of $G$.

\noindent\newline{\bf Problem.} Prove the following:

\begin{enumerate}
\item Permutation $f=(1\ 5)(2\ 4)$ (where $f\in S_6$) is an automorphism of $\Z_6$;
\item Permutations $f_1=(1\ 2\ 4\ 3)$, $f_2=(1\ 3\ 4\ 2)$ and $f_3=(1\ 4)(2\ 3)$ (where $f_i\in S_5$) are all automorphisms of $\Z_5$;
\item If $G$ is any group, and $a$ is any element of $G$, then $f(x)=a x a^{-1}$ is an automorphism of $G$;
\item The set $\textnormal{Aut}(G)$ of all automorphisms of $G$ is a subgroup of $S_G$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it Permutation $f=(1\ 5)(2\ 4)$ (where $f\in S_6$) is an automorphism of $\Z_6$.} Group $\Z_6$ is a cyclic group generated by $1$ with a defining equation $6\cdot 1=0$ (multiplication here is a shorthand for more consecutive additive operations). Now, notice that $\Z_6$ can also be generated by $5$ with a defining equation $6\cdot 5=0$. We will see that it is true, as we have $f(0)=0$, $f(1)=5$, $f(1+_6 1)=f(2)=4=5+_6 5$, $f(2+_6 1)=f(3)=3=5+_6 5+_6 5$, $f(3+_6 1)=f(4)=2=5+_6 5+_6 5+_6 5$, $f(4+_6 1)=f(5)=1=5+_6 5+_6 5+_6 5+_6 5$ and $f(5+_6 1)=f(0)=0$. As generators and defining equations correspond, $f$ is an automorphism of $\Z_6$.

\item {\it Permutations $f_1=(1\ 2\ 4\ 3)$, $f_2=(1\ 3\ 4\ 2)$ and $f_3=(1\ 4)(2\ 3)$ (where $f_i\in S_5$) are all automorphisms of $\Z_5$.} Elements $2$, $3$ and $4$ all generate $\Z_5$ (as they are relatively prime to $5$; reader can easily check this fact). The defining equations are $5\cdot 2=0$, $5\cdot 3=0$ and $5\cdot 4=0$, respectively. Now, for $f_1$, we have $f_1(1)=2$, $f_1(2)=4=2+_5 2$, $f_1(3)=1=2+_5 2+_5 2$, $f_1(4)=3=2+_5 2+_5 2+_5 2$ and $f_1(0)=0$. For $f_2$, we have $f_2(1)=3$, $f_2(2)=1=3+_5 3$, $f_2(3)=4=3+_5 3+_5 3$, $f_2(4)=2=3+_5 3+_5 3+_5 3$ and $f_2(0)=0$. Furthermore, $f_3(1)=4$, $f_3(2)=3=4+_5 4$, $f_3(3)=2=4+_5 4+_5 4$, $f_3(4)=1=4+_5 4+_5 4+_5 4$ and $f_3(0)=0$. Therefore, $f_1$, $f_2$ and $f_3$ are automorphisms of $\Z_5$.
\item {\it If $G$ is any group, and $a$ is any element of $G$, then $f(x)=a x a^{-1}$ is an automorphism of $G$.} First, we will check whether $f$ is bijective. As $f(x)=f(y)$ will, when we multiply the equation $a x a^{-1}=a y a^{-1}$ with $a^{-1}$ on the left and $a$ on the right, imply $x=y$, function $f$ is injective. Surjectivity will hold as for any $y\in G$ we can find $x\in G$ such that $y=f(x)$; we have $y=a x a^{-1}$ and multiplying it with $a^{-1}$ on the left and $a$ on the right will yield $x=a^{-1} y a$. Then, $x$ is obviously in $G$, as $G$ is a group, closed with respect to inverses and multiplication. Therefore, $f$ is bijective. For now we can say that $f$ is a permutation. But, also:

\begin{equation*}
f(x y)=a x y a^{-1}=a x a^{-1} a y a^{-1}=f(x)f(y).
\end{equation*}

\noindent\newline Therefore, $f$ is an automorphism of $G$.

\item {\it The set $\textnormal{Aut}(G)$ of all automorphisms of $G$ is a subgroup of $S_G$.} As $f\in\textnormal{Aut}(G)$ implies $f$ is an automorphism, and by that a permutation, then $f\in S_G$. From that we have that $\textnormal{Aut}(G)\subseteq S_G$. If we take $f,g\textnormal{Aut}(G)$, their product $f g$ will be in $\textnormal{Aut}(G)$, as product of two permutations is a permutation, and as $f(x y)=f(x)f(y)$ and $g(x y)=g(x)g(y)$, we have $f(g(x y))=f(g(x)g(y))=f(g(x))f(g(y))$, their product is (an isomorphism and) an automorphism of $G$. If we take $f\in\textnormal{Aut}(G)$, we have that $f(x y)=f(x)f(y)$.  From that, we have $f(f^{-1}(x)f^{-1}(y))=f(f^{-1}(x))f(f^{-1}(y))=x y$. Then, applying $f^{-1}\in S_G$ on this equation gives us $f^{-1}(x)f^{-1}(y)=f^{-1}(x y)$. Therefore, $f^{-1}$ is an automorphism of $G$, and $\textnormal{Aut}(G)$ is closed with respect to multiplication and inverses. Thus, $\textnormal{Aut}(G)$ is a subgroup of $S_G$.
\end{enumerate}

\newpage

\begin{center}
{\bf Order of group elements}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group, $n\in\N$ and $a\in G$. We define:

\begin{eqnarray*}
a^n&=&\underbrace{a\cdot a\cdots a}_{n\textnormal{ times}},\\\\
a^{-n}&=&\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{n\textnormal{ times}},\\\\
a^0&=&e,
\end{eqnarray*}

\noindent\newline where $e\in G$ is a neutral element of $G$ and $a^{-1}\in G$ inverse of $a$.

\noindent\newline{\bf Proposition.} If $G$ is a group, $a\in G$ and $m,n\in\Z$, then the following holds:

\begin{enumerate}
\item $a^m a^n=a^{m+n}$,
\item $\left(a^m\right)^n=a^{m n}$,
\item $a^{-n}=\left(a^{-1}\right)^n=\left(a^n\right)^{-1}$.
\end{enumerate}

\noindent{\bf Proof.} First we will prove for $m>0$ and $n>0$. Ad $1$. If $m>0$ and $n>0$, we have:

\begin{equation*}
a^m a^n=\underbrace{a\cdot a\cdots a}_{m\textnormal{ times}}\underbrace{a\cdot a\cdots a}_{n\textnormal{ times}}=\underbrace{a\cdot a\cdots a}_{m+n\textnormal{ times}}=a^{m+n}.
\end{equation*}

\noindent\newline Ad $2$. For $m>0$ (or $m\leq 0$; observe it does not change anything) and $n>0$:

\begin{equation*}
\left(a^m\right)^n=\underbrace{a^m\cdot a^m\cdots a^m}_{n\textnormal{ times}}=\underbrace{a\cdot a\cdots a}_{m n\textnormal{ times}}.
\end{equation*}

\noindent\newline Ad $3$. If we have $n>0$, then:

\begin{equation*}
a^{-n}=\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{n\textnormal{ times}}=\left(a^{-1}\right)^n.
\end{equation*}

\noindent\newline Also, we have, by a previous proposition, that:

\begin{equation*}
(a^n)^{-1}=\left(\underbrace{a\cdot a\cdots a}_{n\textnormal{ times}}\right)^{-1}=\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{n\textnormal{ times}}=(a^{-1})^n=a^{-n}.
\end{equation*}

\noindent\newline Now, we will prove other cases. Ad $1$. If $m<0$ and $n>0$. We will take $k=-m$. That way, $k\in\N$ and we have:

\begin{equation*}
a^m a^n=a^{-k} a^n=\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{k\textnormal{ times}}\underbrace{a\cdot a\cdots a}_{n\textnormal{ times}}.
\end{equation*}

\noindent\newline If $k>n$ then we have $n$ pairs of $a^{-1} a$ and the rest is $a^{-1}$, and $k-n$ of them. Therefore we have $a^m a^n=\left(a^{-1}\right)^{k-n}$. As $k>n$ then $k-n$ is positive, and by $(3)$ we have $a^m a^n=\left(a^{-1}\right)^{k-n}=a^{n-k}=a^{m+n}$. If $k<n$ then we have $k$ pairs of $a^{-1} a$ and the $n-k$ rest is $a$. Therefore we have again $a^{m+n}=a^{n-k}=a^{m+n}$. The same proof goes when $m>0$ and $n<0$. If $m<0$ and $n<0$, and we take $p=-m$ and $q=-n$, then $p$ and $q$ are natural numbers. Then, $a^m a^n=a^{-p} a^{-q}$. By definition:

\begin{equation*}
a^m a^n=a^{-p} a^{-q}=\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{p\textnormal{ times}}\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{q\textnormal{ times}}.
\end{equation*}

\noindent\newline We have $a^{-1}$ multiplied by itself $p+q$ times, therefore $a^m a^n=\left(a^{-1}\right)^{p+q}$. As $(p+q)\in\N$, we can use $(3)$ and get:

\begin{equation*}
a^m a^n=a^{-(p+q)}=a^{-p+(-q)}=a^{m+n}.
\end{equation*}

\noindent\newline If $m=0$ or $n=0$ (or both) it's trivial to see that $a^m a^0=a^m e=a^m=a^{m+0}$, $a^0 a^0=e e=e=a^0=a^{0+0}$, et cetera. Ad $2$. If $m<0$ and $n>0$. Take $m=-p$ and we get, with positive $p$,

\begin{equation*}
\left(a^m\right)^n=\left(a^{-p}\right)^n=\left(\underbrace{a^{-1}\cdot a^{-1}\cdots a^{-1}}_{p\textnormal{ times}}\right)^n=\left(a^{-1}\right)^{p n}.
\end{equation*}

\noindent\newline By using $(3)$ we have $\left(a^m\right)^n=a^{-p n}=a^{m n}$. If $m>0$ and $n<0$, and we take $n=-q$, then $q\in\N$, and we have:

\begin{equation*}
\left(a^m\right)^n=\left(a^m\right)^{-q}=\underbrace{\left(a^m\right){-1}\cdot\left(a^m\right)^{-1}\cdots\left(a^m\right)^{-1}}_{q\textnormal{ times}}.
\end{equation*}

\noindent\newline As $m>0$, by using $(3)$, we have $\left(a^m\right)^{-1}=a^{-m}$. Furthermore,

\begin{equation*}
\left(a^m\right)^n=\underbrace{a^{-m}\cdot a^{-m}\cdots a^{-m}}_{q\textnormal{ times}}=\left(a^{-m}\right)^q.
\end{equation*}

\noindent\newline Now we have the case we already proved just a moment ago (for $m<0$ and $n>0$) and $\left(a^m\right)^n=a^{-m q}=a^{m n}$. Now, to prove when $m<0$ and $n<0$. We take $m=-p$ and $n=-q$. Then,

\begin{equation*}
\left(a^m\right)^n=\left(a^{-p}\right)^{-q}=\underbrace{\left(a^{-p}\right)^{-1}\cdot\left(a^{-p}\right)^{-1}\cdots\left(a^{-p}\right)^{-1}}_{q\textnormal{ times}}.
\end{equation*}

\noindent\newline Providing we prove that $\left(a^{-p}\right)^{-1}=a^{-(-p)}=a^p$ (for positive $p$) we will have:

\begin{equation*}
\left(a^m\right)^n=\left(a^{-p}\right)^{-q}=\underbrace{a^p\cdot a^p\cdots a^p}_{q\textnormal{ times}}=a^{p q}=a^{(-m)\cdot(-n)}=a^{m n}.
\end{equation*}

\noindent\newline If $m=0$ or $n=0$ (or both), then the cases are trivial. We have $\left(a^0\right)^n=e^n=e=a^0=a^{n\cdot 0}$, then $\left(a^m\right)^0=e=a^0=a^{m\cdot 0}$ and $\left(a^0\right)^0=e^0=e=a^0=a^{0\cdot 0}$. Ad $3$. Now, to justify for $n<0$, we will take $n=-p$ and have $a^{-n}=a^{-(-p)}=a^p$. As proven previously that $\left(a^{-1}\right)^{-1}=a$, we have:

\begin{equation*}
a^{-n}=a^p=\left(\left(a^{-1}\right)^{-1}\right)^p=\left(a^{-1}\right)^{-p}=\left(a^{-1}\right)^n.
\end{equation*}

\noindent\newline Similarly, we have:

\begin{equation*}
\left(a^{-p}\right)^{-1}=\left(\left(a^{-1}\right)^p\right)^{-1}=\underbrace{\left(a^{-1}\right)^{-1}\cdot\left(a^{-1}\right)^{-1}\cdots\left(a^{-1}\right)^{-1}}_{p\textnormal{ times}}=\underbrace{a\cdot a\cdots a}_{p\textnormal{ times}}=a^p=a^{-n}.
\end{equation*}

\noindent\newline Finally, we have $a^{-0}=a^0=e=e^{-1}=\left(a^0\right)^{-1}$ and $a^{-0}=a^0=e=\left(a^{-1}\right)^0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If there exists $m\in\Z^{\ast}$ such that $a^m=e$, then the {\bf order of the element} $a$ is defined to be the least $n\in\N$ such that $a^n=e$, that is:

\begin{equation*}
n=\min\left(\{m\in\Z^{\ast}:\ a^m=e\}\cap\N\right).
\end{equation*}

\noindent\newline Then we write $\ord{a}=n$. If there does not exist $m\in\Z^{\ast}$ such that $a^m=e$, we say that $a$ has {\bf order infinity} and we write $\ord{a}=\infty$.

\noindent\newline{\bf Proposition.} Let $G$ be a group. If there exists $m\in\Z^{\ast}$ such that $a^m=e$, where $a,e\in G$, then there exists $n\in\N$ such that $a^n=e$.

\noindent\newline{\bf Proof.} If $m>0$ then $n=m$. If $m<0$, then we take $m=-n$ and have $a^{-n}=e$. Multiplying by $a^n$ on the right gives us $a^{-n}a^n=e a^n$ and that is $e=a^n$. This concludes the proof.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent Furthermore, let us remind ourselves of one important theorem (divison with remainder). I will ommit the proof, as I already have it in my work on elementary number theory, discussed in much detail.

\noindent\newline{\bf Theorem (division with remainder).} Let $a\in\Z$ and $b\in\Z^{\ast}$. There exist unique $q,r\in\Z$ such that $a=b q+r$ and $0\leq r<|b|$.

\noindent\newline{\bf Proof.} {\it Discussed in my work on elementary number theory.}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent This we shall use to prove the following theorem.

\noindent\newline{\bf Theorem.} Let $G$ be a group and $a\in G$. If $\ord{a}=n$, then there exists $k\in\{0,1,\ldots,n-1\}$ such that $a^k=a^m$ for all $m\in\Z$. Furthermore $a^i\neq a^j$, for all $i,j\in\{0,1,\ldots,n-1\}$ and $i\neq j$.

\noindent\newline{\bf Proof.} As $m\in\Z$ and $n\in\N$, id est $n\in\Z^{\ast}$ (note that order of a group element is defined as a natural number, and is therefore a positive integer) then, by division with remainder theorem, there exist unique $q,r\in\Z$ such that $m=n q+r$ and $0\leq r<|n|=n$. Therefore $a^m$ can be written as $a^{n q+r}$. By previous proposition, we have that $a^m a^n=a^{m+n}$ and $\left(a^m\right)^n=a^{m n}$, for all $m,n\in\Z$. Therefore:

\begin{equation*}
a^m=a^{n q+r}=a^{n q}a^r=\left(a^n\right)^q a^r.
\end{equation*}

\noindent\newline By definition of order of group element, as we have $\ord{a}=n$, then $a^n=e$. So we have $a^m=e^q a^r=a^r$. But, by division with remainder theorem, we have that $0\leq r<n$, i.e. $r\in\{0,1,\ldots,n-1\}$. So we can take $k=r$ and have $a^m=a^k$.

Now suppose that $a^i=a^j$, for some $i,j\in\{0,1,\ldots,n-1\}$ and $i\neq j$. Also suppose that $i<j$. Note that $i,j<n$. Then we can take some $k\in\N$ such that $j=i+k$. As $j<n$ then $i+k<n$, that is $k<n-i<n$. From that it follows that $a^i=a^{i+k}=a^i a^k$. But, that would mean, if we multiply this expression by $a^{-i}$ on the left, that $e=a^k$. Then the order of $a$ is either $k<n$, which is a contradiction to the assumption that the order is $n$ (that $n$ is the least positive integer such that $a^n=e$), or $k=n$, which would mean that $j=i+n$, contradicting assumption that $j<n$. Thus, it has to be that $a^i\neq a^j$ for all $i,j\in\{0,1,\ldots,n-1\}$ where $i\neq j$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$. If $\ord{a}=\infty$, then all powers of $a$ are different, i.e. $a^i\neq a^j$ for all $i,j\in\Z$ such that $i\neq j$.

\noindent\newline{\bf Proof.} Suppose that $a^i=a^j$ for some $i,j\in\Z$ and $i\neq j$. Suppose that $i<j$. That means that $j=i+k$, where $k\in\Z$. If we multiply the expression $a^i=a^{i+k}$ by $a^{-i}$ on the left, we get $e=a^{-i}a^{i+k}$, that is, $e=a^{i+k}$. But, as $\ord{a}=\infty$, there does not exist $n\in\Z^{\ast}$ such that $a^n=e$. Therefore, the only possible option is that $i+k=0$ so that $e=a^0$. That would mean that $k=-i$ and that $j=i-i=0$. Then from assumption that $a^i=a^j$ would follow that $a^i=a^0=e$ and it must be that $a^i=e$. Again, as $\ord{a}=\infty$, it must be that $i=0$, contradicting our assumption that $i\neq j$. In conclusion, all powers of $a$ are different (if $\ord{a}=\infty$).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$. If $\ord{a}=n$ and $a^t=e$ for some $t\in\Z$, then $n|t$.

\noindent\newline{\bf Proof.} Suppose that $\ord{a}=n$, $a^t=e$ for some $t\in\Z$, but $n\nmid t$, i.e. by division with remainder theorem, we have that $t=n q+r$, where $q,r\in\Z$ such that $0\leq r<|n|=n$. That means that $a^{n q+r}=e$. We have $a^{n q} a^r=e$ and from that $(a^n)^q a^r=e$. As $\ord{a}=n$ we have $a^n=e$ and from that $e^q a^r=e$, that is, $a^r=e$. But, $n$ is the least positive number such that $a^n=e$ and $r<n$. Therefore, as $r$ cannot be $n$, it can only be zero, that is $r=0$. Then, from $t=n q+r$, we have $t=n q+0=n q$. In other words, there exists $q\in\Z$ such that $t=n q$, which means that $n|t$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a,e\in G$. Then, $\ord{a}=1$ if and only if $a=e$.

\noindent\newline{\bf Proof.} {\it Necessity.} If $\ord{a}=1$ then $a^1=e$ and that is $a=e$. {\it Sufficiency.} If $a=e$, i.e. $a^1=e$, obviously $\ord{a}=1$, as there is no $n\in\N$ such that $n<1$ (that is, $1$ is the least natural number such that $a^n=e$).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Determine the order of: (a) $10\in\Z_{25}$, (b) $6\in\Z_{16}$, (c) $f=\left(1\ 6\ 4\ 2\right)\in S_6$, (d) $1\in\R^{\ast}$, (e) $1\in\R$, (f) $f\in S_A$, where $A=\R\backslash\{0,1,2\}$ and $f(x)=\frac{2}{2-x}$.

\noindent\newline{\bf Solution.} (a) $\ord{10}=5$ as $5\cdot 10\equiv 0\pmod 25$, and it's the smallest one (notice that $5=\gcd{(10,25)}$). (b) $\ord{6}=8$ as $8\cdot 6\equiv 0\pmod 16$, and it's the smallest one as $\gcd{(6,16)}=48$. (c) For a cycle $c$ of length $n$ we have that $c^n=\epsilon$ (proven previously). As length of $f$ is $4$, we have $f^4=\epsilon$, thus $\ord{f}=4$. (d) In $\R^{\ast}$ neutral element is $1$, therefore, by previous proposition, $\ord{1}=1$. (e) Obviously $\ord{1}=\infty$ as there does not exist $n\in\N$ such that $n\cdot 1=0$. (f) We have:

\begin{eqnarray*}
\left[f\circ f\right](x)&=&\frac{2}{2-\frac{2}{2-x}}=\frac{2}{\frac{2-2 x}{2-x}}=\frac{2-x}{1-x}.\\\\
\left[f\circ f\circ f\right](x)&=&\frac{2-\frac{2}{2-x}}{1-\frac{2}{2-x}}=\frac{\frac{2-2x}{2-x}}{\frac{-x}{2-x}}=\frac{2x-2}{x}.\\\\
\left[f\circ f\circ f\circ f\right](x)&=&\frac{2\cdot\frac{2}{2-x}-2}{\frac{2}{2-x}}=\frac{\frac{2x}{2-x}}{\frac{2}{2-x}}=x.
\end{eqnarray*}

\noindent\newline Therefore, $\ord{f}=4$.

\noindent\newline{\bf Problem.} Can an element of an infinite group have finite order?

\noindent\newline{\bf Solution.} Group $\C^{\ast}$ is infinite. It is a group, as associativity holds $z_1(z_2 z_3)=(z_1 z_2)z_3$, for all $z_1,z_2,z_3\in\C^{\ast}$. Neutral element is $1\in\C^{\ast}$ as $z\cdot 1=z$, for all $z\in\C^{\ast}$. Each element has an inverse, $(a+b i)^{-1}=\frac{a-b i}{a^2+b^2}$, for all $(a+b i)\in\C^{\ast}$. But, taking roots of unity $\omega_n^i\in\C^{\ast}$, where $i\in\{0,\ldots,n-1\}$ and $n\in\N\backslash\{1\}$, we have that $\omega_n^n=1$, i.e. $\ord{\omega_n}=n$. Same thing would go for $-1\in\R^{\ast}$, as $(-1)^2=1$, that is $\ord{-1}=2$ (notice that $-1$ is also a root of unity for $n=2$).

\noindent\newline{\bf Problem.} In $\Z_{24}$, list all the elements of order (a) $2$, (b) $3$, (c) $4$, (d) $6$.

\noindent\newline{\bf Solution.} In order to solve $n x\equiv 0\pmod 24$ for $x$, where $n\in\{2,3,4,6\}$, we actually have $n x-0=24k$, i.e. $n x=24 k$, where $k\in\Z$. In other words we need to observe only multiples of $24$, e.g. $24$, $48$, $72$, $96$, $120$, etc. (a) We only have $2\cdot 12=24$, there is no other element between $12$ and $24$ that would yield $24$ or $48$ when multiplied by $2$. So, $\ord{12}=2$. (b) We have $3\cdot 8=24$ and $3\cdot 16=48$, so $\ord{8}=3$ and $\ord{16}=3$. (c) We have $4\cdot 6=24$, $4\cdot 12=48$ (but this one does not count as also $2\cdot 12=24$) and $4\cdot 18=72$, therefore $\ord{6}=4$ and $\ord{18}=4$. (d) We have $6\cdot 4=24$, $6\cdot 8=48$ (but this one does not count as $3\cdot 8=24$), then $6\cdot 12=72$ (also does not count as $2\cdot 12=24$), $6\cdot 16=96$ (does not count as $3\cdot 16=48$) and $6\cdot 20=120$. So, $\ord{4}=6$, $\ord{20}=6$.

\noindent\newline{\bf Theorem.} Let $G$ be a group and $a_1,\ldots,a_n\in G$. Then, for any $m\in\N$ and $1<k<n$ we have that

\begin{equation*}
\left(a_1 a_2\cdots a_n\right)^{m}=e,
\end{equation*}

\noindent\newline implies that

\begin{equation*}
\left(a_k a_{k+1}\cdot a_n a_1 a_2\ldots a_{k-1}\right)^{m}=e.
\end{equation*}

\noindent\newline{\bf Proof.} We rewrite the first expression as:

\begin{equation*}
\left(a_1 a_2\cdots a_n\right)\left(a_1 a_2\cdots a_n\right)^{m-2}\left(a_1 a_2\cdots a_n\right)=e.
\end{equation*}

\noindent\newline Multiplying by $\left(a_1 a_2\cdots a_{k-1}\right)^{-1}$ on the left and by $\left(a_1 a_2\cdots a_{k-1}\right)$ on the right gives us:

\begin{eqnarray*}
e&=&\left(a_1 a_2\cdots a_{k-1}\right)^{-1}\left(a_1 a_2\cdots a_{k-1} a_k a_{k+1}\cdots a_n\right)\left(a_1 a_2\cdots a_n\right)^{m-2}\\
&\cdot&\left(a_1 a_2\cdots a_{k-1} a_k a_{k+1}\cdots a_n\right)\left(a_1 a_2\cdots a_{k-1}\right).
\end{eqnarray*}

\noindent\newline That leaves us with:

\begin{equation*}
\left(a_k a_{k+1}\cdots a_n\right)\left(a_1 a_2\cdots a_n\right)^{m-2}\left(a_1 a_2\cdots a_{k-1} a_k a_{k+1}\cdot a_n\right)\left(a_1 a_2\cdots a_{k-1}\right)=e.
\end{equation*}

\noindent\newline Using associativity in $G$, and as the elements are "in the same order", and their numbers are equal, i.e. we have $a_i$ for $1+m-2+1$ times for $k\leq i\leq n$ and $m-2+1+1$ times for $1\leq i\leq k-1$, it is obvious that this is equivalent to:

\begin{equation*}
\left(a_k a_{k+1}\cdots a_n a_1 a_2\cdots a_{k-1}\right)^{m}=e.
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $a$, $b$ and $c$ be elements of a group $G$. Prove the following:

\begin{enumerate}
\item If $\ord{a}=n$, then $a^{n-r}=\left(a^r\right)^{-1}$;
\item If $a^k=e$ where $k$ is odd, then the order of $a$ is odd;
\item $\ord{a}=\ord{b a b^{-1}}$;
\item $\ord{a^{-1}}=\ord{a}$;
\item The order of $a b$ is the same as the order of $b a$;
\item $\ord{a b c}=\ord{c a b}=\ord{b c a}$;
\item Let $x=a_1 a_2 \cdots a_n$, and let $y=a_k a_{k+1}\cdots a_n a_1\cdots a_{k-1}$, where $1<k<n$. Then $\ord{x}=\ord{y}$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $\ord{a}=n$, then $a^{n-r}=(a^r)^{-1}$.} Let $\ord{a}=n$. That means that $a^n=e$. If we multiply this by $a^{-r}$ on the right we have $a^n a^{-r}=e a^{-r}$. But, $a^n a^{-r}=a^{n+(-r)}=a^{n-r}$ and $e a^{-r}=a^{-r}=\left(a^r\right)^{-1}$, so $a^{n-r}=\left(a^r\right)^{-1}$.

\item {\it If $a^k=e$ where $k$ is odd, then the order of $a$ is odd.} By a previous proposition, if $m=\ord{a}$, then $m|k$. Therefore, there exists $n\in\Z$ such that $k=n m$. If $m$ were even, then whether $n$ were even or odd, $k$ would be even also, contrary to the assumption that $k$ is odd. Therefore $m=\ord{a}$ must be odd.

\item {\it $\ord{a}=\ord{b a b^{-1}}$.} Let $m=\ord{a}$ and $n=\ord{b a b^{-1}}$. That means that if $a^k=e$, for some $k\in\N$, then $m\leq k$ (order is the least positive integer for which $a^k=e$). The same thing goes if $\left(b a b^{-1}\right)^l=e$, for some $l\in\N$, then $n\leq l$. First, if we take $a^m=e$ and we multiply it by $b^{-1}$ on the right and $b$ on the left, we have $b a^m b^{-1}=b b^{-1}$. Now, we can write that down as:

\begin{equation*}
b\underbrace{a\cdot a\cdots a}_{m\textnormal{ times}}b^{-1}=e.
\end{equation*}

\noindent\newline But, between each $a\cdot a$ we can put $e=b b^{-1}$, so that we have string of $a b^{-1} b a=a a$. We will put in $m-1$ of neutral elements written down as $b^{-1} b$. That is (counting in $b$ on the left and $b^{-1}$ on the right):

\begin{equation*}
\underbrace{\left(b a b^{-1}\right)\cdot\left(b a b^{-1}\right)\cdots\left(b a b^{-1}\right)}_{m\textnormal{ times}}=e.
\end{equation*}

\noindent\newline But, that means that $\left(b a b^{-1}\right)^m=e$ and by definition of order of $b a b^{-1}$, we have that $n\leq m$. On the other hand, we have $\left(b a b^{-1}\right)^n=e$. That can be written as:

\begin{equation*}
\underbrace{\left(b a b^{-1}\right)\cdot\left(b a b^{-1}\right)\cdots\left(b a b^{-1}\right)}_{n\textnormal{ times}}=e.
\end{equation*}

\noindent\newline But, each $b b^{-1}$ can be eliminated as, obviously, $b b^{-1}=e$. All that is left is $b a^n b^{-1}=e$. Multiplying by $b^{-1}$ on the left and $b$ on the right we have $a^n=e$. And, by definition of order of $a$, we have that $m\leq n$. Combining $m\leq n$ and, previously obtained, $n\leq m$, we have $m=n$. In other words, $\ord{a}=\ord{b a b^{-1}}$.

\item {\it $\ord{a^{-1}}=\ord{a}$.} We can take $m=\ord{a^{-1}}$ and $n=\ord{a}$. Then, for all $k\in\N$, if $\left(a^{-1}\right)^k=e$ then $m\leq k$. Also, for all $l\in\N$, if $a^l=e$, then $n\leq l$. First, let's take a look at $\left(a^{-1}\right)^m=e$, that is $a^{-m}=e$. If we multiply that by $a^m$ on the left, we have $a^m a^{-m}=a^m e$, i.e. $a^m=e$. By definition of order of $a$, we have that $n\leq m$. Now, if we take $a^n=e$, and we multiply it by $a^{-n}$ on the right, we have, $a^n a^{-n}=e a^{-n}$, and that is $e=a^{-n}$. That can be written as $\left(a^{-1}\right)^n=e$ and by definition of order of $a^{-1}$ we have that $m\leq n$. As we have $m\leq n$ and $n\leq m$, it's $m=n$, i.e. $\ord{a^{-1}}=\ord{a}$.

\item {\it The order of $a b$ is the same as the order of $b a$.} Let's take $m=\ord{a b}$ and $n=\ord{b a}$. We need to show that $m=n$. As in previous two excercises, we have $(a b)^m=e$. That is:

\begin{equation*}
\underbrace{(a b)(a b)\cdots(a b)}_{m\textnormal{ times}}=e.
\end{equation*}

\noindent\newline We can rearrange that to get $a (b a)^{m-1} b=e$. Multiplying that by $a^{-1}$ on the left and by $a$ on the right, we get $(b a)^{m-1}(b a)=e$ and that is $(b a)^m=e$. By definition of order of $(b a)$ we have $n\leq m$. Similarly, we have $(b a)^n=e$, i.e.

\begin{equation*}
\underbrace{(b a)(b a)\cdots(b a)}_{n\textnormal{ times}}=e.
\end{equation*}

\noindent\newline We can rearrange that to get $b (a b)^{n-1} a=e$. Multiplying by $b^{-1}$ on the left and by $b$ on the right, we have $(a b)^{n-1}(a b)=e$, that is $(a b)^n=e$. By definition of order of $(a b)$ we have $m\leq n$. Combining $m\leq n$ and $n\leq m$ we have $n=m$, which was to be shown.

\item {\it $\ord{a b c}=\ord{c a b}=\ord{b c a}$.} Let $p=\ord{a b c}$, $q=\ord{c a b}$ and $r=\ord{b c a}$. From first expression we have that $(a b c)^p=e$. That is:

\begin{equation*}
(a b c)(a b c)^{p-2}(a b c)=e.
\end{equation*}

\noindent\newline Multiplying by $c^{-1}$ on the right and by $c$ on the left we have:

\begin{equation*}
c a b c (a b c)^{p-2} a b c c^{-1}=c c^{-1},
\end{equation*}

\noindent\newline which is, by careful examination, $(c a b)^p=e$. As $q=\ord{c a b}$ we have that $p\geq q$. Now, taking $(c a b)^q=e$ and transforming it into $(c a b)(c a b)^{q-2}(c a b)=e$ we will have, by multiplying with $c^{-1}$ on the left and $c$ on the right that $a b(c a b)^{q-2}c a b c=e$, i.e. $(a b c)^q=e$. From that we have $q\geq p$ and finally, by combining this with a previous result, $p=q$, that is, $\ord{a b c}=\ord{c a b}$. Now, the same thing goes for $\ord{b c a}$ and this is generalized by previous theorem and following problem.

\item {\it Let $x=a_1 a_2 \cdots a_n$, and let $y=a_k a_{k+1}\cdots a_n a_1\cdots a_{k-1}$, where $1<k<n$. Then $\ord{x}=\ord{y}$.} Let $p=\ord{x}$ and $q=\ord{y}$. By previous theorem we have that $x^p=e$ implies $y^p=e$ and by that we have that $q\leq p$. Then, we have $y^q=e$ and that implies $x^q=e$. Therefore, $p\leq q$. So, that is $p=q$, i.e. $\ord{x}=\ord{y}$.
\end{enumerate}

\noindent{\bf Problem.} Let $a$ be any element of finite order of a group $G$. Prove the following:

\begin{enumerate}
\item If $a^p=e$ where $p$ is a prime number, and $a\neq e$, then $a$ has order $p$;
\item The order of $a^k$ is a divisor (factor) of the order of $a$;
\item If $\ord{a}=k m$, then $\ord{a^k}=m$;
\item If $\ord{a}=n$ where $n$ is odd, then $\ord{a^2}=n$;
\item If $a$ has order $n$, and $a^r=a^s$, then $n$ is a factor of $r-s$;
\item If $a$ is the only element of order $k$ in $G$, then $a$ is in the center of $G$;
\item If the order of $a$ is not a multiple of $m$, then the order of $a^k$ is not a multiple of $m$;
\item If $\ord{a}=m k$ and $a^{r k}=e$, then $r$ is a multiple of $m$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $a^p=e$ where $p$ is a prime number, and $a\neq e$, then $a$ has order $p$.} Let's denote $n=\ord{a}$. Then, as $a^p=e$, by previous theorem, we have that $n|p$. But, $p$ has only two divisors, by definition, $1$ and $p$. If it were $n=1$, then it would be that $a=e$, contradicting that $a\neq e$. So, we are left with $n=p$, i.e. $\ord{a}=p$.

\item {\it The order of $a^k$ is a divisor (factor) of the order of $a$.} Let $n=\ord{a}$ and $m=\ord{a^k}$. We have to prove that $m|n$. First we have $a^n=e$ and, as $e=e^k=\left(a^n\right)^k$, from that follows $\left(a^n\right)^k=e$. Furthermore, as $\left(a^n\right)^k=a^{n k}=\left(a^k\right)^n$, we have that $\left(a^k\right)^n=e$. As $m=\ord{a^k}$, by previous proposition, we have that $m|n$.

\item {\it If $\ord{a}=k m$, then $\ord{a^k}=m$.} Let us denote $n=\ord{a^k}$. We have to prove that $m=n$. As $n$ is the order of $a^k$, by previous proposition it's $n\leq m$. Next, we have $\left(a^{k}\right)^n=e$, i.e. $a^{k n}=e$. So, by definition of order of $a$, it's $k m\leq k n$. Note that $k>0$ (if it were that $k=0$, we would have that order of $a$ is zero, contradicting the definition that order is positive; if it were that $k<0$, then it would have to be that $m<0$ and then $m$ couldn't possibly be order of $a^k$). That said dividing $k m\leq k n$ with $k$ does not change the inequality and we have $m\leq n$. Combining that with $n\leq m$, we have successfuly shown that $n=m$.

\item {\it If $\ord{a}=n$ where $n$ is odd, then $\ord{a^2}=n$.} Let us denote $\ord{a^2}=m$. We have to prove that $m=n$. From a previous problem we have that $m|n$, so $m$ has to be odd too. We have $a^{2 m}=e$, and from that $n|2 m$. As $n$ is odd, it cannot possibly divide $2$ (which is a prime number), so we only have that $n|m$. As $m|n$ and $n|m$ it must be that $m=n$.

\item {\it If $a$ has order $n$, and $a^r=a^s$, then $n$ is a factor of $r-s$.} As $a^r=a^s$, multiplying the equation by $a^{-s}$ on the right gives us $a^{r-s}=e$. By previous proposition, as $n$ is the order of $a$, we have that $n|(r-s)$.

\item {\it If $a$ is the only element of order $k$ in $G$, then $a$ is in the center of $G$.} We have to prove that $a x=x a$ for all $x\in G$. From a previous problem we know that $\ord{a}=\ord{x a x^{-1}}$, for all $x\in G$. The additional condition in assumption is that $a$ is the only element of order $k$ in group $G$. But we see, due to the previous problem, that all $x a x^{-1}$ also have order $k$. Therefore it must be that $x a x^{-1}=a$, for all $x\in G$. Multiplying equality on right by $x$ gives us $x a=a x$, for all $x\in G$. Thus, $a$ is in center of $G$.

\item {\it If the order of $a$ is not a multiple of $m$, then the order of $a^k$ is not a multiple of $m$.} From a previous problem, order of $a^k$ (we will denote it by $z$) is a divisor of the order of $a$ (denoted by $w$). So there must exist $p\in\Z$ such that $w=p z$. As $w$ is not a multiple of $m$, there must exist $q,r\in\Z$, by the division with remainder theorem, such that $w=m q+r$ where $0<r<|m|$. Plugging that information into $w=p z$, we have $m q+r=p z$. Suppose that $z$ is a multiple of $m$. Then there exists $t\in\Z$ such that $z=m t$. If we put that into previous equation, we have that $m q+r=p m t$ and by that $m(p t-q)=r$. That would mean, as $(p t-q)\in\Z$, that $m|r$, and that $|m|\leq r$, which is a contradiction to the fact that $0<r<|m|$. Therefore $z$, order of $a^k$, cannot be a multiple of $m$.

\item {\it If $\ord{a}=m k$ and $a^{r k}=e$, then $r$ is a multiple of $m$.} From a previous problem it follows that if $\ord{a}=k m$, then $\ord{a^k}=m$. Furthermore, from $a^{r k}=e$ we have $\left(a^k\right)^r=e$. From that it follows not only that $m\leq r$, but that $m|r$. That means that there exists $p\in\Z$ such that $r=p m$, in other words, $r$ is a multiple of $m$.
\end{enumerate}


\noindent{\bf Problem.} Let $a$ and $b$ be elements of a group $G$. Let $\ord{a}=m$ and $\ord{b}=n$; let $\lcm{m,n}$ denote the least common multiple of $m$ and $n$. Prove:

\begin{enumerate}
\item If $a$ and $b$ commute, then $\ord{a b}$ is a divisor of $\lcm{m,n}$ (give a counterexample if they don't commute);
\item If $m$ and $n$ are relatively prime, then no power of $a$ can be equal to any power of $b$ (except for $e$);
\item If $m$ and $n$ are relatively prime, then the products $a^i b^j$ ($0\leq i<m$, $0\leq j<n)$ are all distinct;
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $a$ and $b$ commute, then $\ord{a b}$ is a divisor of $\lcm{m,n}$.} From $a^m=e$ and $b^n=e$ it follows that $a^m b^n=e$. It's also true that $a^{m k}=e$ and $b^{n l}=e$ for any $k,l\in\Z$ which implies $a^{m k} b^{n l}=e$. We are looking for $k,l\in\Z$ such that $m k=n l$. Say $t=\lcm{m,n}$. Then from definition of least common multiple $m\mid t$ and $n\mid t$. So we can take $k=\frac{t}{m}$ and $l=\frac{t}{n}$. Then, $m k=n l$ will be true as $m\frac{t}{m}=n\frac{t}{n}$ yielding $t=t$. Therefore, we take $a^t b^t=e$. This is true as $t=m k$ and $t=n l$, where $k,l\in\Z$. As $a$ and $b$ commute we can use the rule $a^t b^t=(a b)^t$ (see problems in the beginning of the script) and then it's $(a b)^t=e$, i.e. $(a b)^{\lcm{m,n}}=e$. Using that and the definition of order it follows that $\ord{a b}\mid\lcm{m,n}$. Still in search of a counterexample, somewhere in $D_n$, quaternions, $S_n$...?

\item {\it If $m$ and $n$ are relatively prime, then no power of $a$ can be equal to any power of $b$ (except for $e$).} As $m$ and $n$ are relatively prime\footnote{In my works on introductory number theory, I define greatest common divisor to be a natural number, so it would only be that $\gcd{(m,n)}=1$.}, then $\gcd{(m,n)}=\pm 1$. Suppose $a^k=b^l$ for some $k,l\in\Z$ such that $a^k\neq e$ and $b^l\neq e$. As they are equal, their orders are also, obviously, equal. Let us denote $q=\ord{a^k}=\ord{b^l}$. By a previous problem $\ord{a^k}\mid\ord{a}$ and $\ord{b^l}\mid\ord{b}$. That would mean that $q|m$ and $q|n$. But the only integer $q$ for which it is true that $q$ divides both $m$ and $n$ is either $1$ or $-1$. Furthermore, order must be positive, so it's $\ord{a^k}=\ord{b^l}=1$ and by a previous problem that means that $a^k=e$ and $b^l=e$, which is a contradiction to assumption that $a^k\neq e$ and $b^l\neq e$.

\item {\it If $m$ and $n$ are relatively prime, then the products $a^i b^j$ ($0\leq i<m$, $0\leq j<n)$ are all distinct.} Suppose $a^i b^j=a^k b^l$, where $0\leq i,k<m$ and $0\leq j,l<n$. Further condition is that $i\neq k$ and $j\neq l$, as equality would then be trivial. If we multiply the equality with $a^{-k}$ on the left and $b^{-j}$ on the right, we get $a^{i-k}=b^{l-j}$. Now, by the same logic from previous problem it must be that $\ord{a^{i-k}}=\ord{b^{l-j}}=q$. So it must be that $q|m$ and $q|n$. But the only such possible number is $1$ (remember order has to be positive). That would mean that $a^{i-k}=e$ and $b^{l-j}=e$. That is possible only when $i=k$ and $l=j$, contradicting our assumption that $i\neq k$ and $j\neq l$.

\end{enumerate}

\noindent{\bf Problem.} Let $a$ be an element of order $12$ in a group $G$.

\begin{enumerate}
\item What is the smallest positive integer $k$ such that $a^{8k}=e$?
\item What is the order of $a^8$?
\item What are the orders of $a^9$, $a^{10}$, $a^5$?
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it What is the smallest positive integer $k$ such that $a^{8k}=e$?} From a previous theorem, it follows that $12|8k$. Therefore, we are looking for $k,l$ such that $8k=12l$. We take $\textnormal{lcm}(8,12)=24$ and $k=3$, $l=2$. Therefore $k=3$.
\item {\it What is the order of $a^8$?} It must be that the order of $a^8$ divides $12$. Therefore, it can be $1$, $2$, $3$, $4$, $6$ and $12$. Obviously it's $3$ as $\left(a^8\right)^3=a^{8\cdot 3}=a^{24}=a^{12\cdot 2}=\left(a^{12}\right)^2=e$.
\item {\it What are the orders of $a^9$, $a^{10}$, $a^5$?} Order of $a^9$ is $4$ as $3|12$ and $\left(a^9\right)^4=a^{36}=\left(a^{12}\right)^3=e$. Order of $a^{10}$ is $6$ as $6|12$ and $\left(a^10\right)^6=a^{60}=\left(a^{12}\right)^5=e$. Order of $a^5$ is $12$ as $12|12$ and $\left(a^5\right)^{12}=a^{60}=\left(a^{12}\right)^5=e$.
\end{enumerate}

\noindent{\bf Lemma.} Let $G$ be a group, $a\in G$ such that $\ord{a}=m$. Then, $k\in\Z$ is relatively prime to $m$, that is $\gcd{(m,k)}=1$, if and only if $\ord{a^k}=m$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $n=\ord{a^k}$. We have to show that $m=n$. Now, we start with $\left(a^k\right)^n=e$. That is equivalent to $a^{k n}=e$ and it must be that $m|(k n)$. But, due to Euclid's lemma, as $\gcd{(m,k)}=1$, it must be that $m|n$. Furthermore, form a previous problem, we have that $\ord{a^k}|\ord{a}$, id est $n|m$. From $m|n$ and $n|m$ we have $m=n$.

{\it Sufficiency.} Let $\ord{a^k}=m$. Take $g=\gcd{(m,k)}$. Let's prove that $g=1$. Let us also denote $l$ as least common multiple of $m$ and $k$. Then it's $l=\frac{m k}{g}$. As $l$ is a multiple of $m$ it's true that $a^l=e$. Substituting $l$ for its expression gives us $a^{\frac{m k}{g}}=e$. It follows that $\left(a^k\right)^{\frac{m}{g}}=e$. But, as $m$ is order of $a^k$, we have that $m|\frac{m}{g}$, i.e. there exists $q\in\N$ such that $\frac{m}{g}=q m$. Dividing by $m$ (which is positive, by definition of order) yields $\frac{1}{g}=q$ and that is possible if and only if $g=1$ (as $g$ and $q$ are both natural numbers).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent\newline{\bf Remark.} For the next propositions we will use {\it fundamental theorem of arithmetic}, stating that every natural number can be decomposed as a product of prime factors in a unique way (up to order of factors). Proof can be found in my works on number theory.

\noindent\newline{\bf Lemma.} Let $a$ and $b$ commute. If $m$ and $n$ are relatively prime, then $\ord{a b}=m n$.

\noindent\newline{\bf Proof.} Let us denote $q=\ord{a b}$ and $l=\lcm{m,n}$. As, $m$ and $n$ are relatively prime, they contain no common divisors (except $1$), and by that, no prime divisors. So we can write them down\footnote{Notice that here we are rather ambiguous as to which $p_i$ equal some $p_j$, that is not of importance right now. On the other hand, we are unambiguous when saying $p_i\neq q_j$, for all $i\leq s$ and $j\leq t$.} as $m=p_1 p_2\cdots p_s$ and $n=q_1 q_2\cdots q_t$, where $s,t\in\N$. Now, from a previous problem we have that $q|l$, so it must be that there exists some $k\in\N$ (as $q\in\N$ and $l\in\N$ it cannot be non-positive) such that $l=q k$. But, as $m$ and $n$ are relatively prime their least common multiple is $m n$. So we may say that:

\begin{equation*}
q=\frac{p_1 p_2\cdots p_t q_1 q_2\cdots q_s}{k}.
\end{equation*}

\noindent\newline Now, $q$ is a natural number, so $k$ must (obviously) divide $m n$. So it contains some or none $p_i$ and some or none $q_j$. So we may say\footnote{I sometimes denote indices differently to add some unnecessary unambiguity.} that:

\begin{equation*}
k=\prod_{i\in S}{p_i}\prod_{j\in T}{q_j},
\end{equation*}

\noindent\newline where $S\subseteq\{i\in\N:\ i\leq s\}$ and $T\subseteq\{j\in\N:\ j\leq t\}$. We may introduce new $k_1,k_2\in\N$ such that $k_1$ is the product of $p_i$'s and $k_2$ is the product of $q_j$'s from the above equation, so we have that:

\begin{equation*}
k=\underbrace{\prod_{i\in S}{p_i}}_{k_1}\underbrace{\prod_{j\in T}{q_j}}_{k_2}=k_1 k_2.
\end{equation*}

\noindent\newline Obviously $k_1|m$ and $k_2|n$. So we may write:

\begin{equation*}
q=\frac{m}{k_1}\frac{n}{k_2}.
\end{equation*}

\noindent\newline It is obvious that both fractions are therefore integers. Also note that $\gcd{(m,n)}=1$ and $\gcd{(k_1,k_2)}=1$. As $(a b)^q=e$, we have $(a b)^{q k_1}=e$. Then,

\begin{equation*}
a^{m\frac{n}{k_2}} b^{m\frac{n}{k_2}}=e.
\end{equation*}

\noindent\newline But, as $\ord{a}=m$ and $\frac{n}{k_2}\in\N$, then $a^{m\frac{n}{k_2}}=e$ and we only have $b^{m\frac{n}{k_2}}=e$. But that would mean that $n|m \frac{n}{k_2}$. Due to Euclid's lemma, as $m$ and $n$ are relatively prime, it must be that $n|\frac{n}{k_2}$, i.e. there exists some $k'\in\N$ such that $\frac{n}{k_2}=n k'$. That means, after dividing by $n$ (which is positive) that $\frac{1}{k_2}=k'$ and it must be that $k_2=k'=1$. Furthermore, we also have that $(a b)^{q k_2}=e$ and by that:

\begin{equation*}
a^{\frac{m}{k_1}n} b^{\frac{m}{k_1}n}=e.
\end{equation*}

\noindent\newline But here $b^{\frac{m}{k_1}n}=e$ so it must be that $a^{\frac{m}{k_1}n}=e$. From that we have $m|\frac{m}{k_1}n$. By Euclid's lemma, as $m$ and $n$ are relatively prime, we have $m|\frac{m}{k_1}$ and that is, by the same reasoning as for $n$, possible only when $k_1=1$. Therefore, from $k_1 k_2 q=m n$ we have $q=m n$, that is, $\ord{a b}=m n$, when $m$ and $n$ are relatively prime.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} In next two theorems we will denote $P$ as the set of all primes, i.e.

\begin{equation*}
P=\{n\in\N\backslash\{1\}:\left(\forall m\in\N\backslash\{1\}\right)\left(m\nmid n\right)\}.
\end{equation*} 

\noindent\newline Then, by fundamental theorem of arithmetic every natural number $n$ can be written down as:

\begin{equation*}
n=\prod_{p\in P}{p^{\alpha(p)}},
\end{equation*}

\noindent\newline where $\alpha:P\rightarrow\N_0$ denotes the power for each prime number $p$. For example, $36$ can be written as:

\begin{equation*}
36=\prod_{p\in P}{p^{\alpha(p)}},
\end{equation*}

\noindent\newline where $\alpha(2)=2$, $\alpha(3)=2$ and $\alpha(p)=0$ for all $p\in P\backslash\{2,3\}$.

\noindent\newline{\bf Theorem.} Let $G$ be a group and $a\in G$ such that $\ord{a}=m$. Then,

\begin{equation*}
\ord{a^k}=\frac{\lcm{m,k}}{k}=\frac{m}{\gcd{(m,k)}}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $q=\ord{a^k}$. Now, let $g=\gcd{(m,k)}$. Then, we can take $(a^k)^{\frac{m}{g}}=a^{k\frac{m}{g}}$. But, $g$ divides both $k$ and $m$ so we can write that down as $(a^k)^{\frac{m}{g}}=a^{m\frac{k}{g}}=\left(a^m\right)^{\frac{k}{g}}=e$ (because $\frac{k}{g}\in\N$ due to the fact that $g|k$). Now, the order of $a^k$ must divide $\frac{m}{g}$, so we know that $q|\frac{m}{g}$. So there must exist $l\in\N$ such that $\frac{m}{g}=q l$, i.e. $q=\frac{m}{g l}$. We have $\left(a^k\right)^q=e$ so it must be $\left(a^k\right)^{\frac{m}{g l}}=e$. That can be written down as $a^{\frac{m k}{g l}}=e$. Therefore it must be that $m|\frac{m k}{g l}$, i.e. there exists $m'$ such that $\frac{m k}{g l}=m m'$. That is equivalent to $m k=m m'g l$. After dividing by $m$ (which is positive) we have $k=m'g l$. But, from $q=\frac{m}{g l}$ we have $m=q g l$. Let us think about what we have. We have $k=m'g l$ and $m=q g l$. Remember that $g$ is the greatest common divisor of $m$ and $k$. But, there is one more common divisor and that is $l$. Therefore, if $l>1$, greatest common divisor would not be $g$ but $g l$, contrary to our assumption. Therefore, $l=1$ and $q=\frac{m}{g}$. In other words, $\ord{a^k}=\frac{m}{\gcd{(m,k)}}$. Of course we can multiply that by $\frac{k}{k}$ and get $\ord{a^k}=\frac{m k}{k\gcd{(m,k)}}=\frac{\lcm{m,k}}{k}$, which proves our theorem.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Example.} We will give an idea for a proof of the next theorem. Suppose $\ord{a}=12$ and $\ord{b}=18$ in a group $G$ where $a$ and $b$ commute. Does there exist some $c\in G$ such that $\ord{c}=\lcm{12,18}=36$? Well, from a previous theorem, if we take $a^i b^j$ we know that $\ord{a^i}=\frac{12}{\gcd{12,i}}$ and $\ord{b^j}=\frac{18}{\gcd{18,j}}$. If we could make these two orders relatively prime, we could use a previous lemma to easily multiply these two orders. Well, luckily, greatest common divisor can be easily {\it controlled} if we pick $i$ to be divisors of $12$. Let's see how we can write $12$ and $18$ down using the fundamental theorem of arithmetic. We have $12=2^2\cdot 3$ and $18=2\cdot 3^2$. If we remove $3$ from $12$ and remove $2$ from $18$ we would have two relatively prime numbers, $4$ and $9$, which multiplied yield $36$, least common multiple of $12$ and $18$. But how to remove $3$ from $12$? Well, $3|12$ so $\gcd{(3,12)}=3$. Same thing goes for $18$ and $2$. Looking in the formula above, we can see that it's a most natural thing to choose $i=3$ and $j=2$. Then we have $\ord{a^3}=\frac{12}{\gcd{3,12}}=\frac{12}{3}=4$. Similarly we get $\ord{b^2}=\frac{18}{2}=9$. Their orders are relatively prime. Now, if we're trying to find $\ord{a^3 b^2}$, it's easily done by a previous lemma. As orders of factors are relatively prime, the order of the product is product of their orders, i.e. $\ord{a^3 b^2}=9\cdot 4=36$.

Another point we'd like to make is how we should handle the divisors more clearly. Let $m=2^4\cdot 3\cdot 5\cdot 7$ (order of $a\in G$) and $n=2^2\cdot 3\cdot 5^2$ (order of $b\in G$, which commutes with $a$). First, we compare the exponents in the prime powers and take all where there is less. Why? Well, least common divisor is $\frac{m n}{\gcd{(m,n)}}$. And what we're actually doing is that we're dividing greatest common divisor in two parts (by taking common divisors, of course) but so that we get two relatively prime factors. Hence, we will make that experiment now. In $m$ we have $2^4$ which is less than $2^2$ (so we will take $2^2$ from $n$ as it's a common divisor). In $m$ we have $3$ and in $n$ we have $3$. It does not matter from which we take, we can as well take from $n$, as we're already taking $2^2$. Then, we have $5$ in $m$ and $5^2$ in $n$, so we'll take $5$ from $m$. Finally, $7$ appears only once in $m$ and never in $n$, therefore we will leave it alone (it's not a common divisor). Thus we take $i=5$ and $j=2^2 3=12$. Then, $\ord{a^5}=\frac{m}{\gcd{(m,5)}}=2^4\cdot3\cdot7$. Also, $\ord{b^{12}}=\frac{n}{\gcd{(12,n)}}=5^2$. Obviously, $\gcd{\left(\ord{a^5},\ord{b^{12}}\right)}=1$, so $\ord{a^5 b^{12}}=2^4\cdot3\cdot5^2\cdot7$, which is the least common multiple of $m$ and $n$. That is how we will prove the general statement.

\noindent\newline{\bf Theorem.} Let $G$ be a group and $a,b\in G$ such that $a b=b a$ and $\ord{a}=m$ and $\ord{b}=n$. Then there exists $c\in G$ such that $\ord{c}=\lcm{m,n}$.

\noindent\newline{\bf Proof.} Let, by using fundamental theorem of arithmetic,

\begin{eqnarray*}
m&=&\prod_{p\in P}{p^{\alpha(p)}},\\\\
n&=&\prod_{p\in P}{p^{\beta(p)}}.
\end{eqnarray*}

\noindent\newline Then, the greatest common divisor of $m$ and $n$ is:

\begin{equation*}
\gcd{(m,n)}=\prod_{p\in P}{p^{\min{\left\{\alpha(p),\beta(p)\right\}}}}.
\end{equation*}

\noindent\newline We will divide $\gcd{(m,n)}$ into two separate products. First product will contain all prime powers for which $\alpha(p)\leq\beta(p)$ and the second all the others, namely all prime powers for which $\beta(p)<\alpha(p)$. So we have:

\begin{equation*}
\gcd{(m,n)}=\prod_{\substack{p\in P\\\alpha(p)\leq\beta(p)}}{p^{\min{\left\{\alpha(p),\beta(p)\right\}}}}\prod_{\substack{p\in P\\\alpha(p)>\beta(p)}}{p^{\min{\left\{\alpha(p),\beta(p)\right\}}}}.
\end{equation*}

\noindent\newline But, for all $\alpha(p)\leq\beta(p)$ the minimum of the two is $\alpha(p)$. Similarly, for all $\beta(p)<\alpha(p)$ the minimum is $\beta(p)$. Therefore we can write the products more simply:

\begin{equation*}
\gcd{(m,n)}=\prod_{\substack{p\in P\\\alpha(p)\leq\beta(p)}}{p^{\alpha(p)}}\prod_{\substack{p\in P\\\alpha(p)>\beta(p)}}{p^{\beta(p)}}.
\end{equation*}

\noindent\newline The first product we will denote by $i$ and the second by $j$, so that we have:

\begin{eqnarray*}
i&=&\prod_{\substack{p\in P\\\alpha(p)\leq\beta(p)}}{p^{\alpha(p)}}\\\\
j&=&\prod_{\substack{p\in P\\\alpha(p)>\beta(p)}}{p^{\beta(p)}}.
\end{eqnarray*}

\noindent\newline Obviously, $\gcd{(m,n)}=i j$. That implies (rather trivial but useful) $i|\gcd{(m,n)}$ and $j|\gcd{(m,n)}$. Then, as $\gcd{(m,n)}|m$ and $\gcd{(m,n)}|n$, then $i|m$ and $j|n$. That also implies that $\gcd{(m,i)}=i$ and $\gcd{(n,j)}=j$. Furthermore, $\frac{m}{i}\in\N$ and $\frac{n}{j}\in\N$. Let us show that these two numbers are relatively prime. We have:

\begin{equation*}
\frac{m}{i}=\frac{\prod_{p\in P}{p^{\alpha(p)}}}{\prod_{\substack{p\in P\\\alpha(p)\leq\beta(p)}}{p^{\alpha(p)}}}.
\end{equation*}

\noindent\newline If $\alpha(p)\leq\beta(p)$ for some $p$, then obviously $\frac{m}{i}$ will not contain that prime power, as we will have expresion of the form:

\begin{equation*}
\frac{f(p) p^{\alpha(p)}}{g(p)p^{\alpha(p)}}.
\end{equation*}

\noindent\newline Therefore $\frac{m}{i}$ will contain only $p^{\alpha(p)}$ such that $\beta(p)<\alpha(p)$. But, $\frac{n}{j}$ is:

\begin{equation*}
\frac{n}{j}=\frac{\prod_{p\in P}{p^{\beta(p)}}}{\prod_{\substack{p\in P\\\beta(p)<\alpha(p)}}{p^{\beta(p)}}}.
\end{equation*}

\noindent\newline So, by similar logic, only $p^{\beta(p)}$ such that $\alpha(p)\geq\beta(p)$ will remain. So if some $p$ were to divide both $\frac{m}{i}$ and $\frac{n}{j}$ then it would have to satisfy both that $\alpha(p)\geq\beta(p)$ and $\beta(p)<\alpha(p)$, which is impossible. Therefore, $\frac{m}{i}$ and $\frac{n}{j}$ are relatively prime.

The final step in the proof is to find $\ord{a^i b^j}$. For that we need $\ord{a^i}$ and $\ord{b^j}$. As $\ord{a}=m$ and $\ord{b}=n$, from a previous theorem we have that $\ord{a^i}=\frac{m}{\gcd{(m,i)}}$ and $\ord{b^j}=\frac{n}{\gcd{(n,j)}}$. But, we have shown previously in the proof, rather trivially, that $\gcd{(m,i)}=i$ and $\gcd{(n,j)}=j$. Thereofore, $\ord{a^i}=\frac{m}{i}$ and $\ord{b^j}=\frac{n}{j}$. The orders of $a^i$ and $b^j$ are relatively prime, so we by a previous problem we have that $\ord{a^i b^j}=\ord{a^i}\ord{b^j}$. That implies that $\ord{a^i b^j}=\frac{m n}{i j}=\frac{m n}{\gcd{(m,n)}}=\lcm{m,n}$, which concludes our proof.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $a$ denote an element of a group $G$. (i) Let $\ord{a}=12$. Prove that if $a$ has a cube root, say $a=b^3$ for some $b\in G$, then $b$ has order $36$. (ii) Let $a$ have order $6$. If $a$ has a fourth root in $G$, say $a=b^4$, what is the order of $b$? (iii) Let $a$ have order $10$. If $a$ has a sixth root in $G$, say $a=b^6$, what is the order of $b$?

\noindent\newline{\bf Solution.} (i) We have that $b^{36}=b^{12\cdot 3}=\left(b^3\right)^{12}=a^{12}=e$. Therefore, as $b^{36}=3$, by a previous problem order of $b$ must divide $36$. So, we have to observe each factor on its own, and we have $1$, $2$, $3$, $4$, $6$, $9$, $12$ and $18$. If $\ord{b}=1$ that would mean that $b=e$ and we would have $a=b^3=e^3=e$, contradicting that $\ord{a}=12$. Now, if $\ord{b}=2$ we have $b^2=e$. That would mean that $a=b$ and it would mean that $\ord{b}=12$ and it could not be that $b^2=e$. Furthermore, if $b^3=e$, that would again mean that $a=e$, which cannot be. If $\ord{b}=4$ we would have $b^4=e$. That would mean that $a b=e$, i.e. $a=b^{-1}$. But, $\ord{b^{-1}}=\ord{b}$ and so $\ord{a}=\ord{b}$ and it cannot be as $12\neq 4$. If $\ord{b}=6$, then $e=\left(b^3\right)^2=a^2$, which cannot be as order of $a$ would be $2$. Then, $e=b^9=\left(b^3\right)^3=a^3$ also cannot be as it would imply that $\ord{a}=3$. Furthermore, if $\ord{b}=12$ we would have $\left(b^3\right)^4=a^4=e$ and from $\ord{b}=18$ would follow $a^6=e$; both of these cases contradict the fact that $\ord{a}=12$. So, the only possible solution is that $\ord{b}=36$.

(ii) We have $\ord{a}=6$ and $a=b^4$. We want to find $\ord{b}$. First time we encounter $e$ is $a^6=a^5 b^4=\left(b^4\right)^5 b^4=b^{20}b^4$ and that is $e=b^{24}$. Now, let's again convince ourselves that $b^{24}=e$. We have $b^{24}=\left(b^4\right)^6=a^6=e$. But, we also need to prove that no divisor of $24$ will be order of $b$. Trivially, we have $1$, and that is $b^1=b=e$ which cannot be as then it would be that $a=e^4=e$. If $\ord{b}=2$, we would have $a=b^2 b^2=e$. Then, suppose $b^3=e$. We have $a=b b^3$ and from that $a=b$, which would mean that $\ord{a}=\ord{b}=3$ which is a contradiction. Now, if $\ord{b}=4$ we would have $a=b^4=e$, and again it cannot be as then order of $a$ would be $1$. If $\ord{b}=6$ we would have $b^6=a b^2=e$, meaning that $a=b^{-2}$. That would mean that $a^3=b^{-6}$, i.e. $a^3=\left(b^{-1}\right)^6$. But $\ord{b}=\ord{b^{-1}}=6$, so we have $a^3=e$, which, again, cannot be. If $\ord{b}=8$ we would have $e=b^8=\left(b^4\right)^2=a^2$, which is a contradiction. Now, if $\ord{b}=12$, we would have $e=b^{12}=\left(b^4\right)^3=a^3$ and that is impossible as $\ord{a}=6$. Therefore, the least positive power of $b$ which equals $e$ is $24$.

(iii) We have $\ord{a}=10$ and $a=b^6$. We will have $e=a^{10}=\left(b^6\right)^10=b^{60}$. Following the previous examples we will have $1$, $2$, $3$, $4$, $5$, $6$, $10$, $12$, $15$, $20$ and $30$ as candidates for order of $b$. Obviously $b^1=e$ would imply $b^6=e=a$, not possible. Then, $b^2=e$ would imply $e=\left(b^2\right)^3=b^6=a$, not possible; $b^3=e$ similarly implies $e=\left(b^3\right)^2=b^6=a$, not possible. Going further, $b^4=e$ implies $a=b^4 b^2=b^2$ and $a=b^2$. That would mean that $a^3=b^6=a$ and from that $a^2=e$, which is impossible. Then, $b^5=e$ would give us $a=b b^5=e$, which is impossible. Following that, $b^6=e$ directly implies $a=e$, not possible. Then, $b^{10}=e$ yields $a^2=b^{12}=b^10 b^2=b^2$. That would mean that $\left(a^2\right)^3=\left(b^2\right)^3$, which is $a^6=b^6=a$ and from that $a^5=e$. Which is impossible. Then, from $b^{12}=e$ we have $\left(b^6\right)^2=e$, i.e. $a^2=e$, impossible. Going further, $b^{15}=e$ would imply $\left(b^6\right)^2 b^3=e$, that is $a^2 b^3=e$. From that we have $a^4 b^6=e$, implying $a^5=e$, not possible. Then, $e=b^{20}=\left(b^6\right)^3 b^2$ would imply $e=a^3 b^2$, which is possible as $e^3=a^9 b^6=a^{10}$. Finaly, $e=b^{30}=\left(b^6\right)^5=a^5$ is impossible. In conclusion, order of $b$ is either $20$ or $60$.

\noindent\newline{\bf Problem.} Let $a$ have order $n$, and suppose $a$ has a $k$-th root in $G$, say $a=b^k$.

\begin{enumerate}
\item Explain why the order of $b$ is a factor of $n k$.

\item Let $\ord{b}=\frac{n k}{l}$. Prove that $n$ and $l$ are relatively prime.

\item Let $k$ be an integer such that every prime factor of $k$ is a factor of $n$. Prove that if $a$ has a $k$-th root $b$, then $\ord{b}=n k$.
\end{enumerate}

\noindent{\bf Solution.} Let $a$ have order $n$, and suppose $a$ has a $k$-th root in $G$, say $a=b^k$.

\begin{enumerate}
\item {\it Explain why the order of $b$ is a factor of $n k$.} From $a=b^k$, by multiplying equation with $a^{n-1}$ we get $a a^{n-1}=a^{n-1}b^k$ which is $a^n=\left(b^k\right)^{n-1} b^k$, i.e. $a^n=\left(b^k\right)^n$. From that, as $\ord{a}=n$, we have $e=b^{k n}$. Therefore, order of $b$ must divide $k n$.

\item {\it Let $\ord{b}=\frac{n k}{l}$. Prove that $n$ and $l$ are relatively prime.} From a previous problem we have that order of $b$ must divide $n k$, i.e. $\frac{n k}{l}|(n k)$. That means that there exists $m\in\N$ such that $n k=m\frac{n k}{l}$, that is $l=m$. Now, suppose that $n$ and $l$ have a factor in common, that is $n=q n'$ and $l=q l'$, where we can take $q=\gcd{(n,l)}$ and suppose $q>1$. Notice that $n'$ and $l'$ are now relatively prime. That implies:

\begin{equation*}
\ord{b}=\frac{n k}{l}=\frac{q n' k}{q l'}=\frac{n'k}{l'}.
\end{equation*}

\noindent\newline Furthermore, as $n'$ and $l'$ are relatively prime, by Euclid's Lemma, it must be that $l'|k$ (we assume that order of $b$ is a natural number, therefore, by canceling out $n$ and $l$, that property must be preserved). Now, as $\frac{k}{l'}\in\N$, we have:

\begin{equation*}
e=b^{\frac{n' k}{l'}}=\left(b^{n'}\right)^{\frac{k}{l'}}
\end{equation*}

\noindent\newline and, using the fact that $e=e^{l'}$, we can obtain:

\begin{equation*}
e^{l'}=\left(\left(b^{n'}\right)^{\frac{k}{l'}}\right)^{l'}=\left(b^{n'}\right)^{k}=b^{n' k},
\end{equation*}

\noindent\newline that is $e=\left(b^k\right)^{n'}$. Furthermore, from $a=b^k$, it follows that, $a^{n'}=\left(b^k\right)^{n'}=e$, i.e. $a^{n'}=e$. As $\ord{a}=n$, it must be that $n\leq n'$. But, from our assumption that $n=q n'$ and $q>1$, it must also be that $n>n'$, which is a contradiction, therefore $n$ and $l$ have no factor in common, that is, they are relatively prime.

\item {\it Let $k$ be an integer such that every prime factor of $k$ is a factor of $n$. Prove that if $a$ has a $k$-th root $b$, then $\ord{b}=n k$.} Suppose that for every $p\in P$ such that $p|k$ it follows that $p|n$. Now, the order of $b$ is $\frac{k n}{l}$ (see the proposition below that summarizes first two problems) and $\gcd{(n,l)}=1$. If $p|k$ it must also be that $p|n$. Therefore, it must be that $p\nmid l$. As $l$ then shares no common prime factors with $k$ (or $n$), and it must divide $k$, it can only be that $l=1$.
\end{enumerate}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$ such that $\ord{a}=n$. If $a=b^k$, for some $k\in\N$, then $\ord{b}=\frac{n k}{l}$, for some $l\in\N$, such that $\gcd{(n,l)}=1$. Furthermore, if $p|k$ implies $p|n$, for all $p\in P$, then $\ord{b}=n k$.

\noindent\newline{\bf Proof.} From the first problem it follows that $\ord{b}$ is a factor of $n k$, i.e. $\ord{b}|n k$. So there exists $l\in\N$ such that $n k=l\ord{b}$, that is, $\ord{b}=\frac{n k}{l}$. As now we have $\ord{b}=\frac{n k}{l}$ it follows, from the second problem, that $\gcd{(n,l)}=1$. From the third problem it follows that $\ord{b}=n k$, if, for all $p\in P$, $p|k$ implies $p|n$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Partitions and equivalence relations}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} A {\bf partition} of a set $A$ is a family\footnote{Family of sets contains a set $A_i$ for each index $i$ as $i$ ranges over $I\subseteq\N$.} $\{A_i:\ i\in I\}$ of nonempty subsets of $A$ which are mutually disjoint\footnote{That is, $A_i\cap A_j=\emptyset$, for all $A_i$ and $A_j$, where $i,j\in I$.} and whose union\footnote{In other words, $\bigcup_{i\in I}{A_i}=A$.} is all of $A$. Subsets $A_i$ from the forementioned family are then called {\bf classes}, and indices are a way of naming them.

\noindent\newline{\bf Remark.} This definition can also be more explicitly defined with two properties (taking all assumptions and namings from the previous definition):

\begin{enumerate}
\item If $x\in A_i$ and $x\in A_j$, for some $i,j\in I$, then $A_i=A_j$ (and by that $i=j$).
\item If $x\in A$, then there exists $i\in I$ such that $x\in A_i$.
\end{enumerate}

\noindent{\bf Definition.} A {\bf relation} $\mathcal{R}$ on a set $A$ is a function $\mathcal{R}:A\times A\rightarrow\{\top,\bot\}$. If $\mathcal{R}(x,y)=\top$, for some $(x,y)\in A^2$, we write $x\mathcal{R}y$.

\noindent\newline{\bf Definition.} By an {\bf equivalence relation} on a set $A$ we mean a relation $\sim$ which is:

\begin{enumerate}
\item {\it Reflexive:} for every $x\in A$, $x\sim x$.
\item {\it Symmetric:} for every $x,y\in A$, if $x\sim y$, then $y\sim x$.
\item {\it Transitive:} for every $x,y,z\in A$, if $x\sim y$ and $y\sim z$, then $x\sim z$.
\end{enumerate}

\noindent{\bf Definition.} Let $\sim$ be an equivalence relation on $A$ and $x\in A$. The set of all the elements equivalent to $x$ is called the {\bf equivalence class} of $x$, and is denoted by $[x]$. In other words:

\begin{equation*}
[x]=\{y\in A:\ y\sim x\}.
\end{equation*}

\noindent\newline{\bf Lemma.} $x\sim y$ if and only if $[x]=[y]$.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $x\sim y$. As $x\sim x$ and $y\sim y$ we have $x\in[x]$ and $y\in [y]$. Then, by symmetry $y\sim x$ and by definition $y\in[x]$. So we have $[y]\subseteq [x]$. Also, as $x\sim y$, we have that $x\in[y]$ and $[x]\subseteq [y]$. From that we have $[x]=[y]$.

{\it Sufficiency.} If $[x]=[y]$, then $x\in[x]$ implies $x\in[y]$ and from that $x\sim y$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $\sim$ is an equivalence relation on $A$, the family of all the equivalence classes, that is, $\{[x]:\ x\in A\}$, is a partition of $A$.

\noindent\newline{\bf Proof.} Obviously, as $[x]=\{y\in A:\ y\sim x\}$, we have that $[x]\subseteq A$. Then, if we take $x\in [y]$ and $x\in[z]$, then we have $x\sim y$ and $x\sim z$. By simmetry $y\sim x$ and $x\sim z$, and by transitivity $y\sim z$, so by a previos lemma $[y]=[z]$. That satisfies the necessity for disjunctivity of classes in definition of partition. Finally, for all $x\in A$ there exists a class such that $x\in[x]$. Therefore, $\{[x]:\ x\in A\}$ is a partition of $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove that (and describe equivalence relation associated with that partition):

\begin{enumerate}
\item $\{A_0,\ldots,A_4\}$ is a partition of $\Z$, where $A_r=\{x\in\Z:\ (\exists q\in\Z)(x=5 q+r)\}$, for $r\in\{0,\ldots,4\}$;

\item $\{A_n:\ n\in\Z\}$ is a partition of $\Q$, where $A_n=\{x\in\Q:\ n\leq x<n+1\}$, for $n\in\Z$;

\item $\{A_r:\ r\in\Q\}$ is a partition of $\Z\times\Z^{\ast}$, where\footnote{Let us remind the reader that $\Z^{\ast}:=\Z\backslash\{0\}$.} $A_r=\{(m,n)\in\Z\times\Z^{\ast}:\ \frac{m}{n}=r\}$, for $r\in\Q$;

\item $\{A_0,\ldots,A_0\}$ is a partition of $\Z$, where $A_r=\{x\in\Z:\ (\exists q\in\Z)(x=10q+r)\}$, for $r\in\{0,\ldots,9\}$;

\item $\{A_r:\ 0\leq r<1,\ r\in\Q\}$ is a partition of $\Q$, where

\begin{equation*}
A_r=\left\{x\in\Q:\ \left(\exists q\in\Z\right)\left(x=q+r\right)\right\},
\end{equation*}

\noindent\newline for each $r\in\left[0,1\right\rangle\cap\Q$;

\item $\{A_r:\ r\in\R\}$ is a partition of $\R\times\R$, where $A_r=\{(x,y)\in\R\times\R:\ x-y=r\}$, for $r\in\R$;

\item $A_r=\{(x,y)\in\R\times\R:\ y=2x+r\}$, for each $r\in\R$, is a partition of $\R\times\R$;

\item $A_r=\{(x,y)\in\R\times\R:\ x^2+y^2=r^2\}$, for each $r\in\R$, is a partition of $\R\times\R$;

\item $A_r=\{(x,y)\in\R\times\R:\ y=|x|+r\}$, for each $r\in\R$, is a partition of $\R\times\R$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $\{A_0,\ldots,A_4\}$ is a partition of $\Z$, where $A_r=\{x\in\Z:\ (\exists q\in\Z)(x=5 q+r)\}$, for $r\in\{0,\ldots,4\}$.} Suppose $x\in \Z$. Then, by division with remainder theorem, there exist $q,r\in\Z$ such that $x=5q+r$ where $0\leq r<5$. Therefore, $x$ is in some $A_r$, for $r\in\{0,\ldots,4\}$. Suppose $x\in A_{r_1}$ and $x\in A_{r_2}$. Then we have $x=5q_1+r_1$ and $x=5q_2+r_2$ for some $q_1,q_2,r_1,r_2\in\Z$ where $0\leq r_1,r_2<5$. It follows\footnote{Here we are actually proving again uniqueness part of division with remainder theorem, but for a special case, for dividing with $5$; see my works on number theory. It's rather redundant but good for excercise.} that $5q_1+r_1=5q_2+r_2$, i.e. $r_1=5(q_2-q_1)+r_2$. But, as $r_2\geq 0$, for $q_2-q_1\geq 1$ we have that $r_1\geq 5\cdot 1$, which is impossible, as it has to be $r_1<5$. So it must be that $q_2=q_1$. Then we would have $r_1=5\cdot 0+r_2=r_2$. Therefore, as $r_1=r_2$ we have $A_{r_1}=A_{r_2}$. Thus, $\{A_0,\ldots,A_4\}$ is a partition of $\Z$. Associated equivalence relation is $x\sim y$ if and only if $5|(x-y)$, for all $x,y\in\Z$.

\item {\it $\{A_n:\ n\in\Z\}$ is a partition of $\Q$, where $A_n=\{x\in\Q:\ n\leq x<n+1\}$, for $n\in\Z$.} Suppose $x\in\Q$. Then $x=\frac{m}{n}$, where $m\in\Z$ and $n\in\N$. By division with remainder theorem, there exist $q,r\in\Z$ such that $m=n q+r$ where $0\leq r<n$. So we have $\frac{m}{n}=\frac{n q+r}{n}=\frac{n q}{n}+\frac{r}{n}=q+\frac{r}{n}$. But, as $r<n$, then $\frac{r}{n}<1$ and we have $\frac{m}{n}=q+\frac{r}{n}<q+1$. Furthermore, as $r\geq 0$ we have $\frac{m}{n}=q+\frac{r}{n}\geq q+0=q$. From $q\in\Z$ and $q\leq x<q+1$ we have that $x\in A_q$. Let us assume that $x\in A_{n_1}$ and $x\in A_{n_2}$. Then we have $n_1,n_2\in\Z$ such that $n_1\leq x<n_1+1$ and $n_2\leq x<n_2+1$. Furthermore, assume that $n_1\leq n_2$. Then we have $n_2\leq x<n_1+1$. From that we have $n_2<n_1+1$ and $n_1\leq n_2<n_1+1$. The only integer between $n_1$ and $n_1+1$ that is greater or equal to $n_1$ is of course $n_1$. Therefore, $n_1=n_2$ and that implies $A_{n_1}=A_{n_2}$. Here, equivalence relation is $x\sim y$ if and only if there exists $n\in\Z$ such that $n\leq x<n+1$ and $n\leq y<n+1$, for all $x,y\in Q$.

\item {\it $\{A_r:\ r\in\Q\}$ is a partition of $\Z\times\Z^{\ast}$, where $A_r=\{(m,n)\in\Z\times\Z^{\ast}:\ \frac{m}{n}=r\}$, for $r\in\Q$.} Suppose $x\in\Z\times\Z^{\ast}$. Then $x=(a,b)$, where $a\in\Z$ and $b\in\Z^{\ast}$. Then there exists $r=\frac{a}{b}$ and that implies $x\in A_r$. Let us assume that $(a,b)\in A_{r_1}$ and $(a,b)\in A_{r_2}$. Then we have $r_1=\frac{a}{b}$ and $r_2=\frac{a}{b}$. That implies $r_1=r_2$, i.e. $A_{r_1}=A_{r_2}$. The associated equivalence relation is $(a,b)\sim(c,d)$ if and only if $\frac{a}{b}=\frac{c}{d}$, for all $(a,b)\in\Z\times\Z^{\ast}$ and $(c,d)\in\Z\times\Z^{\ast}$.

\item {\it $\{A_0,\ldots,A_0\}$ is a partition of $\Z$, where $A_r=\{x\in\Z:\ (\exists q\in\Z)(x=10q+r)\}$, for $r\in\{0,\ldots,9\}$.} By division with remainder theorem for each $x\in\Z$ there exist unique $q,r\in\Z$ such that $x=10q+r$, where $0\leq r<10$. Therefore, as in the first example, $x\in\Z$ will imply $x\in A_r$ (where $r$ is obtained by using division with remainder theorem; which in turn guarantees its existence) and $x\in A_{r_1}$ with $x\in A_{r_2}$ will imply $A_{r_1}=A_{r_2}$ (due to uniqueness of $q$ and $r$ in division with remainder theorem). Associated equivalence relation is $x\sim y$ if and only if $10|(x-y)$, for all $x,y\in\Z$.

\item {\it $\{A_r:\ 0\leq r<1,\ r\in\Q\}$ is a partition of $\Q$, where

\begin{equation*}
A_r=\left\{x\in\Q:\ \left(\exists q\in\Z\right)\left(x=q+r\right)\right\},
\end{equation*}

\noindent\newline for each $r\in\left[0,1\right\rangle\cap\Q$.} By a corollary of division with remainder theorem, proved in my works on number theory, for any $x\in\Q$ there exist $q\in\Z$, $m\in\N_0$ and $n\in\N$ such that $x=q+\frac{m}{n}$ where $0\leq m<n$. Dividing those two inequalities by $n\neq 0$ gives us $0\leq\frac{m}{n}<1$. Therefore taking $r=\frac{m}{n}$ implies $x\in A_r$. Also, if we took $x\in A_{r_1}$ and $x\in A_{r_2}$, by the uniqueness part of the same corollary, we would have $A_{r_1}=A_{r_2}$ (as $x=k_1+r_1$ and $x=k_2+r_2$ implies $k_1=k_2$ and $r_1=r_2$, because $0\leq r_1,r_2<1$; explained in more detail in the proof in my works on elementary number theory). Here, equivalence relation is $x\sim y$ if and only if there exist $q_1,q_2\in\Z$ and $r\in\Q$ such that $x=q_1+r$ and $y=q_2+r$, where $0\leq r<1$ and $x,y\in Q$.

\item {\it $\{A_r:\ r\in\R\}$ is a partition of $\R\times\R$, where $A_r=\{(x,y)\in\R\times\R:\ x-y=r\}$, for $r\in\R$.} Take $(x,y)\in\R$. Then there exists $r\in\R$ such that $x-y=r$, therefore, $(x,y)\in A_r$. Also, suppose $(x,y)\in A_{r_1}$ and $(x,y)\in A_{r_2}$. Then, $r_1=x-y$ and $r_2=x-y$ implies $r_1=r_2$, id est $A_{r_1}=A_{r_2}$. Equivalence relation associated with this partition is, for all $(x,y),(z,w)\in\R\times\R$, $(x,y)\sim(z,w)$ if and only if $x-y=z-w$.

\item {\it $A_r=\{(x,y)\in\R\times\R:\ y=2x+r\}$, for each $r\in\R$, is a partition of $\R\times\R$.} If we take $(x,y)\in\R\times\R$, then there exists $r\in\R$ such that $y=2x+r$ obtained with $r=y-2x$ and then $(x,y)\in A_r$. For $(x,y)\in A_{r_1}$ and $(x,y)\in A_{r_2}$ we have $y=2x+r_1$ and $y=2x+r_2$. From that we have $2x+r_1=2x+r_2$ which in turn implies, after subtracting $2x$, that $r_1=r_2$ and $A_{r_1}=A_{r_2}$. We have $(x,y)\sim(z,w)$ if and only if $y-2x=w-2z$. Geometrically, this is the partition of Euclidean space as parallel lines.

\item {\it $A_r=\{(x,y)\in\R\times\R:\ x^2+y^2=r^2\}$, for each $r\in\R$, is a partition of $\R\times\R$.} Take $(x,y)\in\R\times\R$. Then we have $r=\sqrt{x^2+y^2}$ and $(x,y)\in A_r$, where $r$ is obtained as shown. If we take $(x,y)\in A_{r_1}$ and $(x,y)\in A_{r_2}$, we have $x^2+y^2=r_1^2$ and $x^2+y^2=r_2^2$. That implies $r_1^2=r_2^2$. But, $r_1,r_2\geq 0$, so from that we have $r_1=r_2$ and $A_{r_1}=A_{r_2}$. Equivalence relation is $(x,y)\sim(z,w)$ if and only if $x^2+y^2=z^2+w^2$. Geometrically, this equivalence relation partitions Euclidean space into circles sharing the same center.

\item {\it $A_r=\{(x,y)\in\R\times\R:\ y=|x|+r\}$, for each $r\in\R$, is a partition of $\R\times\R$.} If we take $(x,y)\in\R\times\R$ we have $r=y-|x|$ and by that $(x,y)\in A_r$. Furthermore, if $(x,y)\in A_{r_1}$ and $(z,w)\in A_{r_2}$, we have $y-|x|=r_1$ and $y-|x|=r_2$ from which trivially follows that $r_1=r_2$. Equivalence relation is $(x,y)\sim(z,w)$ if and only if $y-|x|=w-|z|$. Geometrically, this is the partition of Euclidean space as graphs of absolute value function.

\end{enumerate}

\noindent{\bf Problem.} Prove each of the following is an equivalence relation on the indicated set. Then describe the partition associated with that equivalence relation.

\begin{enumerate}
\item In $\Z$, $m\sim n$ iff $|m|=|n|$;
\item In $\Q$, $r\sim s$ iff $r-s\in\Z$;
\item Let $\left\lceil x\right\rceil$ denote the greatest integer less than or equal to $x$. In $\R$, let $a\sim b$ iff $\left\lceil a\right\rceil=\left\lceil b\right\rceil$;
\item In $\Z$, let $m\sim n$ iff $m-n$ is a multiple of $10$;
\item In $\R$, let $a\sim b$ iff $a-b\in\Q$;
\item In $\mathcal{F}(\R)$, let $f\sim g$ iff $f(0)=g(0)$;
\item In $\mathcal{F}(\R)$, let $f\sim g$ iff $f(x)=g(x)$ for all $x>c$, where $c$ is some fixed real number;
\item If $C$ is any set, $\mathcal{P}(C)$ denotes the set of all the subsets of $C$. Let $D\subseteq C$. In $\mathcal{P}(C)$, let $A\sim B$ iff $A\cap D=B\cap D$;
\item In $\R\times\R$, let $(a,b)\sim(c,d)$ iff $a^2+b^2=c^2+d^2$;
\item In $\R^{\ast}$, let $a\sim b$ iff $\frac{a}{b}\in\Q$;

\item In $\R\times\R$, $(x,y)\sim(u,v)$ iff $a x^2+b y^2=a u^2+b v^2$ (where $a,b>0)$;

\item In $\R\times\R$, $(x,y)\sim(u,v)$ iff $x+y=u+v$;

\item In $\R\times\R$, $(x,y)\sim(u,v)$ iff $x^2-y=u^2-v$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it In $\Z$, $m\sim n$ iff $|m|=|n|$.} {\it Reflexivity.} $m\sim n$ holds as $|m|=|m|$. {\it Symmetry.} $m\sim n$ implies $|m|=|n|$ and $|n|=|m|$ implies $n\sim n$. {\it Transitivity.} If $m\sim n$ and $n\sim p$ then we have $|m|=|n|$ and $|n|=|p|$, i.e. $|m|=|n|=|p|$ which implies $|m|=|p|$ and $m\sim p$. We have $A_m=[m]=\{n\in\Z:\ n\sim m\}=\{n\in\Z:\ |n|=|m|\}$. As $A_m=A_{-m}$ we can only consider family $\{A_m:\ m\in\N_0\}$ as partition.

\item {\it In $\Q$, $r\sim s$ iff $r-s\in\Z$.} {\it Reflexivity.} $r\sim r$ is true as $r-r=0\in\Z$. {\it Symmetry.} $r\sim s$ implies $r-s\in\Z$. And, if $r-s\in\Z$ then $s-r=-(r-s)\in\Z$, so $s\sim r$. {\it Transitivity.} If $r\sim s$ and $s\sim t$, then $r-s\in\Z$ and $s-t\in\Z$. But, $(r-s)+(s-t)\in\Z$, that is $r-t\in\Z$, i.e. $r\sim t$. We have $A_r=[r]=\{x\in\Q:\ x-r\in\Z\}$. Partition is the family $\{A_r:\ r\in\Q\}$.

\item {\it Let $\left\lceil x\right\rceil$ denote the greatest integer less than or equal to $x$. In $\R$, let $a\sim b$ iff $\left\lceil a\right\rceil=\left\lceil b\right\rceil$.} Due to equality ("$=$") being reflexive, transitive and symmetric, so is $\sim$. Partition is the family $\{A_r:\ r\in\Z\}$ where $A_r=\{x\in\R:\ \left\lceil x\right\rceil=r\}$.

\item {\it In $\Z$, let $m\sim n$ iff $m-n$ is a multiple of $10$.} {\it Reflexivity.} $m\sim m$ as $10|0$, i.e. $10|m-m$. {\it Symmetry.} $m\sim n$ implies $10|m-n$, i.e. there exists $q\in\Z$ such that $m-n=10q$. Taking $q'=-q$ we have $m-n=-10q'$, from that $n-m=10q'$ which in turn implies $10|n-m$ and $n\sim m$. {\it Transitivity.} If $m\sim n$ and $n\sim p$ then there exist $q,r\in\Z$ such that $m-n=10q$ and $n-p=10r$. Taking the sum we have $m-n+n-p=10q+10r$, id est $m-p=10(q+r)$, which means that $m\sim p$. If we take $A_m=[m]=\{n\in\Z:\ 10|m-n\}$ we get a partition $\{A_m:\ m\in\Z\}$.

\item {\it In $\R$, let $a\sim b$ iff $a-b\in\Q$.} {\it Reflexivity.} We have $a\sim a$ as $0=a-a\in\Q$. {\it Symmetry.} $a\sim b$ implies $a-b\in Q$, but so is $b-a=-(a-b)\in\Q$ and $b\sim a$. {\it Transitivity.} $a\sim b$ and $b\sim c$ imply $a-b\in\Q$ and $b-c\in\Q$; their sum is also in $\Q$ and we have $a-c\in\Q$, i.e. $a\sim c$. Partition is the family $\{A_r:\ r\in\R\}$, where $A_r=[r]=\{p\in\R:\ r-p\in\Q\}$.

\item {\it In $\mathcal{F}(\R)$, let $f\sim g$ iff $f(0)=g(0)$.} Reflexivity, symmetry and transitivity hold due to the same properties being shared with equality ("$=$"). Partition is the family $\{A_r:\ r\in\R\}$, where $A_r=\{g\in\mathcal{F}(\R):\ f(0)=r\}$.

\item {\it In $\mathcal{F}(\R)$, let $f\sim g$ iff $f(x)=g(x)$ for all $x>c$, where $c$ is some fixed real number.} Let $c\in\R$. {\it Reflexivity.} Trivial, as $f(x)=f(x)$, for all $x>c$. {\it Symmetry.} If $f\sim g$ then $f(x)=g(x)$, for all $x>c$ and due to symmetry of equality we have $g(x)=f(x)$ and $g\sim f$. {\it Transitivity.} $g\sim f$ and $f\sim h$ imply $g(x)=f(x)$ and $f(x)=h(x)$, for all $x>c$ and that means that $g(x)=h(x)$, for all $x>c$. Partition is the family $\{A_f:\ f\in\mathcal{F}(\R)\}$, where $A_f=[f]=\{g\in\mathcal{F}(\R):\ (\forall x>c)(f(x)=g(x))\}$, where $c\in\R$.

\item {\it If $C$ is any set, $\mathcal{P}(C)$ denotes the set of all the subsets of $C$. Let $D\subseteq C$. In $\mathcal{P}(C)$, let $A\sim B$ iff $A\cap D=B\cap D$.} {\it Reflexivity.} $A\sim A$ as $A\cap D=A\cap D$. {\it Symmetry.} $A\sim B$ implies $A\cap D=B\cap D$, that is $B\cap D=A\cap D$ which means $B\sim A$. {\it Transitivity.} $A\sim B$ and $B\sim C$ imply $A\cap D=B\cap D$ and $B\cap D=C\cap D$. Combined, we get $A\cap D=C\cap D$, i.e. $A\sim C$. Partition is the family $\{A_C:\ C\in\mathcal{P}_C\}$ where $A_C=\{X\in\mathcal{P}(C):\ A\cap D=X\cap D\}$ and $D\subseteq C$.

\item {\it In $\R\times\R$, let $(a,b)\sim(c,d)$ iff $a^2+b^2=c^2+d^2$.} Reflexivity, symmetry and transitivity hold due to the same property being shared with equality ("$=$"). Partition is the family $\{A_{(a,b)}:\ (a,b)\in\R\times\R\}$ where $A_{(a,b)}=[(a,b)]=\{(c,d)\in\R\times\R:\ a^2+b^2=c^2+d^2\}$.

\item {\it In $\R^{\ast}$, let $a\sim b$ iff $\frac{a}{b}\in\Q$.} {\it Reflexivity.} $a\sim a$ holds as $1=\frac{a}{a}\in\Q$. {\it Symmetry.} $a\sim b$ implies $\frac{a}{b}\in\Q$, but so is $\frac{b}{a}\in\Q$ (note that $a,b\neq 0$ as $a,b\in\R^{\ast}$ and $\Q^{\ast}$ is a group and therefore closed with respect to inverses) and $b\sim a$. {\it Transitivity.} $a\sim b$ and $b\sim c$ impliy that $\frac{a}{b}\in\Q$ and $\frac{b}{c}\in\Q$. Multiplying those two numbers gets us $\frac{a}{b}\frac{b}{c}=\frac{a}{c}\in\Q$ ($\Q^{\ast}$ is closed with respect to multiplication). Partition is the family $\{A_r:\ r\in\R^{\ast}\}$, where $A_r=[r]=\{p\in\R^{\ast}:\ \frac{r}{p}\in\Q\}$.

\item {\it In $\R\times\R$, $(x,y)\sim(u,v)$ iff $a x^2+b y^2=a u^2+b v^2$ (where $a,b>0)$.} Reflexivity, symmetry and transitivity all hold due to the fact that equality defines the relation. Partition associated with it is $\{A_r:\ r\in\R^{+}_0\}$ where $A_r=\{(x,y)\in\R\times\R:\ a x^2+b y^2=r\}$. Geometrically, it partitions Euclidean space into elypses.

\item {\it In $\R\times\R$, $(x,y)\sim(u,v)$ iff $x+y=u+v$.} Again, reflexivity, symmetry and transitivity follow trivially. The partition is $\{A_r:\ r\in\R\}$ such that $A_r=\{(x,y)\in\R\times\R:\ y=r-x\}$. It partitions Euclidean space into parallel lines.

\item {\it In $\R\times\R$, $(x,y)\sim(u,v)$ iff $x^2-y=u^2-v$.} Reflexivity, symmetry and transitivity hold due to equality in definition. Partition is $\{A_r:\ r\in\R\}$ where $A_r=\{(x,y)\in\R\times\R: x^2=y+r\}$. This is the partition of Euclidean space into parabolas.
\end{enumerate}

\noindent{\bf Problem.} Let $G$ be a group. In each of the following, a relation on $G$ is defined. Prove it is an equivalence relation. Then describe the equivalence class of $e$.

\begin{enumerate}
\item If $H$ is a subgroup of $G$, let $a\sim b$ iff $a b^{-1}\in H$;
\item If $H$ is a subgroup of $G$, let $a\sim b$ iff $a^{-1} b\in H$;
\item Let $a\sim b$ iff there is an $x\in G$ such that $a=x b x^{-1}$;
\item Let $a\sim b$ iff there is an integer $k$ such that $a^k=b^k$;
\item Let $a\sim b$ iff $a b^{-1}$ commutes with every $x\in G$;
\item Let $a\sim b$ iff $a b^{-1}$ is a power of $c$ (where $c$ is a fixed element of $G$).
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $H$ is a subgroup of $G$, let $a\sim b$ iff $a b^{-1}\in H$.} {\it Reflexivity.} $a\sim a$ as $a a^{-1}=e\in H$. {\it Symmetry.} $a\sim b$ implies $a b^{-1}\in H$, but as $H$ is closed with respect to multiplication then $a\in H$ and $b^{-1}\in H$. As $H$ is closed with respect to inverses and multiplication we have $b a^{-1}\in H$ which implies $b\sim a$. {\it Transitivity.} If $a\sim b$ and $b\sim c$ then $a b^{-1}\in H$ and $b c^{-1}\in H$. Multiplying those two elements gives us $a b^{-1} b c^{-1}=a c^{-1}\in H$, due to $H$ being closed with respect to multiplication. Finally, $[e]=\{x\in G:\ x\sim e\}=\{x\in G:\ x e^{-1}\in H\}=\{x\in G:\ x\in H\}=H$.

\item {\it If $H$ is a subgroup of $G$, let $a\sim b$ iff $a^{-1} b\in H$.} Reflexivity, symmetry and transitivity follow similarly as in the previous example. Also, $[e]=H$. This is actually the same equivalence relation as in the previous example as $a^{-1} b\in H$ will imply that $a b^{-1}\in H$ and reverse.

\item {\it Let $a\sim b$ iff there is an $x\in G$ such that $a=x b x^{-1}$.} {\it Reflexivity.} $a\sim a$ holds because $a=a a a^{-1}$. {\it Symmetry.} From $a\sim b$ follows that $a=x b x^{-1}$. Multiplying by $x$ on the right and $x^{-1}$ on the left gives us $x^{-1} a x=b$ which implies $b\sim a$ (maybe it's a little bit unclear but taking $y=x^{-1}\in G$ would clarify our conclusion on a more symbolical level). {\it Transitivity.} If $a\sim b$ and $b\sim c$ then $a=x b x^{-1}$ and $b=x c x^{-1}$ which gives us $a=x x c x^{-1} x^{-1}$ and that is $a=x^2 c \left(x^2\right)^{-1}$. As $x^2, x^{-2}\in G$, then $a\sim c$. We have $[e]=\{y\in G:\ (\exists x\in G)(e=x y x^{-1})\}=\{y\in G:\ (\exists x\in G)(x=x y)\}=\{y\in G:\ e=y\}=\{e\}$.

\item {\it Let $a\sim b$ iff there is an integer $k$ such that $a^k=b^k$.} {\it Reflexivity.} $a\sim a$ because $a^k=a^k$. {\it Symmetry.} $a\sim b$ implies that there exists $k\in\Z$ such that $a^k=b^k$. Simply taking $b^k=a^k$ gives us $b\sim a$. {\it Transitivity.} $a\sim b$ and $b\sim c$ implies $a^k=b^k$ and $b^k=c^k$, i.e. $a^k=c^k$ and $a\sim c$. Equivalence class of $e$ is $[e]=\{x\in G:\ (\exists k\in\Z)(x^k=e^k)\}=\{x\in G:\ (\exists k\in\Z)(x^k=e)\}=\{x\in G:\ \ord{x}<\infty\}$.

\item {\it Let $a\sim b$ iff $a b^{-1}$ commutes with every $x\in G$.} If $a b^{-1}$ commutes with every $x\in G$, then $a b^{-1}$ is in the center $C$ of group $G$. So we could have said $a\sim b$ iff $a b^{-1}$ is in center $C$ of group $G$. And, as $C$ is a subgroup of $G$, from first example we have that $\sim$ is an equivalence relation. Also $[e]=\{y\in G:\ (\forall x\in G)(x y e^{-1}=y e^{-1} x)\}=\{y\in G:\ (\forall x\in G)(x y=y x)\}=C$.

\item {\it Let $a\sim b$ iff $a b^{-1}$ is a power of $c$ (where $c$ is a fixed element of $G$).} Let $c\in G$. {\it Reflexivity.} We have $a\sim a$ as $a a{-1}=e=c^0$. {\it Symmetry.} We have $a\sim b$ and from that $a b^{-1}=c^k$, for some $k\in G$. Multiplying by $c^{-k}$ on the right gives us $\left(a b^{-1}\right)c^{-k}=e$. Multiplying by $\left(a b^{-1}\right)^{-1}=b a^{-1}$ on the left gives $c^{-k}=b a^{-1}$ and that implies $b\sim a$. {\it Transitivity.} From $a\sim b$ and $b\sim d$ follows that $a b^{-1}=c^k$ and $b d^{-1}=c^l$. Multiplying first equation by $c^l$ on the right gives us $\left(a b^{-1}\right) c^l=c^k c^l$. That is equivalent to $a b^{-1} b d^{-1}=c^{k+l}$, i.e. $a d^{-1}=c^{k+l}$, which means that $a\sim d$. For $c\in G$ we have $[e]=\{y\in G:\ (\exists k\in\Z)(y e^{-1}=c^k)\}=\{y\in G:\ (\exists k\in\Z)(y=c^k)\}$, i.e. all elements whose $k$-th root is $c$.

\end{enumerate}

\noindent{\bf Proposition.} Let $\{A_i:\ i\in I\}$ be a partition of $A$ and $\{B_j:\ j\in J\}$ a partition of $B$. Then, $\{A_i\times B_j:\ (i,j)\in I\times J\}$ is a partition of $A\times B$. Furthermore, if $\sim_I$ and $\sim_J$ are equivalence relations corresponding to partitions of $A$ and $B$, respectively, then equivalence relation $\sim$ corresponding to $A\times B$ is $(a_1,b_1)\sim(a_2,b_2)$ if and only if $a_1\sim_I a_2$ and $b_1\sim_J b_2$.

\noindent\newline{\bf Proof.} If we take $(a,b)\in A\times B$ then $a\in A$ and $b\in B$. But, as we have partitions of $A$ and $B$, as defined, there exist $i\in I$ and $j\in J$ such that $a\in A_i$ and $b\in B_j$, that is $(a,b)\in A_i\times B_j$. If we take $(a,b)\in A_i\times B_j$ and $(a,b)\in A_k\times B_l$ then we have $a\in A_i$ and $b\in B_j$, and $a\in A_k$ and $b\in B_l$. But, $a\in A_i$ and $a\in A_k$ implies that $A_i=A_k$, and $b\in B_j$ with $b\in B_l$ implies $B_j=B_l$, so we have $A_i\times B_j=A_k\times B_l$.

Let $\sim_I$ and $\sim_J$ be equivalence relations as defined. First we will show that $\sim$ is an equivalence relation. {\it Reflexivity.} $(a,b)\sim(a,b)$ is true as $a\sim_I a$ and $b\sim_J b$. {\it Symmetry.} $(a_1,b_1)\sim (a_2,b_2)$ implies $a_1\sim_I a_2$ and $b_1\sim_J b_2$. As $\sim_I$ and $\sim_J$ are equivalence relations, and are therefore symmetric, we have $a_2\sim_I a_1$ and $b_2\sim_J b_1$, which in turn implies $(a_2,b_2)\sim(a_1,b_1)$. {\it Transitivity.} If $(a_1,b_1)\sim(a_2,b_2)$ and $(a_2,b_2)\sim(a_3,b_3)$ then we have $a_1\sim_I a_2$ and $b_1\sim_J b_2$ with $a_2\sim_I a_3$ and $b_2\sim_J b_3$. As $\sim_I$ and $\sim_J$ are transitive, we have $a_1\sim_I a_3$ and $b_1\sim_J b_3$, that is, $(a_1,b_1)\sim(a_3,b_3)$. Now, if $[a_i]_I=A_i$ and $[b_i]_J=A_j$, then $A_i\times B_j=[(a_i,b_j)]=\{(x,y)\in A\times B:\ (x,y)\sim(a_i,b_j)\}=\{(x,y)\in A\times B:\ (x\sim_I a_i)\wedge(y\sim_J b_j)\}=\{(x,y)\in A\times B:\ (x\in[a_i]_I)\wedge(y\in[b_j]_J)\}=[a_i]_I\times[b_j]_J$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $f:A\rightarrow B$ be a function and $\sim$ a relation such that, for all $a_1,a_2\in A$, $a_1\sim a_2$ if and only if $f(a_1)=f(a_2)$. Then, $\sim$ is an equivalence relation on $A$.

\noindent\newline{\bf Proof.} {\it Reflexivity.} $a\sim a$ as $f(a)=f(a)$. {\it Simmetry.} $a_1\sim a_2$ implies $f(a_1)=f(a_2)$, i.e. $f(a_2)=f(a_1)$ and that is $a_2\sim a_1$. {\it Transitivity.} If $a_1\sim a_2$ and $a_2\sim a_3$ then $f(a_1)=f(a_2)$ and $f(a_2)=f(a_3)$, which combined yields $f(a_1)=f(a_3)$, id est $f(a_1)=f(a_3)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $f:A\rightarrow B$ be a function, and let $\{B_i:\ i\in I\}$ be a partition of $B$. Then\footnote{Keep in mind that, for $f:A\rightarrow B$, and any $C\subseteq B$, we define $f^{-1}(C)=\{x\in A:\ f(x)\in C\}$.} $\{f^{-1}(B_i):\ i\in I\}$ is a partition of $A$.

\noindent\newline{\bf Proof.} If $a\in A$ then there exists $b\in B$ such that $b=f(a)$, i.e. $f(a)\in B$. As $B$ has a forementioned partition, then there exists $i\in I$ such that $f(a)\in B_i$. As $a\in A$ implies $f(a)\in B_i$ then $a\in\{x\in A:\ f(x)\in B_i\}$, i.e. $a\in f^{-1}(B_i)$. Then, if $a\in f^{-1}(B_i)$ and $a\in f^{-1}(B_j)$, then there exists $b\in B_i$ and $b\in B_j$ such that $b=f(a)$. As $B_i$ and $B_j$ are sets in the partition of $B$, $b\in B_i$ and $b\in B_j$ implies $B_i=B_j$ and that further implies $f^{-1}(B_i)=f^{-1}(B_j)$. Therefore, $\{f^{-1}(B_i):\ i\in I\}$ is a partition of $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $\sim_1$ and $\sim_2$ be distinct equivalence relations on $A$. Define $\sim_3$ by $a\sim_3 b$ if and only if $a\sim_1 b$ and $a\sim_2 b$. Then, $\sim_3$ is an equivalence relation of $A$. Furthermore, let $[x]_i$ denote the equivalence class of $x$ for $\sim_i$ (where $i\in\{1,2,3\}$). Then, $[x]_3=[x]_1\cap[x]_2$.

\noindent\newline{\bf Proof.} {\it Reflexivity.} $a\sim_3 a$ holds as $a\sim_1 a$ and $a\sim_2 a$. {\it Symmetry.} $a\sim_3 b$ implies that $a\sim_1 b$ and $a\sim_2 b$. But, $\sim_1$ and $\sim_2$ are equivalence relations, so $a\sim_1 b$ and $a\sim_2 b$ imply $b\sim_1 a$ and $b\sim_2 a$ and from that follows that $b\sim_3 a$. {\it Transitivity.} If $a\sim_3 b$ and $b\sim_3 c$ we have $a\sim_1 b$ and $a\sim_2 b$ with $b\sim_1 c$ and $b\sim_2 c$. Again, as $\sim_1$ and $\sim_2$ are transitive, we have $a\sim_1 c$ and $a\sim_2 c$ which implies $a\sim_3 c$.
If we take $a\in[x]_3$, then $a\sim_3 x$. That implies that $a\sim_1 x$ and $a\sim 2 x$, i.e. $a\in[x]_1$ and $a\in[x]_2$. As $a\in[x]_3$ implies $a\in[x]_1$ and $a\in[x]_2$, then by definition of intersection, $[x]_3=[x]_1\cap [x]_2$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Cyclic groups}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group and $a\in G$. Then, $\cyc{a}=\{a^k:\ k\in\Z\}$ is called a {\bf cyclic group}.

\noindent\newline{\bf Lemma.} Let $G$ be a group and $a\in G$. Then, $a^k\in G$, for all $k\in\Z$.

\noindent\newline{\bf Proof.} Let $k>0$. If we take $k=1$, we have $a^1=a\in G$. Suppose $a^k\in G$. Then, $a^{k+1}=a^k a$. As $a^k\in G$ by assumption and $a\in G$, then their product is in $G$. Therefore, $a^{k+1}\in G$. If $k=0$ we have $a^0=e$ by definition and $e\in G$. If $k<0$, we can take $l=-k$. So $l>0$ and we have $a^k=a^{-l}=\left(a^{-1}\right)^l$. As $a^{-1}\in G$ and $l>0$, by the first part of the proof we have $a^{-l}\in G$, i.e. $a^k\in G$, for all $k\in\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$. Then, $\cyc{a}$ is a subgroup of $G$.

\noindent\newline{\bf Proof.} Let $a^k\in\cyc{a}$. By a previous lemma, we have $a^k\in G$, so $\cyc{a}\subseteq G$. If we take $a^k,a^l\in\cyc{a}$, we have $a^k a^l=a^{k+l}$ and $a^{k+l}\in\cyc{a}$. Also, $a^k a^{-k}=a^{k-k}=a^0=e$ and $a^{-k}\in\cyc{a}$. Therefore, as $\cyc{a}$ is a subset of $G$ and is closed with respect to products and inverses, it is a subgroup of $G$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$. Then, $\left|\cyc{a}\right|=\ord{a}$.

\noindent\newline{\bf Proof.} Let $A=\left\{a^k:\ k\in\left[0,\ldots,\ord{a}-1\right]\cap\Z\right\}$. By a previous proposition all powers of $a$ are different, i.e. if $k\neq l$, then $a^k\neq a^l$. Therefore, $|A|=\ord{a}$. Now, we will show that $A=\cyc{a}$. If we take $a^k\in A$, where $0\leq k<\ord{a}$, as also $k\in\Z$, we have $a^k\in\cyc{a}$. So, $A\subseteq\cyc{a}$. Then, we take $a^k\in\cyc{a}$. By division with remainder theorem we have that there exist $q,r\in\Z$ such that $k=q\ord{a}+r$, where $0\leq r<|\ord{a}|=\ord{a}$. So, $a^k=a^{q\ord{a}+r}$, i.e. $a^k=\left(a^{\ord{a}}\right)^q a^r=e^q a^r=e a^r=a^r$. As $r\in\left[0,\ldots,\ord{a}-1\right]\cap\Z$, we have $a^r\in A$. But, $a^r=a^k$, so $a^k\in A$ and $\cyc{a}\subseteq A$. That implies $\cyc{a}=A$ and $\left|\cyc{a}\right|=|A|$, i.e. $\left|\cyc{a}\right|=\ord{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $n\in\N$. For all $a,b\in\Z$, let $a\sim_n b$ if and only if $a\equiv b\pmod n$.

\noindent\newline{\bf Proposition.} $\sim_n$ is an equivalence relation.

\noindent\newline{\bf Proof.} Follows directly from the fact (proved in my other script) that congruence is an equivalence relation on $\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $n\in\N$. We define equivalence class for each $a\in\Z$ so that $[a]_n=\{b\in\Z:\ a\sim_n b\}$. Let $\Z_n=\{[a]_n:\ a\in\Z\}$ be a family of all equivalence classes for $\sim_n$.

\noindent\newline{\bf Proposition.} $\left|\Z_n\right|=n$.

\noindent\newline{\bf Proof.} Let $\mathcal{Z}=\{[a]_n:\ a\in[0,\ldots,n-1]\cap\Z\}$. Let $[a]_n,[b]_n\in\mathcal{Z}$, with $a\neq b$. Then, $0\leq a,b<n$. Assume $[a]_n=[b]_n$. From that follows that $a\sim_n b$, i.e. $a\equiv b\pmod n$. As $0\leq a,b<n$, from a proposition discussed in my number theory script, we have $a=b$, which is a contradiction. Therefore, for $a\neq b$ we have $[a]_n\neq [b]_n$. Therefore, $\left|\mathcal{Z}\right|=n$. Obviously $\mathcal{Z}\subseteq\Z_n$. But, if we take $[a]_n\in\Z_n$, we can use the division with remainder theorem to get $a=n q+r$, where $q,r\in\Z$ and $0\leq r<|n|=n$. Then, $[a]_n=[n q+r]_n=\{b\in\Z:\ b\equiv n q+r\pmod n\}$. But, again, from a proposition proved in my number theory works, we have that $b\equiv n q+r\pmod n$ is equivalent to $b\equiv r\pmod n$. Therefore, $[a]_n=\{b\in\Z:\ b\equiv r\pmod n\}=[r]_n$. As $0\leq r<n$, we have $[r]_n\in\mathcal{Z}$ and from that $\Z_n\subseteq\mathcal{Z}$. That implies $\Z_n=\mathcal{Z}$ and $|\Z_n|=|\mathcal{Z}|=n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Operation $[a]_n+[b]_n=[a+b]_n$, for all $a,b\in\Z$, is well-defined.

\noindent\newline{\bf Proof.} The operation is obviously defined for all $a,b\in\Z$, as for each element of $\Z$ there exists a corresponding equivalence class (as equivalence relations partition sets). Now, assume $[a]_n=[c]_n$ and $[b]_n=[d]_n$, for some $a,b,c,d\in\Z$. That implies that $a\equiv c\pmod n$ and $b\equiv d\pmod n$. From a property of congruence relation we have that then $a+b\equiv c+d\pmod n$, i.e. $(a+b)\sim_n(c+d)$. That implies $[a+b]_n=[c+d]_n$. That proves uniqueness property.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} $\Z_n$ with operation defined in previous proposition is an Abelian group.

\noindent\newline{\bf Proof.} Let $a,b,c\in\Z$. {\it Associativity.} Holds as $[a]_n+([b]_n+[c]_n)=[a]_n+[b+c]_n=[a+(b+c)]_n=[(a+b)+c]_n=[a+b]_n+[c]_n=([a]_n+[b]_n)+[c]_n$. {\it Neutral element.} Obviously, $[0]_n+[a]_n=[0+a]_n=[a]_n=[a+0]_n=[a]_n+[0]_n$. {\it Inverse elements.} We have $[a]_n+[-a]_n=[a+(-a)]_n=[0]_n=[-a+a]_n=[-a]_n+[a]_n$. {\it Commutativity.} As follows from addition, $[a]_n+[b]_n=[a+b]_n=[b+a]_n=[b]_n+[a]_n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} $\Z_n$ is cyclic.

\noindent\newline{\bf Proof.} If we take $[a]_n\in\Z_n$, then $[a]_n=\underbrace{[1+1+\cdots+1]_n}_{a\textnormal{ times}}=\underbrace{[1]_n+[1]_n+\cdots+[1]_n}_{a\textnormal{ times}}$. Thus, every $[a]_n\in\Z_n$ can be shown as a power (here we use the notation for addition) of $[1]_n\in\Z_n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} As there is no difference between an operation on equivalence class in $\Z_n$ and an operation between representatives of equivalence classes, we will just, for now, denote $a=[a]_n$, for all $[a]_n\in\Z_n$.

\noindent\newline{\bf Theorem.} Every infinite cyclic group, say $G=\left\langle a\right\rangle$, is isomorphic to $\Z$. Similarly, every finite cyclic group, say $G_n=\left\langle a\right\rangle$ with generator $a^n=e$, for some $n\in\N$, is isomorphic to $\Z_n$.

\noindent\newline{\bf Proof.} (i) We will define a function $f:\Z\rightarrow G$ with formula $f(x)=a^x$, where $a\in G$. First we will show that it is a bijection. {\it Injectivity.} Take $f(x)=f(y)$. We have $a^x=a^y$. Multiplying by $a^{-y}$ we get $a^{x-y}=e$. As order of $a$ is infinite, there is no positive (nor negative) integer such that $a^n=e$ except for $n=0$. Thus it must be that $x-y=0$, i.e. $x=y$ and $f$ is, by that, injective. {\it Surjectivity.} We take $y\in G$ and want to find $x\in\Z$ such that $f(x)=y$. But, every $y\in G$ is of the form $y=a^i$, where $i\in\Z$, so it is sufficient to take $x=i$ and get $f(i)=a^i$. As both injectivity and surjectivity hold, the function is bijective. Last thing to show is that $f(x+y)=f(x)f(y)$, for all $x,y\in\Z$. We have $f(x+y)=a^{x+y}=a^x a^y=f(x)f(y)$. Therefore, $f$ is an isomorphism, so $G\cong\Z$. (ii) We use the function $f:\Z_n\rightarrow G_n$ with $f(x)=a^x$. {\it Injectivity.} From $f(x)=f(y)$, where $0\leq x<n$ and $0\leq y<n$, we get $a^x=a^y$ and $a^{x-y}=e$. As $\ord{a}=n$, it must be that $n|(x-y)$ and $n\leq|x-y|$, i.e. $x-y=n k$, for some $k\in\Z$. Furthermore, that implies $|x-y|=n|k|$. Now, $\Z_n$ contains only integers from $0$ to $n$ (excluding), therefore, as $0\leq x<n$ and $0\leq y<n$, then surely $|x-y|<n$. Therefore, the only choice for $k\in\Z$ is $0$ and it must be $x=y$. {\it Surjectivity.} Follows from (i) with restriction that $0\leq x<n$. The property that $f(x+y)=f(x)f(y)$ also follows from (i). Thus, $f$ is an isomorphism and $\Z_n\cong G_n$, for all $n\in\N$.

\begin{flushright}
$\square$\\
\end{flushright} 

\noindent{\bf Theorem.} Every subgroup of a cyclic group is a cyclic group.

\noindent\newline{\bf Proof.} Let $G=\left\langle a\right\rangle$ be a cyclic group. Let $G'$ be a subgroup of $G$. To show that $G'$ is cyclic we need to find a generator $a^g$, for some $g\in\N$, and show that every $x\in G'$ can be shown as $x=\left(a^g\right)^i$ for some $i\in\N$. As $G'$ is a subgroup of $G$, every $x\in G'$ is of the form $x=a^i$, for some $i\in \N$. Let $g\in\N$ be such number that $g\leq i$ for all $a^i\in G'$. By division theorem we divide $i$ by $g$ and get $i=g q+r$, where $q,r\in\Z$ and $0\leq r<|g|=g$. Therefore, $a^i=a^{g q+r}$, i.e. $a^i=a^{g q}a^r$. By multiplying with $a^{-g q}$ on the right we have $a^i\left(a^q\right)^{-m}=a^r$. As $G'$ is closed under multiplication and with respect to inverses, $a^i\left(a^{-q}\right)^m$ is in $G'$, i.e. $a^r\in G'$. But $0\leq r<g$ and we assumed that $g>0$ and $g\leq i$, for all $i\in G$. So it must be $r=0$. Therefore, $a^i=\left(a^q\right)^g$. Thus, $G'$ is generated by $a^g$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every group $G=\left\langle a\right\rangle$ of infinite order is generated by $a$ and $a^{-1}$ and has no other generators.

\noindent\newline{\bf Proof.} From definition of a cyclic group $G$ is generated by $a$. If we take some $b\in G$, it has to be of the form $b=a^n$, where $n\in\Z$. Taking $b=\left(a^{-1}\right)^{-n}$ shows that every element can be written as a power of inverse of $a$, therefore $G$ is also generated by $a^{-1}$. Now suppose that $G$ is generated by some element $g$. As $g\in G$, and $G$ is cyclic, it has to be of form $g=a^k$, for some $k\in\Z$. Now, if we take some $h\in G$, for which it's $h=a^l$, as $g$ is generator by our assumption, we need to have $h=g^n$, for some $n\in\Z$, id est, $a^l=\left(a^k\right)^n$. From that we get $a^l=a^{k n}$. Multiplying equation by $a^{-k n}$ on the right we get $a^{l-k n}=e$. As $a$ is of infinite order (if it were finite then $\left\langle a\right\rangle$ would also be finite, contradicting our statement), the only possibility is that $a^0=e$. Therefore it must be that $l-k n=0$, that is $l=k n$. That would mean that $k|l$. But, as our choice of $h$, and therefore of $l$, is arbitrary, we can take $l=k-1$. But, $k\nmid(k-1)$ and $l$ cannot be written as a product of $k$ and $n$ (if $k-1=k n$, then $k(1-n)=1$ and $k=\frac{1}{1-n}$, which is possible only when $n=2$ or $n=0$).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} $\R$ and $\R^{\ast}$ are not cyclic groups.

\noindent\newline{\bf Proof.} Suppose that $k\in\R$ generates $\R$. Then every $a\in\R$ can be written as $a=\underbrace{k+k+\cdots+k}_{n\textnormal{ times}}=n k$, where $n\in\Z$. Suppose that $k$ is odd. If we take $b=(n+1)k\in\R$, then $\frac{a+b}{2}\in\R$, but $\frac{a+b}{2}=\frac{k(2n+1)}{2}$. As $2\nmid(2n+1)$, obviously $\frac{a+b}{2}\in\R$ is not generated by $k$ (it would have to be of the form $m k$, where $m\in\Z$).

Now, suppose that $k\in\R^{\ast}$ generates $\R^{\ast}$. Then for some $a\in\R^{\ast}$ it has to be $a=k^n$ for some $n\in\Z$. We can also take $b\in\R^{\ast}$ such that $b=k^{n+1}$. We can take $\sqrt{a b}\in\R^{\ast}$ which is $\sqrt{a b}=k^{\frac{2 n+1}{2}}$, which is not an integer power of $k$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $|G|=n$. $G$ is cyclic if and only if $G$ has an element of order $n$.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose that $G$ is cyclic and $|G|=n$. As $G$ is cyclic it has a generator $a\in G$ which is, by definition, of order $n$.

{\it Sufficiency.} Suppose that $|G|=n$ and $G$ has an element of order $n$. Suppose that $\ord{b}=n$ for some $b\in G$. We need to prove that $G$ is cyclic. By a previous proposition, all powers of $b$ up to $n$ are distinct. Also, $b^k\in G$ (due to $G$ being closed under multiplication), for every $k\in\{0,\ldots,n-1\}$. Therefore, $G$ must contain all $n$ powers of $b$ and no other elements (as it would be a contradiction with the fact that it's order is $n$). So, by definition, group $G$ is cyclic.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every cyclic group is Abelian.

\noindent\newline{\bf Proof.} Let $G$ be a cyclic group generated by $a$. Then, every $b,c\in G$ can be written down as $b=a^n$, for some $n\in\{0,\ldots,|G|-1\}$ and $c=a^m$, for some $m\in\{0,\ldots,|G|-1\}$. Thus we have $b c=a^n a^m=a^{n+m}=a^{m+n}=a^m a^n=c b$, so $G$ is Abelian.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $G=\left\langle a\right\rangle$ and $b\in G$, the order of $b$ is a factor of the order of $a$, i.e. $\ord{b}|\ord{a}$.

\noindent\newline{\bf Proof.} We have $\ord{a}=n$ and suppose $\ord{b}=m$. As $b\in G$, then there exists $k\in\{0,\ldots,n-1\}$ such that $b=a^k$. As $a^n=e$, we have $e=\left(a^{n}\right)^k=a^{n k}=\left(a^k\right)^n=b^n$. So, $b^n=e$ and $b^m=e$. By a definition of order we have $m\leq n$. If we divide $n$ by $m$, we have that $n=m q+r$, for some $q,r\in\N_0$ (as order is always positive) and $0\leq r<m$. Then, $e=b^{n}=b^{m q+r}=b^{m q} b^r$, i.e. $e=b^r$. As $b^r=e$ and $0\leq r<m$ and as it must be that either $r\geq m$ or $r=0$, the only possible choice is that $r=0$, so $n=m q$ and from that $m|n$, id est\footnote{I could have just written down that it follows from previous propisitions, but I wanted to repeat the nice proofs one more time.} $\ord{b}|\ord{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a cyclic group of order $n$. For every integer $k$ which divides $n$, there are elements of order $k$.

\noindent\newline{\bf Proof.} Let $G=\left\langle a\right\rangle$, $|G|=n$ and $k\in\Z$ such that $k|n$. We need to prove that there exists $b\in G$ such that $\ord{b}=k$. We have $n=q k$ for some $q\in\Z$. That means that $a^{q k}=e$ and $\left(a^q\right)^k=e$, which means that for some $b=a^q$, $m=\ord{b}|k$. As $\ord{a}=n$, we have that, by a previous proposition that $\ord{a}=\frac{m q}{l}$, where $m$ and $l$ are relatively prime. So, $n=m\frac{q}{l}$ and from that $m=\frac{n l}q$ and $m=\frac{q k l}{q}=k l$, meaning that $k|m$. As we have $m|k$ and $k|m$ then $m=k$, that is, $\ord{b}=k$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be an Abelian group of order $m n$, where $m$ and $n$ are relatively prime. If $G$ has an element of order $m$ and an element of order $n$, $G$ is cyclic.

\noindent\newline{\bf Proposition.} Let $a,b\in G$ such that $\ord{a}=m$ and $\ord{b}=n$ and $\gcd{(m,n)}=1$. By a previous proposition, $\ord{a b}=m n$. Also, by a previos proposition, $G$ is cyclic as its order is $m n$ and it contains element of order $m n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G=\left\langle a\right\rangle$ be a cyclic group of order $n$. If $n$ and $m$ are relatively prime, then the function $f(x)=x^m$ is an automorphism.

\noindent\newline{\bf Proof.} We need to prove that $f:G\rightarrow G$ is bijective and that $f(a b)=f(a)f(b)$, for all $a,b\in G$. First we will prove that it is a function. If we take $x\in G$ there needs to be $y\in G$ such that $y=x^m$. So, if we take $a^k\in G$ so that $\left(a^k\right)^m=y$, we need to show that $y\in G$. We have $y=a^{k m}$. If we divide $k m$ by $n$, then there exist $q,r\in\Z$ such that $0\leq r<|n|=n$ and $k m=n q+r$. So $y=a^{n q+r}$, i.e. $y=a^{n q}a^r$. But, $a^{n q}=e$ as $\ord{a}=e$ and $y=a^r$. As $0\leq r<n$, then $a^r\in G$, that is $y\in G$.

Then, if we take $x_1=x_2$ it must follow that $f(x_1)=f(x_2)$. This one can be proved by induction. From $x_1=x_2$, by multiplying equation on the right with $x_1$ we get $x_1^2=x_2 x_1$. But, $x_1=x_2$, so $x_1^2=x_2^2$. Then we suppose that the statement is true for some $k$, that is, $x_1^k=x_2^k$. If we multiply the equation on the right by $x_1$, we get $x_1^{k+1}=x_2^{k} x_1$ which is $x_1^{k+1}=x_2^{k+1}$. Therefore, the statement is valid for any $n\in\N$ and by that for $m$; from $x_1=x_2$ follows $x_1^m=x_2^m$. Therefore, $f$ is a function.

{\it Injectivity.} We need to show that $f(x)=f(y)$ implies $x=y$ for all $x,y\in G$. Let $x=a^k$ and $y=a^l$ ($G$ is cyclic), where $k,l\in\{0,\ldots,n-1\}$. We have $x^m=y^m$, that is, $a^{k m}=a^{l m}$. Multiplying by $a^{-l m}$ on the right yields $a^{k m-l m}=e$, i.e. $\left(a^m\right)^{k-l}=e$. That means that $\ord{a^m}|(k-l)$. But, as $\gcd{(m,n)}=1$, we have that $\ord{a^m}=n$. That implies that $n|(k-l)$. But, as $0\leq k,l<n$, then also $|k-l|<n$ and due to our conclusion that $n|(k-l)$ it has to be that $k-l=0$, i.e. $k=l$ and from that $y=a^l=a^k=x$.

{\it Surjectivity.} By a previous theorem, $f$ is an injection if and only if it is a surjection and $\left|\textnormal{dom}(f)\right|=\left|\textnormal{cod}(f)\right|$. As $|G|=n$, and $f:G\rightarrow G$ is an injection, then it must be a surjection and a bijection.

As a last thing, we will prove that $f(x y)=f(x)f(y)$, for all $x,y\in G$. Take $x=a^k$ and $y=a^l$. Then:

\begin{eqnarray*}
f(x y)&=&f(a^k a^l)=f(a^{k+l})=\left(a^{k+l}\right)^m=a^{m(k+l)}\\
&=&a^{m k+m l}=a^{m k} a^{m l}=\left(a^k\right)^m\left(a^l\right)^m=f(a^k)f(a^l)\\
&=&f(x)f(y).
\end{eqnarray*}

\noindent\newline Therefore, $f$ is an isomorphism from $G$ to $G$, and by definition, an automorphism on $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G=\left\langle a\right\rangle$ be a cyclic group of order $n$. Then, $r\in\N$ and $\gcd{(n,r)}=1$ if and only if $a^r$ is a generator of $G$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $r\in\N$ and $\gcd{(n,r)}=1$. As $\ord{a}=n$ by definition, then, as $r$ and $n$ are relatively prime, $\ord{a^r}=n$, by a previous proposition. Also, by a previous proposition, all powers of $a^r$ are distinct. Therefore, $\left(a^r\right)^k\in G$, for all $k=0,\ldots,n-1$, i.e. each power of $a$ must correspond to some power of $a^r$ and reverse. So $a^r$ is a generator of $G$.

{\it Sufficiency.} Suppose $a^r$ is a generator of $G$. As $|G|=n$ it follows that $\ord{a^r}=n$. From a previous proposition, that means that $\gcd{(n,r)}=1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $\phi:\N\rightarrow\N$ be a function which, for every natural number $n$, assigns the number of natural numbers, less than $n$, that are relatively prime to $n$. Such a function is called {\it Euler's totient function}

\noindent\newline{\bf Remark.} Euler's totient function can be written using the Iverson brackets as:

\begin{equation*}
\phi(n)=\sum_{k=1}^{n}\left[\gcd{(n,k)}=1\right].
\end{equation*}

\noindent\newline{\bf Proposition.} Each cyclic group $G$ of order $n$ has $\phi(n)$ generators.

\noindent\newline{\bf Proof.} From a previous proposition $x\in G$ is a generator of $G$ iff $\gcd{\left(\ord{x},n\right)}=1$. Also, as we are observing only $x\in G$, they are all distinct and if $x=a^k$ then $k<n$. The number of generators of $G$ is then, by definition of totient function, equal to $\phi(n)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} For any factor $m$ of $n$, let $C_m=\{x\in\left\langle a\right\rangle:\ x^m=e\}$. Then, $C_m$ is a subgroup of $\left\langle a\right\rangle$ and has exactly $m$ elements.

\noindent\newline{\bf Proof.} Let $G=\left\langle a\right\rangle$. By a definition of $C_m$ we have $C_m\subseteq G$. Is $C_m$ non-empty? Due to a previous problem, for every $k$ such that $k|n$ there are elements of order $k$. Therefore, there are elements of the form $x^m=e$, as $m|n$. If we take $x,y\in C_m$ we have to show that $x y\in C_m$. So, from $x^m=e$ and $y^m=e$, we have that $x^m y^m=e$ and, as $x,y\in G$, and $G$ is Abelian, as proven by a previous proposition, we have $\left(x y\right)^m=e$ and by that $x y\in C_m$. Therefore, $C_m$ is closed under multiplication. Furthermore, if we take $x\in C_m$ we need to show that $x^{-1}\in C_m$. As $\left(x^{-1}\right)^m=x^{-m}=\left(x^m\right)^{-1}=e^{-1}=e$, we have that $x^{-1}\in C_m$. That implies that $C_m$ is closed with respect to inverses and $C_m$ is a subgroup of $G$.

As argued for $C_m\neq\emptyset$, there are elements of order $m$ and they are in $C_m$. Say $x\in C_m$ is such that $\ord{x}=m$. Every subgroup of a cyclic group is cyclic. Therefore $C_m$ has to be generated by a single element. We know that $C_m$ has at least $m$ different elements (from $\ord{x}=m$). But, suppose that there exists $y\in C_m$ such that $\ord{y}=q>m$ and $y^m=e$. From that we have that $q|m$, which is a contradiction that $q>m$. Therefore as $\ord{x}=m$ and there is no element of higher order, and all $m$ powers of $C_m$ are distinct, it must be that $|C_m|=m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} An element $x$ in $G=\left\langle a\right\rangle$, $|G|=n$, has order $m$ (where $m|n$) if and only if $x$ is a generator of $C_m$.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $x\in G$ and $\ord{x}=m$. Then $x^m=e$ and by definition of $C_m$ it must be that $x\in C_m$. Similarly to the proof in the proposition above, as $C_m$ has exactly $m$ elements and is cyclic, it must be that $x$ generates $C_m$ (as $\ord(x)=m$ it generates $m$ different powers of itself and in $C_m$ we have exactly $m$ different powers by previous propositions and it contains $x$; so every element of $C_m$ is a power of $x$). {\it Sufficiency.} If $x$ is a generator of $C_m$ then $\ord{x}=m$ (as $C_m$ has exactly $m$ elements).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} There are $\phi(m)$ elements of order $m$ in $G=\left\langle a\right\rangle$, $|G|=n$ (where $m|n$).

\noindent\newline{\bf Proof.} By a previous proposition we have that an element is a generator of a group if and only if the order of the group and order of the element are relatively prime. So, if an element $x\in G$ has an order $m$ in $G$, then it must be in $C_m$, and it also generates $C_m$. As $x$ is of the form $a^k$, where $k\in\{0,\ldots,n-1\}$, we have that $a^k$ generates $C_m$. Then, by a previous proposition, $k$ and $m$ are relatively prime. As there are $\phi(m)$ natural numbers ($k_1,k_2,\ldots,k_{\phi(m)}$) that are relatively prime to $m$, then it must be that there are $\phi(m)$ elements of order $m$ in $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Previous series of propositions actually tell us that if we have $G=\left\langle a\right\rangle$ with $|G|=n$, then there are $\phi(k)$ elements of order $k$ for every $k$ that divides $n$.

\noindent\newline{\bf Proposition.} Let $G$ be a cyclic group generated by $a$ with $|G|=n$. Furthermore, let $n=m k$. $a^r$ has order $m$ if and only if $r=k l$ where $l$ and $m$ are relatively prime.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $a^r$ has order $m$ and $n=m k$. We need to show that $r=k l$ and $\gcd{(m,l)}=1$. Suppose that $q|m$ and $q|l$ where $q\in\N$ and $q>1$. Then $m=q m'$ and $l=q l'$; also, $n=q m' k$ and $r=q l' k$. As $a^r$ generates $C_m$ it must be that $r$ and $m$ are relatively prime. As $r=q l' k$ and $m=q m'$, we have $\gcd{(r,m)}=q$, which is a contradiction. Therefore $m$ and $l$ are relatively prime.

{\it Sufficiency.} Let $n=m k$ and $r=k l$, where $\gcd{(l,m)}=1$. We need to show that $\ord{a^r}=m$. As $a^r=a^{k l}$, we have that $\ord{a^{k l}}=\frac{n}{\gcd{(k l,n)}}$, by a previous proposition. As $\gcd{(k l, n)}=\gcd{(k l, m k)}$ and $\gcd{(m, l)}=1$ it must be\footnote{Details in my proofs from number theory.} that $\gcd{(k l, n)}=k$. We have $\ord{a^{k l}}=\frac{n}{k}=\frac{m k}{k}=m$, i.e. $\ord{a^r}=m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that if $c$ is any generator of $G=\left\langle a\right\rangle$, $|G|=n$, then $\ord{c}=n$. If $c=a^k$, it must be that $\gcd{(k,n)}=1$. Then if $\gcd{(r,n)}=1$, we have $c^r=a^{k r}$ and it must be\footnote{Again, proof in my works on number theory.} that $\gcd{(k r, n)}=1$. Therefore, $c^r$ is generator of $G$ and the set $\{c^r:\ \gcd{(r,n)}=1\}$ contains all generators of $G$.

\noindent\newline{\bf Problem.} Let $G$ be a group and let $a,b\in G$. Prove the following:

\begin{enumerate}
\item If $a$ is a power of $b$, say $a=b^k$, then $\left\langle a\right\rangle\subseteq\left\langle b\right\rangle$;
\item Suppose $a$ is a power of $b$, say $a=b^k$. Then $b$ is equal to a power of $a$ iff $\left\langle a\right\rangle=\left\langle b\right\rangle$;
\item Suppose $a\in\left\langle b\right\rangle$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $a$ and $b$ have the same order;
\item Let $\ord{a}=n$ and $b=a^k$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $n$ and $k$ are relatively prime;
\item Let $\ord{a}=n$, and suppose $a$ has a $k$-th root, say $a=b^k$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $k$ and $n$ are relatively prime;
\item Any cyclic group of order $m n$ has a unique subgroup of order $n$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $a$ is a power of $b$, say $a=b^k$, then $\left\langle a\right\rangle\subseteq\left\langle b\right\rangle$.} If we take some $a^l\in\left\langle a\right\rangle$ then $a^l=b^{l k}$, i.e. every power of $a$ can be shown as a power of $b$, for all $l\in\{0,\ldots,\ord{a}\}$. By definition of a subset of a set, the assumption is true.

\item {\it Suppose $a$ is a power of $b$, say $a=b^k$. Then $b$ is equal to a power of $a$ iff $\left\langle a\right\rangle=\left\langle b\right\rangle$.} {\it Necessity.} Suppose $a=b^k$ and $b$ is equal to a power of $a$, i.e. $b=a^l$. Then, by a previous problem we have $\left\langle a\right\rangle\subseteq\left\langle b\right\rangle$ and $\left\langle b\right\rangle\subseteq\left\langle a\right\rangle$, for both assumptions, respectively. Those two relations combined yield our necessary implication.

{\it Sufficiency.} Suppose $\left\langle a\right\rangle=\left\langle b\right\rangle$. If we take $a\in\left\langle a\right\rangle$, then also $a\in\left\langle b\right\rangle$, so it must be that $a=b^k$. Conversely, if we take $b$, then it's also in a group generated by $a$ and it must be that $b=a^l$, for some $k,l\in\Z$.

\item {\it Suppose $a\in\left\langle b\right\rangle$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $a$ and $b$ have the same order.} Suppose $a=b^k$, for some $k\in\Z$. {\it Necessity.} Let us assume that $\left\langle a\right\rangle=\left\langle b\right\rangle$. We need to prove that $\ord{a}=\ord{b}$. Suppose $\ord{a}=n$ and $\ord{b}=m$. Then, as $a=b^k$, the order of $b$ is $\ord{b}=n\frac{k}{l}$, i.e. $m=n\frac{k}{l}$. Also as $b\in\left\langle a\right\rangle$, then $b=a^p$ and $\ord{a}=m\frac{p}{q}$, i.e. $n=m\frac{p}{q}$, where $\frac{k}{l}\in\N$ and $\frac{p}{q}\in\N$ (which follows from Euclid's lemma as $\gcd{(n,l)}=1$ and $\gcd{(m,q)}=1$). Therefore, as $m|n$ and $n|m$ we have that $\ord{a}=n=m=\ord{b}$.

{\it Sufficiency.} Let $n=\ord{a}=\ord{b}$ and $a=b^k$, for some $k\in\{0,\ldots,n-1\}$. We have that $\ord{b^k}=n$ (as $a=b^k$ implies $\ord{a}=\ord{b^k}$) and $\ord{b}=n$. That means that $k$ and $n$ are relatively prime. Thus, $b^k$ generates $\left\langle b^k\right\rangle$ with order $n$. As $\left\langle b^k\right\rangle$ and $\left\langle b\right\rangle$ contain $n$ distinct powers of $b$, all elements must be equal.

\item {\it Let $\ord{a}=n$ and $b=a^k$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $n$ and $k$ are relatively prime.} {\it Necessity.} We have $\ord{a}=n$, $b=a^k$ and $\left\langle a\right\rangle=\left\langle b\right\rangle$. By a previous proposition $a$ and $b$ have the same order, that is, $\ord{a}=\ord{b}=n$. As $b=a^k$ and $\ord{b}=\ord{a^k}=n$, by a previous theorem $n$ and $k$ are relatively prime.

{\it Sufficiency.} Let $k$ and $n$ be relatively prime. Then, $\ord{b}=\ord{a^k}=n$ and by a previous problem we have $\left\langle a\right\rangle=\left\langle b\right\rangle$.

\item {\it Let $\ord{a}=n$, and suppose $a$ has a $k$-th root, say $a=b^k$. Then $\left\langle a\right\rangle=\left\langle b\right\rangle$ iff $k$ and $n$ are relatively prime.} {\it Necessity.} If $\left\langle a\right\rangle=\left\langle b\right\rangle$ then $\ord{a}=\ord{b}=n$. We also have that $\ord{b}=\frac{n k}{l}$, where $n$ and $l$ are relatively prime. But, that means that $n=\frac{n k}{l}$ and from that we have $l n=n k$, i.e. $l=k$. As $\gcd{(l,n)}=1$ then, as $l=k$, we have $\gcd{(k,n)}=1$.

{\it Sufficiency.} Let $k$ and $n$ be relatively prime. Then, from $\ord{a}=n$ and $a=b^k$ we have that $\ord{a}=\ord{b^k}=n$. As $k$ and $n$ are relatively prime, it must be, by a previous theorem, that $\ord{b}=n$. From a previous problem, as $\ord{a}=\ord{b}=n$, we have that $\left\langle a\right\rangle=\left\langle b\right\rangle$.

\item {\it Any cyclic group of order $m n$ has a unique subgroup of order $n$.} {\it Existence.} Let $G=\left\langle a\right\rangle$ be a cyclic group such that $|G|=m n$, for some $m,n\in\N$. If we take $b\in G$, such that $b=a^m$, then $\ord{b}=\frac{m n}{\gcd{(m n,m)}}$. We have $\gcd{(m n,m)}=m$ and from that $\ord{b}=\frac{m n}{m}=n$. Therefore we will define $G'=\left\langle b\right\rangle$ and, as $\ord{b}=n$, by definition, $|G'|=n$. Let us show that $G'$ is a subgroup of $G$. By a previous problem, as $b=a^m$, we have that $\left\langle b\right\rangle\subseteq\left\langle a\right\rangle$, i.e. $G'\subseteq G$. As $G'$ itself is a cyclic group, and is a subset of $G$, then $G'$ is a subgroup of $G$ with $|G'|=n$.

{\it Uniqueness.} Now, suppose that there exists some cyclic subgroup $H\subseteq G$ such that $|H|=n$. Let $H=\left\langle c\right\rangle$. Then as $c\in H\subseteq G$ we have $c=a^k$ for some $k\in\{0,\ldots,m n-1\}$. As $\ord{c}=n$ and $\ord{a^k}=\frac{m n}{\gcd{(m n,k)}}$ we have $n\gcd{(m n,k)}=m n$ and $\gcd{m n, k}=m$. That means that $m|k$, i.e. $k=m q$. As $c=a^k=a^{m q}=\left(a^m\right)^q$ and $b=a^m$, we have $c=b^q$. Which means that $c\in G'$. As $c\in G'$ and $n=\ord{b}=\ord{c}$, by a previous problem we have that $G'=H$.

\end{enumerate}

\noindent{\bf Problem.} Let $G$ and $H$ be groups, with $a\in G$ and $b\in H$. Prove:

\begin{enumerate}

\item If $(a,b)$ is a generator of $G\times H$, then $a$ is a generator of $G$ and $b$ is a generator of $H$. If $G\times H$ is a cyclic group, then $G$ and $H$ are both cyclic and converse is false;

\item Let $\ord{a}=m$ and $\ord{b}=n$. The order of $(a,b)$ in $G\times H$ is the least common multiple of $m$ and $n$;

\item If $m$ and $n$ are relatively prime, then $(a,b)$ has order $m n$;

\item Suppose $(c,d)\in G\times H$, where $c$ has order $m$ and $d$ has order $n$. If $m$ and $n$ are not relatively prime, then the order of $(c,d)$ is less than $m n$.

\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}

\item {\it If $(a,b)$ is a generator of $G\times H$, then $a$ is a generator of $G$ and $b$ is a generator of $H$.} Take $x\in G$ and $y\in H$. We want to show that $x$ can be shown as a power of $a$ and $y$ as a power of $b$. As $(a,b)$ is a generator of $G\times H$, we have that for $(x,y)\in G\times H$ holds:

\begin{equation*}
(x,y)=(a,b)^k=\underbrace{(a,b)(a,b)\cdots(a,b)}_{k\textnormal{ times}}.
\end{equation*}

\noindent\newline By definition of a direct product we have that $(a,b)(a,b)=(a a, b b)=(a^2,b^2)$. When applied $k$ times, as in the above equality, we have $(x,y)=(a^k,b^k)$, so $x=a^k$ and $y=b^k$. Now, as any $x\in G$ and any $y\in H$ can be shown as a power of $a$ and of $b$, respectively, we have that $a$ generates $G$ and $b$ generates $H$. From this also follows that if $G\times H$ is cyclic then $G$ and $H$ are both cyclic.

Suppose that $G$ and $H$ are cyclic and $G=\Z_2$ and $H=\Z_4$. By a previous problem (on page $70$, excuse me for not labeling problems and theorems, no time for that now, as it's time to practice violin), we have that $G\times H$ is not cyclic, altough generated by two elements.

\item {\it Let $\ord{a}=m$ and $\ord{b}=n$. The order of $(a,b)$ in $G\times H$ is the least common multiple of $m$ and $n$.} As $\ord{a}=m$ and $\ord{b}=n$, we have $a^m=e$ and $b^n=e$. Now, $(a,b)^l=(a^l,b^l)=e$, where $l=\lcm{m,n}$. This is obvious as both $m|l$ and $n|l$. Suppose there is some $k\leq l$ such that $(a,b)^k=e$. That would mean that $a^k=e$ and $b^k=e$, which would mean that $m|k$ and $n|k$, therefore $k$ is a common multiple of $m$ and $n$. As $k\leq l$ and $k$ is a common multiple of $m$ and $n$, we have a contradiction to assumption that $l$ is the least common multiple of $m$ and $n$.

\item {\it If $m$ and $n$ are relatively prime, then $(a,b)$ has order $m n$.} From a previous problem $\ord{(a,b)}=\lcm{m,n}$ and as $\gcd{m,n}=1$ we have that $\lcm{m,n}=m n$, so $\ord{(a,b)}=m n$.

\item {\it Suppose $(c,d)\in G\times H$, where $c$ has order $m$ and $d$ has order $n$. If $m$ and $n$ are not relatively prime, then the order of $(c,d)$ is less than $m n$.} As $\ord{c}=m$ and $\ord{c}=n$, then $\ord{(c,d)}=\lcm{m,n}<m n$.

\end{enumerate}

\noindent{\bf Proposition.} $\left\langle a\right\rangle\times\left\langle b\right\rangle$ is cyclic if and only if $\ord{a}$ and $\ord{b}$ are relatively prime.

\noindent\newline{\bf Proof.} Let $G=\left\langle a\right\rangle$ and $H=\left\langle b\right\rangle$. {\it Necessity.} Suppose $G\times H$ is cyclic. Then, as $(a,b)$ is obviously a generator of $G\times H$, we have that $\ord{(a,b)}=\lcm{m,n}$, where $m=\ord{a}$, and $n=\ord{b}$. Suppose $m$ and $n$ are not relatively prime. Then $\ord{(a,b)}$ is less than $m n$, but $G\times H$ must contain all ordered pairs of the form $(a^k,b^l)$, for $k\in\{0,\ldots,m-1\}$ and $l\in\{0,\ldots,n-1\}$ and there are $m n$ of them. Therefore, some cannot be shown as a power of $(a,b)$ as order of $(a,b)$ is less than $m n$. To conclude, $m$ and $n$ must be relatively prime.

{\it Sufficiency.} Suppose $m$ and $n$ are relatively prime. Then, in group $G\times H$ generator $(a,b)$ has order $m n$ and all its elements are distinct. As there are $m n$ possible combinations of $(a^k,b^l)$ (if $m$ and $n$ are relatively prime, by a previos problem, no power of $a$ can equal a power of $b$), for $k\in\{0,\ldots,m-1\}$ and $l\in\{0,\ldots,n-1\}$ all of them must be contained in $G\times H$, meaning each can be shown as a power of $(a,b)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be an Abelian group of order $m n$, where $m$ and $n$ are relatively prime. If $G$ has an element $a$ of order $m$ and an element $b$ of order $n$, then $G\cong\left\langle a\right\rangle\times\left\langle b\right\rangle$.

\noindent\newline{\bf Proof.} By a previous proposition, as $\ord{a}=m$ and $\ord{b}=n$ and $\gcd{(m,n)}=1$, $(a,b)$ generates $\left\langle a\right\rangle\times\left\langle b\right\rangle$, which has $m n$ elements. If $m$ and $n$ are relatively prime, by a previous problem, no power of $a$ can equal a power of $b$. Thus, as there are $m n$ combinations of $a^i b^j$ and $\ord{a b}=m n$, by a previous proposition, we have that $a b$ generates $G$. As $m$ and $n$ are relatively prime $\left\langle a\right\rangle\times\left\langle b\right\rangle$ is cyclic with $m n$ distinct elements. As $(a b)$ is the only generator of $G$, i.e. $G=\left\langle a b\right\rangle$, and has $m n$ elements, by a previous theorem, both groups are isomorphic to $\Z_{m n}$ and by the property of transitivity of isomorphism, they are isomorphic to each other.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $\left\langle a\right\rangle$ be a cyclic group of order $m n$, where $m$ and $n$ are relatively prime. Then, $\left\langle a\right\rangle\cong\left\langle a^m\right\rangle\times\left\langle a^n\right\rangle$.

\noindent\newline{\bf Proof.} As $\ord{a}=m n$, we have that $\ord{a^m}=\frac{m n}{\gcd{(m, m n)}}=\frac{m n}{m}=n$. Similarly, $\ord{a^n}=m$. Now, as $m$ and $n$ are relatively prime, no power of $a^m$ can equal a power of $a^n$ (due to facts that cyclic groups are Abelian and that $\gcd{(\ord{a^m},\ord{a^n})}=1$). Therefore, there are $m n$ distinct elements in $\left\langle a^m\right\rangle\times\left\langle a^n\right\rangle$. Also, as $m$ and $n$ are relatively prime, $\left\langle a^m\right\rangle\times\left\langle a^n\right\rangle$ is cyclic. Both groups are cyclic with $m n$ elements, so they are isomorphic to $\Z_{m n}$, by a previous theorem, and by that to each other.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Element $a^m$ has a $k$-th root in $G=\left\langle a\right\rangle$, where $|G|=n$, if and only if $\gcd{(k,n)}|m$.

\noindent\newline{\bf Proof.} Let us denote $g=\gcd{(k,n)}$. {\it Necessity.} Suppose $a^m$ has a $k$-th root in $G$, i.e. there exists $b\in G$ such that $a^m=b^k$. But, $b\in G$ so it must be of the form $b=a^x$, for some $x\in\{0,\ldots,n-1\}$ and we have $a^m=a^{x k}$. Multiplying by $a^{-m}$ on the right gives us $e=a^{x k-m}$. Dividing $x k-m$ by $n$ gives us $x k-m=n q+r$, for some $q,r\in\Z$, where $0\leq r<|n|=n$. Then we have $e=a^{n q}a^r$, and as $a^{n q}=e$, we have $a^r=e$. But, $r<n$, and $n$ is order of $a$, so it must be $r=0$. Therefore we have $x k-m=n q$. That is equivalent to $x k-n q=m$. As $m$ can be shown as a linear combination of $k$ and $n$ (we can take $q'=-q$ to have $x k+q'n=m$, and we assumed existence of $x$ and shown existence of $q'$ by division with remainder theorem), then it must be, by corollary of Bezout's lemma (discussed in my works on number theory) that $g|m$.

{\it Sufficiency.} Suppose $g|m$. By a corollary of Bezout's lemma we have that $m=x k+y n$ (because $g|m$), for some $x,y\in\Z$. So we have $a^m=a^{x k}a^{y n}$. As $\ord{a}=n$, then $a^{y n}=e$. Therefore, $a^m=a^{x k}=\left(a^x\right)^k$. Here $a^x$ is obviously $k$-th root of $a^m$ and it's existence is proven by forementioned corollary.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Element $a$ has a $k$-th root in $G=\left\langle a\right\rangle$, where $|G|=n$, if and only if $\gcd{(k,n)}=1$.

\noindent\newline{\bf Proof.} By definition $a=a^1$, so by a previous theorem (taking $m=1$ from $a^1$) $a^1$ will have a $k$-th root in $G$ if and only if $\gcd{(k,n)}|m$, i.e. $\gcd{(k,n)}|1$. But, as $\gcd{(k,n)}\in\Z\backslash\{0\}$, it can only be that $\gcd{(k,n)}=1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $p$ be a prime number and $G=\left\langle a\right\rangle$, where $|G|=n$.

\begin{enumerate}
\item If $n$ is not a multiple of $p$, then every element in $G$ has a $p$-th root.
\item If $n$ is a multiple of $p$, and $a^m$ has a $p$-th root, then $m$ is a multiple of $p$.
\end{enumerate}

\noindent{\bf Proof.} Suppose $p\in P$ and $G=\left\langle a\right\rangle$.

\begin{enumerate}
\item Let us assume that $p\nmid n$. That implies, by fundamental theorem of arithmetic, that $n$ must not contain any positive power of $p$ and that further implies that $\gcd{(p,n)}=1$. If we take $a^k$, for any $k\in\{0,\ldots,n-1\}$, we have that $1|k$. But, $\gcd{(p,n)}=1$, so we have $\gcd{(p,n)}|k$. By a previous theorem it follows that $a^k$ has a $p$-th root.

\item Let $n=p q$ for some $q\in\N$ (as $n,p\in\N$ by definition). If $a^m$ has a $p$-th root, then from a previos theorem it follows that $\gcd{(p,n)}|m$. But, $\gcd{(p,n)}=\gcd{(p,p q)}=p$ (proof discussed in my work on number theory). So we have that $p|m$, i.e. $m$ is a multiple of $p$.
\end{enumerate}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} The set of all elements in $G=\left\langle a\right\rangle$, where $|G|=n$, having a $k$-th root is a subgroup of $G$ and it is cyclic.

\noindent\newline{\bf Proof.} Let $R=\{x\in G:\ (\exists y\in G)(\exists k\in Z)(x=y^k)\}$. By definition $R\subseteq G$. Is $R$ non-empty? Of course, it will contain at least neutral element $e\in G$ because $e=e^k$, for any $k\in\Z$. If we take $x,y\in R$ then $x$ and $y$ have $k$-th roots in $G$ and $x=u^k$ and $y=v^k$, where $u,v\in G$. Multiplying them gives us $x y=u^k v^k$. As $G$ is cyclic and therefore Abelian, we have $x y=(u v)^k$. Due to the fact that $u,v\in G$, and $G$ is a group and therefore closed under multiplication, we have that $u v\in G$. So, $x y$ has a $k$-th root in $G$ and it must be that $x y\in R$. So, $R$ is closed under multiplication. If we take $x\in R$, then $x=u^k$ for some $u\in G$. As $u\in G$, it has an inverse and we have $u^{-1} u=e$. Also, $(u^{-1} u)^k=e$ and, because it is cyclic and therefore Abelian, we have $\left(u^{-1}\right)^k u^k=e$. But, $u^k=x$, so we have $\left(u^{-1}\right)^k x=e$. Multiplying by $x^{-1}\in G$, on the right gives us $\left(u^{-1}\right)^k=x^{-1}$. We already discussed that $u^{-1}\in G$, so $x^{-1}$ has a $k$-th root in $G$ and it must be in $R$. Therefore, $R$ is closed with respect to inverses and it is a subgroup of $G$. As every subgroup of a cyclic group is cyclic, it follows that $R$ is also cyclic.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that as $R$, from previous proposition, is cyclic and contains elements from $G$, it is of the form $R=\left\langle a^m\right\rangle$. But, if $a^m\in R$ then it has a $k$-th root in $G$, so it must be, by a previous theorem that $\gcd{(k,n)}|m$, i.e. there exists $q\in\Z$ such that $m=q\gcd{(k,n)}$. So, $R=\left\langle a^{q\gcd{(k,n)}}\right\rangle$, for some $q\in\Z$. But, if $a^{q\gcd{(k,n)}}$ is a generator of $R$, then it must be that $\gcd{\left(|R|,q\gcd{(k,n)}\right)}=1$. From this point of view $|R|=\ord{a^{q\gcd{(k,n)}}}=\frac{n}{q\gcd{(k,n)}}$. Also, as $g|m$, then there exist $x,y\in\Z$ such that $m=x k+y n$ and $a^m=a^{x k}a^{y n}=a^{x k}$. So we also have that $R=\left\langle a^{x k}\right\rangle$, for some $x\in\Z$. We have $\ord{a^{x k}}=\frac{n}{\gcd{(x k,n)}}$ and it must also be that $\gcd{\left(\frac{n}{\gcd{(x k,n)}},x k\right)}=1$.

\newpage

\begin{center}
{\bf Counting cosets}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group and $H$ a subgroup of $G$. Let $a\in G$. A {\bf left coset} of $H$ in $G$ is defined as:

\begin{equation*}
a H=\{y\in G:\ (\forall h\in H)(y=a h)\}.
\end{equation*}

\noindent\newline Similarly, a {\bf right coset} of $H$ in $G$ is:

\begin{equation*}
H a=\{y\in G:\ (\forall h\in H)(y=h a)\}.
\end{equation*}

\noindent\newline{\bf Remark.} From now on we will refer to the {\it right coset} of $H$ in $G$ simply as the {\it coset} of $H$ in $G$. Thus we avoid any ambiguity.

\noindent\newline{\bf Lemma.} Let $G$ be a group and $H$ a subgroup of $G$. Let $a,b\in G$. If $a\in H b$ then $H a=H b$.

\noindent\newline{\bf Proof.} As $a\in H b$, then there exists $h_1\in H$ such that $a=h_1 b$. If we take $x\in H a$ then there exists $h_2\in H$ such that $x=h_2 a$. That implies that $x=h_2 h_1 b$, i.e. $x=(h_2 h_1) b$. As $H$ is a subgroup of $G$ and $h_1,h_2\in H$, then their product is in $H$. So, as there exists $h\in H$, where $h=h_2 h_1$, such that $x=h b$, it must be that $x\in H b$. As $x\in H a$ implies $x\in H b$ we have $H_a\subseteq H_b$. Conversely, if we take $x\in H_b$ then there exists $h_3\in H$ such that $x=h_3 b$. As $h_3\in H$ and $H$ is a subgroup of $G$, then $H$ is closed with respect to inverses, and there exists $h_3^{-1}\in H$ such that $h_3^{-1} x=b$. Plugging that in $a=h_1 b$ we get $a=h_1 h_3^{-1} x$. Multiplying by $h_1^{-1}$ and $h_3$ on the left gives us $h_1^{-1} h_3 a=x$, i.e. if we take $h'\in H$ such that $h_1^{-1} h_3=h'$ (as $H$ is a subgroup, i.e. $H$ is closed with respect to inverses and multiplication), we have $x=h' a$, i.e. $x\in H a$. As $x\in H b$ implied $x\in H a$ we have $H b\subseteq H a$. That, combined with $H a\subseteq H b$, implies $H a=H b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ be a group and $H$ a subgroup of $G$. Family $\{H a:\ a\in G\}$ is a partition of $G$.

\noindent\newline{\bf Proof.} If we take $x\in G$ then obviously $x\in H x$, as $e\in H$ and $x=e x$. Then, if we take $x\in H a$ and $x\in H b$, there exist $h_1,h_2\in H$ such that $x=h_1 a$ and $x=h_2 b$. That implies that $h_1 a=h_2 b$. Multiplying with $h_1^{-1}\in H$ on the left gives us $a=(h_1^{-1} h_2) b$, i.e. $a\in H b$. From a previous lemma it follows that $H a=H b$. Thus, the forementioned family is a partition of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ be a group and $H$ a subgroup of $G$. For each $a\in G$ there is a bijection $f:H\rightarrow H a$.

\noindent\newline{\bf Proof.} Let $a\in G$ and $f(x)=x a$ with $f:H\rightarrow H a$. {\it Injectivity.} If $f(x)=f(y)$ we have $x a=y a$. Multiplying by $a^{-1}\in G$ on the right gives us $x=y$, for all $x,y\in H$. {\it Surjectivity.} Take $y\in H a$. Then $y=h a$, for some $h\in H$. We need to find $x\in H$ such that $f(x)=y$, i.e. $x a=h a$. Multiplying by $a^{-1}\in G$ on the right gives $x=h$. Indeed, as $h\in H$, also $h\in G$. Therefore, as $f$ is injective and surjective, it is also bijective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Lagrange).} If $G$ is a finite group and $H$ a subgroup of $G$, then $|H|$ divides $|G|$.

\noindent\newline{\bf Proof.} Suppose $|G|=n$ and $|H|=m$. We need to prove that $m|n$. Consider the partition $\{H_i:\ i\in I\}$, where $H_i$ are cosets of $H$ in $G$. As $G$ is finite, so is this partition, say that there are $k$ of $H_i$, where $k\in\N$ (we have at least $H=\{e\}$, a trivial subgroup, so $k\geq 1$). Then, using the fact that union of all $H_i$ is $G$ we have:

\begin{equation*}
\bigcup_{i=1}^{k}{H_i}=G.
\end{equation*}

\noindent\newline Then it also must be that:

\begin{equation*}
\left|\bigcup_{i=1}^{k}{H_i}\right|=|G|.
\end{equation*}

\noindent\newline Keep in mind that all $H_i$ are mutually disjoint. Also by a previos theorem, there exists a bijection from $H$ to each $H_i$, meaning $|H|=|H_i|=m$, i.e. $|H_i|=m$, for each $i\in\{1,\ldots,k\}$. So as there are $k$ of disjoint $H_i$ and $|H_i|=m$ we have $k\cdot m=n$ (remember that $|G|=n$ by assumption), i.e. it must be that $m|n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} If $G$ is a group with a prime number $p$ of elements, i.e. $|G|=p$, then $G$ is a cyclic group. Furthermore, any element $a\neq e$ in $G$ is a generator of $G$.

\noindent\newline{\bf Proof.} If we take $a\in G$ such that $\ord{a}=m$, for some $m\in\N\backslash\{1\}$ we can observe $G'=\left\langle a\right\rangle$. By definition, $|G'|=m$. Also, $G'$ is a cyclic group and is contained in $G$ (as $G$ is closed with respect to inverses and multiplication), so it is a subgroup of $G$. Therefore, by Lagrange's theorem, it must be that $m|p$, but that can only be if $m=1$ or $m=p$. We assumed that $\ord{a}\neq 1$, so it must be that $m=p$. Therefore $G'$ contains all elements of $G$. As $G'$ is cyclic and contains all elements of $G$, then it follows that every element in $G$ can be shown as a power of $a$; it is also cyclic and generated by $a$. Our choice of $a$ was arbitrary so it proves the second part of the corollary.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $G$ be a finite group with $|G|=n$. Let $a\in G$. Then, $\ord{a}|n$.

\noindent\newline{\bf Proof.} Consider the group $G'=\left\langle a\right\rangle$. Then, by definition, $|G'|=\ord{a}$. Also, $G'$ is a subgroup of $G$ as $G'$ is cyclic by definition and $G$ contains all its elements. Then, by Lagrange's theorem $|G'|$ divides $|G|$. But, $|G'|=\ord{a}$ and $|G|=n$ so $\ord{a}|n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a group and $H$ a subgroup of $G$. The {\bf index of H in G} is the number of cosets of $H$ in $G$, denoted by $[G:H]$.

\noindent\newline{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. Then:

\begin{equation*}
[G:H]=\frac{|G|}{|H|}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $G$ be a group with $|G|=n$ and $H$ a subgroup of $G$ such that $|H|=m$. Suppose there are $k$ cosets of $H$. Then all $H_i$, for $i\in\{1,\ldots,k\}$, have the same number of elements. Furthermore, they have the same number of elements as $H$. So we would have $k$ cosets of $H$ with the same number of elements, and that is $m$. As they are all disjoint and their union is all of $G$, it must be that $m\cdot k=n$. So, the number of cosets of $H$ is $k=\frac{n}{m}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} If operation on $G$ is denoted by $+$, it is customary to write $H+x$ for a coset, rather than $H x$. That is true for some other operation $\ast$, where we would have $H\ast x$, et cetera.

\noindent\newline{\bf Problem.} In each of the following, $H$ is a subgroup of $G$. List the cosets of $H$ and their elements. Indicate the order and index of each of the subgroups.

\begin{enumerate}
\item $G=\Z_4$, $H=\{0,2\}$;
\item $G=S_3$, $H=\{\epsilon,\beta,\delta\}$;
\item $G=S_3$, $H=\{\epsilon,\alpha\}$;
\item $G=\Z_{15}$, $H=\left\langle 5\right\rangle$;
\end{enumerate}

\noindent{\bf Remark.} We will use this table for $S_3$:

\begin{center}
\begin{tabular}{c|cccccc}
$\circ$ & $\epsilon$ & $\alpha$ & $\beta$ & $\gamma$ & $\delta$ & $\kappa$\\
\hline
$\epsilon$ & $\epsilon$ & $\alpha$ & $\beta$ & $\gamma$ & $\delta$ & $\kappa$\\
$\alpha$ & $\alpha$ & $\epsilon$ & $\gamma$ & $\beta$ & $\kappa$ & $\delta$\\
$\beta$ & $\beta$ & $\kappa$ & $\delta$ & $\alpha$ & $\epsilon$ & $\gamma$\\
$\gamma$ & $\gamma$ & $\delta$ & $\kappa$ & $\epsilon$ & $\alpha$ & $\beta$\\
$\delta$ & $\delta$ & $\gamma$ & $\epsilon$ & $\kappa$ & $\beta$ & $\alpha$\\
$\kappa$ & $\kappa$ & $\beta$ & $\alpha$ & $\delta$ & $\gamma$ & $\epsilon$\\
\end{tabular}
\end{center}

\noindent\newline We will also use:

\noindent\newline{\bf Solution.}

\begin{enumerate}
\item {\it $G=\Z_4$, $H=\{0,2\}$.} We have $|G|=4$ and $|H|=2$. Therefore, $[G:H]=\frac{4}{2}=2$. Furthermore, $H+0=H+2=\{0,2\}=H$, $H+1=H+3=\{1,3\}$.
\item {\it $G=S_3$, $H=\{\epsilon,\beta,\delta\}$.} From $|S_3|=3!=6$ and $|H|=3$ follows that $[G:H]=\frac{6}{3}=2$. Now, $H\epsilon=H\beta=H\delta=\{\epsilon,\beta,\delta\}=H$, $H\alpha=H\gamma=H\kappa=\{\alpha,\gamma,\kappa\}$.
\item {\it $G=S_3$, $H=\{\epsilon,\alpha\}$.} Similarly to the previous example, we have $[G:H]=\frac{6}{2}=3$. So, $H\epsilon=H\alpha=\{\epsilon,\alpha\}$, $H\beta=H\gamma=\{\beta,\gamma\}$, $H\delta=H\kappa=\{\delta,\kappa\}$.
\item {\it $G=\Z_{15}$, $H=\left\langle 5\right\rangle$.} We have $|\Z_{15}|=15|$ and $H=\{0,5,10\}$, so $|H|=3$ and $[G:H]=\frac{15}{3}=5$. We have $H+0=H+5=H+10=\{0,5,10\}$, $H+1=H+6=H+11=\{1,6,11\}$, $H+2=H+7=H+12=\{2,7,12\}$, $H+3=H+8=H+13=\{3,8,13\}$ and $H+4=H+9=H+14=\{4,9,14\}$.
\end{enumerate}

\noindent{\bf Proposition.} Let $m\in\Z^{+}$. Then, $\left[\Z:\left\langle m\right\rangle\right]=m$.

\noindent\newline{\bf Proof.} Let us observe the cosets of $\left\langle m\right\rangle=\{\ldots,-2m,-m,0,m,2m,3m,\ldots\}=\{k m\in\Z:\ k\in\Z\}$. Then, $\left\langle m\right\rangle+l=\{k m+l\in\Z:\ k\in\Z\}$, for all $l\in\Z$. By division with remainder theorem there exist $q,r\in\Z$ such that $l=q m+r$, where $0\leq r<m$. So we have $\left\langle m\right\rangle+(q m+r)=\{(k+q)m+r\in\Z:\ k\in\Z\}=\{k m+r\in\Z:\ k\in\Z\}$, for all $0\leq r<m$. Therefore, there exist $m$ different cosets of $\left\langle m\right\rangle$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Describe the cosets of the following subgroups:

\begin{enumerate}
\item The subgroup $\left\langle 3\right\rangle$ of $\Z$;
\item The subgroup $\Z$ of $\R$;
\item The subgroup $H=\{2^n:\ n\in\Z\}$ of $\R^{\ast}$;
\item The subgroup $\left\langle\frac{1}{2}\right\rangle$ of $\R^{\ast}$ and of $\R$;
\item The subgroup $H=\{(x,y):\ x=y\}$ of $\R\times\R$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it The subgroup $\left\langle 3\right\rangle$ of $\Z$.} We have $\left\langle 3\right\rangle+r=\{\ldots,-6+r,-3+r,0+r,3+r,6+r,\ldots\}$, for all $0\leq r<3$. So, there are $3$ different cosets.
\item {\it The subgroup $\Z$ of $\R$.} $\Z+r=\{\ldots,-2+r,-1+r,r,1+r,2+r,\ldots\}$, for all $r\in\R$, i.e. $\Z+r=\{k+r\in\R:\ k\in\Z\}$.
\item {\it The subgroup $H=\{2^n:\ n\in\Z\}$ of $\R^{\ast}$.} $H r=\{2^n r\in\R^{\ast}:\ n\in\Z\}$. Of course $H(2^k r)=H r$, for all $k\in\Z$.
\item {\it The subgroup $\left\langle\frac{1}{2}\right\rangle$ of $\R^{\ast}$ and of $\R$.} $\left\langle\frac{1}{2}\right\rangle r=\{\frac{r}{2^k}\in\R:\ k\in\Z\}$ and $\left\langle\frac{1}{2}\right\rangle+r=\{\frac{1+2k r}{2k}\in\R:\ k\in\Z\}$, respectively.
\item {\it The subgroup $H=\{(x,y):\ x=y\}$ of $\R\times\R$.} $H(a,b)=\{(x a,y b)\in\R\times\R:\ x=y\}$.
\end{enumerate}

\noindent{\bf Problem.} Find a subgroup of $\R^{\ast}$ whose index is equal to $2$.

\noindent\newline{\bf Solution.} If we take $H=\{r\in\R^{\ast}:\ r>0\}$ it is obvious that it is a subgroup of $\R^{\ast}$ as $x\cdot y>0$ for $x,y>0$ and for $x>0$ we have $\frac{1}{x}\in H$ because $\frac{1}{x}>0$ for $x>0$. We can see that $H x=H$ for $x>0$ and $H x=\{r\in\R^{\ast}:\ r<0\}$ for $x<0$. We are of course assuming $x\in\R^{\ast}$ for all mentions of $x$. Therefore, there are two different cosets of $H$ in $\R^{\ast}$.

\noindent\newline{\bf Proposition.} If $G$ is a group and has order $n$, then $x^n=e$ for every $x\in G$.

\noindent\newline{\bf Proof.} Suppose $G$ is a group and $|G|=n$. Take $x\in G$. By a previous corollary $\ord{x}|n$, i.e. there exists $q\in\Z$ such that $n=q\ord{x}$. We have $x^n=x^{q\ord{x}}=\left(x^{\ord{x}}\right)^q$. As $x^{\ord{x}}=e$, by definition, we have $x^n=e^q=e$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $|G|=p q$, where $p,q\in P$. Then, either $G$ is cyclic, or every element $x\neq e$ in $G$ has order $p$ or $q$.

\noindent\newline{\bf Proof.} We have, by a previous corollary that $\ord{x}|p q$ for all $x\in G$. But, by Euclid's lemma, as $\gcd{\left(\ord{x},p\right)}=\gcd{\left(\ord{x},q\right)}=1$, the only possible options are that $\ord{x}=p$, $\ord{x}=q$ or $\ord{x}=p q$ (we will ignore when $\ord{x}=1$ for that is true only for a neutral element). If $\ord{x}=p q$ for some $x\in G$, then, by a previous theorem\footnote{On page $159$.}, we have that $G$ is cyclic. If there does not exist $x\in G$ such that $\ord{x}=p q$, then $\ord{x}=p$ or $\ord{x}=q$, for all $x\in G$ (except, of course, $e\in G$ whose order is $1$).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $G$ is a group and $|G|=4$ then either $G$ is cyclic, or every element of $G$ is its own inverse. Every group of order $4$ is Abelian.

\noindent\newline{\bf Proof.} By a previos corollary we have that $\ord{x}|4$ for every $x\in G$. So we can have $\ord{x}=4$ and $\ord{x}=2$ (we will ignore $\ord{x}=1$). Suppose there exists $x\in G$ such that $\ord{x}=4$. Then, by a theorem mentioned in the proof of the previous proposition, $G$ is cyclic. Suppose that there does not exist $x\in G$ such that $\ord{x}=4$. Then, for all $x\in G$ (except $e\in G$) we have $\ord{x}=2$, that is $x^2=x x=e$, which means that every element is its own inverse. We will prove that such group is Abelian. Take $x,y\in G$. We have $x y=z$, where $z\in G$. As $z^2=e$, we have $(x y)^2=e$, i.e. $x y x y=e$. Multiplying by $y$ on the right gives us $x y x=y$. Multiplying by $x$ on the left gives us $y x=x y$. Thus, $G$ is Abelian.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group. If $G$ has an element of order $p$ and an element of order $q$, where $p$ and $q$ are distinct primes, then the order of $G$ is a multiple of $p q$.

\noindent\newline{\bf Proof.} Let $|G|=n$. Suppose there exist $x,y\in G$ such that $\ord{x}=p$ and $\ord{x}=q$. Then, by a previous theorem, it must be that $p|n$ and $q|n$. As $p|n$, there exists $k\in\Z$ such that $n=p k$. Now, as $q|n$, and by that $q|p k$, with $q\neq 1$ and $q\neq p$ by assumption, by Euclid's lemma, as $\gcd{(p,q)}=1$ for any two distinct primes $p,q\in P$, we have that $q|k$, i.e. there exists $l\in\Z$ such that $k=q l$. Plugging that back into $n$, we have $n=p q l$, that is $p q|n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group with $|G|=n$. If $G$ has an element of order $k$ and an element of order $m$, then $\lcm{(k,m)}|n$.

\noindent\newline{\bf Proof.} By a previous corollary we have that $k|n$, i.e. there exists $q\in\Z$ such that $n=k q$. Let $g=\gcd{(k,m)}$. Then $k=g x$ and $m=g y$ for some $x,y\in\Z$ such that $\gcd{(x,y)}=1$. Then, $\lcm{m,k}=\frac{g x g y}{g}=g x y$. We have $n=g x q$. But, as $m|n$, that is, $g y|g x q$, there exists $q'\in\Z$ such that $g x q=g y q'$. From that we have $x q=y q'$. That means that $x|y q'$. But, as $\gcd{(x,y)}=1$, by Euclid's lemma it has to be $x|q'$, i.e. there exists $q''\in\Z$ such that $q'=q''x$. Plugging that back gives us $g x q=g y q''x$, i.e. $n=g y q'' x$. As $\lcm{(m,k)}=g x y$, we have $n=q''\lcm{(m,k)}$ and from that it follows that $\lcm{(m,k)}|n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p$ be a prime number. In any finite group, the number of elements of order $p$ is a multiple of $p-1$.

\noindent\newline{\bf Proof.} Let $G$ be a finite group and $|G|=n$. Suppose there exist $k$ elements whose order is $p$. Choose $a\in G$ such that $\ord{a}=p$. Let's observe subgroup $\cyc{a}$. As $\ord{a}=p$ then $|\cyc{a}|=p$. By a previous theorem $a^r$, for $r\in\{1,\ldots,p-1\}$, is a generator of $\cyc{a}$ if and only if $p$ and $r$ are relatively prime. But, as $p$ is a prime number, then $\gcd{(r,p)}=1$, for all $r\in\{1,\ldots,p-1\}$ (we exclude $e=a^0=a^p$ as we know its order is $1$). Therefore there are $p-1$ generators of $\cyc{a}$. Also we have, by a previous theorem, $\ord{a^r}=p$ if and only if $r$ and $p$ are relatively prime, but that is true for all $r\in\{1,\ldots,p-1\}$. So, in $\cyc{a}$ there are $p-1$ elements of order $p$. Suppose there are $l$ such cyclic subgroups of $G$ (not generated by any power of $a$; then they are disjoint, by a previous problem\footnote{Third problem on page $164$}). Thus, there are $l\times(p-1)$ elements of order $p$, i.e. $k=l\times(p-1)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a finite group and let $H$ and $K$ be subgroups of $G$ such that $H\subseteq K$. Then $[G:H]=[G:K]\cdot[K:H]$.

\noindent\newline{\bf Proof.} Obviously, $H$ is a subgroup of $K$ ($H$ is a group itself, closed with respect to multiplication and inverses and all its elements are in $K$). By Lagrange's theorem $|H|$ divides $|K|$ and index of $H$ in $K$ is $[K:H]=\frac{|K|}{|H|}$. From that we have $|H|=\frac{|K|}{[K:H]}$. Now, as $K$ is a subgroup of $G$, by Lagrange's theorem, we have $[G:K]=\frac{|G|}{|K|}$. From that follows $|K|=\frac{|G|}{[G:K]}$. Also, as $H$ is also a subgroup of $G$, we have $[G:H]=\frac{|G|}{|H|}$. Plugging previous two equalities we have:

\begin{equation*}
[G:H]=\frac{|G|}{\frac{|K|}{[K:H]}}=\frac{|G|\cdot[K:H]}{|K|}=\frac{|G|\cdot[K:H]}{\frac{|G|}{[G:K]}}=[K:H]\cdot[G:K].
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a finite group and $H$ and $K$ subgroups of $G$. Then:

\begin{enumerate}
\item $|H\cap K|$ is a common divisor of $|H|$ and $|K|$;
\item If $\gcd{\left(|H|,|K|\right)}=1$ then $|H\cap K|=\{e\}$;
\item If $H\neq K$ and $|H|=|K|=p$ then $|H\cap K|=\{e\}$;
\item If $[G:H]=p$ and $[G:K]=q$, for $q,p\in P$ such that $q\neq p$, then $p q|[G:(H\cap K)]$.
\end{enumerate}

\noindent{\bf Proof.} Let $|H|=m$, $|K|=k$ and $|G|=n$. Remember\footnote{Proof on page $63$, first problem.} that $H\cap K$ is a subgroup of $G$ if $H$ and $K$ are subgroups of $G$. Now, as in the previous proposition, as $H\cap K$ is a subgroup of $G$ and so are $H$ and $K$, and we have $H\cap K\subseteq H$ and $H\cap K\subseteq K$, then, $H\cap K$ is a subgroup of $H$ and a subgroup of $K$.

{\it Ad $1$.} Let $l=|H\cap K|$. Then, as $|H\cap K|$ is a subgroup of $K$, we have $l|k$, and $l|m$ as it is a subgroup of $H$. Therefore $|H\cap K|$ is a common divisor of $|H|$ and $|K|$.

{\it Ad $2$.} Suppose $\gcd{\left(|H|,|K|\right)}=1$. From a previous problem we have that $|H\cap K|$ is a common divisor of $|H|$ and $|K|$. But, the only divisor is $1$, so it must be that $|H\cap K|=1$. So, $H\cap K=\{x\}$. As $H\cap K$ is a subgroup of $G$, it must be closed with respect to multiplication, so the only option is $x x=x$. As it is closed with respect to inverses also, and the only option is $x^{-1}=x$, we have $x^{-1} x=x$. As $x^{-1} x=e$, where $e$ is a neutral element in $G$ it must be that $x=e$. Thus, $H\cap K=\{e\}$.

{\it Ad $3$.} As $H\neq K$, they don't have all elements in common, but maybe some $k<|H|=|K|=p$. As $|H\cap K|=k$ and $|H\cap K|$ is a common divisor of $|H|$ and $|K|$, it must be that $k|p$. But, that is only possible if $k=1$ or $k=p$. But, we assumed that $H\neq K$, and from that we concluded $k<p$ and the only option is that $k=1$. As in $2$, it implies that $H\cap K=\{e\}$.

{\it Ad $4$.} We have that $[G:(H\cap K)]=[G:H]\cdot[H:(H\cap K)]$ and $[G:(H\cap K)]=[G:K]\cdot[K:(H\cap K)]$. That implies that $[G:H]\cdot[H:(H\cap K)]=[G:K]\cdot[K:(H\cap K)]$, i.e. $p x=q y$, where we took $[H:(H\cap K)]=x$ and $[K:(H\cap K)]=y$. In assumption we have $p\neq q$. Then $\gcd{(p,q)}=1$ (as they are prime) implies, by Euclid's lemma that $p|y$. So from $x=q\frac{y}{p}$ we can take $c\in\N$ to be $c=\frac{y}{p}$ and have $x=q c$. Similarly $y=p\frac{x}{q}=p c'$. Now we have $[G:(H\cap K)]=p q c$. Therefore, $p q|[G:(H\cap K)]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $G$ is an abelian group of order $n$, and $m$ is an integer such that $m$ and $n$ are relatively prime, then the function $f(x)=x^m$ is an automorphism of $G$.

\noindent\newline{\bf Proof.} First we will show that $f:G\rightarrow G$ is a bijection. {\it Injectivity.} If we take $f(x_1)=f(x_2)$ then we have $x_1^m=x_2^m$. If we multiply that by $\left(x_2^m\right)^{-1}$ (which is equal to $\left(x_2^{-1}\right)^m$), we get $x_1^m\left(x_2^{-1}\right)^m=e$. From that, as $G$ is Abelian, it follows that $\left(x_1 x_2^{-1}\right)^m=e$. By a corollary of Lagrange's theorem $\ord{x_1 x_2^{-1}}|n$, but also $\ord{x_1 x_2^{-1}}|m$. Therefore, $\ord{x_1 x_2^{-1}}$ is a common divisor of $m$ and $n$. But, $m$ and $n$ are relatively prime, so it can only be that $\ord{x_1 x_2^{-1}}=1$. That implies that $\left(x_1 x_2^{-1}\right)^1=e$, i.e. $x_1 x_2^{-1}=e$. By multiplying with $x_2$ on the right we have $x_1=x_2$. Therefore, $f$ is injective.

{\it Surjectivity.} If we take $y\in G$, then we need to find $x\in G$ such that $y=x^m$. In other words, we are looking for $m$-th root of $y$. Let's observe $\cyc{y}$. It is a subgroup of $G$, so it's order must divide $n$. Suppose $\left|\cyc{y}\right|=n'$. Then we have, as $\cyc{y}$ is a subgroup of $G$, by Lagrange's theorem that $n'|n$. But, as $\gcd{(m,n)}=1$, then also $\gcd{(m,n')}=1$ (proof in my works on number theory). By a previous proposition\footnote{Corollary on page $169$} we have that, as $n'$ and $m$ are relatively prime, that $y$ has an $m$-th root in $\cyc{y}$. Therefore there exists $x\in\cyc{y}\subseteq G$ such that $y=x^m$

Finally, we want to show that $f(x y)=f(x)f(y)$, for all $x,y\in G$. We have $f(x y)=\left(x y\right)^m$. But, as $G$ is Abelian, it follows that $(x y)^m=x^m y^m$, i.e. $f(x y)=x^m y^m=f(x) f(y)$. Therefore, $f$ is an isomorphism from $G$ to $G$, and by definition an automorphism on $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $S_1$ and $S_2$ be two non-empty sets. We will say that $S_1\subseteq S_2$ if for every $x\in S_1$ there exists $y\in S_2$ such that $x=y$. We will say that $S_1=S_2$ if $S_1\subseteq S_2$ and $S_2\subseteq S_1$, i.e. if for every $x\in S_1$ there exists $y\in S_2$ such that $x=y$ and if for every $x\in S_2$ there exists $y\in S_1$ such that $x=y$. It takes a little observation to see that this is equivalent to saying that $S_1=S_2$ if $x\in S_1$ implies $x\in S_2$ and if $x\in S_2$ implies $x\in S_1$.

\noindent\newline{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. Let $a,b\in G$. Then:

\begin{enumerate}
\item $a\in H a$;
\item $H a=H b$ iff $ab^{-1}\in H$;
\item $H e=H$;
\item $H a=H$ iff $a\in H$;
\item If $a H=H a$ and $b H=H b$, then $(a b)H=H(a b)$;
\item If $a H=H a$, then $a^{-1} H=H a^{-1}$;
\item If $(a b)H=(a c)H$, then $b H=c H$;
\item The number of right cosets of $H$ is equal to the number of left cosets of $H$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} To prove that $a$ is in $H a$, we need to find $h\in H$ such that $a=h a$. But, it must be that $e\in H$, so we can take $h=e$ and have $a=e a$. That is, there exists $e\in H$ such that $a=e a$, and by definition, $a\in H a$. {\it Ad $2$. Necessity.} Suppose $H a=H b$. Then, as $a\in H a$, it must be that $a\in H b$, i.e. there must exist $h\in H$ such that $a=h b$. Multiplying by $b^{-1}\in G$ on the right gives us $a b^{-1}=h$. As $h\in H$, also must be $a b^{-1}\in H$.

{\it Ad $2$. Sufficiency.} Suppose $a b^{-1}\in H$. Then, all elements of $H b$ are of the form $h b$ as $h$ ranges all over $H$. So, $h$ also must equal, in one of the cases, $a b^{-1}\in H$. Therefore, for one of these cases, for $x\in H b$, we have $x=a b^{-1} b=a$. And, as $x\in H b$ and $x=a$ we have $a\in H b$, and by a previous proposition, $H a=H b$. {\it Necessity.} Suppose $H a=H b$. Then, we can take $h a\in H a$ such that $h a=h_1 b$ for some $h_1 b\in H$. Multiplying by $b^{-1}$ on the right gives us $h_1=h a b^{-1}$. Also, if we multiply by $h^{-1}$ on the left we have $h^{-1} h_1=a b^{-1}$. As $H$ is a subgroup, and as $h,h_1\in H$, we have $h^{-1} h_1\in H$, i.e. $a b^{-1}\in H$.

{\it Ad $3$.} If we take $h\in H$, then $h=h e$. And, as $h\in H$ and $e\in G$, we have $h\in H e$. Therefore, $H\subseteq H e$. Conversely, if we take $h\in H e$, then there exists $h'\in H$ such that $h=h' e$, but that means that $h'=h$ and $h\in H$. So, $H e\subseteq H$ and $H=H e$.

{\it Ad $4$.} By taking $b=e$ in $(1)$ and applying $(3)$, the statement is proved by simple substitution in both directions.

{\it Ad $5$.} Take $(a b)h\in (a b)H$. For each $h\in H$ there exists $h_1\in H$ such that $b h=h_1 b$ (as $b H\subseteq H b$). So we have $a b h=a h_1 b$. Also, for every $h\in H$ there exists $h_2\in H$ such that $a h=h_2 a$ (as $a H\subseteq H a$). As our choice of $h$ was arbitrary we could take $h=h_1$ so we have $a h_1=h_2 a$, specifically. From $a b h=a h_1 b$ we have $a b h=h_2 a b$, i.e. for each $h\in H$ there exists $h_2\in H$ such that $(a b)h=h_2(a b)$; in other words, for each $(a b)h\in (a b)H$ there exists $h_2(a b)\in H(a b)$ such that $(a b)h=h_2(a b)$. From that we have $(a b)H\subseteq H(a b)$. Similarly, take $h(a b)\in H(a b)$. For each $h\in H$ there exists $h a\in H a$ and $h_1\in H$ such that $h a=a h_1$. So we have $h a b=a h_1 b$. But, for every $h\in H$ there exists $h b\in H b$ and $h_3\in H$ such that $h b=b h_2$. As our choice of $h$ was arbitrary, again, we can take $h=h_1$ specifically and have $h_1 b=b h_2$. From that we get $h a b=a b h_2$, i.e. for each $h(a b)\in H(a b)$ there exists $(a b)h_2\in H$ such that $h(a b)=(a b)h_2$. In other words, $H(a b)\subseteq (a b)H$. From that and previous relation we have $(a b)H=H(a b)$.

{\it Ad $6$.} For each $h\in H$ there exist $h_1,h_2\in H$ such that $a h=h_1 a$ and $h a=a h_2$. Multiplying first equation by $a^{-1}$ on the left and on the right yields $h a^{-1}=a^{-1} h_1$, and multiplying second equation by $a^{-1}$ on the left and on right gives us $a^{-1} h=h_2 a^{-1}$. Therefore, as for each $h\in H$ there exist $h_1,h_2\in H$ such that $h a^{-1}=a^{-1} h_1$ and $a^{-1} h=h_2 a^{-1}$, we have $a^{-1}H=H a^{-1}$.

{\it Ad $7$.} Let $a,b,c\in G$ such that $(a b)H=(a c)H$. If we take $(a b)h\in(a b)H$, then there exists $h_1\in H$ such that $a b h=a c h_1$. Multiplying by $a^{-1}$ on the left gives us $b h=c h_1$. Similarly, if we take $(a c)h\in H$, there exists $h_2\in H$ such that $a c h=a b h_2$ and multiplying on the left by $a^{-1}$ gives us $c h=b h_2$. Therefore, for each $b h\in b H$ there exists $c h_1\in c H$ such that $b h=c h_1$ (giving $b H\subseteq c H$) and for each $c h\in c H$ there exists $b h_2\in b H$ such that $c h=b h_2$ (giving $c H\subseteq b H$). That implies that $b H=c H$.

{\it Ad $8$.} Let $\mathcal{H}_L=\{x H:\ x\in G\}$ and $\mathcal{H}_R=\{H x:\ x\in G\}$. We define a mapping $f:\mathcal{H}_L\rightarrow\mathcal{R}$ with $f(x H)=H x$. Obviously, $f$ is defined for each $x H$. But, is it uniquely defined? Suppose $x_1 H=x_2 H$. Then, for each $h\in H$ there exists $h_1\in H$ such that $x_1 h=x_2 h_1$. Multiplying by $x_1^{-1}$ on the left and $h_1^{-1}$ on the right gives us $h h_1^{-1}=x_1^{-1} x_2$. That is equivalent to $(h_1^{-1} h)^{-1}=(x_1 x_2^{-1})^{-1}$. Multiplying by $x_1 x_2^{-1}$ on the right and by $h_1 h$ on the left gives us $x_1 x_2^{-1}=h_1^{-1} h$. As $h_1^{-1} h\in H$ (as $H$ is a subgroup of $G$), we have that $x_1 x_2^{-1}\in H$, and by a previous problem $H x_1=H x_2$, which is $f(x_1 H)=f(x_2 H)$. Therefore, $f$ is a function. Now we will prove that it is a bijection. {\it Injectivity.} Suppose $f(x_1 H)=f(x_2 H)$, i.e. $H x_1=H x_2$. By a previous problem we have $x_1 x_2^{-1}\in H$. That means that there exists $h\in H$ such that $h=x_1 x_2^{-1}$. Multiplying by $h^{-1}$ on the left and $(x_1 x_2^{-1})^{-1}$ on the right gives us $\left(x_1 x_2^{-1}\right)^{-1}=h^{-1}$, i.e. $x_1^{-1} x_2=h^{-1}$. That means that $x_1^{-1} x_2\in H$. That means that for some $x_1 h_1\in x_1 H$ we have $x_1 h_1=x_1 x_1^{-1} x_2$, that is $x_1 h=x_2$, which implies that $x_2\in x_1 H$ and $x_1 H=x_2 H$. {\it Surjectivity.} Trivial. If we take $H x\in\mathcal{H}_R$, there exists $x H\in\mathcal{H}_L$ such that $f(x H)=H x$. Therefore, $f$ is bijective and $\left|\mathcal{H}_L\right|=\left|\mathcal{H}_R\right|$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From considerations from above we also have some interesting properties. E.g. if $H$ is a subgroup of $G$ and $a,b\in G$, then $H a=H b$ if and only if $a H=b H$.

\noindent\newline{\bf Proposition.} Let $G$ be a group and $J$, $K$ and $H$ subgroups of $G$ such that $J=K\cap H$. Then for any $a\in G$, $J a=H a\cap K a$.

\noindent\newline {\bf Proof.} Let $a\in G$. If we take $j a\in J a$, we also have $j\in J=K\cap H$. That means that $j\in K$ and $j\in H$. If we observer $H a$ and $K a$, then as $a$ ranges over all elements of $H$ and $K$ it will also be that $j a\in H a$ and $j a\in K a$, i.e. $j a\in H a\cap K a$. Thus we have $J a\subseteq H a\cap K a$. Now, if we take $x a\in H a\cap K a$, then $x a\in H a$, i.e. $x\in H$, and $x a\in K a$, that is $x\in K$. That means that $x\in H\cap K=J$ and $x\in J$. As $x\in J$ it will also be that $x a\in J a$ which implies $H a\cap K a\subseteq J a$. That implies that $J a=H a\cap K a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From the previous proposition follows that if $H$ and $K$ are of finite index in $G$, then their intersection $H\cap K$ is also of finite index in $G$.

\noindent\newline{\bf Definition.} If $a\in G$, a {\bf conjugate} of $a$ is any element of the form $x a x^{-1}$, where $x\in G$.

\noindent\newline{\bf Proposition.} Let $G$ be a group. Relation $\sim:G^2\rightarrow\{\top,\bot\}$ defined for all $a,b\in G$ as $a\sim b$ if and only if $a=x b x^{-1}$, for some $x\in G$ is an equivalence relation on $G$.

\noindent\newline{\bf Proof.} {\it Reflexivity.} $a\sim a$ holds as $a=e a e^{-1}$. {\it Simmetry.} $a\sim b$ implies that there exists $x\in G$ such that $a=x b x^{-1}$. Multiplying by $x$ on the right and $x^{-1}$ on the left yields $x^{-1} a x=b$, so $b\sim a$ (note that if we took $y=x^{-1}$ we would have $y a y^{-1}=b$, the possible confusion arises only in perceived ambiguity of an element and its inverse). {\it Transitivity.} $a\sim b$ and $b\sim c$ implies $a=x b x^{-1}$ and $b=y c y^{-1}$, for some $x,y\in G$. Substituting $y c y^{-1}$ for $b$ in $x b x^{-1}$ gives us $a=x y c y^{-1} x^{-1}$. But, as $(x y)^{-1}=y^{-1} x^{-1}$, we have $a=(x y)c(x y)^{-1}$. Of course, $G$ is a group, so $x y\in G$, and their inverse also.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $a\in G$ and $\sim$ a relation on $G$ such that $a\sim b$ if and only if $a=x b x^{-1}$ for some $x\in G$. {\bf Conjugacy class} of $a$ is $[a]_c=\{x a x^{-1}:\ x\in G\}$.

\noindent\newline{\bf Definition.} For any element $a\in G$, the {\bf centralizer} of $a$, denoted by $C_a$, is the set of all the elements in $G$ which commute with $a$. That is,

\begin{equation*}
C_a=\{x\in G:\ x a=a x\}=\{x\in G:\ x a x^{-1}=a\}.
\end{equation*}

\noindent\newline{\bf Proposition.} For any $a\in G$, $C_a$ is a subgroup\footnote{In chapter on subgroups we have proved that the center of $G$ is a subgroup of $G$. But note the difference of {\it center} and {\it centralizer}. Center of a group is the set of all elements which commute with all other elements in $G$ while the centralizer needs a fixed element $a\in G$ to be defined and then it is a set of all elements which commute with $a$.} of $G$.

\noindent\newline{\bf Proof.} By definition, $C_a\subseteq G$. Take $a\in G$. Then, if we take $x,y\in C_a$, we have $a=x a x^{-1}$ and $a=y a y^{-1}$. Substituting the former into the latter expression we have $a=x y a y^{-1} x^{-1}$ and that is, by the same reasoning as in the previous proposition, equivalent to $a=(x y)a(x y)^{-1}$, i.e. $x y\in C_a$. Then, if we take $x\in C_a$ we have $a=x a x^{-1}$. Multiplying that by $x$ on the right and $x^{-1}\in G$ on the left we have $x^{-1} a x=a$, so $x^{-1}\in C_a$ and $C_a$ is a subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group, $a\in G$ and $C_a$ centralizer of $a$. Then the following statements are equivalent:

\begin{enumerate}
\item $x^{-1} a x=y^{-1} a y$;
\item $x y^{-1} a=a x y^{-1}$;
\item $x y^{-1}\in C_a$;
\item $C_a x=C_a y$.
\end{enumerate}

\noindent{\bf Proof.} From $x^{-1} a x=y^{-1} a y$ we get an equivalent expression (when multiplying by $x$ on the left and $y^{-1}$ on the right) $x y^{-1} a=a x y^{-1}$ and that implies, as $a$ commutes with $x y^{-1}$, that $x y^{-1}\in C_a$. Conversely, $x y^{-1}\in C_a$ implies that $a$ commutes with $x y^{-1}$ and we have $x y^{-1} a=a x y^{-1}$. Now, assume $x y^{-1}\in C_a$. Then, it must be that $x y^{-1} y\in C_a y$, i.e. $x\in C_a y$. From a previous theorem it follows that $C_a x=C_a y$. Conversely, if $C_a x=C_a y$, then it must be that, $x\in C_a x$ (as $e\in C_a$ and $x=e x$). But, also $x\in C_a y$ (as $C_a x=C_a y$). As $x\in C_a y$ it is of the form $x=c y$, where $c\in C_a$, i.e. $c$ commutes with $a$. That means that $c a=a c$. As we have $x=c y$, by multiplying with $y^{-1}$ on the right we have $x y^{-1}=c$. Then, as $c a=a c$, it follows that $x y^{-1} a=a x y^{-1}$. From this point it implies all the equivalences it is tied to.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group and $a\in G$. There is a one-to-one correspondence between the set of all the conjugates of $a\in G$ and the set of all the cosets of $C_a$ in $G$.

\noindent\newline{\bf Proof.} Let $[a]_c$ be the set of all the conjugates of $a\in G$ (i.e. induced by forementioned equivalence relation) and $\mathcal{C}_a$ family of sets containing all the cosets of $C_a$, that is $\mathcal{C}_a=\{C_a x:\ x\in G\}$. Let $f:[a]_c\rightarrow\mathcal{C}_a$ be a mapping defined with $f(x a x^{-1})=C_a x$. It is obvious that this is a function as it is defined for all $x$, and it has the property of uniqueness, as $x a x^{-1}=y a y^{-1}$ would imply, by a previous proposition, that $C_a x^{-1}=C_a y^{-1}$, i.e. $f(x a x^{-1})=f(y a y^{-1})$. Now, we will prove that it is a bijection. {\it Injectivity.} Suppose $f(x a x^{-1})=f(y a y^{-1})$. That means that $C_a x^{-1}=C_a y^{-1}$ and by a previous proposition that $x a x^{-1}=y a y^{-1}$. {\it Surjectivity.} Let $C_a x\in\mathcal{C}_a$. Obviously, as $x\in G$, and $[a]_c$ is a class, it must contain $x a x^{-1}$. Therefore, there exists $x a x^{-1}\in[a]_c$ such that $f(x a x^{-1})=C_a x$. Thus, $f$ is a bijection.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group. Then, $\left|\left[a\right]_c\right|=\left[G:C_a\right]$.

\noindent\newline{\bf Proof.} From a previous lemma we have a bijection from $[a]_c$ to $\mathcal{C}_a$. Therefore, $\left|\left[a\right]_c\right|=\left|\mathcal{C}_a\right|$. But $\left|\mathcal{C}_a\right|$ is the number of all the different cosets of $C_a$ in $G$, i.e. index of $C_a$ in $G$. So, by definition, $\left|\mathcal{C}_a\right|=\left[G:C_a\right]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ be a group. The size of every conjugacy class is a factor of $|G|$.

\noindent\newline{\bf Proof.}. Let $a\in G$. By definition of index of $C_a$ (centralizer of $a$) in $G$:

\begin{equation*}
\left[G:C_a\right]=\frac{\left|G\right|}{\left|C_a\right|}.
\end{equation*}

\noindent\newline Multiplying by $\left|C_a\right|$ gives us $\left|G\right|=\left|C_a\right|\cdot\left[G:C_a\right]$. But, by a previous lemma we have $\left|\left[a\right]_c\right|=\left[G:C_a\right]$, so $\left|G\right|=\left|C_a\right|\cdot\left|\left[a\right]_c\right|$, i.e. $\left|\left[a\right]_c\right|$ divides $\left|G\right|$. Our choice of $a$ was arbitrary so the statement of the theorem is proved.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a set, and $G$ a subgroup of $S_A$ (group of all the permutations of $A$). We say that $G$ is a {\bf group acting on the set} $A$.

\noindent\newline{\bf Definition.} Let $G$ be a finite group acting on a set $A$. If $a\in A$, then the {\bf orbit} of $a$, with respect to $G$, is the set\footnote{Recall that permutation is a bijection with equal domain and codomain, so if $g\in G\subseteq S_a$, then $g:A\rightarrow A$ and $g(x)\in A$ for all $x\in A$.}:

\begin{equation*}
O(a)=\left\{g(a)\in A:\ g\in G\right\}.
\end{equation*}

\noindent\newline{\bf Definition.} Let $G$ be a group acting on a set $A$. The {\bf stabilizer} of $a\in A$, with respect to $G$, is the set $G_a=\{g\in G:\ g(a)=a\}$.

\noindent\newline{\bf Proposition.} Let $G$ be a group acting on a set $A$. $\sim$ be a relation on $A$ defined as $a\sim b$ iff $g(a)=b$, for some $g\in G$. Then, $\sim$ is an equivalence relation on $A$, and orbits are its equivalence classes.

\noindent\newline{\bf Proof.} {\it Reflexivity.} Note that $e\in G$ and $e(a)=a$, for all $a\in G$. Therefore, we have $a\sim a$. {\it Symmetry.} From $a\sim b$ we have $g(a)=b$, for some $g\in G$. But, as $g:A\rightarrow A$ is a bijection, it has an inverse $g^{-1}\in G$. So we have $[g^{-1}\circ g](a)=g^{-1}(b)$. From definition of inverse we have $[g^{-1}\circ g](a)=a$, for all $a\in G$. So, we have $g^{-1}(b)=a$ and $b\sim a$. {\it Transitivity.} From $a\sim b$ and $b\sim c$ we have $g(a)=b$ and $f(b)=c$ for some $f,g\in G$. From that we have $f(g(a))=c$, i.e. $[f\circ g](a)=c$. As $G$ is a group, and $f,g\in G$, so is $[f\circ g]\in G$ and we have $a\sim c$. Let us observe equivalence classes. We have $[a]=\{b\in A:\ a\sim b\}=\{b\in A:\ (\exists g\in G)(g(a)=b)\}=\{g(a)\in A:\ g\in G\}=O(a)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group acting on a set $A$ and $G_a$ stabilizer of $a\in A$. Then, $G_a$ is a subgroup of $G$.

\noindent\newline{\bf Proof.} Obviously $G_a\subseteq G$. If we take $f,g\in G_a$ then $f(a)=a$ and $g(a)=a$. Taking their composition gives us $f(g(a))=a$, i.e. $[f\circ g](a)=a$. So it must be that $[f\circ g]\in G_a$. If we take $g\in G$, then $g(a)=a$. As $g$ is a bijection it has an inverse $g^{-1}\in G$ and it must be that $[g^{-1}\circ g](a)=g^{-1}(a)$, that is $g^{-1}(a)=a$. So it must be that $g^{-1}\in G_a$. Thus, $G_a$ is a subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group acting on a set $A$, $a\in A$ and $f,g\in G$. Then, $f,g\in x G_a$, for some $x\in G$, if and only if $f(a)=g(a)$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $f,g\in x G_a$. That means that $f=[x\circ f']$ and $g=[x\circ g']$, for some $f',g'\in G_a$. As $x\in G$ it has an inverse $x^{-1}\in G$, so we have $[x^{-1}\circ f]=f'$ and $[x^{-1}\circ g]=g'$. And, as $f'(a)=a$ and $g'(a)=a$ we have $a=f'(a)=[x^{-1}\circ f](a)$ and $a=g'(a)=[x^{-1}\circ g](a)$. That implies that $x(a)=f(a)$ and $x(a)=g(a)$. Finally, from that follows $f(a)=g(a)$. {\it Sufficiency.} Obviously, $f\in f G_a$. Suppose $f(a)=g(a)$ for some $f,g\in G$. Applying $f^{-1}\in G$ gives us $a=[f^{-1}\circ g](a)$ and by definition $[f^{-1}\circ g]\in G_a$. So, it must be that $[f\circ[f^{-1}\circ g]]\in f G_a$. As function composition is associative, we have $g\in f G_a$, i.e. there exists $x\in G$ (here $x=f$) such that $f,g\in x G_a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group acting on a set $A$ and $a\in A$. Then, $\left|O(a)\right|=\left[G:G_a\right]$.

\noindent\newline{\bf Proof.} Let $\mathcal{G}_a=\{x G_a:\ x\in G\}$ be a family of left cosets of $G$. We know that the number of left and right cosets of any group is equal, so this particular choice of left cosets is of no importance. Let $f:O(a)\rightarrow\mathcal{G}_a$ be a mapping with $f(x(a))=x G_a$. Mapping is obviously defined for all $x(a)\in O(a)$, where $x\in G$. Assume $x(a)=y(a)$. Then, there exists $b\in A$ such that $x(a)=b$ and $y(a)=b$. Obviously $x\in x G_a$. Now, as we have $a=x^{-1}(b)$, and $y(a)=b$, we have $a=x^{-1}(y(a))$, i.e. $[x^{-1}\circ y](a)=a$, meaning $x^{-1} y\in G_a$. By a previous proposition, $x G_a=y G_a$. {\it Injectivity.} Suppose $x G_a=y G_a$. Then, by a previous proposition, $x^{-1} y\in G_a$, i.e. $[x^{-1}\circ y](a)=a$. From that we have $y(a)=x(a)$. {\it Surjectivity.} Suppose $y G_a\in\mathcal{G}$. As $y\in G$, we have $y(a)=b$, for some $b\in A$. Thus, $y(a)\in O(a)$ and we can have $f(y(a))=y G_a$. Thus, $f$ is bijective and $\left|O(a)\right|=\left|\mathcal{G}_a\right|$ where $\mathcal{G}_a=\left[G:G_a\right]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Size of every orbit with respect to $G$ is a factor of the order of $G$.

\noindent\newline{\bf Proof.} Let $a\in A$, $O(a)$ orbit of $a$ with respect to $G$ and $G_a$ stabilizer of $a$. As $G_a$ is a subgroup of $G$, its index in $G$ is $[G:G_a]=\frac{|G|}{|G_a|}$. From that we have $[G:G_a]\cdot |G_a|=|G|$. As $\left|O(a)\right|=\left[G:G_a\right]$ by a previous proposition, we have $\left|O(a)\right|\cdot|G_a|=|G|$, which implies that the size of $O(a)$ is a factor of the order of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} If $f\in S_A$, then the length of each cycle of $f$ is a factor of the order of $f\in S_A$.

\noindent\newline{\bf Proof.} Let $f\in S_A$ be a permutation decomposed in disjoint cycles $f_1,\ldots,f_n$ such that $f=f_1 f_2\cdots f_n$ and let each have length $l(i)$, for $i\in\{1,\ldots,n\}$. Also, let $m$ be the order of $f\in S_A$. Let us observe $\cyc{f}$ and some cycle $f_i$ which does not leave $a\in A$ fixed. Then, $O(a)=\{a,f_i(a),f^2_i(a),\ldots,f^{l(i)-1}_i(a)\}$. There are no other elements in $O(a)$ as $f$ is decomposed in disjoint cycles, all other $f_j$, $i\neq j$, leave $a$ fixed and copied into $a$. Therefore, the size of the orbit of $a$, which is not left fixed by $f_i$, is equal to the length of the cycle $f_i$. By a previous proposition it follows that size of the orbit of $a$ is a factor of the order of $\cyc{f}$, i.e. length of cycle $f_i$ is a factor of the order of $f\in S_A$ (as the order of $\cyc{f}$ is equal to order of $f$, by definition).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Lagrange's theorem can be proved using the orbit-stabilizer theorem, using only group actions and the fact that $H g=H$ if and only if $g\in H$.

\noindent\newline{\bf Lemma.} Let $G$ be a group, $H\leq G$ and $a,g\in G$. Then, $H(g a)=H a$ if and only if $H g=H$.

\noindent\newline{\bf Proof.} {\it Necessity.} If we take $x\in H g$, then there exists $h_1\in H$ such that $x=h_1 g$. But, as $h_1\in H$ and $H(g a)=H a$, there exists $h_2\in H$ such that $h_1 g a=h_2 a$. That implies, after multiplying by $a^{-1}$ on the right, that $h_1 g=h_2$ and $x=h_1 g=h_2$. Thus, $H g\subseteq H$. If we take $x\in H$, then there exists $h_1\in H$ such that $x=h_1$. But, then there exists $h_2\in H$ such that $h_1 a=h_2 g a$. That implies, again, $h_1=h_2 g$. So, $x=h_1=h_2 g$ and $x\in H g$. From that we get $H\subseteq H g$. Combining that with former result, we have $H=H g$.

{\it Sufficiency.} If $H g=H$, then for all $h\in H$ there exists $h'\in H$ such that $h g=h '$. But, multiplying that by $a$ on the right gives us that for all $h\in H$ there exists $h'\in H$ such that $h g a=h' a$. So, $H g\subseteq H$. Also, for all $h\in H$, there exists $h'\in H$ such that $h'g=h$, i.e. $h'g a=h a$. So, as for all $h\in H$ there exists $h'\in H$ such that $h'g a=h a$, then $H(g a)\subseteq H a$. Thus, $H(g a)=H a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Lagrange).} If $G$ is a finite group and $H\leq G$, then $|H|$ divides $|G|$.

\noindent\newline{\bf Proof.} Let $G\slash H$ be the set of all cosets of $H$ in $G$. Let $G$ act on $G\slash H$ so that $g.H a=H(g a)$. Then, $g\in\Stab{H a}{G}$ if and only if $g.H a=H a$. So, it must be $H(g a)=H a$. Previous lemma tells us that that is equivalent to $H g=H$. That is, furthermore equivalent to $g\in H$. Therefore, $g\in\Stab{H a}{G}$ if and only if $g\in H$. That is equivalent to $\Stab{H a}{G}=H$. Now, observe that $\Orb{H a}{G}=\{H b\in G:\ (\exists g\in G)(g.H a=H b)\}$. That implies $\Orb{H a}{G}\subseteq G\slash H$. But, if we take $H b\in G\slash H$, then we want to find $g\in G$ such that $g.H a=H b$, i.e. $H(g a)=H b$. Therefore, we want $g a=b$. After multiplying by $a^{-1}$ on the right, we get $g=b a^{-1}$. So, $(b a^{-1}).H a=H((b a^{-1})a)=H(b(a^{-1} a))=H(b e)=H b$. Therefore, $H b\in\Orb{H a}{G}$ and $G\slash H\subseteq\Orb{H a}{G}$. That, and the previous result, gives us $\Orb{H a}{G}=G\slash H$. Using the orbit stabilizer theorem, we get $|G|=|\Orb{H a}{G}|\cdot|\Stab{H a}{G}|$, which is equivalent to $|G|=|G\slash H|\cdot |H|$. From that we have that $|H|$ divides $|G|$. Similarly, as $|G\slash H|=[G:H]$ we obtain $|G|=[G:H]\cdot|H|$, i.e. $[G:H]=\frac{|G|}{|H|}$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Homomorphisms}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ and $H$ be groups. Function $f:G\rightarrow H$ is called a {\bf homomorphism}\footnote{Notice that the only difference from isomorphism is that we do not require $f$ to be a bijection. Every isomorphism is therefore homomorphism.} from $G$ to $H$ if $f(a b)=f(a)f(b)$, for all $a,b\in G$. Then, $H$ is called a {\bf homomorphic image} of $G$.

\noindent\newline{\bf Theorem.} Let $G$ and $H$ be groups, and $f$ a homomorphism from $G$ to $H$. Then:

\begin{enumerate}
\item $f(e)=e$, where $e$ is a neutral element in $H$ (on the right-hand side) and a neutral element in $G$ (on the left-hand side);
\item $f\left(a^{-1}\right)=\left[f(a)\right]^{-1}$, for all $a\in G$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} We have that $f(a b)=f(a)f(b)$, for all $a,b\in G$. If we take $f(e e)=f(e)f(e)$, we have $f(e)=f(e)f(e)$. As $f(e)\in H$, and $H$ is a group, there exists $\left[f(e)\right]^{-1}\in H$ such that $[f(e)]^{-1}f(e)=e'$, where $e'$ is a neutral element in $H$. So, we multiply equation by $[f(e)]^{-1}$ on the left and we have $e'=f(e)$. But, due to this property, it will be unambiguous if we use $e$ to represent neutral element in $G$ and in $H$. Therefore, $f(e)=e$.

{\it Ad $2$.} As $a a^{-1}=e$, for all $a\in G$, we have $f(a a^{-1})=f(a)f(a^{-1})$, i.e. $f(e)=f(a)f(a^{-1})$. From previous property we have $f(e)=e$, so we have $e=f(a)f(a^{-1})$. As $f(a)\in H$, and $H$ is a group, we can multiply the former equation by $[f(a)]^{-1}\in H$ on the left to get $[f(a)]^{-1}=[f(a)]^{-1}f(a)f(a^{-1})$, i.e. $[f(a)]^{-1}=f(a^{-1})$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Definition.} Let $f:G\rightarrow H$ be a homomorphism from group $G$ to group $H$. Then, the {\bf kernel} of $f$ is the set:

\begin{equation*}
\ker{f}=\{a\in G:\ f(a)=e\}.
\end{equation*}

\noindent\newline The {\bf range} of $f$ is the set:

\begin{equation*}
\ran{f}=\{f(a)\in H:\ a\in G\}.
\end{equation*}

\noindent\newline{\bf Definition.} Let $G$ be a group and $H$ a subgroup of $G$. If $x y x^{-1}\in H$, for all $y\in H$ and $x\in G$, then $H$ is called a {\bf normal subgroup} of $G$.

\noindent\newline{\bf Remark.} Notice that a normal subroup of $G$ is any nonempty subset of $G$, with operation inherited from $G$, which is closed with respect to multiplication, inverses and conjugates.

\noindent\newline{\bf Theorem.} Let $f:G\rightarrow H$ be a homomorphism. Then:

\begin{enumerate}
\item $\ker{f}$ is a normal subgroup of $G$;
\item $\ran{f}$ is a subgroup of $H$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Obviously $\ker{f}\subseteq G$, by definition. If we take $a,b\in\ker{f}$ we have $f(a)=e$ and $f(b)=e$. Multiplying first equation by $f(b)$ gives us $f(a)f(b)=e f(b)$. But, as $f(b)=e$, we have $f(a)f(b)=e e$, i.e. $f(a)f(b)=e$. As $f$ is a homomorphism, we have $f(a)f(b)=f(a b)$, which, from $f(a)f(b)=e$, implies $f(a b)=e$. Therefore, it must be that $a b\in\ker{f}$ and $\ker{f}$ is closed with respect to products. Now, take $a\in\ker{f}$. We have $f(a)=e$. If we multiply equation by $[f(a)]^{-1}$, we have $f(a)[f(a)]^{-1}=[f(a)]^{-1}$, which further yields $[f(a)]^{-1}=e$. But, by a previous theorem, $[f(a)]^{-1}=f(a^{-1})$, so we have $f(a^{-1})=e$ and it must be that $a^{-1}\in\ker{f}$. So, $\ker{f}$ is closed with respect to inverses. Finally, if we take $x\in G$ and $y\in\ker{f}$ we need to show that $x y x^{-1}\in\ker{f}$. As $f$ is a homomorphism, we have $f(x y x^{-1})=f(x)f(y x^{-1})=f(x)f(y)f(x^{-1})$. But, as $f(y)=e$ and $f(x^{-1})=[f(x)]^{-1}$ by a previous theorem, we have $f(x y x^{-1})=f(x) e [f(x)]^{-1}=f(x)[f(x)]^{-1}=e$. As $f(x y x^{-1})=e$, it must be that $x y x^{-1}\in\ker{f}$. To conclude, $\ker{f}$ is a normal subgroup of $G$.

{\it Ad $2$.} We have, by definition, $\ran{f}\subseteq H$. If we take $f(a),f(b)\in\ran{f}$, as $f$ is a homomorphism we have $f(a)f(b)=f(a b)$, so $f(a b)\in\ran{f}$ and by that $f(a)f(b)\in\ran{f}$. For $f(a)\in\ran{f}$ we have $[f(a)]^{-1}\in H$ such that $f(a)[f(a)]^{-1}=[f(a)]^{-1}f(a)=e$. But, by a previous theorem $[f(a)]^{-1}=f(a^{-1})$, so $f(a^{-1})\in\ran{f}$, and by that $[f(a)]^{-1}\in\ran{f}$. Thus, $\ran{f}$ is a subgroup of $H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Verify that $f:\Z_8\rightarrow\Z_4$, given by
\begin{equation*}
f=\left(\begin{array}{cccccccc}
0 & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
0 & 1 & 2 & 3 & 0 & 1 & 2 & 3\\
\end{array}\right)
\end{equation*}

\noindent\newline is a homomorphism. Find $\ker{f}$ and all the cosets of $\ker{f}$.

\noindent\newline{\bf Solution.} First we will write the table of $\Z_8$:

\begin{center}
\begin{tabular}{c|cccccccc}
$+_8$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ \\
\hline
$0$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$\\
$1$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $0$\\
$2$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $0$ & $1$\\
$3$ & $3$ & $4$ & $5$ & $6$ & $7$ & $0$ & $1$ & $2$\\
$4$ & $4$ & $5$ & $6$ & $7$ & $0$ & $1$ & $2$ & $3$\\
$5$ & $5$ & $6$ & $7$ & $0$ & $1$ & $2$ & $3$ & $4$\\
$6$ & $6$ & $7$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$\\
$7$ & $7$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\
\end{tabular}
\end{center}

\noindent\newline Now, as we transform the table using the definition of $f$ we have:

\begin{center}
\begin{tabular}{c|cccccccc}
$+$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$ \\
\hline
$0$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$\\
$1$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$\\
$2$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$\\
$3$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$\\
$0$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$\\
$1$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$\\
$2$ & $2$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$\\
$3$ & $3$ & $0$ & $1$ & $2$ & $3$ & $0$ & $1$ & $2$\\
\end{tabular}
\end{center}

\noindent\newline From the table above we can see that last four rows are identical to first four rows; the same thing with columns. Therefore, we can eliminate duplicate information to get:

\begin{center}
\begin{tabular}{c|cccc}
$+_4$ & $0$ & $1$ & $2$ & $3$\\
\hline
$0$ & $0$ & $1$ & $2$ & $3$\\
$1$ & $1$ & $2$ & $3$ & $0$\\
$2$ & $2$ & $3$ & $0$ & $1$\\
$3$ & $3$ & $0$ & $1$ & $2$\\
\end{tabular}
\end{center}

\noindent\newline The table above is exactly the table of $\Z_4$. Therefore, $f$ is a homomorphism from $\Z_8$ to $\Z_4$. Let us find its kernel. Kernel is of course the set containing all elements which are sent to a neutral element, in this case, zero. Thus, $\ker{f}=\{a\in\Z_8:\ f(a)=0\}=\{0,4\}$. It is easy to notice that it is a subgroup of $\Z_8$ with the following table:

\begin{center}
\begin{tabular}{c|cc}
$+_8$ & $0$ & $4$\\
\hline
$0$ & $0$ & $4$\\
$4$ & $4$ & $0$\\
\end{tabular}
\end{center}

\noindent\newline The cosets of $\ker{f}$ are (we will use notation for addition):

\begin{eqnarray*}
\ker{f}+0&=&\{0,4\}=\ker{f}+4=\ker{f},\\
\ker{f}+1&=&\{1,5\}=\ker{f}+5,\\
\ker{f}+2&=&\{2,6\}=\ker{f}+6,\\
\ker{f}+3&=&\{3,7\}=\ker{f}+7.
\end{eqnarray*}

\noindent\newline{\bf Problem.} Prove that $f:\Z\rightarrow\{E,O\}$, given by $f(x)=E$ if $x$ is even, and $f(x)=O$ if $x$ is odd, is a homomorphism. Find $\ker{f}$ and all the cosets of $\ker{f}$. The table of $\{E,O\}$ is:

\begin{center}
\begin{tabular}{c|cc}
$+$ & $E$ & $O$\\
\hline
$E$ & $E$ & $O$\\
$O$ & $O$ & $E$\\
\end{tabular}
\end{center}

\noindent\newline{\bf Solution.} We need only to check if $f(a+b)=f(a)+f(b)$. Suppose $a$ and $b$ are odd. Then, their sum is even. If $a$ is odd and $b$ is even, their sum is odd. Same thing if $a$ is even and $b$ is odd. If $a$ and $b$ are even, their sum is even. We can see that this really corresponds to the table above and we have $f(a+b)=f(a)+f(b)$. Therefore, $f$ is a homomorphism from $\Z$ to $\{E,O\}$. Neutral element in $\{E,O\}$ is evidently $E$. Therefore, $\ker{f}=\{n\in\Z:\ f(n)=E\}=\{2n:\ n\in\Z\}$. There are two cosets of $\ker{f}$:

\begin{eqnarray*}
\ker{f}+0&=&\ker{f}+2=\ldots=\ker{f}+2k=\ker{f},\\
\ker{f}+1&=&\ker{f}+3=\ldots=\ker{f}+2(k+1).
\end{eqnarray*}

\noindent\newline{\bf Problem.} Let $G$ be the multiplicative group of all $2\times 2$ matrices $A$ satisfying $\det{A}\neq 0$. Prove that $f:G\rightarrow\R^{ast}$ with $f(A)=\det{A}$ is a homomorphism and describe its kernel.

\noindent\newline{\bf Solution.} Determinant is defined for all $2\times 2$ matrices and returns a unique value. We only need to check if $f(A B)=f(A)f(B)$. We have $f(A B)=\det{A B}$. But, due to Binet-Cauchy theorem, $\det{A B}=\det{A}\det{B}$. Thus, $f(A B)=\det{A B}=\det{A}\det{B}=f(A)f(B)$ and $f$ is a homomorphism from $G$ to $\R^{\ast}$. Neutral element in $\R^{\ast}$ is $1$. Therefore, $\ker{f}=\{A\in G:\ \det{A}=1\}$, i.e. all orthogonal matrices.

\noindent\newline{\bf Problem.} Prove that each of the following\footnote{Keep in mind that $\mathcal{F}(\R):=\left(\mathcal{F}(\R),+\right)$, group of all real functions with real variables under addition. Similarly, $\mathcal{C}(\R):=\left(\mathcal{C}(\R),+\right)$, containing continuous functions and $\mathcal{D}(\R):=\left(\mathcal{D}(\R),+\right)$ containing differentiable functions.} is a homomorphism and describe its kernel:

\begin{enumerate}
\item $\phi:\mathcal{F}(\R)\rightarrow\R$ with $\phi(f)=f(0)$;
\item $\phi:\mathcal{D}(\R)\rightarrow\mathcal{F}(\R)$ with $\phi(f)=f'$;
\item $f:\R\times\R\rightarrow\R$ with $f(x,y)=x+y$;
\item $f:\R^{\ast}\rightarrow\R^{+}$ with $f(x)=|x|$;
\item $f:\C^{\ast}\rightarrow\R^{+}$ with $f(a+b i)=\sqrt{a^2+b^2}$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $\phi:\mathcal{F}(\R)\rightarrow\R$ with $\phi(f)=f(0)$.} All functions in $\mathcal{F}(\R)$ are of the form $f:\R\rightarrow\R$, therefore they are defined in $f(0)$ and then so is $\phi(f)$. Now we have $\phi(f+g)=[f+g](0)=f(0)+g(0)=\phi(f)+\phi(g)$ and $\phi$ is a homomorphism from $\mathcal{F}(\R)$ to $\R$. Remember that $0$ is the neutral element in $\R$. It's kernel is $\ker{\phi}=\{f\in\mathcal{F}(\R):\ f(0)=0\}$, that is, the set containing all functions passing through the origin $O(0,0)$.
\item {\it $\phi:\mathcal{D}(\R)\rightarrow\mathcal{F}(\R)$ with $\phi(f)=f'$.} Set $\mathcal{D}(\R)$ contains all differentiable functions, therefore $\phi$ is defined for all $f\in\mathcal{F}(\R)$. Derivation is unique. Now, we have $\phi(f+g)=[f+g]'=f'+g'=\phi(f)+\phi(g)$. It's kernel is $\ker{\phi}=\{f\in\mathcal{D}(\R):\ f'=0\}=\{f\in\mathcal{D}(\R):\ f=c,\ c\in\R\}$, that is the set of all constant functions.
\item {\it $f:\R\times\R\rightarrow\R$ with $f(x,y)=x+y$.} Obviously $f$ is a function as it is defined for all $(x,y)\in\R^2$ and returns a unique value. We need only $f((a,b)+(c,d))=f(a+c,b+d)=a+c+b+d=a+b+c+d=f(a,b)+f(c,d)$, i.e. $f$ is a homomorphism. Its kernel is $\ker{f}=\{(x,y)\in\R^2:\ x=-y\}$.
\item {\it $f:\R^{\ast}\rightarrow\R^{+}$ with $f(x)=|x|$.} Again, $f$ is a function. We have $f(a b)=|a b|=|a|\cdot|b|=f(a)f(b)$. Thus, $f$ is a homomorphism. Now, $\ker{f}=\{x\in\R^{\ast}:\ |x|=1\}=\{-1,1\}$.
\item {\it $f:\C^{\ast}\rightarrow\R^{+}$ with $f(a+b i)=\sqrt{a^2+b^2}$.} As $f$ is a function we have:

\begin{eqnarray*}
f((a+b i)(c+d i))&=&f((a c-b d)+i(b c+a d))=\sqrt{(a c-b d)^2+(b c+a d)^2}\\
&=&\sqrt{a^2 c^2-2a c b d+b^2 d^2+b^2 c^2+2b c a d+a^2 d^2}\\
&=&\sqrt{a^2 c^2+b^2 d^2+b^2 c^2+a^2 d^2}\\
&=&\sqrt{a^2(c^2+d^2)+b^2(c^2+d^2)}=\sqrt{(c^2+d^2)(a^2+b^2)}\\
&=&\sqrt{a^2+b^2}\sqrt{c^2+d^2}=f(a+b i)f(c+d i).
\end{eqnarray*}

\noindent\newline Therefore, $f$ is a homomorphism. It's kernel is $\ker{f}=\{z\in\C^{\ast}:\ |z|=1\}$, which is a unit circle.

\end{enumerate}

\noindent{\bf Proposition.} Let $G$, $H$ and $K$ be groups. If $f:G\rightarrow H$ and $g:H\rightarrow K$ are homomorphisms, then $g\circ f:G\rightarrow K$ is a homomorphism.

\noindent\newline{\bf Proof.} We have already proved that $g\circ f$, defined as above, is a function. Further we have $f(a b)=f(a)f(b)$, for all $a,b\in G$. As $f(a),f(b)\in H$, and $g:H\rightarrow K$, we have $g(f(a b))=g(f(a)f(b))=g(f(a))g(f(b))$. That is, for all $a,b\in G$, we have $[g\circ f](a b)=[g\circ f](a)[g\circ f](b)$, for all $a,b\in G$. Thus, $g\circ f:G\rightarrow K$ is a homomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ and $H$ be groups. Homomorphism $f:G\rightarrow H$ is injective if and only if $\ker{f}=\{e\}$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $f:G\rightarrow H$ be an injective homomorphism. Then, for all $x_1,x_2\in G$, if $f(x_1)=f(x_2)$, then $x_1=x_2$. Let us observe the set $\ker{f}=\{x\in G:\ f(x)=e\}$. From a previous proposition we have $f(e)=e$, so we can write $\ker{f}=\{x\in G:\ f(x)=f(e)\}$. But, by using forementioned property of injectivity, from $f(x)=f(e)$ it follows $x=e$. Thus, we have $\ker{f}=\{x\in G:\ x=e\}=\{e\}$.

{\it Sufficiency.} Suppose $f:G\rightarrow H$ is a homomorphism and $\ker{f}=\{e\}$. Suppose $f(x_1)=f(x_2)$ for some $x_1,x_2\in G$. Then, as $f(x_1),f(x_2)\in H$, and $H$ is a group, $f(x_2)$ has an inverse $\left[f(x_2)\right]^{-1}\in H$. Therefore, we can multiply the equality with $\left[f(x_2)\right]^{-1}$ on the right to get $f(x_1)\left[f(x_2)\right]^{-1}=f(x_2)\left[f(x_2)\right]^{-1}$, i.e. $f(x_1)\left[f(x_2)\right]^{-1}=e$. From a previous proposition we have $\left[f(x_2)\right]^{-1}=f\left(x_2^{-1}\right)$. Therefore from $f(x_1)\left[f(x_2)\right]^{-1}=e$ we have $f(x_1)f\left(x_2^{-1}\right)=e$. As $f$ is a homomorphism, we have $f(a b)=f(a)f(b)$ for all $a,b\in G$. So, from $f(x_1)f\left(x_2^{-1}\right)=e$ it follows that $f\left(x_1 x_2^{-1}\right)=e$. But, $\ker{f}$ contains all $x\in G$ such that $f(x)=e$, so it must be $x_1 x_2^{-1}\in\ker{f}$. But, there is only one element in $\ker{f}=\{e\}$ and because of that $x_1 x_2^{-1}=e$. As $x_2^{-1}\in G$ and $G$ is a group we can multiply equality by $x_2$ on the right and get $x_1=x_2$. Therefore, $f$ is injective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ and $H$ be groups and $f:G\rightarrow H$ a homomorphism.

\begin{enumerate}
\item If $K$ is a subgroup of $G$, then $f(K)=\{f(x):\ x\in K\}$ is a subgroup of $H$.
\item If $J$ is a subgroup of $H$, then $f^{-1}(J)=\{x\in G:\ f(x)\in J\}$ is a subgroup of $G$ and $\ker{f}\subseteq f^{-1}(J)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} By definition, $f(K)\subseteq H$. Then, if we take $u,v\in f(K)$, there must exist $x,y\in K$ (by definition of $f(K)$) such that $f(x)=u$ and $f(y)=v$. As $K$ is a subgroup of $G$ we have $x y\in K$, and as $u,v\in f(K)\subseteq H$, and $H$ is a group, we have $u v\in H$, i.e. $f(x)f(y)\in H$. As $f$ is a homomorphism, we have $f(x y)\in H$. So, as $x y\in K$ and $f(x y)\in H$, we have $f(x y)=u v\in f(K)$. Now, take $u\in f(K)$ and there must exist $x\in K$ such that $f(x)=u$. From $K$ being a subgroup of $G$, also $x^{-1}\in K$. As $u\in f(K)\subseteq H$ and $H$ is a group, then $u^{-1}\in H$. We have $u^{-1}=\left[f(x)\right]^{-1}=f(x^{-1})$. So, from $x\in K$ and $f(x^{-1})=u^{-1}\in H$, we have $u^{-1}\in f(K)$. So, $f(K)$ is a subgroup of $H$.

{\it Ad $2$.} We have $f^{-1}(J)\subseteq G$ by definition. If we take $x,y\in f^{-1}(J)\subseteq G$, then $f(x),f(y)\in J$. As $G$ is a group and $x,y\in G$, we have $x y\in G$. Also, as $J$ is a subgroup of $H$, we have $f(x)f(y)\in J$, and as $f$ is a homomorphism, $f(x y)\in J$. As $f(x y)\in J$ and $x y\in G$, then $x y\in f^{-1}(J)$. Take $x\in f^{-1}(J)\subseteq G$. Then, we have $f(x)\in J$. As $G$ is a group and $x\in G$, we have $x^{-1}\in G$. As $f(x)\in J$ and $J$ is a subgroup of $H$, we have $[f(x)]^{-1}\in J$, that is $f(x^{-1})\in J$. So, from $f(x^{-1})\in J$ and $x^{-1}\in G$, we have $x^{-1}\in f^{-1}(J)$. If we take $x\in\ker{f}$, then it must be $f(x)=e$. But, $e\in J$ (as it is a subgroup of $H$) and so it must be $f(x)\in J$. As $x\in\ker{f}\subseteq G$, i.e. $x\in G$ and $f(x)\in J$, we have $x\in f^{-1}(J)$. From that we have $\ker{f}\subseteq f^{-1}(J)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ and $H$ be groups. If $f:G\rightarrow H$ is a homomorphism and $J$ is a subgroup of $G$, then we have $\ker{f_J}=J\cap\ker{f}$ (where $f_J$ is a restriction of $f$ to $J$).

\noindent\newline{\bf Proof.} Obviously $f_J$ is a function as it is defined for all $x\in G$, and so for all $x\in J$. Also, uniqueness holds for all $x\in G$ and so for all $x\in J$. Restriction $f_J$ is the same function as $f$ up to domain. So, for all $x\in J$, $f_J(x)=f(x)$. Therefore, if $x,y\in J$, and $J$ is a subgroup of $G$, we have $x y\in J$. So, $f_J(x y)=f(x y)=f(x)f(y)=f_J(x)f_J(y)$, i.e. $f_J:J\rightarrow H$ is a homomorphism. Let us take $x\in\ker{f_J}$. By definition we have $x\in J$, but also, as $f_J(x)=e$, we have $f_J(x)=f(x)=e$, so $x\in\ker{f}$. Therefore, as $x\in\ker{f_J}$ implies $x\in\ker{f}$ and $x\in J$, we have $\ker{f_J}\subseteq J\cap\ker{f}$. If we take $x\in J\cap\ker{f}$, then $x\in J$ and $x\in\ker{f}$. As $x\in\ker{f}$ we have $f(x)=e$ and, as $x\in J$, we have $f_J(x)=f(x)=e$, so it must be that $x\in\ker{f_J}$. As $x\in J\cap\ker{f}$ implies $x\in\ker{f_J}$, we have $J\cap\ker{f}\subseteq\ker{f_J}$ and by that (and a previous relation) we have $\ker{f_J}=J\cap\ker{f}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group.

\begin{enumerate}
\item The function $f:G\rightarrow G$ defined by $f(x)=e$ is a homomorphism.
\item $\{e\}$ and $G$ are homomorphic images of $G$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} If we take $x,y\in G$, then we have $f(x)=e$ and $f(y)=e$. That multiplied gives us $f(x)f(y)=e$. But, as $G$ is a group, from $x,y\in G$, we have $x y\in G$. From that we have $f(x y)=e$, and combining that with $f(x)f(y)=e$ gives us $f(x y)=f(x)f(y)$, i.e. $f$ is a homomorphism.

{\it Ad $2$.} From previous example we have that $f:G\rightarrow G$, with $f(x)=e$, is a homomorphism. Then, $\ran{f}=\{f(x)\in G:\ x\in G\}$. But, as $f(x)=e$, we only have $\ran{f}=\{e\}$ (which is a trivial group; also by a previous proposition it is a subgroup of $\cod{f}=G$). If we take $f:G\rightarrow G$ with $f(x)=x$, then it is easy to check that $f$ is a homomorphism. From $x,y\in G$ we have $f(x)=x$ and $f(y)=y$. Those two expressions multiplied give us $f(x)f(y)=x y$. But, as $x,y\in G$, and $G$ is a group, also $x y\in G$ and we have $f(x y)=x y$, by definition. So, $f(x)f(y)=x y=f(x y)$ and $f$ is a homomorphism. Now, $\ran{f}=\{f(x)\in G:\ x\in G\}$, but for all $x\in G$ we have $f(x)=x$, therefore $\ran{f}=G$ and $G$ is a homomorphic image of $G$; so is $\{e\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} The function $f:G\rightarrow G$ defined by $f(x)=x^2$ is a homomorphism if and only if $G$ is Abelian.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $f$ is a homomorphism. If we take $x,y\in G$, then, as $G$ is a group, also $x y\in G$. By applying $f$ we have $f(x y)=(x y)^2$. But, $f(x y)=f(x)f(y)=x^2 y^2$. So, we have $(x y)^2=x^2 y^2$. From that we have $x y x y=x x y y$. If we multiply that equality with $y^{-1}$ on the right and $x^{-1}$ on the left, we have $y x=x y$. So, for all $x,y\in G$, it's $x y=x y$, which in turn implies that $G$ is Abelian.

{\it Sufficiency.} Suppose $G$ is Abelian. Then, for all $x,y\in G$, we have $x y=y x$, but also, by a previous proposition (we can obtain next equality by multiplying previous equality with $x$ on the left and $y$ on the right), that $(x y)^2=x^2 y^2$. So, we have $f(x y)=(x y)^2=x^2 y^2=f(x)f(y)$ and $f$ is a homomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} The functions $f_1(x,y)=x$ and $f_2(x,y)=y$, from $G\times H$ to $G$ and $H$, respectively, are homomorphisms.

\noindent\newline{\bf Proof.} First, $f_1:G\times H\rightarrow G$, with $f_1(x,y)=x$. We have $f_1((x,y)(z,w))=f_1(x z,y w)=x z=f_1(x,y)f_1(z,w)$. Therefore, $f_1$ is a homomorphism. Similarly, for $f_2:G\times H\rightarrow H$, we have $f_2((x,y)(z,w))=f_2(x z,y w)=y w=f_2(x,y)f_2(z,w)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every subgroup of an Abelian group is normal.

\noindent\newline{\bf Proof.} Let $G$ be an Abelian group and $H$ a subgroup of $G$. Let $y\in H$ and $x\in G$. As $H\subseteq G$, we also have $y\in G$. As $G$ is Abelian, we have $x y=y x$, for all $y\in H$ and $x\in G$. Multiplying that by $x^{-1}\in G$ on the left, we have $y=x^{-1} y x$. As $y\in H$ and $y=x^{-1} y x$, then $x^{-1} y x\in H$, for all $y\in H$ and $x\in G$. Thus, $H$ is a normal subgroup.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} The center of any group $G$ is a normal subgroup of $G$.

\noindent\newline{\bf Proof.} Center of a group $G$ is the subgroup (as proved by a previous proposition) $C=\{x\in G:\ \left(\forall y\in G\right)(x y=y x)\}$. If we take $x\in C$, then $x y=y x$, for all $y\in G$. Multiplying $x y=y x$ on the right by $y^{-1}\in G$, we have $x=y x y^{-1}$. As $x\in C$, then also $y x y^{-1}\in C$, for all $x\in C$ (our choice of $x\in C$ was arbitrary) and for all $y\in G$. Therefore, center of an arbitrary group is a normal subgroup.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. $H$ is normal if and only if for all $a,b\in G$, $a b\in H$ iff $b a\in H$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $H$ is normal. Then, for all $a,b\in G$ such that $a b\in H$ we have, as $H$ is normal, $a^{-1}(a b)a\in H$, i.e. $b a\in H$. Also, for all $a,b\in G$ such that $b a\in H$, as $H$ is normal, we have $b^{-1}(b a)b\in H$, which is equivalent to $a b\in H$.

{\it Sufficiency.} Suppose that $H$ is a subgroup of $G$ and that for all $a,b\in G$ it holds that if $a b\in H$ then $b a\in H$ and reverse. Take some $a\in G$ and $b\in H$. As $b=b a a^{-1}$, we have $(b a)a^{-1}\in H$. It then follows that also $a^{-1}(b a)\in H$. That is, for all $a\in G$ and $b\in H$ we have $a^{-1}b a\in H$, which means that $H$ is normal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $H$ be a subgroup of $G$. $H$ is normal if and only if $a H=H a$, for all $a\in G$.

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $H$ is normal. Take some $a h\in a H$, where $a\in G$ and $h\in H$. As $H$ is normal and $a\in G$ and $h\in H$, we also have $a h a^{-1}\in H$. That means that $a h a^{-1} a\in H a$, i.e. $a h\in H a$. Therefore, $a H\subseteq H a$. Now, take $h a\in H a$. We have, as $H$ is normal, that $a^{-1}h a\in H$ and from that $a a^{-1} h a\in a H$, i.e. $h a\in a H$. That means that, not only $a H\subseteq H a$, but also $H a\subseteq a H$ and from that $a H=H a$.

{\it Sufficiency.} Suppose $a H=H a$, for all $a\in G$. Then, if we take $a h\in a H$, we also have $a h\in H a$, i.e. there exists $h'\in H$ such that $a h=h' a$. Multiplying that by $a^{-1}$ on the right gives us $h'=a^{-1}h a$. As $h'\in H$, then $a^{-1} h a\in H$ also. Therefore, for all $h\in H$ and $a\in G$ we have $a^{-1} h a\in H$, i.e. $H$ is normal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Any intersection of normal subgroups of $G$ is a normal subgroup of $G$.

\noindent\newline{\bf Proof.} Suppose $H_1$ and $H_2$ are normal subgroups of $G$. Let us denote $H=H_1\cap H_2$. Take any $a\in G$ and $h\in H$. Then, $h\in H_1$ and $h\in H_2$, and as $H_1$ and $H_2$ are normal, we also have $a h a^{-1}\in H_1$ and $a h a^{-1}\in H_2$. In other words, $a h a^{-1}\in H_1\cap H_2=H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. If $H$ has index $2$ in $G$, then $H$ is normal.

\noindent\newline{\bf Proof.} We have $[G:H]=2$, which means that there are only two left and two right cosets of $H$. Therefore, as one of the cosets has to be $H e=H$ (and $e H=H$), we have, in the first case $H$ and $a H$, and in the second case $H$ and $H b$, for some $a,b\in G$. As $H=H$, it must be that $a H=H b$. We have $a\in a H$, and by that, as $\{H, a H\}$ is a partition, it must be $a\notin H$. So, observing partition $\{H, H b\}$ and condition that $a\notin H$, it must be that $a\in H b$, i.e. there exists $h\in H$ such that $a=h b$. That means that $h^{-1} a=b$ and that $b\in H a$. As $b\in H a$, it must be that $H a=H b$. Combined with $a H=H b$, we have $a H=H a$. By a previous proposition, we have that $H$ is normal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$ such that $\ord{a}=2$. Then, $\cyc{a}$ is a normal subgroup of $G$ if and only if $a$ is in the center of the group $G$.

\noindent\newline{\bf Proof.} Let us denote center of group $G$ as $C=\{x\in G:\ (\forall y\in G)(x y=y x)\}$. We also have $\left|\cyc{a}\right|=2$, by definition. {\it Necessity.} Suppose $\cyc{a}$ is a normal subgroup of $G$. Then, for all $g\in G$ we have $g a g^{-1}\in\cyc{a}$. But, that means that, either $a=g a g^{-1}$ or $e=g a g^{-1}$. That is equivalent to (after multiplying both equalities by $g$ on the right) either $a g=g a$, or $g=g a$. The second equality would imply that $a=e$, and, as $\ord{e}=1$, we would have $\ord{a}=1$, which is a contradiction to the assumption that $\ord{a}=2$. Therefore, it must be that $a g=g a$, for all $g\in G$, and that implies that $a\in C$.

{\it Sufficiency.} Suppose $a\in C$. Then, $a g=g a$, for all $g\in G$. After multiplying that equality with $g^{1}$ on the right we have $a=g a g^{-1}$, for all $g\in G$. That means that $g a g^{-1}\in\cyc{a}$ for all $g\in G$. Now, the only other element in $\cyc{a}$ is $e$. But, $e=g e g^{-1}$, for all $g\in G$ and we have that $g e g^{-1}\in\cyc{a}$, for all $g\in G$. Thus, as $g x g^{-1}\in\cyc{a}$, for all $g\in G$ and $x\in\cyc{a}$ (as either $x=e$ or $x=a$), it follows that $\cyc{a}$ is a normal subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent\newline{\bf Lemma.} Let $G$ be a group, $x,y\in G$ and $n\in\Z$. Then, $\left(x^{-1} y x\right)^n=x^{-1} y^n x$.

\noindent\newline{\bf Proof.} For $n=1$ we have $\left(x^{-1} y x\right)^1=x^{-1} y^k x$ by definition. Then, suppose the statement is true for $n=k$, i.e. $\left(x^{-1} y x\right)^k=x^{-1} y^k x$. Let us prove that it is valid for $n=k+1$. We have $\left(x^{-1} y x\right)^{k+1}=\left(x^{-1} y x\right)^k\left(x^{-1} y x\right)$. By using assumption of induction we obtain $\left(x^{-1} y x\right)^{k+1}=x^{-1} y^k x x^{-1} y x=x^{-1} y^{k+1} x$. Thus, the statement is true for all $n\in\N$. Now, for $n=0$ we would have $\left(x^{-1} y x\right)^0=e$. But, we could write $e=x^{-1} x=x^{-1} e x=x^{-1} y^0 x$. Now let us prove the statement for $n\in\Z$, i.e. $n=-k$, where $k\in\N$. We have $\left(x^{-1} y x\right)^{-k}=\left(\left(x^{-1} y x\right)^k\right)^{-1}=\left(x^{-1} y^k x\right)^{-1}=x^{-1}\left(x^{-1} y^k\right)^{-1}=x^{-1} y^{-k} x$. Thus we have proved the statement is valid for all $n\in\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a\in G$. Then, $\cyc{a}$ is a normal subgroup of $G$ if and only if for all $x\in G$, there exists $k\in\N$ such that $x a=a^k x$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $\cyc{a}$ be a normal subgroup of $G$. Then, for all $x\in G$ and $y\in\cyc{a}$ we have $x y x^{-1}\in\cyc{a}$. But, that is also true for $a\in\cyc{a}$, i.e. for all $x\in G$, we have $x a x^{-1}\in\cyc{a}$. Therefore, $x a x^{-1}=a^k$, for some $k\in\{1,\ldots,\ord{a}\}$. Multiplying that equality by $x$ on the right, we have $x a=a^k x$.

{\it Sufficiency.} Let for all $x\in G$ exist $k\in\N$ such that $x a=a^k x$. Then, multiplying that equality by $x^{-1}$ on the right gives us $a^k=x^{-1} a x$. Let $a^l\in\cyc{a}$. Then $a^{k l}=\left(x^{-1} a x\right)^l$. By a previous lemma we have $a^{k l}=x^{-1} a^l x$. As $a^{k l}\in\cyc{a}$, then also $x^{-1} a^l x\in\cyc{a}$, for all $x\in G$ and all $a^l\in\cyc{a}$ and $\cyc{a}$ is normal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a group. A {\bf commutator} is any product of the form $a b a^{-1} b^{-1}$, where $a,b\in G$.

\noindent\newline{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. If $H$ contains all the commutators of $G$, then $H$ is normal.

\noindent\newline{\bf Proof.} Let us define the set containing all commutators of $G$ as

\begin{equation*}
C=\{a b a^{-1} b^{-1}\in G:\ a,b\in G\}.
\end{equation*}

\noindent\newline Now, let $C\subseteq H$. If we take $x\in G$ and $y\in H\subseteq G$, then $x y x^{-1} y^{-1}\in C\subseteq H$. As $H$ is a subgroup then also $x y x^{-1} y^{-1} y\in H$, i.e. $x y x^{-1}\in H$. Therefore, $H$ is normal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $H$ and $K$ are subgroups of $G$, and $K$ is normal, then 

\begin{equation*}
H K=\{h k\in G:\ h\in H\ \wedge\ k\in K\}
\end{equation*}

\noindent\newline is a subgroup of $G$.

\noindent\newline{\bf Proof.} By definition we have $H K\subseteq G$. Then, if we take $x,y\in H K$, we have $x=h_1 k_1$ and $y=h_2 k_2$, for some $h_1,h_2\in H$ and $k_1,k_2\in K$. From that we have $h_1^{-1} x\in K$ and $h_2^{-1} y\in K$. But also, $x h_1^{-1}\in K$ and $y h_2^{-1}\in K$. We have $h_1^{-1} x y h_2^{-1}\in K$ and, as $K$ is closed with respect to conjugates, we also have $h_2^{-1} h_1^{-1} x y h_2^{-1} h_2\in K$, that is $h_2^{-1} h_1^{-1} x y\in K$. As $h_1 h_2\in H$, we have $(h_1 h_2)(h_1 h_2)^{-1} x y\in H K$, which means $x y\in H K$. Therefore $H K$ is closed with respect to products. Now, if we take $x\in H K$, we have $x=h k$, for some $h\in H$ and $k\in K$. Multiplying that by $x^{-1}$ on the right and $k^{-1}$ on the left, we have $x^{-1} h=k^{-1}$. Note that $x^{-1} h\in K$ as $k^{-1}\in K$. But, as $K$ is normal, it is closed with respect to conjugates, so it must be that $h x^{-1} h h^{-1}\in K$, i.e. $h x^{-1}\in K$. As $h^{-1}\in H$, we have $h^{-1} h x^{-1}\in H K$, that is $x^{-1}\in H K$. As $H K$ is closed with respect to inverses also, it is a subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. Let

\begin{equation*}
S=\bigcup{\left\{H a:\ H a=a H,\ a\in G\right\}}.
\end{equation*}

\noindent\newline Then $S$ is a subgroup of $G$ and $H$ is a normal subgroup of $S$.

\noindent\newline{\bf Proof.} {\it $S$ is a subgroup of $G$.} If we take $x\in S$, then $x\in H a$, for some $a\in G$, and $H a\subseteq G$, so $S\subseteq G$. If we take $x,y\in S$, then $x\in H a$ and $y\in H b$, for some $a,b\in G$. We have $H a=a H$ and $H b=b H$ so by a previous proposition we have that $(a b)H=H(a b)$. Thus, it must be that $H(a b)\subseteq S$. We have $x=h_1 a$ and $y=h_2 b$ for some $h_1,h_2\in H$. So, we have $x y=h_1 a h_2 b$. But, as $a h_2\in a H$ and $a H=H a$, there exists $h_3\in H$ such that $a h_2=h_3 a$. From that we have $x y=h_1 h_3 a b$, and as $h_1 h_3\in H$, we have $x y\in H(a b)\subseteq S$. If we take $x\in H a$, where $a\in G$, we have $x=h_1 a$, for some $h_1\in H$. Then, multiplying by $x^{-1}$ on the right gives us $e=h_1 a x^{-1}$, and multiplying by $(h_1 a)^{-1}$ on the left yields $x=a^{-1} h^{-1}$. Therefore, $x\in a^{-1} H$. But, as $a H=H a$ implies $a^{-1} H=H a^{-1}$ by a previous proposition, we have $H a^{-1}\subseteq S$. As we have $x=a^{-1} h^{-1}$ and $a^{-1} H=H a^{-1}$, there exists $h'\in H$ such that $a^{-1} h^{-1}=h' a^{-1}$. To conclude, $x^{-1}=h' a^{-1}$ and $h' a^{-1}\in H a^{-1}\subseteq S$. Therefore, $S$ is closed with respect to inverses and products and it is a subgroup of $G$.

{\it $H$ is a normal subgroup of $S$.} As $H=H e=e H$, we have $H\subseteq S$. If we take $x,y\in H\subseteq S$, then, as $H$ is a subgroup of $G$, we have $x y\in H$. Also, for $x\in H$, as $H$ is a subgroup of $G$, we have $x^{-1}\in H$. But, if we take $x\in S$ and $y\in H$, we have $x\in H a$, for some $a\in G$ and $x=h_1 a$, for some $h_1\in H$. But, as $x\in S$, and $S$ is a subgroup of $G$, we also have $x^{-1}\in S$ and $x^{-1}=a^{-1} h_1^{-1}$. Therefore, $x y x^{-1}=h_1 a y a^{-1} h_1^{-1}$. As $y\in H$ we have $a y\in a H$. As $a H=H a$, we have that $a y=h' a$ for some $h'\in H$. That implies $x y x^{-1}=h_1 h' a a^{-1} h_1^{-1}=h_1 h' h_1^{-1}$. As $H$ is a subgroup of $G$ we have $h_1 h' h_1^{-1}\in H$ and from that $x y x^{-1}\in H$. Therefore, $H$ is a normal subgroup of $S$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $f:G\rightarrow H$ be a homomorphism from group $G$ to group $H$, $n\in\N$ and $a\in G$. Then, $f\left(a^n\right)=\left[f(a)\right]^n$.

\noindent\newline{\bf Proof.} Let us prove that the statement holds for $n=1$. We have $f(a^1)=f(a)=\left[f(a)\right]^1$. Suppose statement is true for $n=k$, i.e. $f\left(a^k\right)=\left[f(a)\right]^k$. Now, let us prove that it is true for $n=k+1$. We have $f\left(a^{k+1}\right)=f\left(a^k a\right)$. As $f$ is a homomorphism we have $f\left(a^k a\right)=f\left(a^k\right)f(a)$. Now, due to the assumption of induction, we have $f\left(a^k\right)=\left[f(a)\right]^k$. So, we get $f\left(a^{k+1}\right)=\left[f(a)\right]^k f(a)=\left[f(a)\right]^{k+1}$. Thus, by the principle of mathematical induction, we have proved the statement for all $n\in\N$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $G$ and $H$ be groups, $f:G\rightarrow H$ a homomorphism and $a\in G$. Then\footnote{Note that $f\left(\cyc{a}\right)=\{f(a^k):\ a^k\in\cyc{a}\}$.}, $f\left(\cyc{a}\right)=\cyc{f(a)}$.

\noindent\newline{\bf Proof.} Take $f(a^k)\in f\left(\cyc{a}\right)$, for some $k\in\N$. By a previous theorem we have $f(a^k)=[f(a)]^k$, which is in $\cyc{f(a)}$. Therefore, $f\left(\cyc{a}\right)\subseteq\cyc{f(a)}$. Now, if we take $[f(a)]^k\in\cyc{f(a)}$, for some $k\in\N$, we have $[f(a)]^k=f(a^k)$ and $f(a^k)\in f\left(\cyc{f}\right)$. As also $\cyc{f(a)}\subseteq f\left(\cyc{a}\right)$, we have $f\left(\cyc{a}\right)=\cyc{f(a)}$.

\noindent\newline{\bf Remark.} Notice that from previous corollary we have $\left|f\left(\cyc{a}\right)\right|=\left|\cyc{f(a)}\right|$.

\noindent\newline{\bf Corollary.} Let $f:G\rightarrow H$ be a homomorphism and $a\in G$. Then, $\ord{f(a)}|\ord{a}$.

\noindent\newline{\bf Proof.} Suppose $\ord{a}=n$. That implies that $a^n=e$. As $f(e)=e$ and $a^n=e$, we have $f\left(a^n\right)=e$. From the previous theorem we have $f\left(a^n\right)=\left[f(a)\right]^n=e$. Therefore, as from $x^n=e$ follows that $\ord{x}|n$, we have that $\ord{f(a)}|n$, i.e. $\ord{f(a)}|\ord{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Note that by Lagrange's theorem we have that $\ord{a}$ divides $|G|$. Therefore, $\ord{f(a)}$ also divides $|G|$. Furthermore, as $\ran{f}$ is a subgroup of $H$, for $f(a)\in\ran{f}$ we have that $\ord{f(a)}$ divides $|H|$. Therefore, if $b\in\ran{f}$, then $\ord{b}$ is a common divisor of $|G|$ and $|H|$.

\noindent\newline{\bf Problem.} Let $f:G\rightarrow H$ be a homomorphism from group $G$ to group $H$. Prove:

\begin{enumerate}
\item If $\left|\ran{f}\right|=n$, then $a^n\in\ker{f}$, for every $a\in G$;
\item Let $m\in\Z$ such that $\gcd{\left(m,|H|\right)}=1$. For any $a\in G$, if $a^m\in\ker{f}$, then $a\in\ker{f}$;
\item Let $\left|\ran{f}\right|=m$ and $a\in G$. If $\ord{a}=n$, where $\gcd{m,n}=1$, then $a\in\ker{f}$;
\item Let $p\in P$. If $\ran{f}$ has an element of order $p$, then $G$ has an element of order $p$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $\left|\ran{f}\right|=n$, then $a^n\in\ker{f}$, for every $a\in G$.} Let $f(a)\in\ran{f}$. From Lagrange's theorem we have that $\ord{f(a)}|n$. So it must be $\left[f(a)\right]^n=e$. From a previous theorem we have $\left[f(a)\right]^n=f(a^n)=e$, therefore $a^n\in\ker{f}$.

\item {\it Let $m\in\Z$ such that $\gcd{\left(m,|H|\right)}=1$. For any $a\in G$, if $a^m\in\ker{f}$, then $a\in\ker{f}$.} Suppose $a^m\in\ker{f}$. Then, $f(a^m)=e$ and from that, by a previous theorem, we have $f(a^m)=\left[f(a)\right]^m=e$. Therefore $\ord{f(a)}|m$. But, by Lagrange's theorem, also $\ord{f(a)}$ divides $|H|$. But, as $m$ and $|H|$ are relatively prime, they have no common divisors except $\pm 1$. So it can only be that $\ord{f(a)}=1$, and from that it follows that $f(a)=e$ and $a\in\ker{f}$.

\item {\it Let $\left|\ran{f}\right|=m$ and $a\in G$. If $\ord{a}=n$, where $\gcd{m,n}=1$, then $a\in\ker{f}$.} Let $a\in G$. Then, $f(a)\in\ran{f}$. We have that $\ord{f(a)}|\ord{a}$, i.e. $\ord{f(a)}|n$, and $\ord{f(a)}|m$ (order of $f(a)$ divides $\left|\ran{f}\right|$). But, as $m$ and $n$ are relatively prime, we have $\ord{f(a)}=1$, that is $f(a)=e$ and $a\in\ker{f}$.

\item {\it Let $p\in P$. If $\ran{f}$ has an element of order $p$, then $G$ has an element of order $p$.} Suppose $f(a)\in\ran{f}$ such that $\ord{f(a)}=p$. We have $\ord{f(a)}|\ord{a}$, i.e. $p|\ord{a}$ so it must be $\ord{a}=p k$, for some $k\in\N$. As, by Lagrange's theorem, $\ord{a}$ divides order of $G$, we have $|G|=p k l$, for some $l\in\N$. As $p$ divides $|G|$, by Cauchy's theorem (not yet proved), it has an element of order $p$.
\end{enumerate}

\noindent{\bf Definition.} We say that a group $G$ is {\bf finitely generated} if the set of its generators is finite.

\noindent\newline{\bf Proposition.} Let $G$ and $H$ be groups and $f:G\rightarrow H$ a homomorphism such that\footnote{Same as $H=f(G)$ or $H=\textnormal{Im}(f)$.} $H=\ran{f}$. Then:

\begin{enumerate}
\item If $G$ is Abelian, then $H$ is Abelian.
\item If $G$ is cyclic, then $H$ is cyclic.
\item If every element of $G$ has finite order, then every element of $H$ has finite order.
\item If every element of $G$ is its own inverse, every element of $H$ is its own inverse.
\item If every element of $G$ has a square root, then every element of $H$ has a square root.
\item If $G$ is finitely generated, then $H$ is finitely generated.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} Let us take $y_1,y_2\in H$. As $H=\ran{f}$, then $y_1=f(x_1)$ and $y_2=f(x_2)$ for some $x_1,x_2\in G$. Then, $y_1 y_2=f(x_1)f(x_2)$. As $f$ is a homomorphism, we have $y_1 y_2=f(x_1 x_2)$. But, as $G$ is Abelian, $x_1 x_2=x_2 x_1$. As $f$ is a function, for all $a,b\in G$, $a=b$ implies $f(a)=f(b)$. Therefore, $x_1 x_2=x_2 x_1$ implies $f(x_1 x_2)=f(x_2 x_1)$. So, we have $y_1 y_2=f(x_2 x_1)$, and again as $f$ is a homomorphism, we have $y_1 y_2=f(x_2)f(x_1)=y_2 y_1$, and so $H$ is Abelian.

{\it Ad $2$.} As $G$ is cyclic, then $G=\cyc{a}$ for some $a\in G$ and every element is of the form $a^k\in\cyc{a}$, for some $k\in\{0,1,\ldots,\ord{a}-1\}$. Let us take $y\in\ran{f}$. Then there exists $a^k\in G$ such that $y=f(a^k)$. By a previous proposition, we have $f(a^k)=[f(a)]^k$, so $y=[f(a)]^k$. In other words, every $y\in\ran{f}$ can be shown as a power of $f(a)$; that implies that $f(a)$ generates $\ran{f}$ and we have $H=\ran{f}=\cyc{f(a)}$.

{\it Ad $3$.} Assume that all $x\in G$ have finite order. Suppose that there is some $y\in H$ that has infinite order, i.e. there does not exist $n\in\N$ such that $y^n=e$. As $H=\ran{f}$, we have $y\in\ran{f}$, which means that there exists some $x\in G$ such that $y=f(x)$. Now, as $x\in G$, it has finite order, e.g. $\ord{x}=m$, for some $m\in\N$. Thus, $x^m=e$. When we apply $f$ we get $f(x^m)=e$, and by a previous theorem $\left[f(x)\right]^m=e$. But, $y=f(x)$, so we have $y^m=e$, which is contrary to our assumption that there does not exist such natural number. Furthermore, $\ord{y}|m$.

{\it Ad $4$.} We have that $x^2=e$ for all $x\in G$. Take $y\in H$, then as $H=\ran{f}$, we have $y\in\ran{f}$ and there exists $x\in G$ such that $y=f(x)$. Then, $y^2=[f(x)]^2$. As $f$ is homomorphism, that is equivalent to $y^2=f(x^2)$. But, $x^2=e$, so $y^2=f(e)$. Again, as $f$ is a homomorphism, by a previous theorem, we have that $f(e)=e$ and $y^2=f(e)=e$. Therefore, $y^2=e$, for all $y\in H$.

{\it Ad $5$.} Suppose that for all $a\in G$, there exists $b\in G$ such that $a=b^2$. Let $y_1\in H$, i.e. $y_1\in\ran{f}$. We must show that there exists $y_2\in\ran{f}$ such that $y_1=y_2^2$. As $y_1\in\ran{f}$, there exists $x_1\in G$ such that $y_1=f(x_1)$. But, $x_1\in G$, so there exists $x_2\in G$ such that $x_1=x_2^2$. Applying $f$ gives us $f(x_1)=f(x_2^2)$ and, as $f$ is a homomorphism, $f(x_1)=[f(x_2)]^2$. Therefore $y_1=[f(x_2)]^2$. Also, as $f(x_2)\in\ran{f}$, we can take $y_2=f(x_2)$ so to get $y_1=y_2^2$. Therefore, each $y_1\in H$ has a square root.

{\it Ad $6$.} Suppose that the set $S=\{s_1,\ldots,s_n\}$, where $n\in\N$, generates $G$. Let $T=S\cup\{s^{-1}\in G:\ s\in S\}$ and $\phi:T^k\rightarrow G$, for some $k\in\N$, be a function defined as:

\begin{equation*}
\phi(t_1,\ldots,t_k)=\prod_{i=1}^{k}{t_i}.
\end{equation*}

\noindent\newline Then, for every $a\in G$ there exist $k\in\N$ and $X\in T^k$ such that $\phi(X)=a$. Let $y\in H=\ran{f}$. Then there exists $x\in G$ such that $f(x)=y$. But, as $x\in G$, then there exists $k\in\N$ and $(t_1,\ldots,t_k)\in T^k$ such that $\phi(t_1,\ldots,t_k)=x$, i.e. $x=\prod_{i=1}^{k}{t_i}$. As $f$ is homomorphism we have:

\begin{equation*}
y=f(x)=f\left(\prod_{i=1}^{k}{t_i}\right)=\prod_{i=1}^{k}{f(t_i)}.
\end{equation*}

\noindent\newline Notice that $f(t_i)\in f(T)=\{f(t):\ t\in T\}$. Also, as $S$ is finite so is $T$ and $f(T)$. Now, as each $y\in H$ can be shown as a product of $f(t_i)\in f(T)$, then $H$ is generated by $f(T)$. As $f(T)$ is finite, $H$ is finitely generated.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group with normal subgroups $H$ and $K$ such that $H\cap K=\{e\}$. Then:

\begin{enumerate}
\item For any $h_1,h_2\in H$ and $k_1,k_2\in K$, from $h_1 k_1=h_2 k_2$ follows $h_1=h_2$ and $k_1=k_2$.
\item For any $h\in H$ and $k\in K$, $h k=k h$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} Suppose $h_1,h_2\in H$, $k_1,k_2\in K$ and $h_1 k_1=h_2 k_2$. That expression is equivalent to $h_2^{-1} h_1=k_2 k_1^{-1}$ (after multiplying by $h_2^{-1}$ on the left and $k_1^{-1}$ on the right). As $h_2^{-1} h_1\in H$ and it equals $k_2 k_1^{-1}$ (which is in $K$), it also has to be in $K$. So, $h_2^{-1} h_1\in H\cap K$. But, the only element in $H\cap K$ is $e$, so it must be $h_2^{-1} h_1=e$. That implies, after multiplying by $h_2\in H$ on the left, that $h_1=h_2$. Now, $h_1 k_1=h_2 k_2$ implies $h k_1=h k_2$ (where $h=h_1=h_2$) and, as $G$ is a group and $k_1,k_2\in K\subseteq G$ and $h\in H\subseteq G$, we multiply by $h^{-1}\in G$ on the left (cancellation law) to get $k_1=k_2$.

{\it Ad $2$.} Let $h\in H$ and $k\in K$. Also, $h^{-1}\in H$ and $k^{-1}\in K$ (as $H$ and $K$ are subgroups). As $H$ and $K$ are normal, $k h k^{-1}\in H$ and $h k^{-1} h^{-1}\in K$. Furthermore, as $k h k^{-1}\in H$, then $(k h k^{-1})h^{-1}\in H$. Similarly, as $h k^{-1} h^{-1}\in K$, then $k(h k^{-1} h^{-1})\in K$. Then, as $k h k^{-1} h^{-1}$ in $H$ and $K$, it is in $H\cap K$. So it must be $k h k^{-1} h^{-1}=e$. Multiplying by $(h k)$ on the right gives us $k h=h k$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $H$ and $K$ are normal subgroups of group $G$, such that $H\cap K=\{e\}$ and $G=H K$, then $G\cong H\times K$.

\noindent\newline{\bf Proof.} Let $f:H\times K\rightarrow G$ be a mapping such that $f(h,k)=h k$. It is unique and defined for all $(h,k)\in H\times K$. {\it Surjectivity.} Choose $y\in G$. Then, as $G=H K$, there exist $h\in H$ and $k\in K$ such that $y=h k$. As $h\in H$ and $k\in K$, then $x=(h,k)\in H\times K$. Therefore, for each $y\in G$ there exists $x\in H\times K$ such that $f(x)=y$. {\it Injectivity.} Let $f(h_1,k_1)=f(h_2,k_2)$. Then, $h_1 k_1=h_2 k_2$. By a previous lemma we have $h_1=h_2$ and $k_1=k_2$, which implies $(h_1,k_1)=(h_2,k_2)$ and the function is injective. As it is injective and surjective, it is bijective. Now, if we take $(h_1,k_1),(h_2,k_2)\in H\times K$, we have $f((h_1,k_1)(h_2,k_2))=f(h_1 h_2,k_1 k_2)=h_1 h_2 k_1 k_2$. As, by a previous lemma, $h k=k h$, for all $h\in H$ and $k\in K$, then $h_1(h_2 k_1)k_2=(h_1 k_1)(h_2 k_2)=f(h_1,k_1)f(h_2,k_2)$. Thus, $f$ is an isomorphism from $H\times K$ to $G$ and we have $H\times K\cong G$. As $\cong$ is an equivalence relation, it is also symmetric, so $G\cong H\times K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Group $G$ from the previous theorem is sometimes called {\bf direct inner product} of $H$ and $K$.

\noindent\newline{\bf Definition.} Let $G$ be a group, $H$ a subgroup of $G$ and $a\in G$. A {\bf conjugate} of $H$ is the set:

\begin{equation*}
a H a^{-1}=\left\{a h a^{-1}:\ h\in H\right\}.
\end{equation*}

\noindent\newline{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. Then:

\begin{enumerate}
\item $a H a^{-1}$ is a subgroup of $G$, for each $a\in G$.
\item $H\cong a H a^{-1}$, for each $a\in G$.
\item $H$ is a normal subgroup of $G$ if and only if $H=a H a^{-1}$ for every $a\in G$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a\in G$. Then, if we take $y\in a H a^{-1}$ it is of the form $y=x a x^{-1}$, where $x\in H\subseteq G$. As $x\in G$ and $a^{-1}\in G$ then $a x a^{-1}\in G$. Therefore $a H a^{-1}\subseteq G$. If we take $x,y\in a H a^{-1}$, then $x=a h_1 a^{-1}$ and $y=a h_2 a^{-1}$, for some $h_1,h_2\in H$. Then, $x y=a h_1 a^{-1} a h_2 a^{-1}=a h_1 h_2 a^{-1}$. As $H$ is a subgroup of $G$, we have $h_1 h_2\in H$ and that implies $x y\in a H a^{-1}$. Now, if we multiply expression for $x$ by $x^{-1}$ on the left and by $(a h_1 a^{-1})^{-1}$ on the right we get $x^{-1}=\left(a h_1 a^{-1}\right)^{-1}=\left(h_1 a^{-1}\right)^{-1} a^{-1}=a h_1^{-1} a^{-1}$. As $H$ is a subgroup of $G$, then $h_1^{-1}\in H$ and $x^{-1}\in a H a^{-1}$. Therefore, $a H a^{-1}$ is closed with respect to products and inverses and is a subgroup of $G$.

{\it Ad $2$.} Let $f:H\rightarrow a H a^{-1}$ be a function such that $f(h)=a h a^{-1}$ (obviously it is a function as it is defined for all $h\in H$ and returns a unique element in $a H a^{-1}$). {\it Surjectivity.} Let $y\in a H a^{-1}$. Then there exists $h\in H$ such that $y=a h a^{-1}$. Then, after multiplying by $a$ on the right and $a^{-1}$ on the left we have $h=a^{-1} y a$. As $h\in H$, we have $f(h)=a h a^{-1}=a a^{-1} y a a^{-1}=y$. Thus, $f$ is surjective. {\it Injectivity.} Suppose $f(h_1)=f(h_2)$. Then, $a h_1 a^{-1}=a h_2 a^{-1}$. After multiplying by $a$ on the right we have $a h_1=a h_2$, and after multiplying by $a^{-1}$ on the left $h_1=h_2$. Therefore, $f$ is surjective and injective and by that a bijection. Now take $h_1,h_2\in H$. Then $f(h_1 h_2)=a h_1 h_2 a^{-1}=a h_1 a^{-1} a h_2 a^{-1}=f(h_1)f(h_2)$ and $f$ is an isomorphism from $H$ to $a H a^{-1}$, i.e. $H\cong a H a^{-1}$.

{\it Ad $3$.} {\it Necessity.} Suppose $H$ is a normal subgroup of $G$. Take $a\in G$ and $a h a^{-1}\in a H a^{-1}$. Then, $h\in H$. As $H$ is normal and $a\in G$ with $h\in H$, then $a h a^{-1}\in H$ and $a H a^{1}\subseteq H$. Now, if we take $h\in H$, then, as $H$ is normal, $h'=a^{-1} h a\in H$. But, also, as $h'\in H$, we have $a h' a^{-1}\in a H a^{-1}$, that is $a a^{-1} h a a^{-1}=h\in a H a^{-1}$, for all $a\in G$. As $h\in H$ implied $h\in a H a^{-1}$, we have $H\subseteq a H a^{-1}$ and, from the previous subset relation, $H=a H a^{-1}$, for all $a\in G$. {\it Sufficiency.} Suppose $H=a H a^{-1}$ for every $a\in G$. So, if we take $h\in H$, then $h\in a H a^{-1}$, i.e. there exists $h'\in H$ such that $h=a h' a^{-1}$. Multiplying that expression on the left by $a^{-1}$ and by $a$ on the right yields $h'=a^{-1} h a$. As $h'\in H$, then $a^{-1} h a\in H$, so $H$ is closed with respect to conjugates and is therefore a normal subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a finite group and $H$ a subgroup of $G$. Set

\begin{equation*}
N(H)=\left\{a\in G:\ \left(\forall h\in H\right)\left(a h a^{-1}\in H\right)\right\}
\end{equation*}

\noindent\newline is called {\bf normalizer} of $H$.

\noindent\newline{\bf Proposition.} Let $G$ be a finite group and $H$ a subgroup of $G$. Then:

\begin{enumerate}
\item If $a\in N(H)$, then $a H a^{-1}=H$.
\item $N(H)$ is a subgroup of $G$.
\item $H$ is a normal subgroup of $N(H)$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} Suppose $a\in N(H)$. Then $a h a^{-1}\in H$, for all $h\in H$. If we take $a h a^{-1}\in a H a^{-1}$, then, by the forementioned property, $a h a^{-1}\in H$. That implies $a H a^{-1}\subseteq H$. But, from a previous proposition we have $H\cong a H a^{-1}$, which implies $\left|H\right|=\left|a H a^{-1}\right|$. From a previous lemma we have that $a H a^{-1}=H$.

{\it Ad $2$.} By definition, $N(H)\subseteq G$. Then, if we take $a,b\in N(H)$, then for $a,b\in G$ we have $a h a^{-1}\in H$ and $b h b^{-1}\in H$, for all $h\in H$. As $a h a^{-1}\in H$, then $a h a^{-1} a\in H a$, i.e. $a h\in H a$, for all $a h\in a H$. That means that $a H\subseteq H a$. Now, as $|H a|=|H|=|a H|$, we have $a H=H a$. In the vein of the same reasoning we get $b H=H b$. So, by a previous proposition, $(a b)H=H(a b)$. Therefore, for all $a b h\in H$ there exists $h'\in H$ such that $a b h=h' a b$. Multiplying that by $(a b)^{-1}$ we get $(a b)h(a b)^{-1}=h'$. As $h'\in H$, then $(a b)h(a b)^{-1}\in H$ and by definition $a b\in N(H)$. Now, as for $a\in N(H)$ we have that $a H=H a$ we also have $a^{-1} H=H a^{-1}$ by a previous proposition. Therefore if we take $a^{-1} h\in H$, then there exists $h'\in H$ such that $a^{-1} h=h' a^{-1}$. Multiplying that by $a\in G$ on the right we get $a^{-1} h a=h'$. As $h'\in H$ then $a^{-1} h a\in H$ and we have that $a^{-1}\in N(H)$. Therefore, $N(H)$ is a subgroup of $G$.

{\it Ad $3$.} If we take $h\in H$, then $h e\in H$, i.e. $h h h^{-1}\in H$, so $h\in N(H)$. That implies that $H\subseteq N(H)$. If we take $h_1,h_2\in H$, then $h_1 h_2\in H$ and $h^{-1}\in H$, as $H$ is a subgroup of $G$. Furthermore, if we take $h\in H$ and $a\in N(H)$, then, $a h a^{-1}\in H$ (from definition of $N(H)$) which implies that $H$ is a normal subgroup of $N(H)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group, $H$ a subgroup of $G$ and $N=N(H)$. Then, for all $a,b\in G$, the following statements are equivalent:

\begin{enumerate}
\item $a H a^{-1}=b H b^{-1}$;
\item $b^{-1} a\in N$;
\item $a N=b N$.
\end{enumerate}

\noindent{\bf Proof.} {\it $a H a^{-1}=b H b^{-1}$ implies $b^{-1} a\in N$.} If $a H a^{-1}=b H b^{-1}$, then for all $a h_1 a^{-1}\in a H a^{-1}$ there exists $b h_2 b^{-1}\in b H b^{-1}$ such that $a h_1 a^{-1}=b h_2 b^{-1}$. Multiplying that expression by $b$ on the right and $b^{-1}$ on the left we get $b^{-1} a h_1 a^{-1} b=h_2$, that is, $\left(b^{-1} a\right)h_1\left(a b^{-1}\right)^{-1}\in H$, so $a b^{-1}\in N(H)$.

{\it $b^{-1} a\in N$ implies $a N=b N$.} As $b^{-1} a\in N$, we can take $b b^{-1} a\in b N$, i.e. $a\in b N$. By a previos proposition, as $a\in b N$, we have $a N=b N$.

{\it $a N=b N$ implies $b^{-1} a\in N$ implies $a H a^{-1}=b H b^{-1}$.} As $a N=b N$, then for $a n_1\in a N$, there exists $b n_2\in b N$ such that $a n_1=b n_2$. Multiplying that by $b^{-1}$ on the left and by $n_1^{-1}$ on the right, gives us $b^{-1} a=n_2 n_1^{-1}$, i.e. $b^{-1} a\in N$. If we take $a h_1 a^{-1}\in a H a^{-1}$, then, as $b^{-1} a\in N$, for all $h\in H$, $b^{-1} a h a^{-1} b\in H$. So, it is also true for $h_1$ and we have $b^{-1} a h_1 a^{-1} b\in H$, i.e. $b b^{-1} a h_1 a^{-1} b b^{-1}\in b H b^{-1}$. That means that $a h_1 a^{-1}\in b H b^{-1}$ and $a H a^{-1}\subseteq b H b^{-1}$. If we take $b h_2 b^{-1}\in H$, then, as $b^{-1} a h_2 a^{-1} b\in H$, we have that there exists $h_3\in H$, such that $h_3=b^{-1} a h_2 a^{-1} b\in H$. Multiplying that by $(b^{-1} a)^{-1}$ on the left and $b^{-1} a$ on the right gives us $a^{-1} b h_3 b^{-1} a=h_2$, i.e. $a^{-1} b h_3 b^{-1} a\in H$. Therefore, $a a^{-1} b h_3 b^{-1} a a^{-1}\in a H a^{-1}$, which means $b h_3 b^{-1}\in a H a^{-1}$ and $b H b^{-1}\subseteq a H a^{-1}$. Finally, $a H a^{-1}=b H b^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a subgroup of $G$. Let $\mathcal{N}=\{N a:\ a\in G\}$ and $\mathcal{H}=\{a H a^{-1}:\ a\in G\}$. Then, there exists a bijection $f:\mathcal{N}\rightarrow\mathcal{H}$.

\noindent\newline {\bf Proof.} Let $f$ be defined as above with $f(N a)=a H a^{-1}$. If we take $N a=N b$, then it implies, by a previous proposition, that $a N=b N$ and then that $a H a^{-1}=b H b^{-1}$, i.e. $f(N a)=f(N b)$. Therefore, $f$ satisfies property of uniqueness and is defined for all $a\in G$. {\it Surjectivity.} If we take $a H a^{-1}\in\mathcal{H}$, we can always take $N a\in\mathcal{N}$ to get $f(N a)=a H a^{-1}$ (because $f$ depends on $a$, and $f$ is defined for all $a$, both ways). {\it Injectivity.} If $f(N a)=f(N b)$, i.e. $a H a^{-1}=b H b^{-1}$, by a previous proposition, we have $a N=b N$. But, $a N=b N$ if and only if $N a=N b$. Therefore, $f$ is bijective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} If $G$ is a finite group and $H$ a subgroup of $G$, then $H$ has exactly $\left[G:N(H)\right]$ conjugates.

\noindent\newline{\bf Proof.} As, by a previous proposition, $\left|\mathcal{N}\right|=\left|\mathcal{H}\right|$, that means that the number of conjugates in $G$ is the same as the number of cosets of $N(H)$ in $G$. But, the number of cosets of $N(H)$ is $\left[G:N(H)\right]$ and that implies $\left|\mathcal{H}\right|=\left[G:N(H)\right]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ and $K$ subgroups of $G$. Let $\mathcal{N}_K=\{N a:\ a\in K\}$ and $\mathcal{H}_K=\{a H a^{-1}:\ a\in K\}$. There exists a bijection $f:\mathcal{N}_K\rightarrow\mathcal{H}_K$.

\noindent\newline{\bf Proof.} Let $f$ be defined as $f(N a)=a H a^{-1}$. As $K$ is a subgroup of $G$, it is also a group and the properties of uniqueness and injectivity hold for $f$, as in the latter proposition. Surjectivity holds as $f$ is trivially defined for all $a\in K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From the previous proposition, the number of conjugates of $K$ divides order of $K$, by Lagrange's theorem.

\newpage

\begin{center}
{\bf Quotient groups}
\end{center}

\vskip 0.5cm

\noindent{\bf Theorem.} If $H$ is a normal subgroup of $G$, then $a H=H a$ for every $a\in G$.

\noindent\newline{\bf Proof.} Let $H$ be a normal subgroup of $G$. Then, $a h a^{-1}\in H$, for all $a\in G$ and $h\in H$. If we take $a h\in a H$, then, as $a\in G$ and $h\in H$ (and $H$ being a normal subgroup of $G$) we have $a h a^{-1}\in H$ which implies $a h a^{-1} a\in H a$, i.e. $a h\in H a$. Thus, $a H\subseteq H a$. Also, if we take $h a\in H a$, then $a^{-1} h a\in H$ and, from that $a a^{-1} h a\in a H$, that is $h a\in a H$. Therefore, $H a\subseteq a H$ and $a H=H a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a group and $H$ a normal subgroup of $G$. {\bf Coset multiplication}, for $a,b\in G$, is defined as:

\begin{equation*}
H a\cdot H b=H(a b).
\end{equation*}

\noindent\newline{\bf Remark.} If $G$ is a group with additive-like operation then remember that we write $H+a$ instead of $H a$. So we will also define {\it coset addition} as $[H+a]+[H+b]=[H+(a+b)]$. The difference is just in naming, everything else is the same.

\noindent\newline{\bf Theorem.} Coset multiplication, as defined above, satisfies the property of uniqueness.

\noindent\newline{\bf Proof.} Let $H a=H c$ and $H b=H d$. Then it has to be $H(a b)=H(c d)$. If we take $h a b\in H a b$, then $(h a)\in H a$ and there exists $h_1\in H c$ such that $h a=h_1 c$. Therefore, $h a b=h_1 c b$. But, as $H$ is normal, then there exists $h_2\in H$ such that $h_2=c^{-1} h_1 c$, i.e. $c h_2 c^{-1}=h_1$. From that we have $h a b=c h_2 c^{-1} c b$, that is $h a b=c h_2 b$. As $h_2 b\in H b$ and $H b=H d$, there exists $h_3\in H$ such that $h_2 b=h_3 d$ and we have $h a b=c h_3 d$. Again, as $H$ is normal, there exists $h_4\in H$ such that $h_4=c h_3 c^{-1}$, i.e. $c^{-1} h_4 c=h_3$. That implies $h a b=c c^{-1} h_4 c d$, which is equivalent to $h a b=h_4 c d$. As $h_4 c d\in H(c d)$, then $h a b\in H(c d)$ and it has to be $H(a b)\subseteq H(c d)$.

Now, if we take $h c d\in H(c d)$, then, as $h c\in H c$ and $H c=H a$, there exists $h_1 a\in H a$ such that $h c d=h_1 a d$. As $H$ is normal, we have that there exists $h_2\in H$ such that $h_2=a^{-1} h_1 a$, i.e. $a h_2 a^{-1}=h_1$. So we have $h c d=a h_2 d$. As $h_2 d\in H d$ and $H d=H b$, there exists $h_3\in H$ such that $h_2 d=h_3 b$. We have $h c d=a h_3 b$. As $H$ is normal, there exists $h_4\in H$ such that $h_4=a h_3 a^{-1}$, that is $a^{-1} h_4 a=h_3$. From that follows that $h c d=a a^{-1} h_4 a b$, id est $h c d=h_4 a b$. As $h_4 a b\in H(a b)$, then also $h c d\in H(a b)$. That means that $H(c d)\subseteq H(a b)$ and that, with $H(a b)\subseteq H(c d)$, implies $H(a b)=H(c d)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $G$ be a group and $H$ a normal subgroup of $G$. Let $G\slash H=\{H a:\ a\in G\}$. Then, $G\slash H$ with coset multiplication as respective operation is called a {\bf quotient group}.

\noindent\newline{\bf Theorem.} Quotient group is a group.

\noindent\newline{\bf Proof.} Let $G$ be a group and $H$ a subgroup of $G$. Let quotient group $G\slash H$ be defined as above. From the previous theorem, it follows that coset multiplication satisfies the property of uniqueness. It is also defined for all $H a,H b\in G\slash H$. {\it Associativity.} We have $H a\cdot\left(H b\cdot H c\right)=H a\cdot H(b c)=H(a(b c))$. As $G$ is a group, it is associative and $a(b c)=a b c$, so $H(a(b c))=H(a b c)$. Now, as $\left(H a\cdot H b\right)\cdot H c=H(a b)\cdot H c=H((a b)c)$, for the same reason $H((a b)c)=H(a b c)$ and that means $H a\cdot\left(H b\cdot H c\right)=\left(H a\cdot H b\right)\cdot H c$. {\it Neutral element.} We can see that, as $H=H e$, we have $H a\cdot H e=H(a e)=H a$ and $H e\cdot H a=H(e a)=H a$. Therefore, $H$ is a neutral element. {\it Inverse elements.} For $H a\in G\slash H$ we have $H a^{-1}\in G\slash H$ and $H a\cdot H a^{-1}=H(a a^{-1})=H e=H$ with $H a^{-1}\cdot H a=H(a^{-1} a)=H e=H$. To conclude, $H a^{-1}$ is the inverse element for $H a$, and by all that $G\slash H$ is a group.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Be reminded that a neutral element in $G\slash H$ is $H$, and inverse of $H a$ is $H(a^{-1})$.

\noindent\newline{\bf Proposition.} Let $G$ be a group and $H$ a normal subgroup of $G$. Then, $\left|G\slash H\right|=\left[G:H\right]$.

\noindent\newline{\bf Proof.} Quotient group $G\slash H$ contains all cosets of $H$ in $G$ and $\left[G:H\right]$ is a number of different cosets of $H$ in $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $G$ be a group and $H$ a normal subgroup of $G$. There exists a homomorphism $f:G\rightarrow G\slash H$ such that $\ran{f}=G\slash H$ ($G\slash H$ is a homomorphic image of $G$) and $\ker{f}=H$.

\noindent\newline{\bf Proof.} Let $f$ be defined as above with $f(a)=H a$. Then, $f$ is defined for all $a\in G$. If $a=b$, then obviously also $H a=H b$, so $f$ is uniquely defined for all $a\in G$. Also, $f$ is surjective as for each $H a\in G\slash H$ there exists $a\in G$ such that $f(a)=H a$. That implies $G\slash H=\ran{f}$. Furthermore, $f(a b)=H(a b)$. By definition, $H a\cdot H b=H(a b)$, so we have $f(a b)=H a\cdot H b=f(a)\cdot f(b)$. Thus, $G\slash H$ is a homomorphic image of $H$.

Now, $\ker{f}=\{a\in G:\ f(a)=H\}=\{a\in G:\ H a=H\}$. By a previous proposition, from $H a=H$ it follows that $a\in H$. Therefore, $\ker{f}=\{a\in G:\ a\in H\}$. As $H\subseteq G$, we have $\ker{f}=\{a\in H\}=H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $H x,H y\in G\slash H$ and $m\in\Z$. Then:

\begin{enumerate}
\item $\left(H x\right)^{-1}=H\left(x^{-1}\right)$.
\item $\left(H x\right)^m=H\left(x^m\right)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} We have $H x\cdot H\left(x^{-1}\right)=H\left(x x^{-1}\right)=H e=H$. Therefore, $\left(H x\right)^{-1}=H\left(x^{-1}\right)$.

{\it Ad $2$.} First we will prove that $\left(H x\right)^n=H\left(x^n\right)$, for $n\in\N$. For $n=1$ we have $H x=H x$, which is true. Suppose the statement is valid for $n=k$, i.e. $\left(H x\right)^k=H\left(x^k\right)$. Now let us prove that it is valid for $n=k+1$. We have $\left(H x\right)^{k+1}=\left(H x\right)^k\cdot H x$. By using assumption of induction we have $\left(H x\right)^{k+1}=H\left(x^k\right)\cdot H x$. By definition of coset multiplication we have $\left(H x\right)^{k+1}=H\left(x^k x\right)=H\left(x^{k+1}\right)$. Therefore, statement is true for all $n\in\N$. Now, if $m=0$, we have $\left(H x\right)^0=H=H e=H\left(x^0\right)$. Now, if $m=-n$, we have $\left(H x\right)^{-n}=\left(\left(H x\right)^{-1}\right)^n=\left(H\left(x^{-1}\right)\right)^n=H\left(\left(x^{-1}\right)^n\right)=H\left(x^{-n}\right)$. So, statement is true for all $m\in\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} In each of the following excercises $G$ is a group and $H$ a normal subgroup of $G$. List the elements of $G\slash H$ and then write the table of $G\slash H$.

\begin{enumerate}
\item $G=\Z_6$ and $H=\{0,3\}$;
\item $G=\Z_10$ and $H=\{0,5\}$;
\item $G=S_3$ and $H=\{\epsilon,\beta,\delta\}$;
\item $G=D_4$ and $H=\{e,b^2\}$;
\item $G=D_4$ and $H=\{e,b^2,a,a b^2\}$;
\item $G=\Z_4\times\Z_2$ and $H=\cyc{(0,1)}$;
\item $G=\mathcal{P}_3$ and $H=\{\emptyset,\{1\}\}$;
\item $G=\R\times\R$ and $H=\{(x,0):\ x\in\R\}$;
\item $G=\R\times\R$ and $H=\{(x,y):\ y=-x\}$;
\item $G=\R\times\R$ and $H=\{(x,y):\ y=2x\}$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $G=\Z_6$ and $H=\{0,3\}$.} We have $H=H+0=H+3=\{0,3\}$ and $H+1=H+4=\{1,4\}$, $H+2=H+5=\{2,5\}$. Then, $G\slash H=\{H,H+1,H+2\}$. Notice that, by a previous proposition, as $G\slash H$ is a homomorphic image of $G$, and $G$ is cyclic, then also $G\slash H$ is cyclic. Obviously, $G\slash H=\cyc{H+1}$ as $[H+1]+[H+1]=H+2$ and $[H+1]+[H+1]+[H+1]=[H+2]+[H+1]=H+3=H$. To conclude, as $G\slash H$ has three elements and is cyclic, it is, by a previous theorem, isomorphic to $\Z_3$, that is $\Z_6\slash\{0,3\}\cong\Z_3$.

\item {\it $G=\Z_{10}$ and $H=\{0,5\}$.} We have $H=H+5=\{0,5\}$, $H+1=\{1,6\}$, $H+2=\{2,7\}$, $H+3=\{3,8\}$ and $H+4=\{4,9\}$. Using the fact that $H$ is normal subgroup of $G$ and has a well-defined coset addition, we have $H+6=[H+1]+[H+5]=[H+1]+H=H+1$, $H+7=[H+2]+[H+5]=[H+2]+H=H+2$, $H+8=[H+3]+[H+5]=[H+3]+H=H+4$ and $H+9=[H+4]+[H+5]=[H+4]+H=H+4$. Notice that if we observe a homomorphism $f:G\rightarrow G\slash H$, where $f(x)=H+x$, then $\ker{f}=\{0,5\}=H=H+5$. As in the previous example, generator of $G$ is $1$ and then generator of $G\slash H$ is $f(1)=H+1$. We could also see that $\Z_{10}\slash\{0,5\}\cong\Z_5$. Notice also that, as $G\slash H$ is a family of cosets of $G$, it is also a partition of $G$. As $H=2$, then $[G:H]=\frac{|G|}{|H|}=\frac{10}{2}=5$.

\item {\it $G=S_3$ and $H=\{\epsilon,\beta,\delta\}$.} Refer to page $185$. By $|H|=3$ and $\left|S_3\right|=3!=6$ we have $[S_3:H]=\frac{6}{2}=3$. So there are two different cosets of $H$ in $S_3$. We have $H=H\epsilon=H\beta=H\delta=\{\epsilon,\beta,\delta\}$ and $H\alpha=\{\alpha,\gamma,\kappa\}=H\gamma=H\kappa$. Using the definition (and of course, that we can as $H$ is normal) we get $H\alpha\cdot H\alpha=H(\alpha\alpha)=H\epsilon=H$, the multiplication table is:

\begin{center}
\begin{tabular}{c|cc}
$\cdot$ & $H$ & $H\alpha$\\
\hline
$H$ & $H$ & $H\alpha$\\
$H\alpha$ & $H\alpha$ & $H$\\
\end{tabular}
\end{center}

\item {\it $G=D_4$ and $H=\{e,b^2\}$.} Refer to page $72$. From $\left|D_4\right|=8$ and $|H|=2$ we have $|G\slash H|=\frac{8}{2}=4$. We have $H=H(b^2)=\{e,b^2\}$, $H a=H(a b^2)=\{a,a b^2\}$, $H b=H(b^3)=\{b,b^3\}$, $H(a b)=H(a b^3)=\{a b,a b^3\}$. Notice that $H a\cdot H a=H(a^2)=H e=H$, $H b\cdot H a=H(b a)=H(a b^3)=H(a b)$, etc. The multiplication table for $G\slash H$ is:

\begin{center}
\begin{tabular}{c|cccc}
$\cdot$ & $H$ & $H a$ & $H b$ & $H(a b)$\\
\hline
$H$ & $H$ & $H a$ & $H b$ & $H(a b)$\\
$H a$ & $H a$ & $H$ & $H(a b)$ & $H b$\\
$H b$ & $H b$ & $H(a b)$ & $H$ & $H a$\\
$H(a b)$ & $H(a b)$ & $H b$ & $H a$ & $H$\\
\end{tabular}
\end{center}

\item {\it $G=D_4$ and $H=\{e,b^2,a b,a b^3\}$.} Here $|G\slash H|=2$. We have $H=H(b^2)=H(a b)=H(a b^3)$, $H a=\{b,b^3,a b^2,a\}=H(a b^2)=H(b^3)=H b$. As $a^2=e$, then $H a\cdot H a=H(a^2)=H$ and therefore:

\begin{center}
\begin{tabular}{c|cc}
$\cdot$ & $H$ & $H a$\\
\hline
$H$ & $H$ & $H a$\\
$H a$ & $H a$ & $H$\\
\end{tabular}
\end{center}

\item {\it $G=\Z_4\times\Z_2$ and $H=\cyc{(0,1)}$.} We have only $H=\{(0,1),(0,0)\}$ and $|H|=2$. Also, $G=\{(0,0),(0,1),(1,0),(1,1),(2,0),(2,1),(3,0),(3,1)\}$ and $|G|=8$. We will have $\left|G\slash H\right|=\frac{8}{2}=4$. So, $H=H+(0,0)=H+(0,1)$, $H+(1,0)=H+(1,1)=\{(0,1),(1,1)\}$, $H+(2,0)=H+(2,1)=\{(2,0),(2,1)\}$, $H+(3,0)=H+(3,1)=\{(3,0),(3,1)\}$. The addition table is:

\begin{center}
\begin{tabular}{c|cccc}
$+$ & $H$ & $H+(1,0)$ & $H+(2,0)$ & $H+(3,0)$\\
\hline
$H$ & $H$ & $H+(1,0)$ & $H+(2,0)$ & $H+(3,0)$\\
$H+(1,0)$ & $H+(1,0)$ & $H+(2,0)$ & $H+(3,0)$ & $H$\\
$H+(2,0)$ & $H+(2,0)$ & $H+(3,0)$ & $H$ & $H+(1,0)$\\
$H+(3,0)$ & $H+(3,0)$ & $H$ & $H+(1,0)$ & $H+(2,0)$\\
\end{tabular}
\end{center}

\noindent\newline If we observe $f:G\slash H\rightarrow\Z_4$ we will see that, with $f(H+(x,0))=x$, $f$ is an isomorphism from $G\slash H$ to $\Z_4$. In other words, $\Z_4\times\Z_2\slash\cyc{(0,1)}=\Z_4$. Actually, we have eliminated $\Z_2$ from the direct product by taking $(0,1)$ (as $0$ is a neutral element for $\Z_4$ and $1$ a generator for $\Z_2$, it left all elements of $\Z_4$ intact while sweeping across all of $\Z_2$).

\item {\it $G=\mathcal{P}_3$ and $H=\{\emptyset,\{1\}\}$.} We have $\left|\mathcal{P}_3\right|=2^3=8$ (refer to page $18$) and $|H|=2$, so $|G\slash H|=4$. First we have $H=H\Delta\{0\}=H\Delta\{1\}$, $H\Delta\{2\}=H\Delta\{1,2\}=\{\{2\},\{1,2\}\}$, $H\Delta\{3\}=H\Delta\{1,3\}=\{\{3\},\{1,3\}\}$ and $H\Delta\{2,3\}=H\Delta\{1,2,3\}=\{\{1,2,3\},\{2,3\}\}$. The multiplication table is:

\begin{center}
\begin{tabular}{c|cccc}
$\cdot$ & $H$ & $H\Delta\{2\}$ & $H\Delta\{3\}$ & $H\Delta\{2,3\}$\\
\hline
$H$ & $H$ & $H\Delta\{2\}$ & $H\Delta\{3\}$ & $H\Delta\{2,3\}$\\
$H\Delta\{2\}$ & $H\Delta\{2\}$ & $H$ & $H\Delta\{2,3\}$ & $H\Delta\{3\}$\\
$H\Delta\{3\}$ & $H\Delta\{3\}$ & $H\Delta\{2,3\}$ & $H$ & $H\Delta\{2\}$\\
$H\Delta\{2,3\}$ & $H\Delta\{2,3\}$ & $H\Delta\{3\}$ & $H\Delta\{2\}$ & $H$\\
\end{tabular}
\end{center}

\item {\it $G=\R\times\R$ and $H=\{(x,0):\ x\in\R\}$.} We have $G\slash H=\{H+(a,b):\ (a,b)\in\R\times\R\}$. As $H+(a,b)=\{(x+a,b):\ x\in\R\}=\{(x,b):\ x\in\R\}$, cosets geometrically represent lines parallel to the $x$ axis and the operation is addition which returns a line parallel to the $x$-axis, whose distance from the $x$-axis is the sum of distances from $x$-axis, of the two operands.

\item {\it $G=\R\times\R$ and $H=\{(x,y):\ y=-x\}$.} It's $G\slash H=\{H+(a,b):\ (a,b)\in\R\times\R\}$ and $H+(a,b)=\{(x+a,y+b):\ y=-x\}=\{(x,y):\ y-b=-x+a\}=\{(x,y):\ y=-x+a+b\}$, i.e. a collection of lines parallel with $y=-x$. Operation is similar to the operation above, but with distances measured from $y=-x$.

\item {\it $G=\R\times\R$ and $H=\{(x,y):\ y=2x\}$.} We have $G\slash H=\{H+(a,b):\ (a,b)\in\R\times\R\}$ with $H+(a,b)=\{(x,y):\ y-b=2x-2a\}=\{(x,y):\ y=2x+b-2a\}$, i.e. a collection of line parallel with. The operation is similar to the two previous operations.
\end{enumerate}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a normal subgroup of $G$. Then:

\begin{enumerate}
\item Every element of $G\slash H$ is its own inverse if and only if $x^2\in H$, for every $x\in G$.
\item The order of every element in $G\slash H$ is a divisor of $m$ if and only if $x^m\in H$, for every $x\in G$.
\item Every element of $G\slash H$ has a finite order if and only if for every $x\in G$ there exists $n\in\Z^{\ast}$ such that $x^n\in H$.
\item Every element of $G\slash H$ has a square root if and only if for every $x\in G$, there is some $y\in G$ such that $x y^2\in H$.
\item $G\slash H$ is cyclic if and only if there exists $a\in G$ such that for every $x\in G$ there exists $n\in\Z$ such that $x a^n\in H$.
\item If $G$ is an Abelian group, let $H_p$ be the set of all $x\in G$ whose order is a power of $p$. Then, $H_p$ is a normal subgroup of $G$ and $G\slash H_p$ has no elements whose order is a nonzero power of $p$.
\item If $G\slash H$ is Abelian, then $H$ contains all the commutators of $G$.
\item Let $K$ be a normal subgroup of $G$, and $H$ a normal subgroup of $K$. If $G\slash H$ is Abelian, then $G\slash K$ and $K\slash H$ are both Abelian.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Necessity.} Let $H x\in G\slash H$. Then, $H x\cdot H x=H$, i.e. $H(x^2)=H$, for all $x\in G$. As $x^2\in H(x^2)$ it is also in $H$. So we have $x^2\in H$, for all $x\in G$. {\it Sufficiency.} As $H$ is normal, then $G\slash H$ is a group and $H x\in G\slash H$, for all $x\in G$ Let $x^2\in H$, for every $x\in G$. Then we have\footnote{Remember that $H a=H$ if and only if $a\in H$.} $H(x^2)=H$. By definition $H x\cdot H x=H(x^2)$ and we have $H x\cdot H x=H$. As $H\in G\slash H$ is a neutral element in $G\slash H$, $H x$ is its own inverse for all $x\in G$.

{\it Ad $2$.} {\it Necessity.} Let $\ord{H x}|m$, for every $H x\in G\slash H$. That means that there exists $k\in\Z$ such that $m=k\ord{H x}$. From that follows $\left(H x\right)^m=\left(H x\right)^{k\ord{H x}}=\left(\left(H x\right)^{\ord{H x}}\right)^{k}=e^k=e$. As, by definition, $\left(H x\right)^m=H(x^m)$, $x^m\in H(x^m)$ and $\left(H x\right)^m=H$ (so $H=H(x^m)$), we have $x^m\in H$, for all $x\in G$. {\it Sufficiency.} Let $x^m\in H$, for all $x\in G$. As $H$ is normal, $G\slash H$ is a group and $H x\in G\slash H$ for all $x\in G$. As $x^m\in H$, for all $x\in G$, then $H=H(x^m)$. By definition, $H(x^m)=\left(H x\right)^m$, so $H=\left(H x\right)^m$. As $H$ is a neutral element in $G\slash H$, we have $\ord{H x}|m$, for all $x\in G$.

{\it Ad $3$.} {\it Necessity.} Suppose that for all $H x\in G\slash H$ there exists $n\in\N$ such that $\ord{H x}=n$. That implies $\left(H x\right)^n=H$, i.e. $H(x^n)=H$. As $x^n\in H(x^n$, then also $x^n\in H$. In other words, there exists $n\in\N\subseteq\Z^{\ast}$, i.e. $\ord{H x}$, such that $x^n\in H$. {\it Sufficiency.} As $H$ is normal, we have $G\slash H$ is a group and $H x\in G\slash H$, for all $x\in G$. Suppose that for each $x\in G$ there exists $n\in\Z^{\ast}$ such that $x^n\in H$ and it follows that $H(x^n)=H$, for all $x\in G$. By definition, $\left(H x\right)^n=H$ and then there also exists $m\in\N$ such that $\left(H x\right)^m=H$. Therefore, $0<\ord{H x}\leq m$; in other words, order of each $H x\in G\slash H$ is finite.

{\it Ad $4$.} {\it Necessity.} Suppose that every $H x\in G\slash H$ has a square root, i.e. there exists $H y\in G\slash H$ such that $H x=\left(H y\right)^2$. That gives us $H x=H(y^2)$. As $x\in H x$, then also $x\in H(y^2)$, i.e. there exists $h\in H$ such that $x=h y^2$. Multiplying that by $y^{-2}$ on the right gives us $x y^{-2}=h\in H$. We can take $z\in G$ such that $z=y^{-1}$, so $z^2=y^{-2}$. Therefore, for all $x\in G$ there exists $z\in G$ such that $x z^2\in H$. {\it Sufficiency.} Let for all $x\in G$ exist $y^2\in G$ such that $x y^2\in H$. As $x y^2\in H$, then also $x z^{-2}\in H$, where $z^{-1}=y$. As $x z^{-2}\in H$, it follows that $H\left(x z^{-2}\right)=H$. By definition, $H x\cdot H\left(z^{-2}\right)=H$, i.e. $H x\cdot\left(H z\right)^{-2}=H$. If we multiply that equality by $\left(H z\right)^2\in G\slash H$ on the right, we get $H x=H\cdot\left(H z\right)^2$. As $H$ is a neutral element in $G\slash H$, we have $H x=\left(H z\right)^2$. In other words, for all $x\in G$ there exists $z\in G$ such that $H x=\left(H z\right)^2$. By that, every element in $G\slash H$ has a square root.

{\it Ad $5$.} {\it Necessity.} Suppose $G\slash H$ is cyclic. Then, $G\slash H=\cyc{H a}$, for some $a\in G$. Also, any $H x\in G\slash H$ can be written as a power of $H a$, i.e. $H x=\left(H a\right)^k=H\left(a^k\right)$, for some $k\in\{0,1,\ldots,\ord{H x}-1\}$. As $H x=H\left(a^k\right)$, by a previous proposition it follows that $x a^{-k}\in H$. If we take $n=-k$, then we have that for all $x\in G$ there exists $n\in\Z$ such that $x a^n\in H$. {\it Sufficiency.} Let $a\in G$. Suppose that for all $x\in G$ there exists $n\in\Z$ such that $x a^n\in H$. From that we have $H x=H\left(a^{-n}\right)$, that is $H x=\left(H a\right)^{-n}$. If we take $k=-n$ we have that for all $x\in G$ there exists $k\in\Z$ such that $H x=\left(H a\right)^{k}$, for some $a\in G$. As $H x,H a\in G\slash H$ and $G\slash H$ is a group, that means that $G\slash H=\cyc{H a}$.

{\it Ad $6$.} Let $G$ be an Abelian group, $p\in P$ and

\begin{equation*}
H_p=\{a\in G:\ \left(\exists k\in\N_0\right)\left(\ord{a}=p^k\right)\}.
\end{equation*}

\noindent\newline From definition we have $H_p\subseteq G$. If we take $a,b\in H_p$, then $\ord{a}=p^m$ and $\ord{b}=p^n$, for some $m,n\in\N_0$. Let $l=\lcm{\ord{a},\ord{b}}=p^{\min{\{m,n\}}}$. Let us denote $i=\min{\{m,n\}}$. Then, as $\ord{a}|l$ and $\ord{b}|l$, we have $a^l b^l=e$. From that, as $G$ is Abelian, $\left(a b\right)^l=e$. That implies $\ord{a b}|l$, i.e. $\ord{a b}|p^i$. In other words, there exists $q\in\N$ (as order is positive integer) such that $p^i=q\ord{a b}$. Let, by fundamental theorem of arithmetic, $q=q_1\cdots q_s$, for some $s\in\N$, where $q_j\in P$, for $j\in\{1,\ldots,s\}$. We have $p^i=q_1\cdots q_s\ord{a b}$. Dividing by $q_1,\ldots,q_s$ give us:

\begin{equation*}
\ord{a b}=\frac{p^i}{q_1\cdots q_s}.
\end{equation*}

\noindent\newline As $q_j\in P$ and $\ord{a b}\in\N$, there is only one possibility, and that is $q_j=p$, for all $j\in\{1,\ldots,s\}$. So we have:

\begin{equation*}
\ord{a b}=\frac{p^i}{p^s}=p^{i-s}.
\end{equation*}

\noindent\newline Also, notice that it is necessary that $i-s\geq 0$. So, order of $a b$ is a power of $p$ and $a b\in H_p$. Also, if we had $a\in H_p$ with $\ord{a}=p^m$, where $m\in\N_0$, then, as $\ord{a^{-1}}=\ord{a}$, we have $\ord{a^{-1}}=p^m$ and $a^{-1}\in H_p$. Now, let $x\in G$. If we took $y\in H_p$, we have $\ord{y}=p^m$, for some $m\in\N_0$. Let $k=\ord{y}$. Now, $x x^{-1}=e$ and $y^k=e$, we have $x y^k x^{-1}=e$. From a previous proposition, $x y^k x^{-1}=\left(x y x^{-1}\right)^k$ and we have $\left(x y x^{-1}\right)^k=e$. Therefore, $\ord{x y x^{-1}}|p^m$. That implies, following the same line of reasoning as for $\ord{a b}$, that order of $x y x^{-1}$ is a power of $p$ and $x y x^{-1}\in H_p$. Thus, $H_p$ is a normal subgroup of $G$. From that follows that $G\slash H_p$ is a quotient group.

Assume there exists $H_p x\in G\slash H_p$ such that $\ord{H_p x}=p^k$, for some $k\in\N$ and $x\in G$. Let us denote $l=p^k$. Then, $\left(H_p x\right)^l=H_p$, i.e. $H_p\left(x^l\right)=H_p$. That would mean that $x^l\in H_p$ and from that $\ord{x^l}=p^n$, for some $n\in\N_0$. From a previous proposition,

\begin{equation*}
\ord{x^l}=\frac{\lcm{\ord{x},p^m}}{p^m}.
\end{equation*}

\noindent\newline Therefore, as $\ord{x^l}=p^n$, we have:

\begin{equation*}
p^n=\frac{\lcm{\ord{x},p^m}}{p^m}.
\end{equation*}

\noindent\newline That equality multiplied by $p^m$ is equivalent to $p^{n+m}=\lcm{\ord{x},p^m}$. Let $l=\lcm{\ord{x},p^m}$. That means that $l=p^{n+m}$ and, as $\ord{x}|\lcm{\ord{x},p^m}$, that $\ord{x}|p^m$. That would imply that $\ord{x}$ is a power of $p$ and $x\in H_p$. But, that means that $G\slash H=\{H_p\}$. Then, as $H_p$ is a neutral element, it must be $\ord{H_p}=1=p^0$, which is contrary to our assumption that there exists $H_p x\in G\slash H$ whose order is a non-zero power of $p\in P$.

{\it Ad $7$.} Suppose $G\slash H$ is an Abelian group. Then, $H x\cdot H y=H y\cdot H x$, for all $H x,H y\in G\slash H$. If we multiply that equality by $\left(H y\right)^{-1}$ on the right and then by $\left(H x\right)^{-1}$ on the right, we have $H=H y\cdot H x\cdot\left(H y\right)^{-1}\cdot\left(H x\right)^{-1}$. By definition, that is equivalent to $H=H\left(y x y^{-1} x^{-1}\right)$. By a previous proposition, that implies $y x y^{-1} x^{-1}\in H$, for all $x,y\in H$. In other words, $H$ contains all commutators of $G$.

{\it Ad $8$.} Let $H$ be a normal subgroup of $K$ and $K$ a normal subgroup of $G$. Then, $H$ is a normal subgroup of $G$. Suppose $G\slash H$ is Abelian. By the previous property, $H$ contains all commutators of $G$. As $H$ is a subset of $K$, then $K$ also contains all commutators of $G$, i.e. $x y x^{-1} y^{-1}\in K$, for all $x,y\in G$. That implies, by a previous proposition, that $K\left(x y x^{-1} y^{-1}\right)=K$. As $G\slash K$ is a quotient group, by definition we have $K x\cdot K y\cdot K x^{-1}\cdot K y^{-1}=K$, for all $x,y\in G$. Multiplying by $K y\cdot K x$ on the right, we have $K x\cdot K y=K y\cdot K x$, for all $x,y\in G$. Thus, $G\slash K$ is Abelian. Similarly, as $H$ contains all commutators of $G$, and $K$ is a subset of $G$, then it also contains all the commutators of $K$. In the same line of reasoning, $K\slash H$ is also Abelian.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Group $G$ is called a {\bf $p$-group} if the order of all elements of $G$ is a power of $p$.

\noindent\newline{\bf Proposition.} Let $G$ be a group, and $H$ a normal subgroup of $G$. Then:

\begin{enumerate}
\item If every element of $G\slash H$ has a finite order, and every element of $H$ has finite order, then every element of $G$ has finite order.
\item If $G$ is Abelian, and if every element of $G\slash H$ has a square root, and every element of $H$ has a square root, then every element of $G$ has a square root.
\item Let $p\in P$. If $G\slash H$ and $H$ are $p$-groups, then $G$ is a $p$-group.
\item If $G\slash H$ and $H$ are finitely generated, then $G$ is finitely generated.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Suppose that for all $H x\in G\slash H$ exists $k\in\N$ such that $\ord{H x}=k$. Furthermore, assume that for all $h\in H$ there exists $k\in\N$ such that $\ord{h}=k$. Let $a\in G$. Then there exists $H a\in G\slash H$ with $\ord{H a}=m$, for some $m\in\N$. So we have $\left(H a\right)^m=H$, i.e. $H\left(a^m\right)=H$. From that we have $a^m\in H$ and there exists $n\in\N$ such that $\ord{a^m}=n$. By a previous proposition:

\begin{equation*}
\ord{a^m}=\frac{\lcm{\ord{a},m}}{m}.
\end{equation*}

\noindent\newline But, as $\ord{a^m}=n$, we get $m n=\lcm{\ord{a},m}$. As $\ord{a}|\lcm{\ord{a},m}$, there exists $k\in\N$ (as $m$ is order of $H a$) such that $\lcm{\ord{a},m}=k\ord{a}$. From that follows $m n=k\ord{a}$, and from that $\ord{a}=\frac{m n}{k}$. As $m$ and $n$ are finite by assumption, and $k\in\N$, then $\frac{m n}{k}$ is finite and so is $\ord{a}$, for all $a\in G$.

{\it Ad $2$.} Let $x\in G$. Then there exists $H x\in G\slash H$. Then, as all elements in $G\slash H$ have a square root, there exists $H y\in G\slash H$, for some $y\in G$, such that $H x=\left(H y\right)^2$, i.e. $H x=H\left(y^2\right)$. But, that implies $x y^{-2}\in H$. So, as all elements in $H$ have a square root, there exists $h\in H$ such that $x y^{-2}=h^2$. If we multiply that equality by $y^2$ on the right, we have $x=h^2 y^2$. As $x\in G$ (and by that also $h^2 y^2\in G$) and $G$ is Abelian, we have $x=\left(h y\right)^2$. As $h\in H\subseteq G$ and $y\in G$ and $G$ is a group, then $h y\in G$. We can denote $h y=z$. Therefore, for all $x\in G$ there exists $z\in G$ such that $x=z^2$, i.e. all elements in $G$ have a square root.

{\it Ad $3$.} Let $x\in G$. Then there exists $H x\in G\slash H$. As $G\slash H$ is a $p$-group, we have $\ord{H x}=m$, where $m=p^k$, for some $k\in\N_0$. Thus, $\left(H x\right)^m=H$, that is $H\left(x^m\right)=H$. From that we have $x^m\in H$. As $H$ is a $p$-group, then $\ord{x^m}=n$, where $n=p^l$, for some $l\in\N_0$. By a previous proposition:

\begin{equation*}
\ord{x^m}=\frac{\lcm{\ord{x},m}}{m}.
\end{equation*}

\noindent\newline Multiplying by $m$ and using the fact that $\ord{x^m}=p^l$ and $m=p^k$ we get:

\begin{equation*}
p^{l+k}=\lcm{\ord{x},p^l}.
\end{equation*}

\noindent\newline As $\ord{x}|\lcm{\ord{x},p^l}$, i.e. $\ord{x}|p^{l+k}$, then, following the similar reasoning as in a previous proposition, we have $\ord{x}=p^s$, where $s\in\N_0$, for all $x\in G$.

{\it Ad $4$.} Let $x\in G$. Then there exists $H x\in G\slash H$. As $G\slash H$ is finitely generated, then there exists a family $S=\{H s_1,\ldots,H s_m\}$, for some $m\in\N$, family $T=S\cup\{\left(H s\right)^{-1}\in G:\ H s\in S\}$ and a function $f:T^k\rightarrow G\slash H$, where $k\in\N$ defined with:

\begin{equation*}
f\left(H t_1,\ldots,H t_n\right)=\prod_{i=1}^{k}{\left(H t_i\right)}.
\end{equation*}

\noindent\newline But, as $G\slash H$ is a group with coset multiplication such that $H x\cdot H y=H(x y)$, then:

\begin{equation*}
\prod_{i=1}^{k}{\left(H t_i\right)}=H\left(\prod_{i=1}^{k}{t_i}\right).
\end{equation*}

\noindent\newline From that we have $f\left(H t_1,\ldots,H t_n\right)=H\left(\prod_{i=1}^{k}{t_i}\right)$. In addition, for every element $H x\in G\slash H$ there exists $k\in\N$ and $X\in T^k$ such that $f(X)=H x$. Also, as $H$ is finitely generated then there exists $U=\{u_1,\ldots,u_n\}$, for some $n\in\N$, $V=U\cup\{u^{-1}\in G:\ u\in U\}$ and $g:V^k\rightarrow H$, where $k\in\N$ defined with:

\begin{equation*}
g(v_1,\ldots,v_k)=\prod_{i=1}^{k}{v_i}.
\end{equation*}

\noindent\newline Also, for every element $h\in H$ there exists $l\in\N$ and $Y\in V^k$ such that $g(Y)=h$. Let $x\in G$, then there exists $H x\in G\slash H$, with forementioned property. As $f(X)=H x$, i.e. $H\left(\prod_{i=1}^{k}{t_i}\right)=H x$, then, by a previous proposition $\left(\prod_{i=1}^{k}{t_i}\right) x^{-1}\in H$. Let $\left(\prod_{i=1}^{k}{t_i}\right) x^{-1}=h$. As $h\in H$, then it has a forementioned property with $g(Y)=h$, i.e. there exists $l\in\N$ such that $g(v_1,\ldots,v_l)=h$. But, $g(v_1,\ldots,v_l)=\prod_{i=1}^{l}{v_i}$, so:

\begin{equation*}
\left(\prod_{i=1}^{k}{t_i}\right) x^{-1}=\prod_{i=1}^{l}{v_i}.
\end{equation*}

\noindent\newline Multiplying the above equation by $x$ on the right and $\left(\prod_{i=1}^{l}{v_i}\right)^{-1}=\prod_{i=l}^{1}{\left(v_i^{-1}\right)}$ on the left, we have:

\begin{equation*}
x=\left(\prod_{i=1}^{k}{t_i}\right)\left(\prod_{i=l}^{1}{\left(v_i^{-1}\right)}\right).
\end{equation*}

\noindent\newline Therefore, each $x\in G$ can be shown as a finite product of $t_i\in T$ and $v_i\in V$, i.e. a finite product of $g\in T\cup V$. As $T$ and $V$ are finite, then also is $T\cup V$ and that implies that $G$ is finitely generated.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $H$ a normal subgroup of $G$. Then,

\begin{enumerate}
\item For each $a\in G$, the order of the element $H a\in G\slash H$ is a divisor of the order of $a\in G$.
\item If $[G:H]=m$, the order of every element of $G\slash H$ is a divisor of $m$.
\item If $[G:H]=p$, for $p\in P$, then the order of every element\footnote{Be careful not to confuse $G\slash H$ which is a quotient group and $G\backslash H$ which is set difference, i.e. the set containing all elements of $G$ which are not in $H$.} $a\in G\backslash H$ is a multiple of $p$.
\item If $G$ has a normal subgroup of index $p$, where $p\in P$, then $G$ has at least one element of order $p$.
\item If $[G:H]=m$, then $a^m\in H$ for every $a\in G$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a\in G$ and $\ord{a}=m$. Then there exists $H a\in G\slash H$. From $\ord{a}=m$ we have $a^m=e$. As $H$ is a normal subgroup of $G$, it must be that $e\in H$, that is $a^m\in H$. That implies $H\left(a^m\right)=H$. By a previous proposition that is equivalent to $\left(H a\right)^m=H$. As $H$ is a neutral element in $G\slash H$, then $\ord{H a}|m$, i.e. $\ord{H a}|\ord{a}$.

{\it Ad $2$.} Let $[G:H]=m$, i.e. the number of different cosets of $H$ in $G$ equals $m$. But, $G\slash H$ contains, by definition, all the cosets of $H$ in $G$. From that it follows $\left|G\slash H\right|=m$. By corollary of Lagrange's theorem, order of all $H a\in G\slash H$ divide order of $G\slash H$; in other words, $\ord{H a}|m$, for all $a\in G$.

{\it Ad $3$.} Let $[G:H]=p$ and $a\in G\backslash H$ with $\ord{a}=m$. As $a\in G$, then there exists $H a\in G\slash H$. We have $\ord{H a}|\ord{a}$, i.e. $\ord{H a}|m$. From that we have that there exists $q\in\N$ such that $m=\ord{H a} q$. Also, as $\ord{H a}|p$, we have that either $\ord{H a}=1$ or $\ord{H a}=p$. Suppose $\ord{H a}=1$. That would mean that $H a=H$ which would impliy $a\in H$ bringing us to a contradiction with $a\in G\backslash H$. So it can only be $\ord{H a}=p$ and we have $m=p q$, i.e. $\ord{a}=p q$, for some $q\in\N$.

{\it Ad $4$.} Let $[G:H]=p$ and $a\in G$. If it were $G\backslash H=\emptyset$, we would have that $G=H$ and that would mean $[G:H]=\frac{|G|}{|H|}=\frac{|G|}{|G|}=1$, but $1\notin P$. Therefore, there must exist $a\in G\backslash H$ and from a previous proposition, $\ord{a}=p q$, for some $q\in\N$, for all $a\in G\backslash H$. Suppose $a^p\neq e$, for all $a\in G\backslash H$. That would mean that $a^p\notin H$, for all $a\in G\backslash H$. As $a^p\notin H$, then $a^p\in G\backslash H$ and we have that $\ord{a^p}=p r$, for some $r\in\N$. But, also:

\begin{equation*}
\ord{a^p}=\frac{\ord{a}}{\gcd{\left(\ord{a},p\right)}}=\frac{p q}{\gcd{(p q,p)}}=\frac{p q}{p}=q.
\end{equation*}

\noindent\newline As $\ord{a^p}=q$, we have that $p$ and $q$ are relatively prime, which is a contradiction to the previous conclusion that $\ord{a^p}=p r$. Therefore, there must exist some $a\in G\backslash H$ such that $a^p=e$, i.e. $\ord{a}=p$.

{\it Ad $5$.} Suppose $[G:H]=m$. Then, $\ord{H a}|m$ and we have $\left(H a\right)^m=H$. That is equivalent to $H\left(a^m\right)=H$, meaning $a^m\in H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $C=\{a\in G:\ \left(\forall x\in G\right)\left(x a=a x\right)\}$ center of $G$. If $G\slash C$ is cyclic, then $G$ is Abelian.

\noindent\newline{\bf Proof.} We already proved that $C$ is a normal subgroup of $G$. Suppose $G\slash C=\cyc{C x}$ for some $x\in G$. Then, if we take $a,b\in G$, there exist $m,n\in\left\{0,1,\ldots,\ord{C x}-1\right\}$ such that $C a=\left(C x\right)^m=C\left(x^m\right)$ and $C b=\left(C x\right)^n=C\left(x^n\right)$. Then, by previous proposition, $a x^{-m},b x^{-n}\in C$, that is, there exist $c_1,c_2\in C$ such that $a x^{-m}=c_1$ and $b x^{-n}=c_2$. Multiplying those two equalities by $x^m$ and $x^n$, respectivily, we get $a=c_1 x^m$ and $b=c_2 x^n$. Then, $a b=c_1 x^m c_2 x_n$. As $c_1 x=x c_1$ and $c_2 x=x c_2$, for all $x\in G$, we can rearrange those elements to get $a b=x^m c_1 c_2 x_n=x^m c_2 c_1 x_n$, that is $a b=c_2 x^m x^n c_1$. As $x^m x^n=x^{m+n}=x^{n+m}=x^n x^m$, we have $a b=c_2 x^n x^m c_1$, and from that $a b=c_2 x^n c_1 x^m=b a$. Therefore, $a b=b a$, for all $a,b\in G$ and $G$ is Abelian.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group. Then:

\begin{enumerate}
\item $G\slash G=\{G\}$ with $[G:G]=1$.
\item $G\slash\{e\}=\{\{a\}:\ a\in G\}$ with $[G:\{e\}]=|G|$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} First, $G$ is a subgroup of $G$ as $a b\in G$, for all $a,b\in G$ and $a^{-1}\in G$, for all $a\in G$, by group axioms. Also if we take $x\in G$ and $y\in G$, then $y^{-1}\in G$, and also $x y^{-1}\in G$. But, also $y x y^{-1}\in G$. Therefore, $G$ is a normal subgroup of $G$. Now, we will prove that $G a=G$, for all $a\in G$. If we take $g a\in G a$, for some $g\in G$, then also $g a\in G$ ($G$ is a group and closed with respect to products), so $G a\subseteq G$. If we take $g\in G$, we have $g=g e=g(a^{-1} a)$ which implies $(g a^{-1})a\in G$. But, also $(g a^{-1})a\in G a$, i.e. $g\in G a$. Therefore, $G\subseteq G a$. From that we have $G=G a$. So, $G\slash G=\{G\}$. The number of different cosets is obviously $1$ so $[G:G]=1$.

{\it Ad $2$.} We have that $\{e\}$ is a trivial subgroup of $G$. It is also normal as $x e x^{-1}=e\in\{e\}$, for all $x\in G$. Now, let $E=\{e\}$. Then $E a=\{e a:\ e\in E\}=\{a\}$, for all $a\in G$. Therefore, $G\slash\{e\}=\{\{a\}: a\in G\}$. $G\slash\{e\}$ contains each $a\in G$ (altough embedded in a set) so $[G:\{e\}]=|G|$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $C=\{y\in G:\ (\forall x\in G)(x y=y x)\}$ be center of group $G$ and $[x]=\{y x y^{-1}\in G:\ y\in G\}$ conjugacy class for $x\in G$. Then, $a\in C$ if and only if $\left|[a]\right|=1$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $a\in C$. Then, for all $x\in G$, $x a=a x$. That is, for all $x\in G$, $a=x a x^{-1}$. We have $[a]=\{x a x^{-1}\in G:\ x\in G\}=\{a\}$. Therefore, $\left|[a]\right|=1$. {\it Sufficiency.} Suppose $\left|[a]\right|=1$. Now, taking that fact with $a\in [a]$, then $[a]=\{a\}$. That means that, for all $x\in G$, $x a x^{-1}=a$. In other words, for all $x\in G$, $x a=a x$ and it must be that $a\in C$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a finite group, $C$ center of a group and

\begin{equation*}
\mathcal{K}=\{[x]:\ \left(x\in G\right)\wedge\left(\left|[x]\right|\neq 1\right)\}.
\end{equation*}

\noindent\newline Then, $\mathcal{K}\cup\{C\}$ is a partition of $G$ and

\begin{equation*}
|G|=|C|+\sum_{K\in\mathcal{K}}{|K|}.
\end{equation*}

\noindent\newline{\bf Proof.} If we take $x\in G$, then $x\in[x]$. If $|[x]|\neq 1$, then $x\in[x]\in\mathcal{K}\cup\{C\}$ such that $x\in[x]$. If $|[x]|=1$, then, by a previous proposition, $x\in C\in\mathcal{K}\cup\{C\}$. Let $x_1,x_2\in G$. Then, there exist $S_1,S_2\in\mathcal{K}\cup\{C\}$ such that $x_1\in S_1$ and $x_2\in S_2$. Then, either $S_i=[x_i]$, or $S_i=C$, for $i\in\{1,2\}$. Suppose $x_1=x_2$. That way, $[x_1]=[x_2]$. If $|[x_1]|=|[x_2]|=1$, we have $x_1,x_2\in C$ and $S_1=C=S_2$. If $|[x_1]|,|[x_1]|\neq 1$, then simply, $S_1=[x_1]=[x_2]=S_2$. Therefore, $\mathcal{K}\cup\{C\}$ is a partition of $G$. The number of elements of $G$ is equal to the sum of elements in $C$ and sums of elements in each conjugacy class whose order is not $1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From now on, we will denote $A-B=A\backslash B$ to aviod confusion with quotient group $A\slash B$.

\noindent\newline{\bf Lemma.} Let $p\in P$ and $k\in\N$. Then, $p\nmid(p^k-1)$.

\noindent\newline{\bf Proof.} Suppose $p|(p^k-1)$. Then, $p^k-1=p q$, for some $q\in\Z$. That implies $p^k-p q=1$ and $p(p^{k-1}-q)=1$. That would mean that $p=1$ and $(p^{k-1}-q)=1$. Yet, $p\in P$ and $1\notin P$, so $p\notin P$ which is a contradiction to our assumption.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $k\in\N-\{0\}$. Let $G$ be a group with $|G|=p^k$, where $p\in P$. If $C$ is a center of $G$, then $|C|=p^l$, where $l\in\N-\{0\}$ and $l\leq k$.

\noindent\newline{\bf Proof.} From a previos proposition we have $|G|=|C|+\sum_{K\in\mathcal{K}}{|K|}$, where $\mathcal{K}$ is a family of conjugacy classes whose order is not $1$. Then, due to a previous proposition, each conjugacy class divides order of $|G|$. Also $|C|$ divides $|G|$ and it must be that $|C|=p^l$, where $l\in\N$ and $|K|=p^{f(K)}$, for all $K\in\mathcal{K}$. So, we have:

\begin{equation*}
p^k=p^l+\sum_{K\in\mathcal{K}}{p^{f(K)}}.
\end{equation*}

\noindent\newline Let $m=\min{\{l\}\cup\{f(K):\ K\in\mathcal{K}\}}$. Then,

\begin{equation*}
p^k=p^m\left(p^{l-m}+\sum_{K\in\mathcal{K}}{p^{f(K)-m}}\right).
\end{equation*}

\noindent\newline As $k\in\N-\{0\}$, we have $p^k\neq 1$. Therefore, it must be that $p^m\neq 1$ or $p^{l-m}+\sum_{K\in\mathcal{K}}{p^{f(K)-m}}\neq 1$. The sum could equal $1$ only if $p^m=p^k$ and if it had no conjugacy classes with $p^{l-m}=1$ (as order of center can never be zero). But, that would mean that $l=m$ and we would have $p^k=p^l$, so $l=k$ and $|C|=p^l$, where $l\in\N-\{0\}$. Now, if it were that $p^m=1$, the sum would have to equal $p^k$. But, as $p^m=1$, i.e. $m=0$ it would mean that either $f(K)=0$ for some $K\in\mathcal{K}$ or $|C|=1$. It cannot be that $f(K)=0$, for some $K\in\mathcal{K}$ as then that conjugacy class would have order $1$ and its representative would be in $C$, contradicting our assumption. But, if it were that $|C|=1$, we would have $p^k=1+p S$, where $S$ is the sum of conjugacy class orders (we can factor out $p$ as all $p^{f(K)}\neq 1$; that would put them in center of the group). Then, $p^k-1=p S$ would mean $p|(p^k-1)$ which is, by a previous lemma, impossible. Therefore, all conjugacy classes must contain more that one element and so does center and we have $|C|=p^l$, $l\in\N-\{0\}$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group with $|G|=p$, where $p\in P$. Then, $G$ is cyclic.

\noindent\newline{\bf Proof.} Let $a\in G$, $a\neq e$. Then, due to corollary of Lagrange's theorem, we have $\ord{a}|p$. So, either $\ord{a}=1$ or $\ord{a}=p$. But, only a neutral element has order $1$ ($a^1=e$ would imply $a=e$) so $\ord{a}=p$. We have, by a previous proposition, $|\cyc{a}|=\ord{a}=p$. As also $\cyc{a}\subseteq G$, it follows that $\cyc{a}=G$, i.e. $G$ is cyclic with generator $a$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group with $|G|=p^2$, where $p\in P$. Then, $G$ is Abelian.

\noindent\newline{\bf Proof.} By a previous proposition, either $|C|=p$ or $|C|=p^2$. If it were that $|C|=p^2$, we would have $|C|=|G|$ and, as $C\subseteq G$, that $C=G$. From that, as $C$ is Abelian, $G$ is Abelian. Now, assume $|C|=p$. Then, as $C$ is a normal subgroup of $G$, we have $\left|G\slash C\right|=[G:C]=\frac{p^2}{p}=p$. From that we have, by a previous lemma, that $G\slash C$ is cyclic. From a previous proposition it follows that $G$ is Abelian.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group with $|G|=p^2$, where $p\in P$. Then, either $G\cong\Z_{p^2}$ or $G\cong\Z_{p}\times\Z_{p}$.

\noindent\newline{\bf Proof.} By corollary of Lagrange's theorem, orders of elements in a group divide order of $G$. Therefore, for all $a\in G$, $\ord{a}\in\{1,p,p^2\}$. Only one element has order $1$ and that is $e$, leaving $p^2-1$ other elements in disposal. Now, suppose there exists $a\in G$ such that $\ord{a}=p^2$. Then, $|\cyc{a}|=p^2$. So, as $|\cyc{a}|=|G|$ and $\cyc{a}\subseteq G$, we have $G=\cyc{a}$, i.e. $G$ is cyclic group of order $p^2$. By a previous theorem, $G\cong\Z_{p^2}$.

If there does not exist $a\in G$ such that $\ord{a}=p^2$, there must be $p^2-1$ elements of order $p$. If we take $a\in G$, $a\neq e$, we have $\cyc{a}=p$. Then $|G-\cyc{a}|=p^2-p$, i.e. $G-\cyc{a}\neq\emptyset$. Then we can take $b\in G-\cyc{a}$. Again, it must be that $\ord{b}=p$ and we have $|\cyc{b}|=p$. Both $\cyc{a}$ and $\cyc{b}$ are normal subgroups of $G$. If it were that $b^k\in\cyc{a}$, we would have $\cyc{a}=\cyc{b^k}$ (as, by a previous proposition, $\ord{b^k}=p$ and $\ord{a}=p$ implies $\cyc{b^k}=\cyc{a}$). But, as $b^k\in\cyc{b}$ and $\ord{b^k}=\ord{b}$ we have $\cyc{b^k}=\cyc{b}$, i.e. $\cyc{a}=\cyc{b}$. So, if we take $b\in\cyc{b}$ then there exists $a^l\in\cyc{a}$ such that $b=a^l$. But, that would mean that $b\in\cyc{a}$ which is contrary to our assumption that $b\in G-\cyc{a}$. Therefore, $\cyc{a}\cap\cyc{b}=\{e\}$. Now, let us observe $\cyc{a}\cyc{b}=\{a^k b^l\in G:\ k,l\in[0,\ldots,p-1]\cap\Z\}$. Obviously, $\cyc{a}\cyc{b}\subseteq G$ (as it contains only elements from $G$). And, as $a^k\neq b^l$, for all $k\in[1,\ldots,p-1]\cap\Z$ and $l\in[0,\ldots,p-1]$, we have $(p-1)p=p^2-p$ different elements of the form $a^k b^l$. But if we allow $k=0$, we have $e b^l$, where there are $p$ elements, we have $\left|\cyc{a}\cyc{b}\right|=p^2-p+p=p^2$. Therefore, we have $\cyc{a}\cyc{b}=G$. By a previous proposition, we have $G\cong\cyc{a}\times\cyc{b}$. But, as $|\cyc{a}|=|\cyc{b}|=p$ then $\cyc{a}\cong\cyc{b}\cong\Z_p$. So we have $G\cong\Z_p\times\Z_p$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} In $\Q\slash\Z$, every element has finite order.

\noindent\newline{\bf Proof.} As $\Q$ is Abelian, then $\Z$ is a normal subgroup of $\Q$ (as it also satisfies $a+b\in\Z$ for all $a,b\in\Z$ and $-a\in\Z$ for all $a\in\Z$). Let $\Z+q\in\Q\slash\Z$. Let $q\in\Q$. Then there exists $\Z+q=\{z+q\in\Q:\ z\in\Z\}$. Suppose that for all $n\in\N$ we have $n\left(\Z+q\right)\neq\Z$. That expression is equivalent to $\Z+n q\neq\Z$. That would impliy that $n q\notin\Z$, for all $q\in\Q$. But, as $n\in\N$, and $\Q=\{\frac{m}{n}:\ m\in\Z,\ n\in\N\}$, there exists $q=\frac{m}{n}\in\Q$, where $m\in\Z$. Thus, $n\frac{m}{n}=m\in\Z$. Therefore, there exists $q\in\Q$ such that $n q\in\Z$, which is a contradiction to our assumption that for all $n\in\N$ we have $n q\notin\Z$. So there must exist $n\in\N$ such that $n\left(\Z+q\right)=\Z$. In other words, all $\Z+q\in\Q\slash\Z$ are of finite order.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf The fundamental homomorphism theorem}
\end{center}

\vskip 0.5cm

\noindent{\bf Remark.} From now on, we will denote homomorphism from $G$ to $H$ with kernel $K$ as $f:\homo{G}{H}{K}$.

\noindent\newline{\bf Lemma.} Let $G$ be a group and $H$ a subgroup of $G$. Let $f:\homo{G}{H}{K}$ be a homomorphism. Then $f(a)=f(b)$ if and only if $K a=K b$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $f(a)=f(b)$. Then, $f(a)\left[f(b)\right]^{-1}=e$. As $f$ is homomorphism, we have $f(a)f(b^{-1})=f(a b^{-1})=e$. But, that means that $a b^{-1}\in K$. From a previous proposition, as kernel is a normal subgroup of $G$, that implies $K a=K b$. {\it Sufficiency.} Let $K a=K b$. Then, by a previous proposition $a b^{-1}\in K$, i.e. $f(a b^{-1})=e$. As $f$ is a homomorphism, we have $f(a)f(b^{-1})=f(a)\left[f(b)\right]^{-1}=e$. Multiplying by $f(b)$ on the right gives us $f(a)=f(b)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (FHT).} Let $f:\homo{G}{H}{K}$ be a homomorphism, where $G$ is a group and $H$ a subgroup of $G$. Suppose\footnote{If it is not, we can always disregard elements in $H$ that have no originals by selecting $f:\homo{G}{\ran{f}}{K}$.} $\ran{f}=H$. Then, $H\cong G\slash K$.

\noindent\newline{\bf Proof.} As $K$ is a normal subgroup of $G$, quotient group $G\slash K$ is well defined. Let us define a mapping $g:G\slash K\rightarrow H$ with $g(K x)=f(x)$. As $K x\in G\slash H$ if and only if $x\in G$, then $g$ is defined for $x\in G$ as $f$ is defined for all $x\in G$ ($f$ is a function). If $K x_1=K x_2$, then, by a previous lemma, $f(x_1)=f(x_2)$, i.e. $g(K x_1)=g(K x_2)$. Therefore, $g$ satisfies property of uniqueness and is a function. {\it Injectivity.} Let $g(K x_1)=g(K x_2)$. Then, $f(x_1)=f(x_2)$. Previous lemma then implies $K x_1=K x_2$. {\it Surjectivity.} As $H=\ran{f}$, we can take $f(x)\in H$, where $x\in G$. As $x\in G$, there exists $K x\in G\slash K$. Therefore $g(K x)=f(x)$. Thus, $g$ is a bijection. Now, $g((K x_1)(K x_2))=g(K(x_1 x_2))=f(x_1 x_2)$. As $f$ is a homomorphism, $f(x_1 x_2)=f(x_1)f(x_2)=g(K x_1)g(K x_2)$. Then, $g$ is an isomorphism and $H\cong G\slash K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ and $H$ be groups and $K\trianglelefteq G$. If $G\slash K\cong H$, then there exists a homomorphism $f:\homo{G}{H}{K}$.

\noindent\newline{\bf Proof.} As $G\slash K\cong H$, then there exists an isomorphism $g:G\slash K\rightarrow H$. Also, by a previous proposition, $G\slash K$ is a homomorphic image of $G$, so there exists a surjective homomorphism $k:\homo{G}{G\slash K}{K}$ with $k(x)=K x$. As $g$ and $k$ are well defined functions with $\dom{g}=\cod{k}$, then there exists a well-defined function $g\circ k:G\to H$. We will show that $g\circ k$ is a homomorphism. We have $[g\circ k](x y)=g(k(x y))=g(K(x y))=g((K x)(K y))=g(K x)g(K y)=g(k(x))g(k(y))=[g\circ k](x)[g\circ k](y)$. Therefore, $g\circ k$ is a homomorphism (and it is surjective, as projective homomorphism $k$ is surjective and isomorphism $g$ is surjective). Now, $\ker{[g\circ k]}=\{x\in G:\ [g\circ k](x)=e\}=\{x\in G:\ g(K x)=e\}$. But, $g(K x)=e$ implies that $K x\in\ker{g}$. But, as $g$ is an isomorphism, kernel of $g$ contains only one element, and that is the neutral element in $G\slash K$ and that is $K$. In other words $\ker{g}=\{K\}$. So, if $K x\in\ker{g}$, then $K x\in\{K\}$ and it has to be $K x=K$. From that we have $x\in K$. So, $\ker{[g\circ k]}=\{x\in G:\ x\in K\}=K$. Thus, we can take $f(x)=[g\circ k](x)$, for all $x\in G$ and we have $f:\homo{G}{H}{K}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Map $f:\homo{\zmod{(n k)}}{\zmod{k}}{\cyc{n k\Z+k}}$ defined with $f\left(n k\Z+x\right)=k\Z+x$ is a homomorphism. Also, $\left(\zmod{n k}\right)\slash\cyc{n k\Z+k}=\Z\slash k\Z$.

\noindent\newline{\bf Proof.} Obviously, $f$ is defined for all $n k\Z+x\in\Z\slash n k\Z$. If we take $n k\Z+x=n k\Z+y$, then $x\equiv y\pmod n k$. But, that implies that $x\equiv y\pmod k$, so $k\Z+x=k\Z+y$. Therefore, $f$ is well-defined. Now, $f\left(\left(n k\Z+x\right)+\left(n k\Z+y\right)\right)=f\left(n k\Z+(x+y)\right)=k\Z+(x+y)$. By definition of coset multiplication (here addition), $k\Z+(x+y)=\left(k\Z+x\right)+\left(k\Z+y\right)=f\left(n k\Z+x\right)+f\left(n k\Z+y\right)$.

Now, $\ker{f}=\left\{n k\Z+x\in\Z\slash n k\Z:\ f\left(n k\Z+x\right)=k\Z\right\}$. As $f\left(n k\Z+x\right)=k\Z+x$, $k\Z+x=k\Z$ implies $x\in k\Z$, i.e. $x=k m$, for some $m\in\Z$. In other words, $k|x$. Therefore, $\ker{f}=\{n k\Z+x\in\Z\slash n k\Z:\ k|x\}$. If we take $n k\Z+k m\in\ker{f}$, then $n k\Z+k m=m\left(n k\Z+k\right)$. That is, $n k\Z+k m$ is a "power" (in additive notation) of $n k\Z+k$, so $n k\Z+k m\in\left\langle n k\Z+k\right\rangle$. That means that $\ker{f}\subseteq\left\langle n k\Z+k\right\rangle$. If we take $m\left(n k\Z+k\right)\in\cyc{n k\Z+k}$, then, $m\left(n k\Z+k\right)=n k\Z+k m$. So, $m\left(n k\Z+k\right)\in\ker{f}$. From that we have $\cyc{n k\Z+k}\subseteq\ker{f}$ and $\ker{f}=\cyc{n k\Z+k}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From the following theorem, we have, adopting less formal notations, that e.g. $\Z_{20}\slash\cyc{5}=\Z_5$, $\Z_6\slash\cyc{2}=\Z_2$, $\Z_6\slash\cyc{3}=\Z_3$, etc.

\noindent\newline{\bf Problem.} Let $\alpha:\mathcal{F}(\R)\rightarrow\R$ be defined by $\alpha(f)=f(1)$ and let $\beta:\mathcal{F}\rightarrow\R$ be defined by $\beta(f)=f(2)$.

\begin{enumerate}
\item Prove that $\alpha$ and $\beta$ are homomorphisms from $\mathcal{F}(\R)$ onto $\R$;
\item Let $J=\{f\in\mathcal{F}(\R):\ f(1)=0\}$ and $K=\{f\in\mathcal{F}(\R):\ f(2)=0\}$. use the FHT to prove that $\R\cong\mathcal{F}\slash J$ and $\R\cong\mathcal{F}\slash K$.
\end{enumerate}

\noindent{\bf Solution.} {\it Ad $1$}. As $\mathcal{F}(\R)$ contains all functions that are defined on $\R$ and so in $1,2\in\R$, then $\alpha$ and $\beta$ are also defined. Same goes for uniqueness. Then, $\alpha(f+g)=[f+g](1)$. By definition, $[f+g](x)=f(x)+g(x)$, for all $x\in\R$. Then, $\alpha(f+g)=f(1)+g(1)=\alpha(f)+\alpha(g)$. Same goes for $\beta$, i.e. they are both homomorphisms from $\mathcal{F}(\R)$ onto $\R$. {\it Ad $2$.} We have $\ker{\alpha}=\{f\in\mathcal{F}(\R):\ \alpha(f)=0\}=\{f\in\mathcal{F}(\R):\ f(1)=0\}$. Similarly, $\ker{\beta}=\{f\in\mathcal{F}(\R):\ f(2)=0\}$. We see that $\ker{\alpha}=J$ and $\ker{\beta}=K$. By FHT, we have $\R\cong\mathcal{F}\slash J$ and $\R\cong\mathcal{F}\slash K$.

\noindent\newline{\bf Proposition.} Let $G$ be an Abelian group. Let $H=\{x^2:\ x\in G\}$ and $K=\{x\in G:\ x^2=e\}$. Then, $H\cong G\slash K$.

\noindent\newline{\bf Proof.} We have $H\subseteq G$ by definition. Then, if we take $x^2,y^2\in H$, then $x^2,y^2\in G$ and, as $G$ is Abelian, $x^2 y^2=(x y)^2$. Therefore, as $(x y)^2\in H$, also $x^2 y^2\in H$. Also, as $x^2\in H$ then $x^2\in G$ and there exists $x^{-2}\in G$ such that $x^2 x^{-2}=e$. But, $x^{-2}=\left(x^{-1}\right)^2$, so $\left(x^{-1}\right)^2\in H$. Thus, $H$ is a subgroup of $G$ and by a previous proposition, as $G$ is Abelian, then $H$ is a normal subgroup of $G$. Let $f:G\rightarrow H$ be a mapping with $f(x)=x^2$. Then, $f$ is obviously a well-defined function. We have $f(x y)=(x y)^2$. As $G$ is Abelian, so is $H$ and $(x y)^2=x^2 y^2=f(x)f(y)$, for all $x,y\in G$. Therefore, $f$ is a homomorphism and $\ker{f}=\{x\in G:\ f(x)=e\}=\{x\in G:\ x^2=e\}=K$. By FHT we have $G\slash K\cong H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group, $I(G)=\left\{\phi_a\in\Aut{G}:\ a\in G\right\}$, where $\phi_a(x)=a x a^{-1}$, and let $C$ be the center of $G$. Then, $I(G)\cong G\slash C$.

\noindent\newline{\bf Proof.} We have already proved that $\Aut{G}$ is a group (page $139$) and that $\phi_a$ is an automorphism (page $138$). Also, $C$ is a normal subgroup of $G$. Let's prove that $I(G)$ is a subgroup of $\Aut{G}$. By definition, $I(G)\subseteq\Aut{G}$. Then, if we take $\phi_a,\phi_b\in I(G)$, we have $\phi_a(\phi_b(x))=\phi_a(b x b^{-1})=a b x b^{-1} a^{-1}=\phi_{a b}(x)$. So, as $\phi_{a b}\in I(G)$, also $\phi_a\phi_b\in I(G)$. We have $a^{-1} a x a^{-1} a=x$ and that means that $\phi_{a^{-1}}(\phi_a(x))=x$ (and $x=e x e^{-1}$), i.e. $\phi_{a^{-1}}\phi_a=\phi_e$. From that we have $\phi_{a^{-1}}=[\phi_a]^{-1}$. Therefore, $\phi_{a^{-1}}\in I(G)$ and so $[\phi_a]^{-1}\in I(G)$. Thus, $I(G)$ is a subgroup of $\Aut(G)$.

Let $f:G\rightarrow I(G)$ such that $f(a)=\phi_a$, for all $a\in G$. Mapping is thus defined for all $a\in G$. Assume $a=b$. Then, $a x=b x$, for all $x\in G$. As $a=b$, then also $a^{-1}=b^{-1}$. From that we get $a x a^{-1}=b x b^{-1}$, i.e. $\phi_a(x)=\phi_b(x)$, for all $x\in G$. That means that $\phi_a=\phi_b$ (as they also have the same domain and the same codomain). From that follows $f(a)=f(b)$. Therefore, $f$ satisfies property of uniqueness. Now, $f(a b)=\phi_{a b}$. But, $\phi_{a b}(x)=a b x (a b)^{-1}=a b x b^{-1} a^{-1}=a \phi_{b}(x) a^{-1}=\phi_{a}\left(\phi_b(x)\right)$, for all $x\in G$. So, $\phi_{a b}=\phi_a\phi_b$ and we have $f(a b)=\phi_{a b}=\phi_a\phi_b=f(a)f(b)$ and $f$ is a homomorphism.

Now, $\ker{f}=\{a\in G:\ f(a)=\phi_e\}$. That means, using $f(a)=\phi_a$ and $\phi_e(x)=x$, for all $x\in G$, that $\ker{f}=\{a\in G:\ (\forall x\in G)(\phi_a(x)=x)\}$. Furthermore, as $\phi_a(x)=a x a^{-1}$, we have $\ker{f}=\{a\in G:\ (\forall x\in G)(a x a^{-1}=x)\}$. When we multiply the equality $a x a^{-1}=x$ by $a$ on the right, we get $a x=x a$. So, $\ker{f}=\{a\in G:\ (\forall x\in G)(a x=x a)\}$, and that is obviously the definition of the center $C$ of $G$ and we have $\ker{f}=C$. By FHT, we have $G\slash C\cong I(G)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ and $H$ be groups and $J$ and $K$ normal subgroups of $G$ and $H$, respectively. Then, $(G\times H)\slash(J\times K)\cong(G\slash J)\times(H\slash K)$.

\noindent\newline{\bf Proof.} We have already proved that $G\times H$ and $J\times K$ are groups as $G$, $H$, $J$ and $K$ are groups. We want a homomorphism from $(G\times H)$ to $(G\slash J)\times (H\slash K)$. We will also want to map from $G$ to $G\slash J$ and from $H$ to $H\slash K$ (those quotient groups exist as $J$ and $K$ are normal). We will use a sort of projective homomorphism, i.e. $f: G\times H\rightarrow (G\slash J)\times (H\slash K)$ defined with $f(x,y)=(J x, K y)$. Then, $f$ is defined for all $(x,y)\in G\times H$. Also, $(x,y)=(z,w)$ implies $x=z$ and $y=w$. So, $J x=J z$ and $K y=K w$. From definition of ordered pair, we have $(J x,K y)=(J z,K w)$. Therefore $f$ satisfies property of uniqueness. Then, $f((x,y)(z,w))=f(x z,y w)=(J(x z),K(y w))$. From definition of coset multiplication, $(J(x z),K(y w))=(J x\cdot J z,K y\cdot K w)$. By definition of direct product, $(J x\cdot J z,K y\cdot K w)=(J x,K y)(J y,K w)=f(x,y)f(y,w)$. Thus, $f$ is a homomorphism.

Now, $\ker{f}=\{(x,y)\in G:\ f(x,y)=(J,K)\}$. As $f(x,y)=(J x,K y)$, then $\ker{f}=\{(x,y)\in G:\ (J x,K y)=(J,K)\}$. From $(J x,K y)=(J,K)$ we get $J x=J$ and $K y=K$. From that we gat $x\in J$ and $y\in K$, respectively. Therefore, $\ker{f}=\{(x,y)\in G:\ (x\in J)\wedge(y\in K)\}$. That yields, by definition of Cartesian product, $\ker{f}=J\times K$. Thus, by FHT, we get $(G\times H)\slash(J\times K)\cong(G\slash J)\times(H\slash K)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Second isomorphism theorem.} Let $G$ be a group, $H$ and $K$ subgroups of $G$. Also, let $H$ be a normal subgroup of $G$. Then, $K\slash(H\cap K)\cong H K\slash H$.

\noindent\newline{\bf Proof.} We have already proved that $H K$ is a subgroup of $G$. Now, we will prove that $H$ is a normal subgroup of $H K$. If we take $h\in H$, then $h e\in H K$, so $H\subseteq H K$. As $H$ is contained in $H K$ and both are subgroups of $G$, then $H$ is also a subgroup of $H K$, by previous proposition. Then, as $H$ is normal, we have that for all $x\in G$ and $h\in H$, $x h x^{-1}\in H$. As it is true for all $G$ and $H K\subseteq G$, then it is true for all $x\in H K$. So, $H$ is a normal subgroup of $H K$ and $H K\slash H$ is a well-defined quotient group. Then $H x\in H K\slash H$, for all $x\in H K$. Let us take $f:K\rightarrow H K\slash H$ with $f(x)=H x$, for all $x\in K$. So, if we take $x\in K$, there exists $e x\in H K$ such that $f(x)=H(e x)=H x$. Also, if $x=y$, then obviously $H x=H y$, for all $x,y\in K$. Also, if $x,y\in K$, then $f(x y)=H(x y)$. By definition of coset multiplication $H(x y)=H x\cdot H y$, so $f(x y)=f(x)f(y)$. Thus $f$ is a homomorphism from $K$ onto $H K\slash H$.

Then, $\ker{f}=\{x\in K:\ f(x)=H\}=\{x\in K:\ H x=H\}$. But, $H x=H$ implies $x\in H$, so $\ker{f}=\{x\in K:\ x\in H\}$. That is, by definition equal to $K\cap H$. Therefore, by FHT we have $H K\slash H\cong K\slash (H\cap K)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Cayley).} Let $H$ be a subgroup of $G$ and $\mathcal{H}=\{x H:\ x\in G\}$. If $H$ contains no normal subgroups of $G$, except for $\{e\}$, then $G\cong S_{\mathcal{H}}$, where $S_{\mathcal{H}}$ is a group of permutations on $\mathcal{H}$.

\noindent\newline{\bf Proof.} Let $\pi_a:\mathcal{H}\rightarrow\mathcal{H}$ be defined with $\pi_a(x H)=(a x)H$, for all $a\in G$. We will show that $\pi_a$ is a permutation of $\mathcal{H}$. First, it is defined for all $H x\in \mathcal{H}$, as $a x\in G$, for all $a,x\in G$. Assume $x H=y H$. If we take $a x h_1\in (a x)H$, then $a x h_1=h_2$, for some $h_2\in H$. That is equivalent to $a x h_1=h_2$. As $x h_1\in x H$, then there exists $y h_3\in y H$ such that $x h_1=y h_3$. So, $a y h_3=h_2$, i.e. $a y h_3\in (a y)H$. That is really $a x h_1\in(a y)H$ and from that we have $(a x)H\subseteq(a y)H$. If we take $a y h_1\in(a y)H$, then there exists $h_2\in H$ such that $a y h_1=h_2$. Again, as $x H=y H$ and $y h_1\in y H$, then there exists $h_3\in H$ such that $y h_1=x h_3$. We have $a x h_3=h_2$, i.e. $a x h_3\in(a x)H$ (which is really $a y h_1\in(a x)H$). That implies $(a y)H\subseteq(a x)H$ and we have $(a x)H=(a y)H$, i.e. from $x H=y H$ we get $\pi_a(x H)=\pi_a(y H)$. Therefore, $\pi_a$ is a well-defined function. {\it Injectivity.} Suppose $\pi_a(x H)=\pi_a(y H)$, i.e. $(a x)H=(a y)H$. Then, for all $a x h_1\in (a x)H$, there exists $a y h_2\in H$ such that $a x h_1=a y h_2$. But, that implies that for all $h_1\in H$ there exists $h_2\in H$ such that $x h_1=y h_2$, i.e. $x H\subseteq y H$. Then, for all $a y h_1\in H$ there exists $a x h_2\in(a x)H$ such that $a y h_1=a x h_2$. That implies that for all $h_1\in H$ there exists $h_2\in H$ such that $y h_1=x h_2$ and then $y H\subseteq x H$. That is, $x H=y H$. {\it Surjectivity.} If we take $y H\in\mathcal{H}$, then, as $y a^{-1}\in G$, we can take $(a^{-1} y)H\in\mathcal{H}$ so that $f((a^{1} y)H)=(a(a^{-1} y))H=y H$. Therefore, $\pi_a$ is bijective and, as $\dom{\pi_a}=\cod{\pi_a}=\mathcal{H}$, it is a permutation of $\mathcal{H}$.

As we have $S_{\mathcal{H}}=\{\pi_a:\ a\in G\}$, we will consider $f:G\rightarrow S_{\mathcal{H}}$, such that $f(a)=\pi_a$, for all $a\in G$. Obviously $f$ is defined for all $a\in G$ in this way. But, if $a=b$ and if we take $a x h_1\in (a x)H$, we have $a x h_1=h_2$, for some $h_2\in H$. But, as $a=b$, then $b x h_1=h_2$, so $a x h_1=b x h_1\in(b x)H$ and $(a x)H\subseteq(b x)H$. Now, if we take $b x h_1\in (b x)H$, we have $b x h_1=h_2$, i.e. $a x h_1=h_2$ for some $h_2\in H$. Then, $a x h_1\in(a x)H$ and $b x h_1\in(a x)H$. Thus, $(b x)H\subseteq(a x)H$ and $(a x)H=(b x)H$. Therefore, $f$ is well-defined. Then, $f(a b)=\pi_{a b}$. But, $\pi_{a b}(x H)=((a b)x H)=(a(b x)H)=a\pi_b(x H)=\pi_a(\pi_b(x H))$, for all $x H\in\mathcal{H}$. That means that $\pi_{a b}=\pi_a\pi_b$ and we have $f(a b)=\pi_{a b}=\pi_a\pi_b=f(a)f(b)$ and $f$ is a homomorphism from $G$ onto $S_{\mathcal{H}}$.

Now, $\ker{f}=\{a\in G:\ f(a)=\pi_e\}=\{a\in G:\ (\forall x H\in\mathcal{H})(\pi_a(x H)=x H)\}$. Now, as $\pi_a(x H)=(a x)H$, we have $\ker{f}=\{a\in G:\ (\forall x\in G)(a x H=x H)\}$. From $a x H=x H$ we conclude that $a x x^{-1}\in H$, i.e. $a\in H$. Therefore $\ker{f}=\{a\in H:\ (\forall x\in G)(a x H=x H)\}$. From $a x H=x H$, we have that for all $h_1\in H$ there exists $h_2\in H$ such that $a x h_1=x h_2$. But, that means that $x^{-1} a x=h_2 h_1^{-1}$. And, as $h_2 h_1^{-1}\in H$, we can say that $\ker{f}=\{a\in H:\ (\forall x\in G)(x^{-1} a x\in H)\}$. By FHT, we have $G\slash\ker{f}\cong S_{\mathcal{H}}$. But, if only $\ker{f}=\{e\}$, i.e. there are no $a\in H$ such that $(\forall x\in G)(x^{-1} a x\in H)$ (except for $e\in G$), then $G\slash\{e\}\cong S_{\mathcal{H}}$. But, we have shown that $G\slash\{e\}=\{\{a\}:\ a\in G\}$. Obviously, $G\slash\{e\}\cong G$, so, by transitivity of relation of isomorphism, we have $G\cong S_{\mathcal{H}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $\cis{\phi}:=\cos{\phi}+i\sin{\phi}$. Then $S'=\{\cis{\phi}\in\C:\ \phi\in\left[0,2\pi\right\rangle\}$ is the set of all complex numbers lying on the unit circle. Then, $S'$ with multiplication of complex numbers is a group and $S'\cong\R\slash\Z$.

\noindent\newline{\bf Proof.} We have already proved that $\C^{\ast}$ with multiplication is an Abelian group. Obviously $S'\subset\C^{\ast}$ (notice that $0\notin S'$). Then, if we take $\cis{x},\cis{y}\in S'$, we have, by an already familiar result from elementary algebra, $\cis{x}\cis{y}=\cis{x+y}$ and, as $\cis{x+y}\in S'$, then $\cis{x}\cis{y}\in S'$. If we take $\cis{x}\in S'$, then $\cis{x}\cis{2\pi-x}=\cis{2\pi}$. As $\cis{2\pi}=\cos{(2\pi)}+i\sin{(2\pi)}=1+i\cdot 0=1$, and as $1\in\C^{\ast}$ is a neutral element with respect to multiplication, then $\cis{-x}\in S'$ is an inverse of $\cis{x}$. Thus, $S'$ is a subgroup of $\C^{\ast}$ and a group by itself, when it comes to multiplication.

Now, we will consider $f:\R\rightarrow S'$ with $f(x)=\cis{k x}$, where $k\in\R$. Then, $f$ is defined for all $x\in\R$. Also, if $x=y$, then obviously $\cis{k x}=\cis{k y}$. Now, $f(x+y)=\cis{k(x+y)}=\cis{k x}\cis{k y}=f(x)f(y)$ and $f$ is a homomorphism. Also, $\ker{f}=\{x\in\R:\ f(x)=1\}=\{x\in\R:\ \cis{k x}=1\}$. When is $\cis{k x}=1$? Obviously, if and only if $\sin{k x}=0$ and $\cos{k x}=1$. That is true for $k x=2n\pi$, for all $n\in\Z$. So, $\cis{k x}=1$ when $x=\frac{2n\pi}{k}$, where $n\in\Z$, and we have $\ker{f}=\{\frac{2n\pi}{k}\in\R:\ n\in\Z\}$. By choosing $k=2\pi$, we have $\ker{f}=\{n\in\R:\ n\in\Z\}=\Z$. Then, by FHT, $S'\cong\R\slash\Z$. Similarly, if we chose $k=1$, we would get $\ker{f}=\{2n\pi\in\R:\ n\in\Z\}$. But, that is actually $\ker{f}=\cyc{2\pi}$, so $S'\cong\R\slash\cyc{2\pi}$, which makes sense as, by a previous theroem, both $\Z$ and $\cyc{2\pi}$ are cyclic groups of infinite order so $\Z\cong\cyc{2\pi}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From now on, we will introduce notation, $H\leq G$ will denote the fact that $H$ is a subgroup of $G$. Also, let $H\trianglelefteq G$ denote the fact that $H$ is a normal subgroup of $G$. It is a trivial fact that if $K\trianglelefteq H\trianglelefteq G$ that $K\triangleleft G$ and $H\triangleleft G$. Also we will denote coset multiplication $K x\cdot K y$ as $(K x)(K y)$.

\noindent\newline{\bf Third isomorphism theorem.} Let $G$ be a group and $K\trianglelefteq H\trianglelefteq G$. Then $H\slash K\leq G\slash K$ and

\begin{equation*}
\left(G\slash K\right)\slash\left(H\slash K\right)\cong G\slash H.
\end{equation*}

\noindent\newline{\bf Proof.} Let $f:G\slash K\rightarrow G\slash H$. Groups $G\slash K$ and $G\slash H$ are well defined quotient groups as $K\trianglelefteq G$ and $H\trianglelefteq G$. Let $f(K x)=H x$, for all $x\in G$. If we take $K x\in G\slash K$, then $x\in G$ and there exists $H x\in G\slash H$ such that $f(K x)=H x$. If $K x=K y$, by a previous theorem we have that $x y^{-1}\in K$. But, as $K\trianglelefteq H$, we have $x y^{-1}\in H$. From that we have $H x=H y$, i.e. $f(K x)=f(K y)$. Now, $f((K x)(K y))=f(K(x y))$, by definition of coset multiplication. Then, $f(K(x y))=H(x y)=(H x)(H y)=f(K x)f(H y)$. From that we have that $f$ is a homomorphism from $G\slash K$ onto $G\slash H$.

If we take $K x\in H\slash K$, then as $x\in H$, it is also in $G$ (as $H\subseteq G$). So, $K x\in G\slash K$ and $H\slash K\subseteq G\slash K$. As $H\slash K$ is itself a group, as is $G\slash K$, it follows that $H\slash K\leq G\slash K$.

Furthermore, $\ker{f}=\{K x\in G\slash K:\ f(K x)=H\}$, that is $\ker{f}=\{K x\in G\slash K:\ H x=H\}$. As $H x=H$ if and only if $x\in H$, then $\ker{f}=\{K x\in G\slash K:\ x\in H\}$. From that $\ker{f}=K\slash H$ (due to the fact that $H\slash K\subseteq G\slash K$). By FHT, $\left(G\slash K\right)\slash\left(H\slash K\right)\cong G\slash H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Correspondence theorem.} Let $f:G\longrightarrow_{K} H$ be a homomorphism. Also, let $S\leq H$ and $S^{\ast}=\{x\in G:\ f(x)\in S\}$. Then $S^{\ast}\leq G$ and $S\cong S^{\ast}\slash K$. In addition, if $S\trianglelefteq H$ then $S^{\ast}\trianglelefteq G$.

\noindent\newline{\bf Proof.} First, $S^{\ast}\subseteq G$. Then, $x,y\in S^{\ast}$ implies $f(x),f(y)\in S$. As $f(x)f(y)=f(x y)$ and $f(x y)\in S$ (it is a subgroup of $H$), it must be that $x y\in S^{\ast}$. Also, as $f(x)\in S$, then, as $S\leq H$, $[f(x)]^{-1}\in S$, i.e. $f(x^{-1})\in S$. So, $x^{-1}\in S^{\ast}$. Therefore, $S^{\ast}\leq G$. Let $g:S^{\ast}\rightarrow S$ be a mapping with $g(x)=f(x)$. Then, for $x\in S^{\ast}$ there exists $f(x)\in S$, by definition; so $f(x)=g(x)$ makes $g$ defined for all $x\in S^{\ast}$. Then, uniqueness follows from uniqueness of $f$. Also, $g$ is a homomorphism as $f$ is a homomorphism.

We have $\ker{f}=\{x\in S^{\ast}:\ g(x)=e\}$, i.e. $\ker{f}=\{x\in S^{\ast}:\ f(x)=e\}$. If we take $x\in K$, then $f(x)=e$ (and $e\in S$ as $S\leq H$) and $x\in G$, meaning $x\in S^{\ast}$ and, as $f(x)=e$, also $x\in\ker{f}$. Therefore, $K\subseteq\ker{f}$. Then, if we take $x\in\ker{f}$, then $x\in S^{\ast}$ and $f(x)=e$. As $S^{\ast}\leq G$ and $f(x)=e$, then $x\in K$. So, $\ker{f}\subseteq K$ and $\ker{f}=K$. We have, by FHT, $S^{\ast}\slash K\cong S$.

We have already proved that $S^{\ast}\leq G$ if $S\leq H$. Suppose $S\triangleleft H$. Now, if we take $g\in G$ and $x\in S^{\ast}$, then it must be $f(x)\in S\subseteq H$. As also $f(g)\in H$, then it must be that $f(g)f(x)[f(g)]^{-1}\in S$ (due to the fact that $S\trianglelefteq H$). But, $f(g)f(x)[f(g)]^{-1}=f(g x g^{-1})$ (due to $f$ being a homomorphism), so we have $f(g x g^{-1})\in S$. Then, by definition, $g x g^{-1}\in S^{\ast}$ from which follows that $S^{\ast}\trianglelefteq G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Cauchy's theorem.} Let $G$ be a group such that $|G|=m$ and $p\in P$ so that $p|m$. Then, there exists $a\in G$ such that $\ord{a}=p$.

\noindent\newline{\bf Proof.} First we will prove that the statement is true for Abelian groups. Let $p\in P$. Let $a\in G$ where $a\neq e$. Such element exists as $G$ cannot be a trivial group ($p>1$ so $|G|>1$). The statement holds for $|G|=p$ as then $G\cong\Z_p$, i.e. $G$ is cyclic of order $p$. Then there must exist $x\in G$ such that $G=\cyc{x}$. Then $\ord{x}=p$. The statement, specifically holds for $m=2$.

Now, assume that the following statement is true for all $2<n<m$: {\it if $G_n$ is an Abelian group with $|G_n|=n$ and $p|n$ then there exists $a\in G$ such that $\ord{a}=p$.}

We will prove that it is also true for $|G|=m$. Then, $p|m$ and there exists $q\in\Z$ such that $m=q p$. by Lagrange's theorem, it must be that $\ord{a}|q p$. We choose $a\neq e$ so that $\ord{a}\neq 1$. If $\ord{a}=p$, then we are done. If $\ord{a}=k p$, where $k|q$, then:

\begin{equation*}
\ord{a^k}=\frac{k p}{\gcd{(k p,k)}}=\frac{k p}{k}=p.
\end{equation*}

\noindent\newline Now, assume $\ord{a}=k$, where $k|q$. As $G$ is Abelian, then $\cyc{a}\trianglelefteq G$ and we have a well-defined quotient group $G\slash\cyc{a}$. Let us denote $\cyc{a}$ with $H$. Then,

\begin{equation*}
\left|G\slash H\right|=[G:H]=\frac{|G|}{|H|}=\frac{q p}{k}=\frac{q}{k} p.
\end{equation*}

\noindent\newline Let us denote $r=\frac{q}{k}$. As $a\neq e$, then $k\neq 1$ and $\frac{q}{k} p<q p$, i.e. $\left|G\slash H\right|<m$. Then, by assumption of induction there exists $H b\in G\slash H$ such that $\ord{H b}=p$. Then, $\left(H b\right)^p=H\left(b^p\right)=H$. From that we have $b^p\in H=\cyc{a}$. That means that for some $l\in\N$, $b^p=a^l$. As $\ord{a}=k$, $b^{p k}=a^{k l}$, i.e. $b^{k p}=e$. From that we have $\left(b^k\right)^p=e$ and, as $\ord{b^k}|p$, it can only be that either $\ord{b^k}=1$ or $\ord{b^k}=p$. If $\ord{b^k}=p$, we are done. However, if $\ord{b^k}=1$, then $b^k=e$ and we would have $\ord{b}|k$. Then, as there exists a projective homomorphism $f:G\rightarrow G\slash H$, with $f(x)=H x$, from $f(b^k)=f(e)$, (where we have $f(b^k)=H\left(b^k\right)=\left(H b\right)^k$ and $f(e)=H$) it follows that $\left(H b\right)^k=H$. Then, it must be that $\ord{H b}|k$, i.e. $p|k$. Then, $k=k' p$, for some $k'\in\Z$ and $\ord{b}=k' p$ gives us:

\begin{equation*}
\ord{b^{k'}}=\frac{k'p}{\gcd{(k' p,k')}}=\frac{k'p}{k'}=p.
\end{equation*}

\noindent\newline Thus we have proved that there exists an element of order $p$ in an Abelian group $G$ where $p$ divides order of $G$.

Suppose $G$ is not Abelian. As $G$ is not Abelian $C$ is a proper subgroup of $G$, i.e. $G-C\neq\emptyset$. Let us observe $C_a=\{x\in G:\ x a=a x\}$. We know that $C_a\leq G$, so $|C_a|$ divides order of $|G|=n p$. If $|C_a|=l p$, where $l|n$, then, as $C_a$ is Abelian, it must be that, by first part of the proof, there exists $a\in C_a\subseteq G$ such that $\ord{a}=p$ and we are done. But, assume $|C_a|=l$, for all $a\in G-C$, where $l|n$, i.e. there exists $q\in\N$ such that $n=q l$. Then, $|G|=|C_a|q p$. From that we have $[G:C_a]=\frac{|G|}{|C_a|}=\frac{|C_a|q p}{|C_a|}=q p$. Therefore, $p$ divides $\left|G\slash C_a\right|$. But, by a previous proposition, $|[a]|=[G:C_a]$. Therefore, $p$ divides conjugacy classes of all $a\in G-C$. Let us observe class equation $|C_a|q p=|C|+k_1+\cdots+k_t$, where $k_i$ is the order of $i$-th conjugacy class that is not of order $1$. As $k_i=p s_i$, for some $s_i\in\N$, we have $|C_a| q p-p(s_1+\cdots+s_t)=|C|$, i.e. $p(q|C_a|-(s_1+\cdots+s_t))=|C|$ and from that $p$ divides order of $C$. As $C$ is Abelian, then, by first part of the proof, there exists $a\in C$ such that $\ord{a}=p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group, $m,n\in\N$, $n<m$ and $|G|=p^m$. Then, there exists $H\trianglelefteq G$ such that\footnote{Notice that as $n<m$ then $p^n<p^m$, we have that $H$ is a proper subgroup of $G$. Also, obviously it has to be $m\geq 2$.} $|H|=p^n$.

\noindent\newline{\bf Proof.} If $|G|=p^2$, by previous lemma there exists $a\in G$ such that $\ord{a}=p$ and then $\left|\cyc{a}\right|=p$. Assume that the statement is true for all $1<k<m$. Let $n\in\N$ and $n<m$. By a previous proposition, as $G$ is a $p$-group, it has a non-trivial center $C$ of the order $p^l$, for some $0<l\leq m$. Then, $|C|=p p^{l-1}$, i.e. $p$ divides $|C|$. Also, as $C$ is Abelian, by Cauchy's theorem, we have that there exists $a\in C$ such that $\ord{a}=p$. Now, let us observe $\cyc{a}$ (notice that $\left|\cyc{a}\right|=p$). As $a\in C$, then $\cyc{a}\leq C$. But, as $C$ is Abelian, then $\cyc{a}\trianglelefteq C$. As $\cyc{a}$ is a normal subgroup of $C$, for all $x\in C\subseteq G$ and $y\in\cyc{a}$, we have $x^{-1} y x\in\cyc{a}$. It is obvious that $\cyc{a}\trianglelefteq G$. Let us observe $G\slash\cyc{a}$. We have $[G:\cyc{a}]=\frac{p^{m}}{p}=p^{m-1}$. By assumption, as $m-1<m$ and $n<m$, then $n\leq m-1$. But, surely then $n-1<m-1$, so there exists $H\trianglelefteq G\slash\cyc{a}$ such that $|H|=p^{n-1}$. Also, as $G\cyc{a}$ is a homomorphic image of $G$, there exists a projective homomorphism $f:G\rightarrow G\cyc{a}$ with $f(x)=\cyc{a}x$, for all $x\in G$. Obviously $H=\{f(x)\in H:\ x\in G\}\trianglelefteq\ran{f}$. Let us denote $H'=\{x\in G:\ f(x)\in H\}$. Then, by the correspondence theorem, $H'\trianglelefteq G$ and $H\cong H'\slash\ker{f}$. But, $\ker{f}=\{x\in G:\ \cyc{a}x=\cyc{a}\}=\{x\in G:\ x\in\cyc{a}\}=\cyc{a}$, i.e. $H\cong H'\slash\cyc{a}$. As $|H|=p^{n-1}$ and $H\cong H'\slash\cyc{a}$, then $\left|H'\slash\cyc{a}\right|=p^{n-1}$. Furthermore, $p^{n-1}=[H':\cyc{a}]=\frac{|H'|}{p}$, so $|H'|=p^n$. As also $H'\trianglelefteq G$, $G$ has a normal subgroup of order $p^n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group. For all $a\in G$, $\ord{a}=p^{k(a)}$, where $k(a)\in\N$, if and only if $|G|=p^m$ for some $m\in\N$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume for all $a\in G$ we have $\ord{a}=p^{k(a)}$. Then, as $\left|\cyc{a}\right|=p^{k(a)}$, and $\cyc{a}\leq G$, for all $a\in G$, by Lagrange's theorem, $p^{k(a)}$ divide order of $G$. Therefore, $|G|=p^k m$, for some $m\in\N$ where $k=\max{\{k(a):\ a\in G\}}$. Assume that $p\nmid m$. By fundamental theorem of arithmetic, $m$ can be written as a product of distinct powers of prime numbers $q\in P$ (and they are different from $p$). Then, as each $q$ divides $m$, they divide order of $G$. By Cauchy's theorem there must exist element of order $q$ for each $q\in P$, where $q\neq p$. But, $G$ has only elements of order $p^{k(a)}$, for all $a\in G$. Therefore, $p|m$, and $m=p r$, for some $r\in\N$. Then, as we cannot have infinite regression, $|G|=p^t$, for some $t\in\N$ and $t\geq k$.

{\it Sufficiency.} If $|G|=p^m$, suppose there exists some element $a\in G$ such that $\ord{a}=q$ and $q\neq p^l$, for all $l\in\N$. But, as $\cyc{a}\leq G$, by Lagrange's theorem it must be that $q|p^m$, which is possible only if $q=p^l$, for some $l\in\N$ and $l\leq m$, and that is contrary to our assumption.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Previous proposition actually proves equivalence of two definitions of $p$-groups. Therefore, either one we choose, we can freely interchange it with another.

\newpage

\begin{center}
{\bf Sylow's theorems}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $G$ be a group and $H\leq G$. Let $p\in P$. If $H$ is a $p$-group we say that $H$ is a {\bf $p$-subgroup} of $G$.

\noindent\newline{\bf Definition.} Let $G$ be a group, $p\in P$ and $H\leq G$ a $p$-subgroup of $G$. We say that $H$ is a {\bf $p$-Sylow subgroup} of $G$ if there does not exist $p$-subgroup $K\leq G$ such that $H\leq K$.

\noindent\newline{\bf Definition.} Let $G$ be a group acting on set $S$. Then,

\begin{equation*}
\Fix{S}{G}=\left\{s\in S:\ \left(\forall g\in G\right)\left(g.s=s\right)\right\}.
\end{equation*}

\noindent\newline{\bf Remark.} Notice that the difference from stabilizer of some element $s\in S$, that is $\Stab{s}{G}$ is that a point $s$ is in $\Fix{S}{G}$ if all elements of group $G$ hold it fixed (it cannot be moved to another point by any element of the group). Stabilizer of $s$ is, on the other hand, set of elements of $G$ which fix $s$. We will clarify that with the following proposition.

\noindent\newline{\bf Proposition.} Let $G$ be a group acting on set $S$. Then the following statements are equivalent:

\begin{enumerate}
\item $s\in\Fix{S}{G}$;
\item $\left|\Orb{s}{G}\right|=1$;
\item $\Orb{s}{G}=\{s\}$;
\item $\left|\Stab{s}{G}\right|=\left|G\right|$;
\item $\Stab{s}{G}=G$.
\end{enumerate}

\noindent{\bf Proof.} {\it $1$ implies $2$.} If $s\in\Fix{S}{G}$, then $g.s=s$, for all $g\in G$. We know that $s\in\Orb{s}{G}$ as $e.s=s$, so it must be $|\Orb{s}{G}|\geq 1$. Assume that $|\Orb{s}{G}|>1$. Then there exists $t\in\Orb{s}{G}$, $t\neq s$, such that $h.s=t$, for some $h\in G$. But, as $s\in\Fix{S}{G}$, then $h.s=s$ and we get $s=t$, which is contrary to our assumption that $s\neq t$ and that $|\Orb{s}{G}|>1$. It must be that $|\Orb{s}{G}|\leq 1$ and, combined with $|\Orb{s}{G}|\geq 1$ we have $|\Orb{s}{G}|=1$.

{\it $2$ implies $3$.} We have $|\Orb{s}{G}|=1$ and $s\in\Orb{s}{G}$, as $e.s=s$. Therefore, $\Orb{s}{G}=\{s\}$.

{\it $3$ implies $4$.} From orbit stabilizer theorem, $|G|=|\Orb{s}{G}|\cdot|\Stab{s}{G}|$. As $\Orb{s}{G}=\{s\}$, then $|\Orb{s}{G}|=1$. From that it follows $|G|=1\cdot|\Stab{s}{G}|=|\Stab{s}{G}|$.

{\it $4$ implies $5$.} We have $\left|\Stab{s}{G}\right|=|G|$. If we take $g\in\Stab{s}{G}$, then $g\in G$ such that $g.s=s$. As $g\in\Stab{s}{G}$ implies $g\in G$, then $\Stab{s}{G}\subseteq G$. That, and $\left|\Stab{s}{G}\right|=|G|$ implies $\Stab{s}{G}=G$.

{\it $5$ implies $1$.} As $\Stab{s}{G}=G$, then for all $g\in G$ there exists $h\in\Stab{s}{G}$ such that $g=h$. But, as $h\in\Stab{s}{G}$ implies $h.s=s$, and as $g=h$, it also implies that $g.s=s$. Therefore, for all $g\in G$, we have $g.s=s$ and from that, by definition, $s\in\Fix{S}{G}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Fixed point congruence).} Let $S$ be a set and $G$ a group acting on $S$ such that $|G|=p^m$, for some $m\in\N$ and $p\in P$. Then,

\begin{equation*}
\left|S\right|\equiv\left|\Fix{S}{G}\right|\pmod p.
\end{equation*}

\noindent\newline{\bf Proof.} Let for all $s,t\in S$, $s\sim t$ if and only if $|\Orb{s}{G}|=|\Orb{t}{G}|$. It is easy to see that $\sim$ is an equivalence relation. From orbit-stabilizer theorem we know that $|G|=|\Orb{s}{G}|\cdot|\Stab{s}{G}|$, that is $|\Orb{s}{G}|$ divides $|G|=p^m$. Our choices for sizes of orbits are in $C=\{1,p,p^2,\ldots,p^m\}$. Therefore, there are $m+1$ equivalence classes, disjoint and their union being all of $S$. Now, we know that $s\in\Fix{S}{G}$ if and only if $|\Orb{s}{G}=1|$, from the previous proposition. So, there will be $|\Fix{S}{G}|$ elements in the equivalence class where $|\Orb{s}{G}|=1$. For $|\Orb{s}{G}|=p^i$, $i\in\N$, we can assume there are $c(i)$ different orbits ($c:\N\rightarrow\N\cup\{0\}$). Therefore,

\begin{equation*}
|S|=|\Fix{S}{G}|\cdot 1+\sum_{i=1}^{m}{c(i)p^{i}}.
\end{equation*}

\noindent\newline As $i>1$, then $p^{i}=p\cdot p^{i-1}$ and we can write:

\begin{equation*}
|S|=|\Fix{S}{G}|+\sum_{i=1}^{m}{c(i)p\cdot p^{i-1}}=|\Fix{S}{G}|+p\sum_{i=1}^{m}{c(i)\cdot p^{i-1}}.
\end{equation*}

\noindent\newline Let us denote $\sum_{i=1}^{m}{c(i)\cdot p^{i-1}}=\Sigma$ for simplicity. Then, $|S|=|\Fix{S}{G}|+p\Sigma$. That is equivalent to $|S|-|\Fix{S}{G}|=p\Sigma$. From that we have that $p$ divides $|S|-|\Fix{S}{G}|$ and by definition of congruence that is equivalent to $|S|\equiv|\Fix{S}{G}|\pmod p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group, $H\leq G$ and $a\in G$. Let $H$ act on $G\slash H$ with $h.H a=H(a h)$. Then, $h.H a=H a$, for all $h\in H$, if and only if $H a\in\Norm{H}{G}\slash H$.

\noindent\newline{\bf Proof.} We can see that $H$ and $G\slash H$ satisfy axioms for group actions. We have $h_1.(h_2.H a)=h_1.H(a h_2)=H(a h_2 h_1)=(h_2 h_1).H a$, for all $h_1,h_2\in H$ and $H a\in G\slash H$. Also, $e.H a=H(a e)=H a$, for all $H a\in G\slash H$. {\it Necessity.} Let $a\in G$. Then $H a\in G\slash H$. We can see that $h.H a=H a$ is equivalent to $H(a h)=H a$, for all $h\in H$. That is equivalent to $a h a^{-1}\in H$, for all $h\in H$. That implies $a\in\Norm{H}{G}$. As $a\in\Norm{H}{G}$, then $H a\in\Norm{H}{G}\slash H$. {\it Sufficiency.} If we take $H a\in\Norm{H}{G}\slash H$, then $a\in\Norm{H}{G}$. That implies that $a h a^{-1}\in H$, for all $h\in H$. That is equivalent to $(a h)a^{-1}\in H$ which implies $H(a h)=H a$, for all $h\in H$. Then, as $h.H a=H(a h)$, we have $h.H a=H a$, for all $h\in H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group, $H\leq G$ with $|H|=p^m$, for some $m\in\N$ and $p\in P$. Then,

\begin{equation*}
[G:H]\equiv[\Norm{H}{G}:H]\pmod p.
\end{equation*}

\noindent\newline{\bf Proof.} Let $H$ act on the set of all right cosets of $H$ ($G\slash H$, but not necessarily a quotient group) with $h.H a=H(a h)$, for all $h\in H$ and $H a\in G\slash H$. By fixed point definition, we have:

\begin{equation*}
\Fix{G\slash H}{H}=\{H a\in G\slash H:\ (\forall h\in H)(h.H a=H a)\}.
\end{equation*}

\noindent\newline That implies that $h.H a=H a$ for all $h\in H$ if and only if $H a\in\Fix{G\slash H}{H}$. By previous lemma that is equivalent to $H a\in\Norm{H}{G}\slash H$ if and only if $H a\in\Fix{G\slash H}{H}$, giving us $\Norm{H}{G}\slash H=\Fix{G\slash H}{H}$. By fixed point congruence theorem,

\begin{equation*}
\left|G\slash H\right|\equiv\left|\Fix{G\slash H}{H}\right|\pmod p.
\end{equation*}

\noindent\newline Combining that with previous result gives us:

\begin{equation*}
\left|G\slash H\right|\equiv\left|\Norm{H}{G}\slash H\right|\pmod p.
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Sylow I).} Let $G$ be a group with $|G|=p^m n$, where $\gcd{(p,n)}=1$, $m\in\N_0$, $n\in\N$ and $p\in P$. Then, there exists a $p$-Sylow subgroup of $G$ with order $p^m$.

\noindent\newline{\bf Proof.} We will show that there exists a subgroup of order $k$ for all $k\in\{0,\ldots,m\}$. We have a trivial $p$-subgroup $H_0=\{e\}\leq G$ (obviously $|H_0|=1=p^0$). By Cauchy's theorem, as $p$ divides order of $G$, there exists $a\in G$ such that $\ord{a}=p$. Then, $H_1=\left|\cyc{a}\right|=p=p^1$. Now, assume that there exists $H_i\leq G$ such that $|H_i|=p^i$, for some $i\in\{2,\ldots,m-1\}$. By previous proposition, as $H_i\leq G$ and $|H_i|=p$, we have $[G:H_i]\equiv[\Norm{H_i}{G}:H_i]\pmod p$. As $[G:H_i]=\frac{|G|}{|H_i|}=\frac{n p^m}{p^i}=n p^{m-i}$, then $n p^{m-i}\equiv[\Norm{H_i}{G}:H_i]\pmod p$. That means $n p^{m-i}-p q=[\Norm{H_i}{G}:H_i]$ for some $q\in\N$. Then, as $i<m$ we have $0<m-i$ and it must be that $p^{m-i-1}\in\N$. So, $p(n p^{m-i-1}-q)=[\Norm{H_i}{G}:H_i]$. Therefore, $p$ divides $[\Norm{H_i}{G}:H_i]$ and we have $[\Norm{H_i}{G}:H_i]=p r$, for some $r\in\N$. As $p$ divides order of $\Norm{H_i}{G}\slash H_i$ (which is a group because $H_i\trianglelefteq\Norm{H_i}{G}$), then by Cauchy's theorem there exists $H a\in\Norm{H_i}{G}\slash H_i$ such that $\ord{H a}=p$. Then, $\cyc{H a}\leq\Norm{H_i}{G}\slash H_i$ and $\left|\cyc{H a}\right|=p$. Let $f:\Norm{H_i}{G}\rightarrow_{H_i}\Norm{H_i}{G}\slash H_i$ be a projective homomorphism, i.e. $f(x)=H_i x$, for all $x\in\Norm{H_i}{G}$. Then, by correspondence theorem, $f^{-1}\left(\cyc{H a}\right)\Norm{H_i}{G}$ and $f^{-1}\left(\cyc{H_i a}\right)\slash H_i\cong\cyc{H_i a}$. The latter implies $\left|f^{-1}\left(\cyc{H_i a}\right)\slash H_i\right|=\left|\cyc{H_i a}\right|=p$. In other words, $\left[f^{-1}\left(\cyc{H_i a}\right):H_i\right]=p$. Then,

\begin{equation*}
p=\left[f^{-1}\left(\cyc{H_i a}\right):H_i\right]=\frac{\left|f^{-1}\left(\cyc{H_i a}\right)\right|}{|H_i|}=\frac{\left|f^{-1}\left(\cyc{H_i a}\right)\right|}{p^i}.
\end{equation*}

\noindent\newline If we multiply the equation above with $p^i$, we get

\begin{equation*}
\left|f^{-1}\left(\cyc{H_i a}\right)\right|=p^{i+1}.
\end{equation*}

\noindent\newline As $H_{i+1}=f^{-1}\left(\cyc{H a}\right)\leq\Norm{H_i}{G}\leq G$, we have shown that existence of a $p$-subgroup of order $p^i$ implies the existence of a $p$-subgroup of $G$ of order $p^{i+1}$. The correspondence theorem also implies that $H_i\trianglelefteq f^{-1}\left(\cyc{H_i a}\right)$ (as $\ker{f}=H_i$). The process obviously stops at $H_m$ with $|H_m|=p^m$. We can assume that there exists $H_s\leq G$ such that $H_m\leq H_s\leq G$ and $|H_s|=p^{s}$, where $s\in\N$ and $s\geq m$. But, $|H_s|$ divides $|G|$ and we have $|G|=p^s t$, for some $t\in\N$. That implies $p^m n=p^s t$. Then, as $m\leq s$ we have $n=p^{s-m} t$. If $s-m\neq 0$ we get that $p|n$ which brings us into contradiction with assumption that $p\nmid n$. Therefore it must be that $s-m=0$, i.e. $s=m$. From that we have $|H_s|=|H_m|$. As also $H_m\leq H_s$ then $H_s=H_m$. That actually means that $H_m$ is a $p$-Sylow subgroup of $G$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $G$ is a group with $|G|=p^m n$, for some $p\in P$, $m\in\N_0$ and $n\in\N$, and $H$ is a $p$-Sylow subgroup of $G$, then $|H|=p^m$.

\noindent\newline{\bf Proof.} If $m=0$, then the only $p$-Sylow subgroup is $\{e\}$, so $|H|=|\{e\}|=1$. If $m=1$, then by Cauchy's theorem there exists $a\in G$ such that $\ord{a}=p$. We have that $\cyc{a}$ is a $p$-subgroup of $G$. If it were that $H$ was a $p$-Sylow subgroup of order $1$, we would have $H=\{e\}$. But, as $\{e\}\leq\cyc{a}$, $H$ would be contained in $\cyc{a}$ and could not be a $p$-Sylow subgroup of $G$. Therefore, $|H|=p$. Assume $m>1$. and $|H|<p^m$, i.e. $|H|=p^k$, where $k<m$. Then, as $H$ is a $p$-Sylow (and a) $p$-subgroup of $G$, by a previous lemma, we have:

\begin{equation*}
[G:H]\equiv[\Norm{H}{G}:H]\pmod p.
\end{equation*}

\noindent\newline That implies, as $\frac{|G|}{|H|}=\frac{p^m}{p^k}=p^{m-k}=p^l$, where $l\in\N$ (due to $m>k$), $\left|\Norm{H}{G}\slash H\right|-p^l=p q$, for some $q\in\Z$. That is, $\left|\Norm{H}{G}\slash H\right|=p(q+p^{l-1})$. By Cauchy's theorem, there exists $H a\in\Norm{H}{G}\slash H$ such that $\ord{a}=p$. Then, by correspondence theorem, $\cyc{H a}\cong f^{-1}(\cyc{H a})\slash H$, where $f$ is a projective homomorphism $f:\Norm{H}{G}\rightarrow_{H}\Norm{H}{G}\slash H$ with $f(x)=H x$. So, $|f^{-1}(\cyc{H a})\slash H|=p$, which gives us that $\frac{|f^{-1}(\cyc{H a})|}{|H|}=p$. That is, $|f^{-1}(\cyc{H a})|=p^{l+1}$. As $H\trianglelefteq f^{-1}(\cyc{H a})$, and $p^l<p^{l+1}$, $H$ cannot be a $p$-Sylow subgroup of $G$ with order less than $p^m$. So, $|H|=p^m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Previous proposition actually tells us that all $p$-Sylow subgroups are of the same size and that $[G:H]=m$, where $p\nmid m$. Also, it is obvious that, if $|G|=p^m n$, $\gcd{(p,n)}=1$, it cannot be that $|H|=p^{m'}$, where $m'>m$. That would imply that, due to Lagrange's theorem, $p^{m'}|p^m n$, i.e. that there exists $q\in\Z$ such that $p^{m} n=p^{m'} q$. That is equivalent to $p^{m'-m} q=n$, i.e. $p(p^{m'-m-1}q)=n$. And, as $m'>m$, and we would have $p|n$.

\noindent\newline{\bf Lemma.} Let $G$ be a group, $a\in G$ and $H,K\leq G$. If $a H a^{-1}\subseteq K$, then $H\subseteq a^{-1} K a$.

\noindent\newline{\bf Proof.} Let $h\in H$. Then there exists $k\in K$ such that $a h a^{-1}=k$. But, that is equivalent to $h=a^{-1} k a$. Thus, for all $h\in H$ there exists $k\in K$ such that $h=a^{-1} k a$. So, $h\in a^{-1} K a$ and $H\subseteq a^{-1} K a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Sylow II).}  If $H$ and $K$ are $p$-Sylow subgroups of $G$, then:

\begin{enumerate}
\item For all $a\in G$, $a H a^{-1}$ is a $p$-Sylow subgroup of $G$.
\item There exists $a\in G$ such that $H=a K a^{-1}$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} By a previous proposition, we know that $a H a^{-1}\cong H$ and $a H a^{-1}\leq G$. From that follows $\left|a H a^{-1}\right|=|H|$. As order of $H$ is a power of $p$, then order of $a H a^{-1}$ is a power of $p$. Thus, $a H a^{-1}$ is a $p$-subgroup of $G$. Assume $a H a^{-1}\leq K$ where $K\leq G$ is a $p$-subgroup of $G$. Then, as $a H a^{-1}\subseteq K$, previous lemma implies $H\subseteq a^{-1} K a$. But, $a^{-1} K a\cong K$, so it is also a $p$-subgroup of $G$. Also, $a^{-1} K a\leq G$ and that implies $H\leq a^{-1} K a$. As $H$ is a $p$-Sylow subgroup of $G$ it cannot be contained in any $p$-subgroup of $G$ and it must be $H=a^{-1} K a$. That is equivalent to $a H a^{-1}=K$. As $a H a^{-1}\leq K$ implied $a H a^{-1}=K$ and both are $p$-subgroups of $G$, then $a H a^{-1}$ is a $p$-Sylow subgroup of $G$.

{\it Ad $2$.} Let $H$ and $K$ be $p$-Sylow subgroups of $G$. Then, let $H$ act on $G\slash K$ with $h.K a=K(a h)$, for all $K a\in G\slash K$ and $h\in H$. Obviously, $K(a h)=K a$ if and only if $a h a^{-1}\in K$, for all $h\in H$. That is equivalent to $a H a^{-1}\subseteq K$. But, as conjugate of a $p$-Sylow subgroup is a $p$-Sylow subgroup, it follows that $a H a^{-1}=K$. Also, if $a H a^{-1}=K$ it is obvious that $a H a^{-1}\subseteq K$. Therefore, $K a\in\Fix{H}{G\slash K}$ if and only if $a H a^{-1}=K$. But, by FPC theorem we have:

\begin{equation*}
\left|G\slash K\right|\equiv\left|\Fix{H}{G\slash K}\right|\pmod p.
\end{equation*}

\noindent\newline Assume $\left|\Fix{H}{G\slash K}\right|=0$, i.e. $\Fix{H}{G\slash K}=\emptyset$. Then, we would have $[G:K]\equiv 0\pmod p$, meaning $[G:K]=p q$, for some $q\in\Z$. But, due to previous corollary, $[G:K]=m$, where $p\nmid m$. Therefore, $\Fix{H}{G\slash K}\neq\emptyset$, i.e. there exists $K a\in\Fix{H}{G\slash K}$ such that $K=a H a^{-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From previous theorem, it also follows that all $p$-Sylow subgroups are conjugate.

\noindent\newline{\bf Definition.} Let $G$ be a group and $p\in P$. Then we define $\Syl{p}{G}$ as a set containing all $p$-Sylow subgroups of $G$.

\noindent\newline{\bf Lemma.} Let $G$ be a group and $H$ a $p$-Sylow subgroup of $G$, for some $p\in P$. Let $a\in G$ such that $\ord{a}=p^m$, for some $m\in\N_0$. Then, $a H a^{-1}=H$ implies $a\in H$.

\noindent\newline{\bf Proof.} First, as $a H a^{-1}=H$, it must be that $a\in\Norm{H}{G}$, by definition of subgroup normalizer. Let $f:\Norm{H}{G}\rightarrow_{H}\Norm{H}{G}\slash H$ be a projective homomorphism. Then, $f(x)=H x$, for all $x\in\Norm{H}{G}$. Then, as $a\in\Norm{H}{G}$, then $H a\in\Norm{H}{G}\slash H$. As $\ord{a}=p^m$, then $\cyc{H a}\leq\Norm{H}{G}$ with $\left|\cyc{H a}\right|=p^m$. By correspondence theroem, it follows that

\begin{equation*}
\ker{f}=H\trianglelefteq f^{-1}\left(\cyc{H a}\right)\leq\Norm{H}{G},
\end{equation*}

\noindent\newline with $f^{-1}\left(\cyc{H a}\right)\slash H\cong\cyc{H a}$. That implies $\left|f^{-1}\left(\cyc{H a}\right)\slash H\right|=\left|\cyc{H a}\right|=p^m$. As $H$ is a $p$-subgroup, $|H|=p^k$, for some $k\in\N$. We have:

\begin{equation*}
p^m=\left[f^{-1}\left(\cyc{H a}\right):H\right]=\frac{\left|f^{-1}\left(\cyc{H a}\right)\right|}{|H|}=\frac{\left|f^{-1}\left(\cyc{H a}\right)\right|}{p^k}.
\end{equation*}

\noindent\newline That implies $\left|f^{-1}\left(\cyc{H a}\right)\right|=p^{m+k}$, i.e. $f^{-1}\left(\cyc{H a}\right)$ is a $p$-subgroup of $\Norm{H}{G}$ and by that also of $G$. As $\ker{f}=H\leq f^{-1}\left(\cyc{H a}\right)$, we have that $H$ is contained in $p$-subgroup $f^{-1}\left(\cyc{H a}\right)$. But, as $H$ is a $p$-Sylow subgroup of $G$, it must be that $f^{-1}\left(\cyc{H a}\right)=H$. That would imply

\begin{equation*}
\left[f^{-1}\left(\cyc{H a}\right):H\right]=1=\left|\cyc{H a}\right|.
\end{equation*}

\noindent\newline From that we have $\ord{H a}=1$, and that implies $H a=H$, i.e. $a\in H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $G$ be a group and $H\in\Syl{p}{G}$. Then, $K\in\Syl{p}{G}$ if and only if there exists $a\in G$ such that $a H a^{-1}=K$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $K\in\Syl{p}{G}$. Then, by second Sylow's theorem, there exists $a\in G$ such that $a K a^{-1}=H$. {\it Sufficiency.} By second Sylow's theorem, $a^{-1} H a$ is a $p$-Sylow subgroup of $G$, and that means that so is $K$, i.e. $K\in\Syl{p}{G}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Sylow III).} Let $G$ be a group with $|G|=p^m n$, where $p\in P$, $m\in\N_0$ and $n\in\N$ such that $\gcd{(p,n)}=1$. Then,

\begin{enumerate}
\item $\left|\Syl{p}{G}\right|\equiv 1\pmod p$.
\item If $H\in\Syl{p}{G}$, then $\left|\Syl{p}{G}\right|=\left[G:\Norm{H}{G}\right]$.
\item $\left|\Syl{p}{G}\right|$ divides $|G|$. Specifically, $\left|\Syl{p}{G}\right|$ divides $n$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} By first Sylow theorem, as $p$ divides $|G|$, we know that $\Syl{p}{G}\neq\emptyset$. Take $H\in\Syl{p}{G}$ and let $H$ act on $\Syl{p}{G}$ with $h.K=h K h^{-1}$, for all $h\in H$ and $K\in\Syl{p}{G}$. Let us observe $\Fix{\Syl{p}{G}}{H}$. As $h.H=h H h^{-1}=H$, for all $h\in H$, then $H\in\Fix{\Syl{p}{G}}{H}$. That means that $\left|\Fix{\Syl{p}{G}}{H}\right|\geq 1$. Assume $\left|\Fix{\Syl{p}{G}}{H}\right|>1$. Then, there exists $K\in\Fix{\Syl{p}{G}}{H}$ such that $H\neq K$. But, that also means that $h.K=K$, for all $h\in H$, i.e. $h K h^{-1}=K$ for all $h\in H$. As $H$ is a $p$-group, all elements in it have order a power of $p$, i.e. $\ord{h}=p^{\alpha(h)}$, for all $h\in H$, where $\alpha:H\rightarrow\N_0$. Then, by previous lemma, $h K h^{-1}=K$ implies $h\in K$, for all $h\in H$. So, $H\subseteq K$, and as $H$ is a group, $H\leq K$. As $H$ is a $p$-Sylow subgroup of $G$, it cannot be that $H\leq K$ with $H\neq K$, and it must be $H=K$. That is in contradiction with assumption that $\left|\Fix{\Syl{p}{G}}{H}\right|>1$. So it is $\left|\Fix{\Syl{p}{G}}{H}\right|\leq 1$. That, and $\left|\Fix{\Syl{p}{G}}{H}\right|\geq 1$ imply $\left|\Fix{\Syl{p}{G}}{H}\right|=1$. Then, by FPC, we have $\left|\Syl{p}{G}\right|\equiv\left|\Fix{\Syl{p}{G}}{H}\right|\pmod p$, i.e. $\left|\Syl{p}{G}\right|\equiv 1\pmod p$.

{\it Ad $2$.} Let $G$ act on $\Syl{p}{G}$ so that $g.H=g H g^{-1}$, for all $g\in G$ and $H\in\Syl{p}{G}$. Then, by Sylow II, we have that $g H g^{-1}\in\Syl{p}{G}$. Also, $g.(h.H)=g.(h H h^{-1})=g h H h^{-1} g^{-1}=(g h) H (g h)^{-1}=(g h).H$ and $e.H=e H e^{-1}=H$. Let $H\in\Syl{p}{G}$ and let us observe:

\begin{equation*}
\Stab{H}{G}=\{g\in G:\ g.H=H\}=\{g\in G:\ g H g^{-1}=H\}=\Norm{H}{G}.
\end{equation*}

\noindent\newline Now, the orbit of $H$ in $G$ is:

\begin{eqnarray*}
\Orb{H}{G}&=&\{K\in\Syl{p}{G}:\ (\exists g\in G)(g.H=K)\}\\
&=&\{K\in\Syl{p}{G}:\ (\exists g\in G)(g H g^{-1}=K)\}.
\end{eqnarray*}

\noindent\newline Notice that $K\in\Orb{H}{G}$ if and only if there exists $g\in G$ such that $g H g^{-1}=K$. But, by previous lemma, $K\in\Syl{p}{G}$ if and only if there exists $g\in G$ such that $g H g^{-1}=K$. That actually means that $K\in\Orb{H}{G}$ if and only if $K\in\Syl{p}{G}$. In other words, $\Orb{H}{G}=\Syl{p}{G}$. Therefore, by orbit-stabilizer theorem,

\begin{equation*}
|G|=\left|\Orb{H}{G}\right|\cdot\left|\Stab{H}{G}\right|=\left|\Syl{p}{G}\right|\cdot\left|\Norm{H}{G}\right|.
\end{equation*}

\noindent\newline If we divide the equality above with $\left|\Norm{H}{G}\right|$, we get:

\begin{equation*}
\left|\Syl{p}{G}\right|=\frac{|G|}{\Norm{H}{G}}=[G:\Norm{H}{G}].
\end{equation*}

\noindent\newline That concludes proof for $2$.

{\it Ad $3$.} From $2$, we have that $|G|=\left|\Syl{p}{G}\right|\cdot\left|\Norm{H}{G}\right|$. Then, $\left|\Syl{p}{G}\right|$ divides $|G|=p^m n$. But, as due to $1$, we have $\left|\Syl{p}{G}\right|\equiv 1\pmod p$, i.e. $\left|\Syl{p}{G}\right|=p q+1$, for some $q\in\Z$. Assume $p$ divides $\left|\Syl{p}{G}\right|$, i.e. $p|p q+1$. Then, $p q+1=p r$, for some $r\in\Z$. We have $p q-p r=1$, i.e. $p(q-r)=1$ which would imply $p=1$, but that is a contradiction as $p\neq 1$. Therefore, $\gcd{\left(\left|\Syl{p}{G}\right|,p\right)}=1$. Then, $p^m n=\left|\Syl{p}{G}\right| s$, where $s=\left|\Norm{H}{G}\right|$. As $p\nmid\left|\Syl{p}{G}\right|$, then $p^m\nmid\left|\Syl{p}{G}\right|$, so by Euclid's lemma\footnote{Another way to prove this is by Lagrange's theorem: as $H\trianglelefteq\Norm{H}{G}$, then $|H|$ divides $\Norm{H}{G}$ and we have that $\Norm{H}{G}=|H|t=p^m t$, for some $t\in\Z$.}, $p^m|s$. We have $n=\left|\Syl{p}{G}\right|\frac{s}{p^m}$, where $\frac{s}{p^m}\in\Z$ as $p^m|s$. That implies that $\left|\Syl{p}{G}\right|$ divides $n$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Dihedral groups}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $n\in\N-\{1,2\}$ and $R\in\R^{+}$. Let $V_n$ be the set of vertices of a regular polygon in Euclidean plane centered at origin, that is:

\begin{equation*}
V_n=\left\{\left(R\cos{\frac{2k\pi}{n}},R\sin{\frac{2k\pi}{n}}\right)\in\R^2:\ k\in\{0,\ldots,n-1\}\right\}.
\end{equation*}

\noindent\newline Set of all rotations and reflections which leave $A$ unchanged (i.e. for all $v\in V$, $r(v)\in V_n$ and $s(v)\in V_n$, for all rotations $r$ and reflections $s$) along with composition as respective operation is called a {\bf dihedral group} $D_{2n}$.

\noindent\newline{\bf Proposition.} For all $n\in\N-\{1,2\}$, $|D_{2n}|=2n$.

\noindent\newline{\bf Proof.} Here we can use the orbit-stabilizer theorem to show that $|D_{2n}|=2n$. Choose any vertex, e.g. $v_1\in V_n$, without loss of generality. Then, that vertex can be moved to all other vertices $v_2,\ldots,v_n$ (by rotations and reflections), including to itself (by identity). Therefore, $\Orb{v_1}{D_{2n}}=\{v_1,\ldots,v_n\}$ which implies $\left|\Orb{v_1}{D_{2n}}\right|=n$. All rotations, except identity, move all vertices, therefore, they cannot be in the stabilizer of $v_1$. Reflection $s$ through vertex $v_1$, however, will leave $v_1$ unchanged (and so will identity). Thus, $\Stab{v_1}=\{e,s\}$ from which we have $\left|\Stab{v_1}{D_{2n}}\right|=2$. Note that reflection through $v_1$ will be the bisector of the angle at $v_1$ and will pass through opposite vertex if $n$ is even. If $n$ is odd, it will bisect the opposite side. To conclude, by orbit-stabilizer theorem, $|D_{2n}|=\left|\Stab{v_1}{D_{2n}}\right|\cdot\left|\Orb{v_1}{D_{2n}}\right|=2 n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Note that we could have proven (at least argued) for the previous theorem by counting permutations (with certain properties). For example, if we fix $v(i)$, where $i\in\{0,\ldots,n-1\}$, then it can be sent to $n$ different vertices (including itself). Say we chose $v(i)\rightarrow v(k)$. But, that leaves open the possibility of his neighbour, $v(i+1)$ to be sent to all other $n$ vertices with us still having $n!$ bijections. Therefore, we will want to preserve the structure of the polygon, so that only rotation and reflection are considered. In both, if $v(i)\rightarrow v(k)$, then $v(i+1)\rightarrow v(k+1)$ (then it must be $v(i-1)\rightarrow v(k-1)$) or $v(i+1)\rightarrow v(k-1)$ (when it must be $v(i-1)\rightarrow v(k+1)$), where $i+1$, $i-1$, $k+1$, $k-1$ is taken modulo $n$. In other words, all adjacent vertices must remain adjacent after rotation and reflection. Thus, we have only two options left (either send $v(i+1)$ to $v(k+1)$ or to $v(k-1)$) and with $n$ possibilities to choose for $v(i)$, that is $2\cdot n$ permutations with desired properties.

\noindent\newline{\bf Remark.} Let $v(k)\in V_n$, for $k\in\{0,\ldots,n-1\}$, where $v(k)=\left(R\cos{\frac{2k\pi}{n}},R\sin{\frac{2k\pi}{n}}\right)$. We will now define rotation $r\in D_{2n}$ (for $\frac{2\pi}{n}$) as $r:V_n\rightarrow V_n$ such that $r\left(v(k)\right)=v(k+1)$. We will also define reflection $s\in D_{2n}$ (across $x$-axis) as $s:V_n\rightarrow V_n$ with $s\left(v(k)\right)=v(-k)$. Also, identity is $e:V_n\rightarrow V_n$ with $e\left(v(k)\right)=v(k)$. Note that $D_{2n}\leq S_{V_n}$, where $S_{V_n}$ is the set of all permutations of $V_n$. Thus, all group axioms for $D_{2n}$ are satisfied (except commutativity).

\noindent\newline{\bf Remark.} From now on, when considering $D_{2n}$, we will assume $n\in\N-\{1,2\}$.

\noindent\newline{\bf Proposition.} Let $v(k)\in V_n$ be defined as above and $r\in D_{2n}$. Then\footnote{Note that $r^m\left(v(k)\right)=\underbrace{r\circ r\circ\cdots\circ r}_{m\textnormal{ times}}$.}, for all $m\in\N$, $r^m\left(v(k)\right)=v(k+m)$ and $r^n=e$. Also, $\ord{r}=n$.

\noindent\newline{\bf Proof.} By definition, $r(v(k))=v(k+1)$. Assume $r^m(v(k))=v(k+m)$. Then,

\begin{equation*}
r^{m+1}(v(k))=[r^m\circ r](v(k))=r^m(r(v(k)))=r^m(v(k+1))=v(k+m+1).
\end{equation*}

\noindent\newline Finally,

\begin{eqnarray*}
r^n(v(k))&=&v(k+n)=\left(R\cos{\frac{2(k+n)\pi}{n}},R\sin{\frac{2(k+n)\pi}{n}}\right)\\
&=&\left(R\cos{\left(2\pi+\frac{2k\pi}{n}\right)},R\sin{\left(2\pi+\frac{2k\pi}{n}\right)}\right)\\
&=&\left(R\cos{\frac{2k\pi}{n}},R\sin{\frac{2k\pi}{n}}\right)=v(k).
\end{eqnarray*}

\noindent\newline That implies $r^n(v(k))=v(k)$, that is $r^n=e$. Now, assume that $r^m=e$, where $0<m<n$. That implies $r^m(v(k))=v(k)$, that is:

\begin{equation*}
r^m(v(k))=\left(R\cos{\frac{2(k+m)\pi}{n}},R\sin{\frac{2(k+m)\pi}{n}}\right)=\left(R\cos{\frac{2k\pi}{n}},R\sin{\frac{2k\pi}{n}}\right).
\end{equation*}

\noindent\newline By definition of ordered pair, and after cancelling out $R$, we have $\cos{\frac{2(k+m)\pi}{n}}=\cos{\frac{2k\pi}{n}}$. That implies $\frac{2(k+m)\pi}{n}=\frac{2k\pi}{n}+2l\pi$, for all $l\in\Z$. That gives us $\frac{2m\pi}{n}=2l\pi$, i.e. $\frac{m}{n}=l$. But, as $0<m<n$, then $n\nmid m$ and $\frac{m}{n}\notin\Z$ while $l\in\Z$. This is a contradiction, therefore, there does not exist $m$ such that $r^m=e$ and $0<m<n$. The other possibility is that $\frac{2(k+m)\pi}{n}=-\frac{2k\pi}{n}+2l\pi$. Then, $\frac{4k+2m\pi}{n}=2l\pi$. After cancelling out equal terms, $\frac{2k}{n}+\frac{m}{n}=l$, i.e. $\frac{2k+m}{n}=l$. For the same case, but for sine function, from $\sin{\frac{2(k+m)\pi}{n}}=\sin{\frac{2k\pi}{n}}$ we get $\frac{2(k+m)\pi}{n}=\pi-\frac{2k\pi}{n}+2l'\pi$, where $l'\in\Z$. Then, $\frac{k+m}{n}=\frac{1}{2}-\frac{k}{n}+l'$, i.e. $\frac{2k+m}{n}-\frac{1}{2}=l'$. Therefore, $l-\frac{1}{2}=l'$. But, if $l\in\Z$, then $l-\frac{1}{2}\notin\Z$ and that cannot be. If $l\notin\Z$, then, again, we are done. In conclusion, there does not exist $m\in\Z$ such that $0<m<n$ and it must be that $\ord{r}=n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $i\in\Z$ and $r^i\in D_{2 n}$. Then $r^i r^{-i}=e$ (in other words $\left(r^{i}\right)^{-1}=r^{-i}$).

\noindent\newline{\bf Proof.} Let $v(k)\in V_n$. Then $(r^i r^{-i})(v(k))=r^i(r^{-i})(v(k))=r^i(v(k-i))=v(k-i+i)=v(k)$. Therefore, $r^i r^{-i}=e$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $s\in D_{2n}$. Then, $\ord{s}=2$.

\noindent\newline{\bf Proof.} Let $v(k)\in V_n$. We have $s(v(k))=v(-k)$. Then,

\begin{equation*}
s^2(v(k))=[s\circ s](v(k))=s(s(v(k)))=s(v(-k))=v(-(-k))=v(k).
\end{equation*}

\noindent\newline Therefore, $s^2=e$. As $s\neq e$ and $\ord{s}=1$ if and only if $s=e$, it must be that $\ord{s}=2$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $i\in\N$ and $r^i,s\in D_{2n}$. Then, $s r^i=r^{-i} s$. Also, $s r^k\neq s r^j$ and $s r^k\neq r^j$, for all $k\neq j$, $k,j\in\{0,\ldots,n-1\}$.

\noindent\newline{\bf Proof.} Let $v(k)\in V_n$. Then, $(s r^i)(v(k))=[s\circ r^i](v(k))=s(r^i(v(k)))=s(v(k+i))=v(-k-i)$. Now, as $v(-k+(-i))=r^{-i}(v(-k))$, we have $v(-k-i)=r^{-i}(v(-k))=r^{-i}(s(v(k)))=(r^{-i} s)(v(k))$, i.e. $s r^i=r^{-i} s$. Assume $s r^k=s r^j$. Then, multiplying by $s$ on the left, gives us $r^k=r^j$. But, $r^k\neq r^j$ for all $k\neq j$ such that $k,j\in\{0,\ldots,n-1\}$. Also, assume $s r^k=r^j$. That means that $s=r^{j-k}$. That cannot be as $s$ would be a rotation.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $d\in D_{2n}$, then $d=s^i r^j$, where $i\in\{0,1\}$ and $j\in\{0,\ldots,n-1\}$.

\noindent\newline{\bf Proof.} From a previous proposition, we have that $s r^i\neq s r^j$, for all $i\neq j$ and $i,j\in\{0,\ldots,n-1\}$. If we define $D=\{s r^i:\ i\in\{0,\ldots,n-1\}\}$, then $|D|=n$. Note that $s\in D$ as $s=s r^0$. Now, $|\cyc{r}|=n$. Notice that $D\cap\cyc{r}=\emptyset$. If we take $s r^i\in D$, then by a previous proposition $s r^i\neq r^j$, for all $r^j\in\cyc{r}$. Therefore, $|D\cup\cyc{r}|=2 n$. As $|D\cup\cyc{r}|=D_{2n}$ and $D\cup\cyc{r}\subseteq D_{2n}$, it follows that $D\cup\cyc{r}=D_{2 n}$. As $r^j=e r^j=s^0 r^j$, for all $r^j\in\cyc{r}$, and $s r^j\in D$, all elements of $D_{2n}$ can be written as $s^i r^j$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $s^i r^j\in D_{2n}$, where $i\in\{0,1\}$ and $j\in\{0,\ldots,n-1\}$. Then, $\left(s^i r^j\right)^{-1}=r^{-j} s^{i}$.

\noindent\newline{\bf Proof.} Let $i=1$. Then, $(s r^j)(r^{-j} s)=s^2=e$. Let $i=0$. Then $r^j r^{-j}=e$, by a previous proposition. That can be written as $(s^0 r^j)(r^{-j}s^0)=e$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $r,s\in D_{2n}$. Then, $\cyc{r}\trianglelefteq D_{2n}$ and $D_{2n}=\cyc{r}\cyc{s}$.

\noindent\newline{\bf Proof.} Let $r^k\in\cyc{r}$ and $s^i r^j\in D_{2n}$. Then, $(s^i r^j)r^k(r^{-j} s^i)=s^i r^{j+k-j} s^i=s^i r^k s^i$. Assume $i=0$. Then $s^i r^k s^i=s^0 r^k s^0=r^k$, so $(s^0 r^j)r^k(r^{-j} s^0)\in\cyc{r}$. Assume $i=1$. Then $(s r^k)s=(r^{-k} s)s$, by a previous proposition. Then, as $D_{2n}\leq S_{V_n}$, associativity implies $s r^k s=r^{-k} s^2=r^{-k}\in\cyc{r}$. Therefore, $(s r^j)r^k(r^{-j} s)\in\cyc{r}$ and $\cyc{r}\trianglelefteq D_{2n}$. If we take $s^i r^j\in D_{2 n}$, then, if $i=0$, $s^0 r^j=r^j=r^j e=r^j s^0\in\cyc{r}\cyc{s}$ and, if $i=1$, $s r^j=r^{-j} s\in\cyc{r}\cyc{s}$. That is, $D_{2n}\subseteq\cyc{r}\cyc{s}$. Now, if $r^j s^i\in\cyc{r}\cyc{s}$, then, if $i=0$, $r^j\in D_{2n}$, and, if $i=1$, $r^j s=s r^{-j}\in D_{2n}$. So, $\cyc{r}\cyc{s}\subseteq D_{2n}$ and $D_{2n}=\cyc{r}\cyc{s}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that $\cyc{s}$ is not a normal subgroup of $D_{2 n}$. For example, $r s r^{-1}=s r^{-1} r^{-1}=s r^{-2}\notin\cyc{s}$. That is why $D_{2n}$ is not isomorphic to $\cyc{r}\times\cyc{s}$.

\noindent\newline{\bf Proposition.} Let $s,r\in D_{2n}$. Then $D_{2n}=\cyc{s,r:\ r^n=s^2=e,\ r s=s r^{-1}}$.

\noindent\newline{\bf Proof.} Let $D'=\cyc{s,r:\ r^n=s^2=e,\ r s=s r^{-1}}$. If we take $s^i r^j\in D_{2 n}$, where $i\in\{0,1\}$, $j\in\{0,\ldots,n-1\}$, we know that $s^2=e$ and $r^n=e$ for $s,r\in D_{2_n}$. But also, if we take $r s\in D_{2 n}$, by using the rule $s^i r^j=r^{-j} s^i$, we have $r s=s r^{-1}$, when $i=1$ and $j=1$. Therefore, all $s^i r^j\in D_{2 n}$satisfy rules set for $D'$ and it must be that $D_{2 n}\subseteq D'$.

Now, let us take $x\in D$. Then $x=x_1^{i_1} x_2^{i_2}\cdots x_m^{i_m}$, where $x_1,\ldots,x_m\in\{s,r\}$ and $i_1,\ldots,i_m\in\Z$. By induction, using $s^2=r^n=e$ and $r s=s r^{-1}$, we will show that $x$ can be written as $s^i r^j$, for some $i,j\in\Z$. First, assume $x=x_1^{i_1} x_2^{i_2}$. {\it First case.} If $x_1=s$ and $x_2=s$, we have $x=s^{i_1} s^{i_2}=s^{i_1+i_2}$. Set $i=i_1+i_2$. Then $x=s^i=s^i e=s^i r^0$. Set $j=0$. Then, $x=s^i r^0$. {\it Second case.} Assume $x_1=r$ and $x_2=r$. Then $x=r^{i_1} r^{i_2}=r^{i_1+i_2}$. Set $j=i_1+i_2$. Then, $x=r^j=e r^j$ and $x=s^0 r^j$. Set $i=0$ and then we have $x=s^i r^j$. {\it Third case.} If $x_1=s$ and $x_2=r$, we have $x=s^{i_1} r^{i_2}$. Then, $i=i_1$ and $j=i_2$ gives us $x=s^i r^j$. {\it Fourth case.} Finally, if $x_1=r$ and $x_2=s$, we have $x=r^{i_1} s^{i_2}$. As $i_2,2\in\Z$, then there exist $q,p\in\Z$ such that $i_2=2 q+p$, where $0\leq p<2$, that is $p\in\{0,1\}$. Then, $s^{i_2}=s^{2 q+p}=s^{2 q}s^p$. Now, as $s^2=r$ and $s^{2 q}=\left(s^2\right)^q=e^q=e$, we have $s^{2 q}s^p=e s^p=s^p$ which implies $x=r^{i_1} s^p$, where $p\in\{0,1\}$. If $p=0$, we have $x=r^{i_1} s^0=r^{i_1} e=r^{i_1}=e r^{i_1}=s^0 r^{i_1}$. Setting $i=0$ and $j=i_1$ we have $x=s^i r^j$. Assume $p=1$. Then $x=r^{i_1} s^1=r^{i_1} s$. By induction we will prove that $r^{i_1} s=s r^{-i_1}$. For $i_1=0$ we have $r^0 s=e s=s=s e=s r^{0}=s r^{-0}$. Now, assume that $r^{k} s=s r^{-k}$. Then, $r^{k+1} s=r^k r s=r^k s r^{-1}=s r^{-k} r^{-1}=s r^{-(k+1)}$. Therefore, we proved that if $m=2$, we have $x=s^i r^j$, for some $i,j\in\Z$. Now, assume that $x=x_1^{i_1}\cdots x_m^{i_m}=s^i r^j$. Then, we need to prove that $y=x_1^{i_1}\cdots x_m^{i_m} x_{m+1}^{i_{m+1}}=s^z r^w$, for some $z,w\in\Z$. But, $y=x x_{m+1}^{i_{m+1}}$ and $x=s^i r^j$ by assumption of induction, for some $i,j\in\Z$, so $x=s^i r^j x_{m+1}^{i_{m+1}}$. If $x_{m+1}=s$, then, $y=s^i r^j s^{i_{m+1}}=s^i(r^j s^{i_{m+1}})$. Therefore, by using basis of induction, obviously $r^j s^{i_{m+1}}=s^v r^w$, for some $v,w\in\Z$ and $y=s^i s^v r^w=s^{i+v} r^w$. Setting $z=i+v$ we have $y=s^z r^w$. If $x_{m+1}=r$, then $y=s^i r^j r^{i_{m+1}}=s^i r^{j+i_{m+1}}$. Setting $z=i$ and $w=j+i_{m+1}$ we have $y=s^z r^w$. Therefore, every element $x\in D'$ can be shown as $x=s^i r^j$, where $i,j\in\Z$. By using division with remainder theorem, as $i,j,2,n\in\Z$, we have $i=2q_1+r_1$ and $j=n q_2+r_2$, where $q_1,q_2,r_1,r_2\in\Z$ with $0\leq r_1<2$ (or equivalently, $r_1\in\{0,1\}$) and $0\leq r_2<n$ (that is, $r_2\in\{0,\ldots,n-1\}$). We can see that then,

\begin{equation*}
x=s^i r^j=s^{2 q_1+r_1}r^{n q_2+r_2}=\left(s^2\right)^{q_1} s^{r_1}\left(r^n\right)^{q_2} r^{r_2}.
\end{equation*}

\noindent\newline As $s^2=e$ and $r^n=e$, then $x=s^{r_1} r^{r_2}$, where $r_1\in\{0,1\}$ and $r_2\in\{0,\ldots,n-1\}$. Then, obviously $x\in D_{2 n}$ and $D_{2 n}\subseteq D'$. That implies, from the first part of the proof that $D_{2 n}=D'=\cyc{s,r:\ r^n=s^2=e,\ r s=s r^{-1}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $n\in 2\Z^{+}-\{2\}$, $Z\left(D_{2n}\right)=\left\{e,r^{\frac{n}{2}}\right\}$. If $n\in\left(\Z^{+}-\{1,2\}\right)-2\Z$, then $Z\left(D_{2n}\right)=\left\{e\right\}$.

\noindent\newline{\bf Proof.} We know that at least $e\in Z(D_{2n})$, for both cases. Take $r^i,s,r^j\in D_{2n}$, where $i,j\in\{0,\ldots,n-1\}$. First, we know that $r^i r^j=r^{i+j}=r^{j+i}=r^j r^{i}$. Therefore, $r^i$ commutes with all $r^j$. Now, we check if it commutes with all other elements. Assume $r^i(s r^j)=(s r^j)r^i$. Then, that is equivalent to $s r^{-i} r^{j}=s r^j r^i$, i.e. to $r^{j-i}=r^{j+i}$ (after multiplying equality with $s$ on the left). After multiplying by $r^{-(j-i)}$ on the left we have $r^{j+i-j+i}=e$, that is, $r^{2i}=e$. As $\ord{r}=n$, it must be that $n|2i$. Then, there exists $q\in\N_0$ (as $n\in\N$ and $i\in\N_0$) such that $2i=n q$.

Assume $n$ is even, i.e. $n=2k$, for some $k\in\N-\{1\}$. Then, $2i=2kq$, which is $i=kq$. For $q=0$ we have $i=0$, i.e. $r^0=e$ (but we already know that $e\in Z(D_{2n})$. For $q=1$ we have $i=k$, i.e. $r^k=r^{\frac{n}{2}}$. For $q=2$ we have $i=2k$, but $i<n$, so here we stop. Therefore, $r^{\frac{n}{2}}\in Z(D_{2n})$. Now, if we were considering $s r^i$, we would have to check first whether it commutes with $r^j$. But, then $s r^i r^j=r^j s r^i$ would give us $s r^{i+j}=s r^{i-j}$. That is, $r^{2j}=e$, and that can be either $e$ or $r^{\frac{n}{2}}$. Now that $s r^{i}$ would only commute with $e$ and $r^{\frac{n}{2}}$ it cannot be in the center (it has to commute with all elements of $D_{2n}$). Thus, $Z(D_{2n})=\{e,r^{\frac{n}{2}}\}$ if $n$ is even.

Assume $n$ is odd, i.e. $n=2k+1$, for some $k\in\N$. Then, from $2i=n q$ we have $2i=2k q+q$, i.e. $i=kq+\frac{q}{2}$. Now, if $q$ is odd, $i\notin\Z$. So, we will only check when $q$ is even, i.e. $q=2q'$, for some $q'\in\N_0$. Then, $i=2kq'+q'$. That gives us $i=0$ for $q'=0$ and $i=2k+1=n$ if $q'=1$. But, $i<n$, so we only have $e$ that commutes. As we can only consider $e$ for $r^i$, then, it is useless to check for $s r^i$ (as then it would commute only with $e$, if viewing it in the same light as for when $n$ is even). Therefore, $Z(D_{2n})=\{e\}$ when $n$ is odd.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $n\in\N-\{1,2\}$ and $s,r^i\in D_{2 n}$, for some $i\in\{0,\ldots,n-1\}$. Then, $\ord{s r^i}=2$.

\noindent\newline{\bf Proof.} We have $(s r^i)(s r^i)=s (r^i s) r^i=s (s r^{-i}) r^i=s^2 r^{-i+i}=e e=e$. Therefore, as $s r^i\neq e$, for all $i\in\{0,\ldots,n-1\}$, it must be that $\ord{s r^i}=2$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Symmetries in $\R^3$}
\end{center}

\vskip 0.5cm

\noindent{\bf Proposition\footnote{Author's remark: as soon as I complete more precise drawings (which is a problem in all my writings), I will argue more strongly using the orbit-stabilizer theorem and preserve the following reasoning, perhaps, as a remark after the proposition. Then I will also add symmetries for pyramids and prisms and reaveal more about the group structure (which is not a big deal actually, just a lot of work).}}. Let $G$ be a group of rigid motions (without reflections, i.e. only reflections) and $G_S$ a group of rotations and reflections of a Platonic solid. Then,

\begin{enumerate}
\item For a tetrahedron, $|G|=12$ and $|G_S|=24$.
\item For a cube and an octahedron, $|G|=24$ and $|G_S|=48$.
\item For a dodecahedron and an icosahedron, $|G|=60$ and $|G_S|=120$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Tetrahedron has $4$ vertices and $4$ faces. Then, the number of edges is $6$ as $4-6+4=-2+4=2$. Let $e$ be the number od edges that pass through each vertex. Then, that should be $\frac{e\cdot 4}{2}=6$, as each edge contains two vertices. Then, $e\cdot 4=12$, so $e=3$. Therefore, there are $3$ edges through each vertex, and so each vertex has three adjacent vertices. For $G$, we will count only rotations. So, if we choose some vertex $v(i)$ we can send it to $v(k)$, which can be chosen in $4$ different ways. Also, we want to send vertex adjacent to $v(i)$ some vertex adjacent to $v(k)$ (and there are three of them). Therefore, the total number is $|G|=4\cdot 3=12$. If we want to make our counting weaker, we can allow one of two remaining vertices adjacent to $v(i)$ to be sent to some of remaining vertices adjacent to $v(k)$ (two left). Therefore, we have two times more possibilities and that is $|G_S|=4\cdot 3\cdot 2=24$.

{\it Ad $2$.} Cube and octahedron are dual, so we will use a different counting method to grasp both. Now, cube has $6$ faces with $4$ vertices on each face. Octahedron has $8$ faces with $3$ vertices on each face. If we fix a face on a cube, it can be sent to $6$ different faces and then rotated. There are $4$ rotations, of course, so $|G|=6\cdot 4=24$. Same reasoning goes for octahedron, so $|G|=8\cdot 3=24$. If we allow reflections, we will allow not only rotations of faces, but also their reflections. That is the dihedral group $D_{8}$ for a cube and $D_{6}$ for octahedron (meaning, for each of their faces). Therefore, we have $|G_S|=6\cdot 8=8\cdot 6=48$.

{\it Ad $3$.} Same as above, we only need to know that dodecahedron has $20$ vertices, $12$ faces and $5$ vertices on each. Icosahedron has $20$ faces, as it is dual to dodecahedron. As it is composed of triangles, it has $3$ vertices on each face. So, by the same reasoning as above, we have $|G|=12\cdot 5=20\cdot 3=60$ and $|G_S|=12\cdot 10=20\cdot 6=120$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Direct product}
\end{center}

\vskip 0.5cm

\noindent{\bf Theorem.} Let $G_1,G_2,H_1,H_2$ be groups. If $G_1\cong G_2$ and $H_1\cong H_2$, then $G_1\times H_1\cong G_2\times H_2$.

\noindent\newline{\bf Proof.} From $G_1\cong G_2$ we have that there exists an isomorphism $g:G_1\rightarrow G_2$ and from $H_1\cong H_2$ that there exists an isomorphism $h:H_1\rightarrow H_2$. Then, we define mapping $f:G_1\times H_1\rightarrow G_2\times H_2$ with $f(x,y)=(g(x),h(y))$. We will show that $f$ is well-defined. Take $(x,y)\in G_1\times H_1$. Then, $x\in G_1$ and $y\in H_1$, and as $g$ and $h$ are well-defined we have $g(x)\in G_2$ and $h(y)\in H_2$, meaning $(g(x),h(y))\in G_2\times H_2$. If $(x_1,y_1)=(x_2,y_2)$, then $x_1=x_2$ and $y_1=y_2$, and, as $g$ is well-defined that implies $g(x_1)=g(x_2)$ and, as $h$ is well-defined, $h(y_1)=h(y_2)$. From that we have $(g(x_1),h(y_1))=(g(x_2),h(y_2))$, i.e. $f(x_1,y_1)=f(x_2,y_2)$. Thus, $f$ is also a well-defined function.

Now we will show that $f$ is a bijection. {\it Surjectivity.} Take $(x',y')\in G_2\times H_2$. Then, $x'\in G_2$ and $y'\in H_2$. As $g$ and $h$ are surjective, there exist $x\in G_1$ and $y\in H_1$ such that $g(x)=x'$ and $h(y)=y'$. Therefore, there exists $(g(x),h(y))\in G_1\times H_1$ such that $(g(x),h(y))=(x',y')$ and $f$ is surjective. {\it Injectivity.} Take $f(x_1,y_1)=f(x_2,y_2)$. From that follows $(g(x_1),h(y_1))=(g(x_2),h(x_2))$. That implies $g(x_1)=g(x_2)$ and $h(y_1)=h(y_2)$. As $g$ and $h$ are injective, then that implies $x_1=x_2$ and $y_1=y_2$, i.e. $(x_1,y_1)=(x_2,y_2)$, so $f$ is injective also. In conclusion it is also bijective. Finally, we have $f((x_1,y_1)(x_2,y_2))=f(x_1 x_2,y_1 y_2)=(g(x_1 x_2),h(y_1 y_2))=(g(x_1)g(x_2),h(y_1)h(y_2))=(g(x_1),h(y_1))(g(x_2),h(y_2))=f(x_1,y_1)f(x_2,y_2)$, so $f$ is an isomorphism from $G_1\times H_1$ to $G_2\times H_2$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $G$, $K$ and $H$ be groups. If $G\cong K$, then $G\times H\cong K\times H$.

\noindent\newline{\bf Proof.} The relation of being isomorphic is a relation of equivalence, so it is reflexive, and for group $H$, we have $H\cong H$. Along with $G\cong K$, using the previous theorem, we get $G\times H\cong K\times H$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $G$, $H$, $H_1$, $K$ be groups. If $G\cong H\times K$ and $H\cong H_1$, then $G\cong H_1\times K$.

\noindent\newline{\bf Proof.} We have $H\cong H_1$ and $K\cong K$. Then, by previous theorem, $H\times K\cong H_1\times K$. As relation of being isomorphic is a relation of equivalence, it is also transitive, so $G\cong H\times K$ and $H\times K\cong H_1\times K$ implies $G\cong H_1\times K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $n\in\Z^{+}$ and $G_1,\ldots,G_n$ be groups with $e_1,\ldots,e_n$ as their neutral elements, respectively, and $i\in\{1,\ldots,n\}$. Then, we define:

\begin{equation*}
\proj{G}{i}=\{e_1\}\times\cdots\times\{e_{i-1}\}\times G_i\times\{e_{i+1}\}\times\cdots\{e_n\}.
\end{equation*}

\noindent\newline{\bf Lemma.} Let $n\in\Z^{+}$ and let $G_1,\ldots,G_n$ be groups. Then, for all $i\in\{1,\ldots,n\}$,

\begin{equation*}
\proj{G}{i}\cong G_i.
\end{equation*}

\noindent\newline{\bf Proof.} Let us denote $\proj{x}{i}=(e_1,\ldots,e_{i-1},x,e_{i+1},\ldots,x_n)$, for all $x\in G_i$. We define $f:\proj{G}{i}\rightarrow G_i$ with $f(\proj{x}{i})=x$. Then, $f$ is well-defined because, if we take $\proj{x}{i}\in\proj{G}{i}$, we have $x\in G_i$, so $f(\proj{x}{i})=x$. Also, $f$ satisfies property of uniqueness, because if $\proj{x}{i}=\proj{y}{i}$, then obviously $x=y$ (that is, $f(\proj{x}{i})=f(\proj{y}{i})$) as they are both on $i$-th place. {\it Surjectivity.} Take $x\in G_i$. Then, it is obvious that there exists $\proj{x}{i}\in\proj{G}{i}$, as then $x$ is on the $i$-th place and is therefore in $G_i$, such that $f(\proj{x}{i})=x$. {\it Injectivity.} Let $f(\proj{x}{i})=f(\proj{y}{i})$, i.e. $x=y$. Then it is easy to see that $\proj{x}{i}=\proj{y}{i}$, because all other places have $e_j$, where $j\in\{0,\ldots,n\}-\{i\}$. Thus, $f$ is bijective. Finally, $f(\proj{x}{i}\proj{y}{i})=f(\proj{x y}{i})=x y=f(\proj{x}{i})f(\proj{y}{i})$ and $f$ is an isomorphism which brings us to $\proj{G}{i}\cong G_i$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $n\in\Z^{+}$ and let $G_1,\ldots,G_n$ be groups. Then,

\begin{equation*}
\pi_{i}:G_1\times\cdots\times G_n\rightarrow G_1\times\cdots\times G_{i-1}\times G_{i+1}\times\cdots\times G_n,
\end{equation*}

\noindent\newline defined with

\begin{equation*}
\pi_{i}(x_1,\ldots,x_n)=(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n),
\end{equation*}

\noindent\newline for all $i\in\{1,\ldots,n\}$, is a surjective homomorphism and $\ker{\pi_i}\cong G_i$.

\noindent\newline{\bf Proof.} Let us denote $G=G_1\times\cdots\times G_n$ and $G'=G_1\times\cdots\times G_{i-1}\times G_{i+1}\times\cdots\times G_n$. First, we will prove that $\pi_i$ is well-defined. Take $X\in G$. Then, $X=(x_1,\ldots,x_n)$, where $x_j\in G_j$, for all $j\in\{1,\ldots,n\}$. That implies that $X'=(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n)\in G'$, so $\pi_i(X)=X'$. Now, if $(x_1,\ldots,x_n)=(y_1,\ldots,y_n)$, then, $x_j=y_j$, for all $j\in\{1,\ldots,n\}$, which implies $(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n)=(y_1,\ldots,y_{i-1},y_{i+1},\ldots,y_n)$, that is $\pi_i(x_1,\ldots,x_n)=\pi_i(y_1,\ldots,y_n)$. Let $X'\in G'$. Then $X'=\left(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n\right)$, where $x_j\in G_j$, for all $j\in\{1,\ldots,n\}-\{i\}$. But, we can take, e.g. $e_i\in G_i$, to have $\pi_i(x_1,\ldots,x_{i-1},e_i,x_{i+1},\ldots,x_n)=X'$. Therefore, $\pi_i$ is well-defined and surjective. Finally,

\begin{eqnarray*}
\pi_i((x_1,\ldots,x_n)(y_1,\ldots,y_n))&=&\pi_i(x_1 y_1,\ldots,x_n y_n)\\
&=&(x_1 y_1,\ldots,x_{i-1} y_{i-1},x_{i+1} y_{i+1},\ldots,x_n y_n)\\
&=&(x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n)(y_1,\ldots,y_{i-1},y_{i+1},\ldots,y_n)\\
&=&\pi_i(x_1,\ldots,x_n)\pi_i(y_1,\ldots,y_n).
\end{eqnarray*}

\noindent\newline Thus, $\pi_i$ is a surjective homomorphism. Finally, $\ker{\pi_i}=\{(x_1,\ldots,x_n)\in\dom{\pi_i}:\ \pi_i(x_1,\ldots,x_n)=(e_1,\ldots,e_{i-1},e_{i+1},\ldots,e_n)\}$. But, that means $\ker{\pi_i}=\{(x_1,\ldots,x_n)\in\dom{\pi_i}:\ (x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_n)=(e_1,\ldots,e_{i-1},e_{i+1},\ldots,e_n)\}$, i.e. $\ker{\pi_i}=\{(e_1,\ldots,e_{i-1},x,e_{i+1},\ldots,e_n)\in\dom{\pi_i}:\ x\in G_i\}=\proj{G}{i}$. By previous lemma, $\proj{G}{i}\cong G_i$, so $\ker{\pi_i}\cong G_i$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $n\in\Z^{+}$ and let $G_1,\ldots,G_n$ be groups and $i\in\{1,\ldots,n\}$. Then,

\begin{equation*}
G_1\times\cdots\times G_n\slash\proj{G}{i}\cong G_1\times\cdots\times G_{i-1}\times G_{i+1}\times\cdots\times G_n.
\end{equation*}

\noindent\newline{\bf Proof.} Follows directly from the previous lemma, by the fundamental homomorphism theorem (applicable as $\pi_i$ is surjective homomorphism).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $n\in\Z^{+}$ and let $G_1,\ldots,G_n$ be groups and $i\in\{1,\ldots,n\}$. Then,

\begin{equation*}
\proj{G}{i}\trianglelefteq G_1\times\cdots\times G_n.
\end{equation*}

\noindent\newline{\bf Proof.} Follows from the previous theorem and its lemma, as $\ker{\pi_i}=\proj{G}{i}$ and we know $\ker{\pi_i}\trianglelefteq\dom{\pi_i}=G_1\times\cdots\times G_n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} This is most easily seen in Euclidean spaces. For example, if we have $\R^3=\R\times\R\times\R$ and $\R^2=\R\times\R$, then getting the quotient group $\R^3\slash\proj{\R}{i}\cong\R^2$, for any $i\in\{1,2,3\}$, is the same as projecting the Euclidean space on the Euclidean plane.

\newpage

\begin{center}
{\bf Rings}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be a non-empty set, $+:A\times A\rightarrow A$ (addition) and $\cdot:A\times A\rightarrow A$ (multiplication) binary operations defined on $A$. An ordered triple $\left(A,+,\cdot\right)$ is called a {\bf ring} if the following axioms are satisfied:

\begin{enumerate}
\item $\left(A,+\right)$ is an Abelian group.
\item $\left(A,\cdot\right)$ is a semigroup\footnote{I.e. it is associative.}.
\item Multiplication is distributive over addition. In other words, for all $a,b,c\in A$:

\begin{eqnarray*}
a (b+c)&=&a b+a c,\\
(b+c) a&=&b a+c a.
\end{eqnarray*}

\end{enumerate}

\noindent{\bf Remark.} A neutral element in $A$ is denoted with $0$ and inverse of $a\in A$ is denoted as $-a$. Also, as in group theory, we will denote ring $(A,+,\cdot)$ only with $A$. Notice that the corresponding set of $(A,+,\cdot)$ is $A$.

\noindent\newline{\bf Definition.} Let $A$ be a ring. Then, we define {\bf subtraction} as $-:A\times A\rightarrow A$ with $a-b=a+(-b)$, for all $a,b\in A$.

\noindent\newline{\bf Remark.} We can see that subtraction is well defined as each element in a $A$ has an additive inverse ($A$ with addition is an Abelian group).

\noindent\newline{\bf Example.} Here are some examples of rings:

\begin{itemize}
\item $\Z=\left(\Z,+,\cdot\right)$;
\item $\Q=\left(\Q,+,\cdot\right)$;
\item $\R=\left(\R,+,\cdot\right)$;
\item $\C=\left(\C,+,\cdot\right)$;
\item $\Z_n=\left(\Z_n,+_n,\cdot_n\right)$;
\item $\mathcal{F}(\R)=\left(\mathcal{F}(\R),+,\cdot\right)$ with $[f+g](x)=f(x)+g(x)$ and $[f g](x)=f(x)g(x)$, for all $x\in\R$.
\end{itemize}

\noindent{\bf Definition.} Let $A$ be a ring. If corresponding set of ring $A$ is finite, then we say that $A$ is finite.

\noindent\newline{\bf Theorem.} Let $A$ be a ring and $a,b\in A$. Then:

\begin{enumerate}
\item $a 0=0 a=0$;
\item $a(-b)=-(a b)$ and $(-a)b=-(a b)$;
\item $(-a)(-b)=a b$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} We use the fact that $a+0=a$. We have $a a+0=a a$. But, also, as $a=a+0$, we can write $a a+0=a(a+0)$. By distributive law, $a a+0=a a+ a 0$. We can write that as $a^2+0=a^2+a 0$. As $A$ is closed with respect to multiplication, then $a^2\in A$ and so $a^2$ has an additive inverse $-a^2$. We apply that on the left side of equality to get $-a^2+a^2+0=-a^2+a^2+a 0$. That gives us $0+0=0+a 0$, i.e. $0=a 0$. Now to prove that $0 a=0$, we simply use again $a a+0=a a$, but substitute $a=a+0$ so that $a a+0=(a+0) a$. From that we have $a^2+0=a^2+0 a$. Again, applying $-a^2$ on the left gives us $-a^2+a^2+0=-a^2+a^2+0 a$. Then, $0+0=0+0 a$, which is $0=0 a$.

{\it Ad $2$.} We have $a 0=0$. Also, $b+(-b)=0$. From that we get $a(b+(-b))=0$. By distributive law, $a b+a(-b)=0$. As $a b\in A$ (closed with respect to multiplication), then $-(a b)\in A$ is the additive inverse of $a b$. Therefore, applying $-a b$ on the left we get $-a b+a b+a(-b)=-a b+0$. From that we have $0+a(-b)=-(a b)$, i.e. $a(-b)=-(a b)$. Similarly, as $0 b=0$ and $a+(-a)=0$ we have $(a+(-a))b=0$. By distributive law $a b+(-a)b=0$. Then, using the inverse $-(a b)$ we get $(-a)b=-(a b)$.

{\it Ad $3$.} We have $a+(-a)=0$ and $0(-b)=0$, so $(a+(-a))(-b)=0$. By distributive law, $a(-b)+(-a)(-b)=0$. From previous property, we have $a(-b)=-(a b)$. Therefore, $-(a b)+(-a)(-b)=0$. As $-(-(a b))=a b$ (result from group theory), we can apply $a b$ on the left of the equality to get $a b+(-a b)+(-a)(-b)=a b+0$. From that follows $0+(-a)(-b)=a b$, i.e. $(-a)(-b)=a b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $\left(A,+,\cdot\right)$ be a ring. We say that $A$ is:

\begin{itemize}
\item {\bf Commutative ring} if $\left(A,\cdot\right)$ is a commutative semigroup, i.e. associativity holds ($a(b c)=(a b)c$, for all $a,b,c\in A$) and $a b=b a$, for all $a,b\in A$.
\item {\bf Commutative ring with unity} if $\left(A,\cdot\right)$ is a commutative monoid, i.e. it has a neutral element usually designated as $1$ ($1 a=a 1=a$, for all $a\in A$) and $a b=b a$, for all $a,b\in A$.
\item {\bf Skew field} if $\left(A-\{0\},\cdot\right)$ is a group.
\item {\bf Field} if $\left(A-\{0\},\cdot\right)$ is an Abelian group.
\end{itemize}

\noindent{\bf Proposition.} Let $A$ be a commutative ring with unity. Then, $0$ has a multiplicative inverse if and only if $A$ is trivial ($A=\{0\}$).

\noindent\newline{\bf Proof.} {\it Necessity.} Suppose $0$ is invertible. Then, there exists $a\in A$ such that $a 0=1$. But, by a previos theorem $a 0=0$, so $0=1$. Suppose that there exists some $a\in A$, $a\neq 0,1$. Then, as $A$ is a commutative ring, $a 1=a$, but, as $0=1$, we have $a 0=a$, i.e. $a=0$. Therefore, $A=\{0\}$. {\it Sufficiency.} Suppose $A=\{0\}$. Then, $A$ is a commutative ring with unity. Neutral element for addition is $0$. Inverse is $-0=0$ (as $0+(-0)=0$ and, as $0$ is a neutral element, $-0=0$). It is commutative and associative. As for multiplication it is also commutative and associative. Neutral element is $0$ as $0 0=0$. Inverse element is $0$, again, as $0 0=0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring. Element $a\in A$, $a\neq 0$ is called {\bf divisor of zero} if there exists $b\in A$, $b\neq 0$, such that $a b=0$ or $b a=0$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring. If there does not exist a divisor of zero in $A$, then $a b=0$ implies $a=0$ or $b=0$.

\noindent\newline{\bf Proof.} Let $A$ be a ring with no divisors of zero. Let $a b=0$. Assume that $a\neq 0$ and $b\neq 0$. Then, from $a b=0$ it follows by definition that $a$ and $b$ are divisors of zero, which is contrary to our assumption.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring. We say that $A$ has the {\bf cancellation property} if $a b=a c$ or $b a=c a$ implies $b=c$, for all $a,b,c\in A$ with $a\neq 0$.

\noindent\newline{\bf Remark.} Let $A$ be a ring. Then $0 b=0 c$ gives no information about the nature of $b=c$. For example, in $\Z$, $0\cdot 5=0\cdot 2$, but $5\neq 2$. Thus, we have excluded $0$ in the previous definition.

\noindent\newline{\bf Proposition.} Ring $A$ has a cancellation property if and only if it has no divisors of zero.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume that $a b=a c$ or $b a=c a$ implies $b=c$, for all $a,b,c\in A$, $a\neq 0$. Assume that there exist $d,e\in A$, $d\neq 0$ and $e\neq 0$ such that $d e=0$. As $e 0=0$, then $d e=0 e$. By cancellation property (applicable as $e\neq 0$), we have $d=0$ which is contrary to our assumption that $d\neq 0$. Therefore, there do not exist divisors of zero in $A$. {\it Sufficiency.} Assume $A$ has no divisors of zero. Then, $a b=0$ implies $a=0$ or $b=0$, for all $a,b\in A$. Assume $a b=a c$, with $a\neq 0$. As $A$ is Abelian with respect to addition, then there exists $-(a c)\in A$ such that $a c+(-(a c))=0$. Applying $-(a c)$ on the right gives us $a b+(-(a c))=a c+(-(a c))$, i.e. $a b+(-(a c))=0$. By a previous theorem, $-(a c)=a(-c)$. Therefore, $a b+a(-c)=0$. By distributive law, $a(b+(-c))=0$. As $A$ has no divisors of zero, it follows that $a=0$ or $b+(-c)=0$. But, we assumed $a\neq 0$, therefore the only remaining possibility is that $b+(-c)=0$. From that we have, by applying $c\in A$ on the right, $b+(-c)+c=c$, i.e. $b=c$. Similarly we can prove that $b a=c a$ implies $b=c$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a non-trivial commutative ring with unity such that it has cancellation property (or, equivalently, no divisors of zero). Then we say that $A$ is an {\bf integral domain}.

\noindent\newline{\bf Problem.} In each of the following, a set $A$ with operation of addition and multiplication is given. Prove that $A$ satisfies all the axioms to be a commutative ring with unity. Indicate the zero element, the unity, and the negative of an arbitrary $a$.

\begin{enumerate}
\item $A=\left(\Z,\oplus,\odot\right)$, where $a\oplus b=a+b-1$ and $a\odot b=a b-(a+b)+2$;
\item $A=\left(\Q,\oplus,\odot\right)$, where $a\oplus b=a+b+1$ and $a\odot b=a b+a+b$;
\item $A=\left(\Q\times\Q,\oplus,\odot\right)$, where $(a,b)\oplus(c,d)=(a+c,b+d)$ and $(a,b)\odot(c,d)=(a c-b d,a d+b c)$;
\item $A=\{x+y\sqrt{2}:\ x,y\in\Z\}$ with conventional addition and multiplication.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $A=\left(\Z,\oplus,\odot\right)$, where $a\oplus b=a+b-1$ and $a\odot b=a b-(a+b)+2$;} From problem on page $2$ we have that $\R$ with $\oplus$ is an Abelian group (reader will notice that all the properties on page $12$ do not depend on properties of $\R$ specifically and can be narrowed down to $\Z$ without much problem). We will assume associativity for $\odot$, but we have already proved it for a similar operation on page $12$. Now we will check distributive laws. We observe $a\odot\left(b\oplus c\right)=a\odot\left(b+c-1\right)=a (b+c-1)-(a+(b+c-1))+2=a b+a c-a-a-b-c+1+2=a b+a c-2a-b-c+3$. Now, $a\odot b\oplus a\odot c=(a b-(a+b)+2)\oplus(a c-(a+c)+2)=a b-(a+b)+2+a c-(a+c)+2-1=a b-2 a-b-c+3$. Therefore, $a\odot\left(b\oplus c\right)=a\odot b\oplus a\odot c$. Similarly, we can show that $(a\oplus b)\odot c=a\odot c\oplus b\odot c$. But, we will first prove commutativity for $\Z$ with $\odot$. We have $a\odot b=a b-(a+b)+2=b a-(b+a)+2=b\odot a$. Therefore, $A$ is commutative. So, $(a\oplus b)\odot c=c\odot(a\oplus b)=c\odot a\oplus c\odot b=a\odot c\oplus b\odot c$ and $A$ is a commutative ring. It's zero element is the neutral element for $\oplus$ and that can be obtained from $a+e-1=a$, i.e. $e=1$. So, here, zero is $1$. Neutral element in $\Z$ with $\odot$ can be obtained from $a e-(a+e)+2=a$. From that we have $a e-a-e+2=a$, i.e. $e(a-1)=2a+2$. That implies $e(a-1)=2(a-1)$. Assume $a\neq 1$. Then $e=2$. But, if $a=1$, we have $2\odot 1=2\cdot 1-(2+1)+2=2-2-1+2=1=1\odot 2$. Therefore, neutral element for $\odot$ in $\Z$ is $2$. Therefore, $A$ is a commutative ring with unity. Then, we will find multiplicative inverses. It must be that $a\odot a^{-1}=2$, for all $a\neq 1$. That implies $a a^{-1}-(a+a^{-1})+2=2$, i.e. $a^{-1}(a-1)=a$. From that we have, as $a\neq 1$, that $a^{-1}=\frac{a}{a-1}$. But, $\frac{a}{a-1}\in\Z$ if and only if $a-1|a$, i.e. if there exists $q\in\Z$ such that $a=q(a-1)$. From that we have that $q|a$ (assuming $q\neq 0$) and then it's $\frac{a}{q}=a-1$. That is equivalent to (for $a\neq 1$, of course) $\frac{a}{q}\frac{1}{a-1}=1$. Therefore, as $\frac{a}{q}=k$, for some $k\in\Z$, it must be that $\frac{1}{a-1}=\pm1$. Then, $\pm1=a-1$ implies $a=2$ or $a=0$. If $q=0$ then it would mean that $a=0$, again. Therefore, the only invertible elements in $A$ are in $A^{*}=\{0,2\}$.

\item {\it $A=\left(\Q,\oplus,\odot\right)$, where $a\oplus b=a+b+1$ and $a\odot b=a b+a+b$;} We also know from page $12$ that $(\Q,\oplus)$ is a commutative group (see reasoning from previous problem). We have proven associativity for the similar operation on page $12$. Now, we will check distributivity. We have $a\odot\left(b\oplus c\right)=a\odot(b+c+1)=a(b+c+1)+a+(b+c+1)=a b+a c+2a+b+c+1$. Then, $a\odot b\oplus a\odot c=(a b+a+b)\oplus(a c+a+c)=a b+a+b+a c+a+c+1=a b+2a+b+c+1$. Therefore, first distributive law holds. The second one will hold if we prove $\odot$ is commutative. We have $a\odot b=a b+a+b=b a+b+a=b\odot a$. Therefore, $(a\oplus b)\odot c=c\odot(a\oplus b)$, wherein follows the first distributive law. Then we seek zero element from $a\oplus e=a$, i.e. $a+e+1=a$. That gives us $e=-1$. Now, we get unity from $a e+a+e=a$. We have $e(a+1)=0$, so $e=0$. Therefore, zero element is $-1$ and unity is $0$. Multiplicative inverses are obtained from $a a^{-1}+a+a^{-1}=0$. Then, $a^{-1}(a+1)=-a$, i.e. $a^{-1}=\frac{-a}{a+1}$, where $a\neq-1$ (zero element). As $\frac{-a}{a+1}\in\Q$, for all $a\in\Q$, $a\neq-1$, all non-zero elements have an inverse and therefore, $A$ is a field.

\item {\it $A=\left(\Q\times\Q,\oplus,\odot\right)$, where $(a,b)\oplus(c,d)=(a+c,b+d)$ and $(a,b)\odot(c,d)=(a c-b d,a d+b c)$;} We already know from a previous exercise that $\Q\times\Q$ with $\oplus$ is an Abelian group. Also, we know that $\Q\times\Q-\{(0,0)\}$ is also an Abelian group (similar to multiplication of complex numbers). So, we will check distributive law. We have $(a,b)\odot((c,d)\oplus(e,f))=(a,b)\odot(c+e,d+f)=(a(c+e)-b(d+f),a(d+f)+b(c+e))=(a c+a e-b d-b f,a d+a f+b c+b e)$. Similarly, $(a,b)\odot(c,d)\oplus(a,b)\odot(e,f)=(a c-b d,a d+b c)\oplus (a e-b f,a f+b e)=(a c+a e-b d-b f,a d+a f+b c+b e)$. Therefore, distributivity holds (as $\odot$ is commutative, second distributive law holds - see previous two exercises). As $\Q\times\Q-\{(0,0)\}$ is an Abelian group, $A$ is a field. Note that $(0,0)$ is zero and $(1,0)$ is unity (as $(1,0)(a,b)=(1a+0b,1b+0a)=(a,b)$, etc.).

\item {\it $A=\{x+y\sqrt{2}:\ x,y\in\Z\}$ with conventional addition and multiplication.} We will prove that $(A,+)\leq\R$ and $(A,\cdot)\leq\R^{\ast}$. First, if $a\in A$, then $a=x+y\sqrt{2}$, where $x,y\in\Z$. Then, $a\in\R$. So, $A\subseteq\R$. If we take $a,b\in A$, then $a=x+y\sqrt{2}$ and $b=z+w\sqrt{2}$. We have $a+b=(x+z)+(y+w)\sqrt{2}$ so $a+b\in A$. Similarly, $a b=(x+y\sqrt{2})(z+w\sqrt{2})=(x z+2y w)+(y z+x w)\sqrt{2}$ and $a b\in A$. If $a\in A$, then $-a=(-x)+(-y)\sqrt{2}$ and $-a\in A$. But, $a^{-1}=\frac{1}{x+y\sqrt{2}}$ is not necessarily in $A$. Therefore, $(A,+)$ is an Abelian group and $(A,\cdot)$ is a commutative monoid. Distributivity is inherited from addition and multiplication in $\R$ and that means that $(A,+,\cdot)$ is a commutative ring with unity.

\end{enumerate}

\noindent{\bf Problem.} Verify that $\mathcal{F}(\R)$ satisfies all the axioms for being a commutative ring with unity. Indicate the zero and unity, and desribe the negative of any $f$. Describe the divisors of zero and invertible elements in $\mathcal{F}(\R)$ and explain why $\mathcal{F}(\R)$ is neither a field nor an integral domain.

\noindent\newline{\bf Solution.} We know that $\mathcal{F}(\R)$ when considering only addition is an Abelian group. Zero is obviously $f(x)=0$. But, $\mathcal{F}(\R)$ when observing multiplication is only a commutative monoid, as not all functions have multiplicative inverses. As $[f g](x)=f(x)g(x)$, for all $x\in\R$, we need $\left[f f^{-1}\right](x)=f(x)\frac{1}{f(x)}=x$, for all $x\in\R$. But, if e.g. $f(x)=x-a$, then $f^{-1}(a)=\frac{1}{a-a}=\frac{1}{0}$ is not defined. Unity is $f(x)=1$. Distributive law holds as $f(x)(g(x)+h(x))=f(x)[g+h](x)=[f(g+h)](x)=[f g+f h](x)=[f g](x)+[f h](x)=f(x)g(x)+f(x)h(x)$. Second distributive law holds as $\mathcal{F}(\R)$ is commutative. Therefore, $\mathcal{F}(\R)$ is a commutative ring with unity. If we have $f(x)g(x)=0$, then, not necessarily $f(x)=0$ or $g(x)=0$. Take $f(x)=x\mathcal{I}_{\left\langle-\infty,0\right\rangle}$ and $g(x)=x\mathcal{I}_{\left[0,\infty\right\rangle}$. Then obviously $f(x)g(x)=0$, but $f(x)\neq 0$ and $g(x)\neq 0$. Thus, $\mathcal{F}(\R)$ is not an integral domain.

\noindent\newline{\bf Problem.} Let $\mathcal{M}_2$ designate the set of all $2\times2$ matrices (with entries from $\R$) with usual addition and multiplication. Verify that $\mathcal{M}_r(\R)$ satisfies the ring axioms and unity, but not commutativity. Explain why $\mathcal{M}_2(\R)$ is not an integral domain or a field.

\noindent\newline{\bf Solution.} We know from previous exercises that $\mathcal{M}_2$ under matrix addition is an Abelian group and a monoid under matrix mulitiplication. Checking distributivity is easy but tedious (as I am really tired and want to focus on more important stuff). Thus, $\mathcal{M}_2$ is a ring with unity. Also, it is a trivial fact from linear algebra that $A B=0$ does not imply $A=0$ or $B=0$, so $\mathcal{M}_2$ is not an integral domain.

\noindent\newline{\bf Problem.} If $D$ is a set, then the power set of $D$ is the set $\mathcal{P}_D$ of all the subsets of $D$. Addition and multiplication are defined with $A+B=(A-B)\cup(B-A)$ and $A B=A\cap B$. Prove that $\mathcal{P}_D$ is a commutative ring with unity. Describe the divisors of zero and invertible elements in $\mathcal{P}_D$ and explain why it is neither a field nor an integral domain.

\noindent\newline{\bf Solution.} We know from a previous exercise that $\mathcal{P}_D$ with addition is an Abelian group and $\mathcal{P}_D$ with multiplication is a commutative monoid (we have no inverse for intersection). We will also assume distributivity. Proving it is a bit tedious but elementary. Zero is $\emptyset$ as $A+\emptyset=(A-\emptyset)\cup(\emptyset-A)=A\cup\emptyset=A$. Unity is $D$ as $A\cap D=A$. Now, from $A B=\emptyset$ we have $A\cap B=\emptyset$. Divisors of zero are sets that are disjoint as $A\cap B=\emptyset$ implies that. Invertible elements for addition is $-A=A$ as $A+A=(A-A)\cup(A-A)=\emptyset\cup\emptyset=\emptyset$. Invertible elements for multiplication are $A A^{-1}=D$. But, that is $A\cap A^{-1}=D$. From that we have that the only invertible element is actually $D$ whose inverse is $D$. Therefore, $\mathcal{P}_D$ is not a field, and as it has divisors of zero, it is not an integral domain.

\noindent\newline{\bf Definition.} Let $(G,+)$ be an Abelian group. An {\bf endomorphism} of $G$ is a homomorphism from $G$ to $G$. By $\End{G}$ we denote the set of all endomorphisms of $G$ and define $[f+g](x)=f(x)+g(x)$ and $[f g](x)=[f\circ g](x)$, for every $x\in G$.

\noindent\newline{\bf Proposition.} Let $(G,+)$ be an Abelian group. Then, $\End{G}$ is a ring with unity.

\noindent\newline{\bf Proof.} First we will check the group axioms for $+$. {\it Associativity.} For all $f,g,h\in\End{G}$ we have $[[f+g]+h](x)=[f+g](x)+h(x)=(f(x)+g(x))+h(x)$. As $G$ is associative, then $(f(x)+g(x))+h(x)=f(x)+(g(x)+h(x))=f(x)+[g+h](x)=[f+[g+h]](x)$. {\it Identity (zero).} If $f\in\End{G}$ then we need $0\in\End{G}$ such that $[f+0](x)=[0+f](x)=f(x)$, for all $x\in G$. As $G$ is a group it has an identity $0\in G$. Then, $0(x)=0$ would work as $[f+o](x)=f(x)+0(x)=f(x)+0$. As $f(x)\in G$ (it is an endomorphism), then $f(x)+0=f(x)$. It is easy to see that $0$ is an endomorphism, as $0:G\rightarrow G$ and $o(x+y)=0=0+0=0(x)+0(y)$. Also, this works for $[0+f](x)=f(x)$, as $G$ is Abelian. {\it Inverses.} We need $-f\in\End{G}$ such that $[f+(-f)](x)=[-f+f](x)=0(x)$. Then, as $f(x)\in G$, it has an inverse $-[f(x)]\in G$. Therefore, we will define $-f(x)=-[f(x)]$. Also, $-f$ is an endomorphism as $-f(x+y)=-[f(x+y)]=-[f(x)+f(y)]=-[f(y)]+(-[f(x)])$. As $G$ is Abelian, $-[f(y)]+(-[f(x)])=-[f(x)]+(-[f(y)])=-f(x)+(-f(y))$. Now, $[f+(-f)](x)=f(x)+(-f(x))$. As $f(x)\in G$ and $-f(x)=-[f(x)]$, where $-[f(x)]\in G$ is an inverse of $f(x)\in G$, we have $f(x)+(-[f(x)])=0=0(x)$. As $G$ is Abelian, $[-f+f](x)=0(x)$, also. Now, from $f,g\in\End{G}$ we have $[f+g](x)=f(x)+g(x)$. As $G$ is Abelian, $f(x)+g(x)=g(x)+f(x)=[g+f](x)$. Therefore, $\End{G}$ is an Abelian group.

We will check distributive laws. We have $[f [g+h]](x)=[f\circ[g+h]](x)=f([g+h](x))=f(g(x)+h(x))$. Now, as $f$ is an endomorphism, i.e. a homomorphism from $G$ to $G$, then $f(g(x)+h(x))=f(g(x))+f(h(x))=[f\circ g](x)+[f\circ h](x)=[f g](x)+[f h](x)$. In other words, $f(g+h)=f g+f h$. As composition is not generally commutative, we need to check $[[g+h] f](x)=[[g+h]\circ f](x)=[g+h](f(x))=g(f(x))+h(f(x))=[g f](x)+[h f](x)$, i.e. $(g+h)f=g f+h f$. {\it Associativity.} Function composition is associative and so is composition in $\End{G}$. {\it Identity (unity).} We need to find $1\in\End{G}$ such that $[f 1](x)=[1 f](x)=f(x)$. If we take $1(x)=x$, for all $x\in G$, then $1:G\rightarrow G$ is an endomorphism as $1(x+y)=x+y=1(y)+1(y)$. Now, $[f 1](x)=f(1(x))=f(x)$ and $[1 f](x)=1(f(x))=f(x)$. Therefore, $1\in\End{G}$ is a unity and $\End{G}$ is a ring with unity. Notice that endomorphisms are not necessarily bijections and might have no inverses. Also, composition is not generally commutative.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Definition.} Let $A$ and $B$ be rings. We define the {\bf direct product of rings} $A$ and $B$ as $A\times B$ such that for all $(x_1,y_1),(x_2,y_2)\in A\times B$ it holds:

\begin{eqnarray*}
(x_1,y_1)+(x_2,y_2)&=&(x_1+x_2,y_1+y_2),\\
(x_1,y_1)(x_2,y_2)&=&(x_1 x_2,y_1 y_2).
\end{eqnarray*}

\noindent\newline{\bf Proposition.} Let $A$ and $B$ be rings. Then:

\begin{enumerate}
\item $A\times B$ is a ring.
\item If $A$ and $B$ are commutative, then so is $A\times B$.
\item If $A$ and $B$ have unity, then so does $A\times B$.
\item $A\times B$ can never be an integral domain or a field.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} As $A$ and $B$ are Abelian groups when considering addition, then, by a previous proposition, set $A\times B$ with addition is an Abelian group. Similarly, from the proof of the same proposition, we can conclude that, when considering multiplication, from $A$ and $B$ are semigroups it follows that $A\times B$ is a semigroup. Therefore, we only need to check distributivity. First, $(x_1,y_1)((x_2,y_2)+(x_3,y_3))=(x_1,y_1)(x_2+x_3,y_2+y_3)=(x_1(x_2+x_3),y_1(y_2+y_3))$. As $x_1,x_2,x_3\in A$, and $A$ is a ring, distributivity holds, so $x_1(x_2+x_3)=x_1 x_2+x_1 x_3$. Similarly, as $B$ is a ring, $y_1(y_2+y_3)=y_1 y_2+y_1 y_3$. Therefore, $(x_1(x_2+x_3),y_1(y_2+y_3))=(x_1 x_2+x_1 x_3,y_1 y_2+y_1 y_3)=(x_1 x_2,y_1,y_2)+(x_1 x_3,y_1 y_3)=(x_1,y_1)(x_2,y_2)+(x_1,y_1)(x_3,y_3)$. Similarly we prove the second distributive law. Therefore, $A\times B$ is a ring.

{\it Ad $2$.} If $A$ and $B$ are commutative rings, then for all $a_1,a_2\in A$ we have $a_1 a_2=a_2 a_1$ and for all $b_1,b_2\in B$ that $b_1 b_2=b_2 b_1$. Thus, if we take $(a_1,b_1),(a_2,b_2)\in A\times B$ we have $(a_1,b_1)(a_2,b_2)=(a_1 a_2,b_1 b_2)=(a_2 a_1,b_2 b_1)=(a_2,b_2)(a_1,b_1)$.

{\it Ad $3$.} Assume that there exists $1_A\in A$ and $1_B\in B$ such that $1_A a=a 1_A=a$, for all $a\in A$ and $1_B b=b 1_B=b$, for all $b\in B$. Then, $(1_A,1_B)\in A\times B$ is unity in $A\times B$ as $(1_A,1_B)(a,b)=(1_A a,1_B b)=(a,b)$. Also $(a,b)(1_A,1_B)=(a 1_A,b 1_B)=(a,b)$.

{\it Ad $4$.} Zero in $A\times B$ is $(0_A,0_B)\in A\times B$, such that $0_A+a=a+A_0=a$ and $0_B+b=b+B_0=b$, for all $a\in A$ and $b\in B$. Thus, let us observe $(a_1,b_1)(a_2,b_2)=(0_A,0_B)$. Assume $(a_1,b_1)\neq(0_A,0_B)$ and $(a_2,b_2)\neq(0_A,0_B)$. Then, $(a_1 a_2,b_1 b_2)=(0_A,0_B)$ which implies $a_1 a_2=0_A$ and $b_1 b_2=0_B$. As $A$ and $B$ are not integral domains, then $a_1 a_2=0_A$ and $b_1 b_2=0_B$ implies $a_1\neq 0_A$ and $a_2\neq 0_A$; similarly $b_1\neq 0_B$ and $b_2\neq 0_B$. Therefore, $(a_1,b_1)(a_2,b_2)=(0_A,0_B)$ implies $(a_1,b_1)\neq(0_A,0_B)$ and $(a_2,b_2)\neq(0_A,0_B)$. Also, as there are non-invertible elements in $A$ and $B$, say $a\in A$ and $b\in B$, then, $(a,b)\in A\times B$ is also not invertible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $a,b,c\in A$. Then:

\begin{enumerate}
\item $a(b-c)=a b-a c$ and $(b-c)a=b a-c a$.
\item If $a b=-b a$, then $(a+b)^2=(a-b)^2=a^2+b^2$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} As $a-b=a+(-b)$, we have $a(b-c)=a(b+(-c))$. By distributive law, from $A$ is a ring, we have $a(b+(-c))=a b+a(-c)$. From a previous proposition, $a(-c)=-a c$, we have $a b+a(-c)=a b-a c$. Similarly, we can prove that $(b-c)a=(b+(-c))a=b a+(-c)a=b a-c a$.

{\it Ad $2$.} Assume $a b=-b a$. Then, $(a+b)^2=(a+b)(a+b)$. By distributive law, $(a+b)(a+b)=(a+b)a+(a+b)b$. Again, by distributive law, $(a+b)a+(a+b)b=a^2+b a+a b+b^2$. But, as $a b=-b a$, we have $a^2+b a+a b+b^2=a^2+b a+(-b a)+b^2=a^2+0+b^2=a^2+b^2$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring. Then $a^2-b^2=(a-b)(a+b)$.

\noindent\newline{\bf Proof.} By using distributivity, $(a-b)(a+b)=(a-b)a+(a-b)b=a^2-b a+a b-b^2$. As $A$ is commutative, $b a=a b$, so $a^2-b a+a b-b^2=a^2-a b+a b-b^2=a^2-b^2$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring with unity. Then $a^2-1=(a+1)(a-1)$.

\noindent\newline{\bf Proof.} We have $a^2-1=a^2+0-1=a^2-a+a-1=a a-a 1+1 a-1 1$. By using distributive law, $a^2-1=a(a-1)+1(a-1)$. Using the distributive law again, we get $a^2-1=(a+1)(a-1)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} If $a=b$ or $a=-b$ we will write $a=\pm b$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain and $a,b\in A$. Then,

\begin{enumerate}
\item $a^2=b^2$ implies $a=\pm b$.
\item $a^n=0$, for some $n\in\Z^{+}$, implies $a=0$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} Let $a,b\in A$. We have $a^2=b^2$, that is $a^2-b^2=0$. From a previous proposition, as every integral domain is a commutative ring, $a^2-b^2=(a-b)(a+b)=0$. As integral domain has no divisors of zero, $(a-b)(a+b)=0$ implies $a-b=0$ or $a+b=0$. From first equation we get $a=b$, and from the second one, $a=-b$. Therefore, $a=b$ or $a=-b$, i.e. $a=\pm b$.

{\it Ad $2$.} Let $a\in A$, $n\in\Z^{+}$ and $a^n=0$. First, we will prove proposition for $n=1$. We have $a^1=0$. Then, as $a^1=a$ we get $a=0$. Assume $a^n=0$ implies $a=0$, for some $n\in\N$. As $a^{n+1}=0$ is equivalent to $a a^n=0$ and, as $A$ is an integral domain either $a=0$ or $a^n=0$. If $a=0$ we are done. If $a^n=0$, then, by assumption $a=0$ and we are done.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Note that, if $A$ is an integral domain, if $x\in A$ is its own multiplicative inverse, i.e. $x^2=1$, then it follows, from a previous proposition, that $x=\pm 1$.

\noindent\newline{\bf Proposition.} Let $\left(A,+\right)$ be a group and $\left(A,\cdot\right)$ a monoid such that $a(b+c)=a b+a c$ and $(a+b)c=a c+b c$, for all $a,b,c\in A$. Then, $a+b=b+a$, for all $a,b\in A$.

\noindent\newline{\bf Proof.} We have $(a+b)(1+1)=(a+b)(1+1)$. Using the distributive law we get $(a+b)1+(a+b)1=a(1+1)+b(1+1)$. That is, $(a+b)+(a+b)=a+a+b+b$. We apply $-(a+b)$ on the right and get $a+b=a+a+b+b-(a+b)$. But, $-(a+b)=-b+(-a)$, therefore $a+b=a+a+b+b+(-b)+(-a)$. That is equivalent to $a+b=a+a+b+(-a)$. We apply $a$ on the right and get $a+b+a=a+a+b$. Finally, we apply $(-a)$ on the left and obtain $(-a)+a+b+a=(-a)+a+a+b$, i.e. $b+a=a+b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a non-trivial ring with unity and $a,b\in A$. Then:

\begin{enumerate}
\item If $a^2=0$ then $a+1$ and $a-1$ are invertible.
\item If $a$ and $b$ are invertible, their product $a b$ is invertible. The converse holds if $A$ is commutative.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a^2=0$. After some thinking (done on author's part), we have $(-a+1)(a+1)=(-a+1)a+(-a+1)1=-a^2+a-a+1=-0+0+1=1$. Also, $(a+1)(-a+1)=-a^2-a+a+1=1$. Therefore, $(a+1)^{-1}=(-a+1)$. Similarly, $(-a-1)(a-1)=(-a-1)a+(-a-1)(-1)=-a^2-a+a+1=-0+0+1=1$. Similarly $(a-1)(-a-1)=(a-1)(-a)+(a-1)(-1)=a^2+a-a+1=0+0+1=1$.

{\it Ad $2$.} Let $a$ and $b$ be invertible, i.e. there exist $a^{-1},b^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$ and $b b^{-1}=b^{-1} b=1$. Then, as $a b=a b$ and $b$ is invertible, we can multiply equality with $b^{-1}$ on the right to obtain $a b b^{-1}=a$. Finally, multiplying by $a^{-1}$ on the right gives us $a b b^{-1} a^{-1}=1$. That can be grouped as $(a b)(b^{-1} a^{-1})=1$. Therefore, $(a b)^{-1}=b^{-1} a^{-1}$. To prove the converse, assume $A$ is commutative and there exists $(a b)^{-1}\in A$ such that $(a b)(a b)^{-1}=(a b)^{-1}(a b)=1$. Then, $a b=a b$ implies, after multiplying by $(a b)^{-1}$ on the right and using associativity, $(a b)(a b)^{-1}=a (b (a b)^{-1})$. That is, $1=a (b (a b)^{-1})$. As $A$ is commutative, also $(b (a b)^{-1}) a=1$. Therefore, $a^{-1}=b (a b)^{-1}$. Similarly, we can show that $(a b)^{-1}(a b)=((a b)^{-1} a) b$ implies $1=((a b)^{-1} a) b$. As $A$ is commutative, then $b((a b)^{-1} a)=1$ and $b^{-1}=(a b)^{-1} a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $\left(A,+,\cdot\right)$ be a non-trivial ring with unity and

\begin{equation*}
A^{\ast}=\left\{a\in A:\ \left(\exists a^{-1}\in A\right)\left(a a^{-1}=a^{-1} a=1\right)\right\}.
\end{equation*}

\noindent\newline Then, $\left(A^{\ast},\cdot\right)$ is a group.

\noindent\newline{\bf Proof.} As $1\in A$ and $1\cdot 1=1\cdot 1=1$, then $1\in A^{\ast}$, therefore, $A^{\ast}$ has a neutral element and is non-empty. Also, due to $(A,\cdot)$ being associative (from $A$ is a ring), then $A^{\ast}$ is also associative (because $A^{\ast}\subseteq A$). By definition, every $a\in A^{\ast}$ has an inverse and from that we conclude that $\left(A^{\ast},\cdot\right)$ is a group.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Assume $0\in A^{\ast}$. Then, there exists $a^{-1}\in A$ such that $0 a^{-1}=a^{-1} 0=1$. As $0 a^{-1}=0$ and also $a^{-1} 0=0$, then we get $0=1$, which is true only in a trivial ring. Therefore $0\notin A^{\ast}$. Furthermore, if $A$ is a field, then all elements are invertible (except obviously zero). So, $A^{\ast}=A-\{0\}$.

\noindent\newline{\bf Proposition.} Let $F$ be a finite field and $|F|=m$, for some $m\in\N-\{1\}$. Then, $x^{m-1}=1$, for every $x\neq 0$.

\noindent\newline{\bf Proof.} As $F$ is a field, then $F-\{0\}$ with multiplication is an Abelian group, by definition. But, as $|F|=m$, then $|F-\{0\}|=m-1$. As $F-\{0\}$ is an Abelian group, then by a result from group theore, $x^{|F|}=1$, for all $x\in F-\{0\}$. That implies $x^{m-1}=1$, for all $x\in F-\{0\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a non-trivial ring. If $(A,+)$ is cyclic, then $A$ is a commutative ring.

\noindent\newline{\bf Proof.} Let $(A,+)$ be cyclic, i.e. if $x\in A$, then $x=\underbrace{a+a+\cdots+a}_{n\textnormal{ times}}$ (then, e.g. $-x=\underbrace{(-a)+(-a)+\cdots+(-a)}_{n\textnormal{ times}}$, just to remind ourselves of the difference in notation in group theory) , for some generator $a\in A$. Then, we can write $x=m a$ (remember that this is the same as $a^m$ in group theory, but only in additive notation), where $m\in\Z$. Then, if we take $y\in A$, we have $y=n a$, for some $n\in\Z$. Then, $x y=(m a)(n a)=\left(\underbrace{a+\cdots+a}_{m\textnormal{ times}}\right)\left(\underbrace{a+\cdots+a}_{n\textnormal{ times}}\right)$. Now, when using distributive law, there will be $m n$ elements of the form $a^2$, so, $x y=(m n) a^2=(n m) a^2$. Following the same reasoning, obviously $(n a)(m a)=\left(\underbrace{a+\cdots+a}_{n\textnormal{ times}}\right)\left(\underbrace{a+\cdots+a}_{m\textnormal{ times}}\right)=(n m) a^2$. Therefore, $x y=(n m) a^2=(n a)(m a)=y x$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $A$ be a non-trivial ring and $a,b\in A$. Then,

\begin{enumerate}
\item If $a\neq\pm 1$ and $a^2=1$, then $a+1$ and $a-1$ are divisors of zero;
\item If $a b$ is a divisor of zero, then $a$ or $b$ is a divisor of zero;
\item In a commutative ring with unity, a divisor of zero cannot be invertible;
\item Suppose $a b\neq 0$ in a commutative ring. If either $a$ or $b$ is a divisor of zero, so is $a b$;
\item Suppose $a\neq 0$ nor a divisor of zero. If $a b=a c$, then $b=c$;
\item If $A$ and $B$ are rings, $A\times B$ always has divisors of zero.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $a\neq\pm 1$ and $a^2=1$, then $a+1$ and $a-1$ are divisors of zero.} Let $a^2=1$ and $a\neq\pm 1$. From that we have $a^2-1=1-1$, i.e. $a^2-1=0$. From that we have $a^2-a+a-1=0$. By using distributive law, $a(a-1)+1(a-1)=0$. Then, using the distributive law again, $(a+1)(a-1)=0$. As $a\neq 1$ and $a\neq 1$, we have that $a+1\neq 0$ and $a-1\neq 0$. From that, and $(a+1)(a-1)=0$, it follows that $a+1$ and $a-1$ are divisors of zero.

\item {\it If $a b$ is a divisor of zero, then $a$ or $b$ is a divisor of zero.} It must be that $a b\neq 0$. Also, we have that there exists $c\neq 0$ such that $(a b)c=0$. As $a b\neq 0$, then $a\neq 0$ and $b\neq 0$. As $c\neq 0$, then $b c\neq 0$. From that we have, using associative law, $a(b c)=0$, but $a\neq 0$ and $b c\neq 0$. Therefore, $a$ is a divisor of zero. If it were that $c(a b)=0$, then $(c a)b=0$ would in the same manner imply that $b$ is a divisor of zero. Therefore, $a$ or $b$ is a divisor of zero.

\item {\it In a commutative ring with unity, a divisor of zero cannot be invertible.} Assume $a\in A$, $a\neq 0$, is an invertible divisor of zero and $A$ is commutative ring with unity. Then, there exists $b\in A$, $b\neq 0$, such that $a b=0$ (or, equivalently as $A$ is commutative, $b a=0$). As $a$ is invertible, there exists $a^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$. If we multiply $a b=0$ with $a^{-1}$ on the left, we would get $a^{-1}a b=a^{-1} 0$, that is, $b=0$, which is a contradiction with $b\neq 0$.

\item {\it Suppose $a b\neq 0$ in a commutative ring. If either $a$ or $b$ is a divisor of zero, so is $a b$.} Assume $a b\neq 0$. Assume $a$ is a divisor of zero, without loss of generality (proof for $b$ goes the same way). Then, $a\neq 0$ and there exists $c\in A$, $c\neq 0$, such that $a c=0$. If we multiply that equality by $b$ we would get $a c b=0 b$, i.e. $(a b)c=0$, as $A$ is commutative. Then, as $(a b)c=0$ and $(a b)\neq 0$ and $c\neq 0$, it follows that $(a b)$ is a divisor of zero.

\item {\it Suppose $a$ is neither $0$ nor a divisor of zero. If $a b=a c$, then $b=c$.} We have that $a b=a c$ implies $a b-a c=0$. By distributive law that is equivalent to $a(b-c)=0$. As $a$ is not a divisor of zero, then either $a=0$ or $b-c=0$. But, as $a\neq 0$ by assumption, then it must be that $b-c=0$. That implies $b=c$.

\item {\it If $A$ and $B$ are non-trivial rings, $A\times B$ always has divisors of zero.} As $A$ and $B$ are non-trivial rings, we have that there exist $a\in A$ and $b\in B$ such that $a\neq 0$ and $b\neq 0$. Also, as $0_A\in A$ and $0_B\in B$, there exist $(a,0_B),(0_A,b)\in A\times B$. Then, $(a,0_B)\neq(0_A,0_B)$ and $(0_A,b)\neq(0_A,0_B)$, i.e. they are both non-zero. But, $(a,0_B)(0_A,b)=(a 0_A,0_B b)=(0_A,0_B)$, that is, their product is zero, making $(a,0_B)$ and $(0_A,b)$ divisors of zero.

\end{enumerate}

\noindent{\bf Definition.} If $A$ is a ring and $a^2=a$ for every $a\in A$, then, we say that $A$ is a {\bf boolean ring}.

\noindent\newline{\bf Proposition.} Every boolean ring $A$ is a commutative ring with no invertible elements except $1\in A$.

\noindent\newline{\bf Proof.} Let $a,b\in A$. We have $(a+a)^2=(a+a)(a+a)=(a+a)a+(a+a)a=a^2+a^2+a^2+a^2$. But, $(a+a)^2=a+a$ and $a^2=a$, so $a+a+a+a=a+a$ from which we get $a+a=0$, i.e. $a=-a$. Then, from $(a+b)^2=a+b$ we get $(a+b)(a+b)=a+b$, i.e. $(a+b)a+(a+b)b=a+b$. By applying distributive law, $a^2+b a+a b+b^2=a+b$. Here we use the fact that $a^2=a$ and $b^2=b$. So we have $a+b a+a b+b=a+b$. Applying $-b$ on the right and $-a$ on the left gives us $b a+a b=0$, i.e. $b a=-a b$. That is equivalent to $b a=(-a)b$. As $-a=a$, we have $b a=a b$.

Let $a\in A$, $a\neq 0$. Assume there exists $1\in A$ such that $a 1=1 a=a$. Then, as $a^2=a$, we have $a^2-a=0$, i.e. $a a-a 1=0$. From that we get $a(a-1)=0$. If $a$ is not a divisor of zero, then $a=0$ or $a-1=0$. But that implies that either $a$ is a divisor of zero, or $a$ is $0$ or $1$. Therefore, all elements in $A$ are divisors of zero, except $0$ and $1$. Note that $1$ cannot be a divisor of zero as it would imply that there exists $x\in A$ such that $1 x=0$. But, $1 x=x$, so $x=0$, meaning that the ring is trivial. As all elements but $0$ and $1$ are divisors of zero, only $1\in A$ (unity) is invertible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a boolean ring and $a\vee b=a+b+a b$. Then,

\begin{enumerate}
\item $a\vee b c=(a\vee b)(a\vee c)$.
\item $a\vee(1+a)=1$.
\item $a\vee a=a$.
\item $a(a\vee b)=a$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} $(a\vee b)(a\vee c)=(a+b+a b)(a+c+a c)=(a+b+a b)a+(a+b+a b)(c+a c)=a^2+b a+a^2 b+a c+b c+a b c+a^2c+a b c+a^2b c$. We use the fact that $a^2=a=-a$, so $(a\vee b)(a\vee c)=a+a b-a b+a c+b c+a b c-a c+a b c-a b c=a+b c+a b c=a\vee b c$. {\it Ad $2$.} $a\vee(1+a)=a+(1+a)+a(1+a)=a+1-a+a+a^2=1+a-a=1$. {\it Ad $3$.} $a\vee a=a+a+a^2=a-a+a=a$. {\it Ad $4$.} $a(a\vee b)=a(a+b+a b)=a^2+a b+a^2 b=a+a b-a b=a$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let us remind ourselves that if $(A,+)$ is a group, $a\in A$ and $n\in\N$, then $n a=\underbrace{a\oplus a\oplus\cdots\oplus a}_{n\textnormal{ times}}$, $(-n) a=\underbrace{(-a)\oplus(-a)\oplus\cdots\oplus(-a)}_{n\textnormal{ times}}$, $0 a=0_A$, where $0_A\in A$ is zero.

\noindent{\bf Proposition.} Let $(A,\oplus,\odot)$ be a ring\footnote{Note that $1\in\Z$ in statement $5$ is not unity $1_A\in A$.}, $a\in A$ and $n,m\in\Z$. Then,

\begin{enumerate}
\item $n(m a)=(n m)a$
\item $n(-a)=(-n)a=-n a$.
\item $n a\oplus m a=(n+m) a$.
\item $-n(-a)=-(-n)a=n a$.
\item $1 a=a$.
\item $(m a)\odot(n b)=(m n) (a\odot b)$.
\end{enumerate}

\noindent{\bf Proof.} The results for $1$, $2$, $3$ and $4$ are obvious if we denote $n a$ as $a^n$ and use result from group theory that $n(m a)$ is actually $(a^n)^m=a^(n m)$. Then, it is easy to see that $2$ and $3$ and $4$ are also true. {\it Ad $5$.} Trivial, from $a^1=a$. {\it Ad $6$}. Let $n=1$ and $m=1$. Then $(1 a)\odot(1 b)=a\odot b$. Assume that is true for all $m$. Then, $((m+1) a)\odot(1 b)=(m a\oplus 1 a)\odot(1 b)=(m a)\odot(1 b)\oplus(1 a)\odot(1 b)$. By assumption, $(m a)\odot(1 b)\oplus(1 a)\odot(1 b)=m (a\odot b)\oplus(a\odot b)=(m+1)(a\odot b)$. Assume that statement is true for $m$ and $n$. Then if we take $n+1$, we have $(m a)\odot((n+1)b)=(m a)\odot(n b\oplus 1 b)=(m a)\odot(n b\oplus 1 b)$. From distributive law, $(m a)\odot(n b\oplus 1 b)=(m a)\odot(n b)\oplus (m a)\odot(1 b)$. By using assumption , we have $(m a)\odot(n b)\oplus (m a)(1 b)=(m n)(a\odot b)\oplus(m 1)(a\odot b)=(m n+m)(a\odot b)=(m(n+1))(a\odot b)$. Similarly we can prove for $m+1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} What observations above really tell us is that we can deal with operations in ring as with multiplication and addition in elementary algebra (algebraic expressions), without using special symbols for addition and multiplication in a ring.

\noindent\newline{\bf Remark.} From now on, we will use following notation for a sum:

\begin{equation*}
\sum_{i=k}^{n}{f(i)}=f(k)+f(k+1)+\cdots+f(n-1)+f(n),
\end{equation*}

\noindent\newline where $n\in\N$, $k\in\Z$, $k\leq n$ and $f(i)$ is some expression involving $i$, e.g. $f(i)=a^i$, $f(i)=(i-1)a+2i$, etc.

\noindent\newline{\bf Proposition.} Let $n\in\N$, $k\in\Z$, $k\leq n$ and $j\in\Z$. Then,

\begin{equation*}
\sum_{i=k}^{n}{f(i)}=\sum_{i=k+j}^{n+j}{f(i-j)}.
\end{equation*}

\noindent\newline{\bf Proof.} We have:

\begin{eqnarray*}
\sum_{i=k+j}^{n+j}{f(i-j)}&=&f((k+j)-j)+\cdots+f((n+j)-j)\\\\
&=&f(k)+\cdots+f(n)=\sum_{i=k}^{n}{f(i)}.
\end{eqnarray*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $n\in\N$, $k\in\Z$, $k\leq n$ and $j\in\Z$ such that $k<j\leq n$. Then,

\begin{equation*}
\sum_{i=k}^{n}{f(i)}=\sum_{i=k}^{j-1}{f(i)}+\sum_{i=j}^{n}{f(i)}.
\end{equation*}

\noindent\newline{\bf Proof.} By definition:

\begin{eqnarray*}
\sum_{i=k}^{j-1}{f(i)}+\sum_{i=j}^{n}{f(i)}&=&f(k)+\cdots+f(j-1)\\\\
&+&f(j)+\cdots+f(n)=\sum_{i=k}^{n}{f(i)}.
\end{eqnarray*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring and $n\in\N$. Then, for all $a,b\in A$,

\begin{equation*}
\left(a+b\right)^n=\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^k}.
\end{equation*}

\noindent\newline{\bf Proof.} Assume $n=1$. Then,

\begin{equation*}
(a+b)^1=a+b=\binom{1}{0}a^{1-0} b^0+\binom{1}{1}a^{1-1} b^1=\sum_{k=0}^{1}{\binom{1}{k}a^{1-k}b^k}.
\end{equation*}

\noindent\newline Assume that the formula is true for some $n\in\N$. Then we will prove it is true for $n+1$. We have

\begin{equation*}
(a+b)^{n+1}=(a+b)^n(a+b)=\left(\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^k}\right)\left(a+b\right).
\end{equation*}

\noindent\newline By distributive law,

\begin{equation*}
\left(\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^k}\right)a+\left(\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^k}\right)b=\sum_{k=0}^{n}{\binom{n}{k}a^{n-k+1}b^k}+\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^{k+1}}.
\end{equation*}

\noindent\newline Now, we can clarify that by adjusting indices:

\begin{equation*}
\sum_{k=0}^{n}{\binom{n}{k}a^{n-k+1}b^k}+\sum_{k=0}^{n}{\binom{n}{k}a^{n-k}b^{k+1}}=\sum_{k=0}^{n}{\binom{n}{k}a^{n-k+1}b^k}+\sum_{k=1}^{n+1}{\binom{n}{k-1}a^{n-k+1}b^{k}}.
\end{equation*}

\noindent\newline We set the first sum as:

\begin{eqnarray*}
&&\sum_{k=0}^{n}{\binom{n}{k}a^{n-k+1}b^k}+\sum_{k=1}^{n+1}{\binom{n}{k-1}a^{n-k+1}b^{k}}\\\\
&=&\binom{n}{0}a^{n+1}+\sum_{k=1}^{n+1}{\binom{n}{k}a^{n-k+1}b^k}-\binom{n}{n+1}b^{n+1}+\sum_{k=1}^{n+1}{\binom{n}{k-1}a^{n-k+1}b^{k}}.
\end{eqnarray*}

\noindent\newline Using the fact that\footnote{I won't bother to prove it here, for now.}

\begin{equation*}
\binom{n}{k}+\binom{n}{k-1}=\binom{n+1}{k},
\end{equation*}

\noindent\newline and that

\begin{equation*}
\binom{n}{n+1}=0,
\end{equation*}

\noindent\newline we have:

\begin{eqnarray*}
&&\binom{n}{0}a^{n+1}+\sum_{k=1}^{n+1}{\binom{n}{k}a^{n-k+1}b^k}-\binom{n}{n+1}b^{n+1}+\sum_{k=1}^{n+1}{\binom{n}{k-1}a^{n-k+1}b^{k}}\\\\
&=&\binom{n+1}{0}a^{n+1-0}b^0+\sum_{k=1}^{n+1}{\binom{n+1}{k}a^{(n+1)-k)}b^k}\\\\
&=&\sum_{k=0}^{n+1}{\binom{n+1}{k}a^{(n+1)-k)}b^k}.
\end{eqnarray*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and $a\in A$. We say that $a$ is {\bf nilpotent} if $a^n=0$ for some $n\in\N$. We say that $a$ is {\bf unipotent} if $1-a$ is nilpotent.

\noindent\newline{\bf Proposition.} Let $A$ be a ring with unity, $a\in A$ and $n\in\N$. Then,

\begin{enumerate}
\item $1-a^n=(1-a)(1+a+a^2+\cdots+a^{n-1})=(1+a+a^2+\cdots+a^{n-1})(1-a)$.
\item $a^n+1=(1+a)(1-a+a^2+\cdots+(-1)^{n-1}a^{n-1})$, for odd $n$.
\item $1-a^{2n}=(1-a^n)(1+a^n)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $n=1$. Then, $1-a^1=1-a=(1-a)1=(1-a)a^0=(1-a)(a^{1-1})$. Assume statement is true for $n$. Then, $1-a^{n+1}=1-1 a+1 a-a^n a=(1-a)+(1-a^n)a$. From that we have $(1-a)+(1-a^n)a=(1-a)+(1-a)(1+a+\cdots+a^{n-1})a=(1-a)1+(1-a)(a+a^2+\cdots+a^n)=(1+a)(1+a+a^2+\cdots+a^n)$. Similarly we can prove the second case.

{\it Ad $2$.} We can see that $(1+a)(1-a+a^2+\cdots+(-1)^{n-1}a^{n-1})=1-a+a^2+\cdots+(-1)^{n-1}a^{n-1}+a-a^2+\cdots+(-1)^{n-1}a^n-a+a^2+\cdots+(-1)^n(-1)^{n-1}a^n$. All elements cancel out except $1$ and $(-1)^{n-1}a^n$. As $n$ is odd, $n-1$ is even, so we have only $1+a^n$.

{\it Ad $3$.} We have $(1-a^n)(1+a^n)=(1-a^n)1+(1-a^n)a^n=1-a^n+a^{2}-a^{2n}=1-a^{2n}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring with unity. If $a$ is nilpotent, then $a+1$ and $a-1$ are both invertible.

\noindent\newline{\bf Proof.} As $a$ is nilpotent, then $a^n=0$. Therefore, $1-a^n=1$. From the previous proposition that is equivalent to $(1-a)(1+a+\cdots+a^{n-1})=(1+a+\cdots+a^{n-1})(1-a)=1$. Using the fact that $(1-a)=-(a-1)$, we have $(a-1)(-(1+a+\cdots+a^{n-1}))=(-(1+a+\cdots+a^{n-1}))(a-1)=1$. Thus, $(a-1)^{-1}=-(1+a+\cdots+a^{n-1})$.

From the previous proposition, for odd $n$, and from $a^n=0$, we have $a^n+1=1$. Then, $a^n+1=(1+a)(1-a+a^2+\cdots+(-1)^{n-1}a^{n-1})=(1-a+a^2+\cdots+(-1)^{n-1}a^{n-1})(1+a)=1$, so $(a+1)^{-1}=(1-a+a^2+\cdots+(-1)^{n-1}a^{n-1})$. If $n$ is even, then, $a^{2k}=0$. That is equivalent to $-a^{2k}=0$ and $1-a^{2k}=1$. From the previous proposition, $(1-a^{2k})=(1-a^k)(1+a^k)$. If $k$ is odd, then $(1+a^k)$ can be factored to give $(1+a)$, as in previous case. If $k$ is even, then we can break it again into $(1-a^{\frac{k}{2}})(1-a^{\frac{k}{2}})$, until we get that $k$ is odd. So, we get $1=(a+1)f(a)=f(a)(a+1)$ (due to distributive law), where $f(a)$ is some expression dependant on $a$. Therefore, $a+1$ and $a-1$ are invertible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring. Then:

\begin{enumerate}
\item Any product $x a$ of a nilpotent element $a\in A$ by any element $x\in A$ is nilpotent.
\item Let $a^n=0$, for some $n\in\N$. Then, let $m\in\N$ such that $m\geq n$. Then, $a^m=0$.
\item Sum of two nilpotent elements is nilpotent.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $x,a\in A$ and $a^n=0$, for some $n\in\N$. Then, as $A$ is commutative, $(x a)^n=x^n a^n=x^n 0=0$. Therefore, $x a$ is nilpotent.

{\it Ad $2$.} As $a^n=0$, then $a^m=a^{n+(m-n)}$. As $m\geq n$, then $m-n\in\N$ and we have $a^m=a^n a^{m-n}=0a^{m-n}=0$.

{\it Ad $3$.} Let $a,b\in A$, where $a^m=0$ and $b^n=0$, for some $m,n\in\N$. Then,

\begin{eqnarray*}
\left(a+b\right)^{n+m}&=&\sum_{k=0}^{n+m}{\binom{n+m}{k}a^{n+m-k}b^k}\\\\
&=&\sum_{k=0}^{n}{\binom{n+m}{k}a^{n+m-k}b^k}+\sum_{k=n+1}^{n+m}{\binom{n+m}{k}a^{n+m-k}b^k}.
\end{eqnarray*}

\noindent\newline In the first sum, $k\leq n$, meaning $n-k\in\N$. Similarly, in the second sum $k\geq n+1$, i.e. $k-(n+1)\in\N$. Let us denote $n-k=x$ and $k-(n+1)=y$. Then the sum above becomes:

\begin{eqnarray*}
&&\sum_{k=0}^{n}{\binom{n+m}{k}a^{m+x}b^k}+\sum_{k=n+1}^{n+m}{\binom{n+m}{k}a^{n+m-k}b^{y+(n+1)}}\\\\
&=&\sum_{k=0}^{n}{\binom{n+m}{k}a^m a^x b^k}+\sum_{k=n+1}^{n+m}{\binom{n+m}{k}a^{n+m-k} b^n b^{y+1}}.
\end{eqnarray*}

\noindent\newline The first sum contains $a^m=0$ and the second sum contains $b^n=0$. Therefore all two sums equal zero, so $(a+b)^{m+n}=0+0=0$ and $a+b$ is nilpotent, as $m+n\in\N$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring with unity and $a,b\in A$ such that $a$ and $b$ are unipotent. Then, $a b$ is unipotent.

\noindent\newline{\bf Proof.} As $a$ and $b$ are unipotent, then $1-a$ and $1-b$ are nilpotent, i.e. there exist $m,n\in\N$ such that $(1-a)^m=0$ and $(1-b)^n=0$. We need to show that $1-a b$ is nilpotent (as then $a b$ will be unipotent). We have that $1-a b=1-a b=1-a+a-a b=(1-a)+a(1-b)$. As $1-b$ is nilpotent, by previous proposition, $a(1-b)$ is also nilpotent. As $(1-a)$ and $a(1-b)$ are nilpotent, then their sum is also nilpotent. Therefore, $1-a b$ is nilpotent and $a b$ is unipotent.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring with unity. Then, if $a\in A$ is unipotent, it is invertible.

\noindent\newline{\bf Proof.} Let $a\in A$ be unipotent. Then, $1-a$ is nilpotent. By first proposition, $1-a-1=-a$ is invertible, i.e. there exists $a^{-1}\in A$ such that $(-a) a^{-1}=1$. But, then $a (-a^{-1})=(-a^{-1}) a=1$ and $a$ is also invertible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If $A$ is a ring with unity, we define the set:

\begin{equation*}
A^{\ast}=\{a\in A:\ (\exists a^{-1}\in A)(a a^{-1}=a^{-1} a=1)\},
\end{equation*}

\noindent\newline with multiplication from $A$ as a respective operation.

\noindent\newline{\bf Proposition.} Let $A$ be a ring with unity. Then, $A^{\ast}$ is a group. If $A$ is commutative then $A^{\ast}$ is Abelian.

\noindent\newline{\bf Proof.} First, obviously $1\in A^{\ast}$ and $A^{\ast}\subseteq A$ so $A^{\ast}$ inherits associativity (and commutativity). If $a,b\in A^{\ast}$ then there exist $a^{-1},b^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$ and $b b^{-1}=b^{-1} b=1$. But, then also $a^{-1},b^{-1}\in A^{\ast}$, so each element has its inverse. Now we only need to check whether $A^{\ast}$ is closed with respect to multiplication. In $A$, we have $(a b)(b^{-1} a^{-1})=a a^{-1}=1$. Also, $(b^{-1} a^{-1})(a b)=b^{-1} b=1$. From that we have that $(b^{-1} a^{-1})=(a b)^{-1}$, therefore $(a b)\in A$ has its inverse $(a b)^{-1}=b^{-1}a^{-1}\in A$. Then it must be that $a b\in A^{\ast}$. Therefore $A^{\ast}$ is a group (Abelian if $A$ is commutative, which is inherited by subset relation).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} This can, of course, be done in group theory for monoids. That is, if $M$ is a (commutative) monoid, then the set

\begin{equation*}
M^{\ast}=\{m\in M:\ (\exists m^{-1}\in M)(m m^{-1}=m^{-1} m=1)\}.
\end{equation*}

\noindent\newline is a (commutative) group. The proof is exactly the same as above.

\noindent\newline{\bf Proposition.} Let $A$ and $B$ be commutative rings with unity. If $A\cong B$, then $A^{\ast}\cong B^{\ast}$.

\noindent\newline{\bf Proof.} As $A\cong B$, then there exists an isomorphism $f:A\rightarrow B$. Then we can take restriction $g:A^{\ast}\rightarrow B^{\ast}$ with $g(x)=f(x)$. We will show that $g$ is a bijection. As $f$ is well-defined then so is $g$. Then, $g(a)=g(b)$ is equivalent to $f(a)=f(b)$. But, as $f$ is an injection, then $a=b$, so $g$ is an injection. Now, if we take $y\in B^{\ast}$, we need to find $x\in A^{\ast}$ such that $g(x)=y$. But, as $f$ is surjective, for each $y\in B^{ast}$ there exists $x\in A^{\ast}$ such that $f(x)=y$, i.e. $g(x)=y$. So, $g$ is bijective. Now, $g(a b)=f(a b)=f(a)f(b)=g(a)g(b)$, so $g$ is an isomorphism from $A^{\ast}$ to $B^{\ast}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be commutative rings with unity. Then,

\begin{equation*}
A^{\ast}\times B^{\ast}=\left(A\times B\right)^{\ast}.
\end{equation*}

\noindent\newline Let $(a,b)\in A^{\ast}\times B^{\ast}$. Then $a\in A^{\ast}$ and $b\in B^{\ast}$, so there exist $a^{-1}\in A^{\ast}$ and $b^{-1}\in B^{\ast}$ such that $a a^{-1}=a^{-1} a=1$ and $b b^{-1}=b^{-1} b=1$. Then, $(a^{-1},b^{-1})\in A^{\ast}\times B^{\ast}$. We have $(a,b)(a^{-1},b^{-1})=(a a^{-1},b b^{-1})=(1,1)=(a^{-1}a,b^{-1}b)=(a^{-1},b^{-1})(a,b)=(1,1)$, so $(a^{-1},b^{-1})=(a,b)^{-1}$, i.e. $(a,b)$ has an inverse $(a,b)^{-1}$, and as $(a,b)\in A\times B$, then also $(a,b)\in (A\times B)^{\ast}$ and we have $A^{\ast}\times B^{\ast}\subseteq(A\times B)^{\ast}$. Now, let $(a,b)\in\left(A\times B\right)^{\ast}$. Then there exists $(a,b)^{-1}$ such that $(a,b)(a,b)^{-1}=(a,b)^{-1}(a,b)=(1,1)$. But, as $(a,b)^{-1}\in\left(A\times B\right)^{\ast}$, then $(a,b)^{-1}=(c,d)$, where $(c,d)\in\left(A\times B\right)^{\ast}$. So, $(a,b)(c,d)=(c,d)(a,b)=(1,1)$. From definition of direct product we have $(a c,b d)=(c a,d b)=(1,1)$. From definition of ordered pair that is equivalent to $a c=c a=1$ and $b d=d b=1$, meaning $c=a^{-1}$ and $d=b^{-1}$, so $a\in A^{ast}$ and $b\in B^{\ast}$. In other words, $(a,b)\in A^{\ast}\times B^{\ast}$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Ideals and homomorphisms}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be a ring and $B\subseteq A$ with $B\neq\emptyset$. Then, if $B$ is closed with respect to addition, multiplication and negatives, we say that $B$ is a {\bf subring} of $A$ and write $B\leq A$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring and $B\leq A$, $B\neq\emptyset$. Then, $B$ is a ring.

\noindent\newline{\bf Proof.} As $B\subseteq A$, we have that $B$, with addition, is a subgroup of $A$ (as it is closed with respect to addition and negatives). Therefore, $A$ with addition is a group. Distributive laws hold for all elements in $A$ and so they hold for all elements in $B\subseteq A$. As $B$ is closed with respect to multiplication and as $A$ is associative (for multiplication), then $B$ is also associative (for multiplication). Therefore, $B$ is a ring.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $B\subseteq A$. Then, $B$ is closed with respect to subtraction if and only if it is closed with respect to addition and negatives.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $B$ be closed with respect to subtraction. Let $a\in B$. Then, for all $b\in B$, we have $b-a\in B$. But, $b-a=b+(-a)\in B$. That also implies that $a+(-a)\in B$. As $a+(-a)=0\in A$, then $0\in B$. That implies that for all $a\in B$, we have $0-a=0+(-a)=-a\in B$. Therefore, $B$ is closed with respect to negatives. Then, as $-a\in B$, for all $a\in B$, we have $a-(-b)=a+(-(-b))=a+b\in B$, for all $b\in B$. So, $B$ is closed with respect to addition. {\it Sufficiency.} Let $B$ be closed with respect to addition and negatives. Then, $a+b\in B$ for all $a,b\in B$ and $-a\in B$ for all $a\in B$. Let $a,b\in B$. Then $-b\in B$. We have $a+(-b)=a-b\in B$. So, $B$ is closed with respect to subtraction.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and $B\subseteq A$. We say that $B$ {\bf absorbs products} in $A$ if $a b\in B$ and $b a\in B$ for all $a\in A$ and $b\in B$. We say that $B\subseteq A$ is an {\bf ideal} of $A$ if $B$ is closed with respect to addition and negatives\footnote{Or simply subtraction due to previous proposition.} and if $B$ absorbs products in $A$. Then, we write $B\trianglelefteq A$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring and $B\trianglelefteq A$. Then, $B\leq A$.

\noindent\newline{\bf Proof.} By definition, $B$ is closed with respect to addition and negatives. Now, if $a,b\in B$, then also $b\in A$. So, $a b\in B$. Same thing for $b a\in B$. Thus, $B\leq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a commutative ring, $a\in A$ and $\cyc{a}=\{x a:\ x\in A\}$. We say that $\cyc{a}$ is a {\bf principal ideal generated by $a$}.

\noindent\newline{\bf Proposition.} Let $A$ be a commutative ring and $a\in A$. Then, $\cyc{a}\trianglelefteq A$.

\noindent\newline{\bf Proof.} We must show that $\cyc{a}$ is closed with respect to subtraction and that it absorbs products in $A$. Let $x a,y a\in\cyc{a}$. Then, by distributive law, $x a+y a=(x+y)a$. Now, as $x+y\in A$, then $(x+y)a\in\cyc{a}$ and also $x a+y a\in\cyc{a}$. If $x a\in\cyc{a}$, then $-x a=(-x)a$. As $(-x)\in A$, then $(-x)a\in\cyc{a}$ and $-x a\in\cyc{a}$. If $x a\in\cyc{a}$ and $b\in A$, then $b(x a)=(b x)a$, due to associativity. As $b x\in A$, then $(b x)a\in\cyc{a}$, where we have $b(x a)\in\cyc{a}$. Also, as $A$ is commutative, $(x a)b=b(x a)$. As $(x a)b\in\cyc{a}$, then also $b(x a)\in\cyc{a}$. Therefore, $\cyc{a}\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} A {\bf homomorphism} from ring $A$ to a ring $B$ is a function $f:A\rightarrow B$ such that for all $x_1,x_2\in A$:

\begin{eqnarray*}
f(x_1+x_2)&=&f(x_1)+f(x_2),\\
f(x_1 x_2)&=&f(x_1)f(x_2).
\end{eqnarray*}

\noindent\newline{\bf Definition.} Let $f:A\rightarrow B$ be a homomorphism from $A$ to $B$. Then, {\bf kernel} of $f$ is $\ker{f}=\{x\in A:\ f(x)=0\}$.

\noindent\newline{\bf Definition.} We say that a bijective homomorphism from $A$ to $B$ is an {\bf isomorphism} from $A$ to $B$ and we write $A\cong B$.

\noindent\newline{\bf Proposition.} Let $f:A\rightarrow B$ be a homomorphism from $A$ to $B$. Then, $\ker{f}\trianglelefteq A$.

\noindent\newline{\bf Proof.} Let $x,y\in\ker{f}$. Then, $f(x)=f(y)=0$. Let $x+y=s$ for some $s\in A$. But, then, $f(x+y)=f(s)$ and, as $f$ is a homomorphism, $f(x)+f(y)=f(s)$. But, as $x,y\in\ker{f}$, we have $f(x)+f(y)=0+0=f(s)$, i.e. $f(s)=0$, so $s\in\ker{f}$. That means $x+y\in\ker{f}$. Let $x\in\ker{f}$. Then, $f(x)=0$ implies $f(x)+(-f(x))=0+(-f(x))$. From that we have $0=-f(x)$. A result from group theory gives us $-f(x)=f(-x)=0$, so $-x\in\ker{f}$. If $x\in\ker{f}$ and $a\in A$, then $f(x a)=f(x)f(a)=0\cdot f(a)=0$, so $x a\in\ker{f}$. Also, $f(a x)=f(a)f(x)=f(a)\cdot 0=0$ and $a x\in\ker{f}$. Therefore, $\ker{f}\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove the following:

\begin{enumerate}
\item $A=\{x+y\sqrt{3}:\ x,y\in\Z\}\leq\R$;
\item $B=\{x+2^{\frac{1}{3}}y+2^{\frac{2}{3}}z:\ x,y,z\in\Z\}\leq\R$;
\item $C=\{x 2^y:\ x,y\in\Z\}\leq\R$;
\item Let $\mathcal{C}(\R)$ be the set of all functions from $\R$ to $\R$ which are continuous on $\R$ and $\mathcal{D}(\R)$ set of functions from $\R$ to $\R$ which are differentiable on $\R$. Then, $\mathcal{C}(\R)\leq\mathcal{F}(\R)$ and $\mathcal{D}(\R)\leq\mathcal{F}(\R)$;
\item Let $\mathcal{U}(\R)$ be the set of all functions from $\R$ to $\R$ which are continuous on the interval $[0,1]$. Then, $\mathcal{C}(\R)\leq\mathcal{U}(\R)\leq\mathcal{F}(\R)$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $A=\{x+y\sqrt{3}:\ x,y\in\Z\}\leq\R$.} Obviously $A\subseteq\R$. Let $a,b\in A$. Then, $a=x_1+y_1\sqrt{3}$ and $b=x_2+y_2\sqrt{3}$, for some $x_1,x_2,y_1,y_2\in\Z$. We have $a+b=(x_1+x_2)+(y_1+y_2)\sqrt{3}$, so $a+b\in A$. Then, $-a=-x_1-y_1\sqrt{3}=(-x_1)+(-y_1)\sqrt{3}$, so $-a\in A$. Finally, $a b=(x_1 x_2+3 y_1 y_2)+(y_1 x_2+x_1 y_2)\sqrt{3}\in A$, so $A\leq\R$. Notice that $A$ is not an ideal of $\R$, as, e.g. $\frac{1}{2}x+\frac{1}{2}y\sqrt{3}\notin A$ (as $\frac{x}{2}$ and $\frac{y}{2}$ are in $\Z$ only for even numbers).
\item {\it $B=\{x+2^{\frac{1}{3}}y+2^{\frac{2}{3}}z:\ x,y,z\in\Z\}\leq\R$.} We have $B\subseteq\R$. Take $a,b\in B$. Then, $a=x_1+2^{\frac{1}{3}}y_1+2^{\frac{2}{3}}z_1$ and $b=x_2+2^{\frac{1}{3}}y_2+2^{\frac{2}{3}}z_2$, for some $x_i,y_i,z_i\in\Z$, where $i\in\{1,2\}$. We have $a+b=(x_1+x_2)+2^{\frac{1}{3}}(y_1+y_2)+2^{\frac{2}{3}}(z_1+z_2)\in B$. Then, $-a=(-x_1)+2^{\frac{1}{3}}(-y_1)+2^{\frac{2}{3}}(-z_1)\in B$. Finally, $a b=(x_1+2^{\frac{1}{3}}y_1+2^{\frac{2}{3}}z_1)(x_2+2^{\frac{1}{3}}y_2+2^{\frac{2}{3}}z_2)=(x_1 x_2+y_1 z_2+z_1 y_2)+2^{\frac{1}{3}}(x_1 y_2+x_2 y_1+2z_1 z_2)+2^{\frac{2}{3}}(y_1 y_2+z_1 x_2+z_2 x_1)\in B$, so $B\leq\R$. Also, $B$ is not an ideal of $\R$ as multiplying it with, e.g. $\sqrt{3}$ would not give an element of $B$.
\item {\it $C=\{x 2^y:\ x,y\in\Z\}\leq\R$.} We have $C\subseteq\R$. Then, $a,b\in C$ imply $a=x_1 2^{y_1}$ and $b=x_2 2^{y_2}$. We have $a+b=2^m\left(2^{y_1-m}x_1+2^{y_2-m}x_2\right)\in C$, where $m=\min{(y_1,y_2)}$. Then, $-a=(-x_1) 2^{y_1}\in C$ and $a b=(x_1 x_2)2^{y_1+y_2}\in C$, so $C\leq\R$. Also, $C$ is not an ideal of $\R$ as multiplying it with $\frac{1}{2}$ would not give $x\in\Z$, necessarily.
\item {\it Let $\mathcal{C}(\R)$ be the set of all functions from $\R$ to $\R$ which are continuous on $\R$ and $\mathcal{D}(\R)$ set of functions from $\R$ to $\R$ which are differentiable on $\R$. Then, $\mathcal{C}(\R)\leq\mathcal{F}(\R)$ and $\mathcal{D}(\R)\leq\mathcal{F}(\R)$.} As sum and negative, and also a product, of continuous functions is a continuous function, obviously $\mathcal{C}(\R)\leq\mathcal{F}(\R)$. The same thing goes for differentiable functions. Also, $\mathcal{C}(\R)$ is not an ideal of $\mathcal{F}(\R)$ as multiplying it with a discontinuous function would yield a discontinuous function.
\item {\it Let $\mathcal{U}(\R)$ be the set of all functions from $\R$ to $\R$ which are continuous on the interval $[0,1]$. Then, $\mathcal{C}(\R)\leq\mathcal{U}(\R)\leq\mathcal{F}(\R)$.} Due to the same reasoning as in previous problem, we have $\mathcal{U}(\R)\leq\mathcal{F}(\R)$. Let $f\in\mathcal{C}(\R)$. Then, $f$ is continuous on $\R$, but so is on $[0,1]$, so $f\in\mathcal{U}(\R)$. That implies $\mathcal{C}(\R)\subseteq\mathcal{U}(\R)$. But, as $\mathcal{C}(\R)$ is a ring (as a subring of $\mathcal{F}(\R)$), then so is a subring of $\mathcal{U}(\R)$.
\end{enumerate}

\noindent{\bf Problem.} Which of the following are ideals of $\Z\times\Z$: (1) $A=\{(n,n):\ n\in\Z\}$, (2) $B=\{(5n,0)\}:\ n\in\Z\}$, (3) $C=\{(n,m):\ 2|n+m\}$, (4) $D=\{(2n,3m):\ n,m\in\Z\}$?

\noindent\newline{\bf Solution.} (1) If we take $(m,m),(n,n)\in A$, then $(m,m)-(n,n)=(m-n,m-n)\in A$. Then, $(m,m)(n,n)=(m n,m n)\in A$. If we take $(x,y)\in\Z$, then $(m,m)(x,y)=(m x,m y)$ is generally not in $A$, as, e.g., $(2,2)\in A$ and $(3,5)\in\Z\times\Z$, but $(2,2)(3,5)=(6,10)\notin A$, so $A$ is not an ideal of $\Z\times\Z$. (2) Let $(5m,0),(5n,0)\in B$. Then, $(5m,0)-(5m,0)=(5m-5m,0-0)=(5\cdot 0,0)\in B$. Then, $(5m,0)(5n,0)=(5(5m n),0)\in B$. Also, if $(x,y)\in\Z\times\Z$ then $(5m,0)(x,y)=(5(m x),0)\in B$. Therefore, $B$ is an ideal of $\Z\times\Z$. (3) Let $(n,m),(p,q)\in C$. Then, $(n,m)-(p,q)=(n-p,m-q)$. If $n+m=2k$ and $p+q=2l$, then $n=2k-m$ and $p=2l-q$. So $n-p=2(k-l)+(q-m)$. Then, $(n-p)+(m-q)=2(k-l)+(q-m)+(m-1)=2(k-l)$. Therefore, $(n-p,m-q)\in C$. Also, $(n,m)(p,q)=(n p,m q)$ and $n p+m q=(2k-m)(2l-q)+m q=4k l-2k q-2l m+m q+m q=2(2k l-k q-l m+m q)$. Therefore, $(n,m)(p,q)\in C$. If $(x,y)\in\Z$, then e.g. $(3,5)(2,1)=(6,5)$, but $6+5=11$ and $(6,5)\notin C$. Thus, $C$ is not an ideal of $\Z\times\Z$. (4) Let $(2n,3m),(2p,3q)\in D$. Then, $(2n,3m)-(2p,3q)=(2(n-p),3(m-q))\in D$. Also, $(2n,3m)(2p,2q)=(2(2 n p),3(3m q))\in D$. If $(x,y)\in\Z\times\Z$ then $(2n,3m)(x,y)=(2(n x),3(m y))\in D$. Therefore, $D$ is an ideal of $\Z\times\Z$.

\noindent\newline{\bf Problem.} List all the ideals of $\Z_{12}$.

\noindent\newline{\bf Solution.} We have $\{0,2,4,6,8,10\}$, $\{0,3,6,9\}$, $\{0,4,8\}$, $\{0,6\}$ and $\{0\}$. Notice that in $\{0,3,6,9\}$, $9$ is unity. Similarly in $\{0,4,8\}$, $4$ is unity. So, in $\{0,6\}$, $6$ is obviously unity. Yet, there is no unity in $\{0,2,4,6,8,10\}$.

\noindent\newline{\bf Problem.} Prove that if $A\leq\Z_n$, then $A\trianglelefteq\Z_n$.

\noindent\newline{\bf Solution.} Let $A\leq Z_n$. Then if $a\in A$ and $b\in\Z_n$, we have $a b=\underbrace{a+a+\cdot+a}_{b\textnormal{ times}}$. As $A$ is closed with respect to addition, $a b\in A$. Same thing for $b a\in A$.

\noindent\newline{\bf Problem.} Prove that each of the following is an ideal of $\mathcal{F}(\R)$: (1) The set $A$ of all $f$ such that $f(x)=0$ for every $x\in\Q$. (b) The set $B$ of all $f$ such that $f(0)=0$.

\noindent\newline{\bf Solution.} (a) If $f,g\in A$, then $[f-g](x)=f(x)-g(x)=0-0=0$, for all $x\in\Q$, so $f-g\in A$. If $g\in\mathcal{F}(\R)$, then $[f g](x)=f(x)g(x)=0\cdot g(x)=0$, for all $x\in\Q$, so $f g\in A$. (b) If $f,g\in B$, then $[f-g](0)=f(0)-g(0)=0-0=0$, so $f-g\in B$. If $g\in\mathcal{F}(\R)$, then $[f g](0)=f(0)g(0)=0\cdot g(0)=0$, so $f g\in B$.

\noindent\newline{\bf Problem.} Give an example of a subring of $\Z_3\times\Z_3$ which is not an ideal.

\noindent\newline{\bf Solution.} We have $A=\{(0,0),(1,1),(2,2)\}$ which is obviously a subring of $\Z_3\times\Z_3$. But, if we take $(1,2)\in\Z_3\times\Z_3$, then $(1,1)(1,2)=(1,2)\notin A$, so $A$ is not an ideal of $\Z_3\times\Z_3$.

\noindent\newline{\bf Definition.} Let $A$ be an integral domain and $B\leq A$. If $B$ is an integral domain, we say that $B$ is a {\bf subdomain} of $A$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain and $B\leq A$. If $1\in B$, then $B$ is a subdomain of $A$.

\noindent\newline{\bf Proof.} As $A$ is an integral domain, then $a b=a c$ implies $b=c$, for all $a,b,c\in A$. Take $a,b,c\in B$ where $a\neq 0$. Then, as also $a,b,c\in A$, $a b=a c$ implies $b=c$. Integral domain is defined as a commutative ring with unity, so it also must be that $1\in B$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every subring containing the unity of a field is an integral domain.

\noindent\newline{\bf Proof.} Let $F$ be a field and $A\leq F$ and $1\in A$. As $F$ is commutative, so is $A$. Let $a,b,c\in A$, $a\neq 0$ and $a b=a c$. As also $a\in F$, then there exists $a^{-1}\in F$ such that $a a^{-1}=a^{-1} a=1$. Then, $a^{-1} a b=a^{-1} a c$ implies $b=c$. Therefore, as $1\in A$, we have that $A$ is a commutative ring with unity. As also $a b=a c$ implies $b=c$, then $A$ is an integral domain.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field and $A\leq F$. If $A$ is a field, we say that $A$ is a {\bf subfield} of $F$.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $B\leq F$. If $B$ is closed with respect to multiplicative inverses, then $B$ is a field.

\noindent\newline{\bf Proof.} As $B\leq F$, we have that $B$ is a ring. As $F$ is commutative, then so is $B$. Assume $B$ is closed with respect to inverses. Then, for all $a\in B$ there exists $a^{-1}\in B$ such that $a a^{-1}=a^{-1} a=1$. As $a\in B$ and $a^{-1}\in B$, then $a a^{-1}\in B$ (as it is closed with respect to multiplication). Therefore, also $a a^{-1}=1\in B$. So, $B$ is a commutative ring with unity, and as every element has an inverse, it is also a field.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Find subrings of $\Z_18$ which illustrate each of the following: (a) $A$ is a ring with unity, $B\leq A$, but $B$ is not a ring with unity; (b) $A$ and $B$ are rings with unity, $B\leq A$, but the unity of $B$ is not the same as the unity of $A$.

\noindent\newline{\bf Solution.} (a) We have $A=\{0,2,4,6,8,10,12,14,16\}$, where unity is $10$ and $B=\{0,6,12\}$, where $A\leq B$, but it has no unity. (b) $A=\{0,2,4,6,8,10,12,14,16\}$ and $B\leq A$, where $B=\{0,9\}$. Unity is $9\in B$, but $10\in A$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring, $f:A\rightarrow A$ a homomorphism, and $B=\{x\in A:\ f(x)=x\}$. Then, $B\leq A$.

\noindent\newline{\bf Proof.} Obviously $B\subseteq A$. Let $x,y\in B$. Then, $f(x)=x$ and $f(y)=y$. We have $x-y=f(x)-f(y)=f(x-y)\in B$, so $x-y\in B$. Then, $x y=f(x)f(y)=f(x y)\in B$, so $x y\in B$ and by that $B\leq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring. Center of ring $A$ is the set:

\begin{equation*}
Z(A)=\{a\in A:\ (\forall x\in A)(x a=a x)\}.
\end{equation*}

\noindent\newline{\bf Proposition.} Let $A$ be a ring. Then, $Z(A)\leq A$.

\noindent\newline{\bf Proof.} First, $Z(A)\subseteq A$. Then, if $a,b\in Z(A)$, we have $a x=x a$ and $b x=x b$, for all $x\in A$. Then, $a x-b x=x a-b x$. But, as $b x=x b$, we have $a x-b x=x a-x b$. By distributive laws, that is equivalent to $(a-b)x=x(a-b)$, so $a-b\in Z(A)$. Also, $a b x=a x b$, as $b x=x b$, for all $x\in A$. Then, $a x b=x a b$, as $a x=x a$, for all $x\in A$. So, as $(a b)x=x(a b)$, we have $a b\in Z(A)$. Therefore, $Z(A)\leq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $J_1,J_2\trianglelefteq A$. Then, $J_1\cap J_2\trianglelefteq A$.

\noindent\newline{\bf Proof.} We see that $J_1\cap J_2\subseteq A$. Take $x,y\in J_1\cap J_2$. Then, $x,y\in J_1$ and $x,y\in J_2$. As $J_1$ and $J_2$ are ideals, then $x-y\in J_1$ and $x-y\in J_2$, so $x-y\in J_1\cap J_2$. Also, $x y\in J_1$ and $x y\in J_2$, therefore $x y\in J_1\cap J_2$. If $a\in A$, then $x a,a x\in J_1$ and $x a,a x\in J_2$, i.e. $x a,a x\in J_1\cap J_2$, so $J_1\cap J_2\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. If $J\trianglelefteq A$ and $1\in J$ then $J=A$.

\noindent\newline{\bf Proof.} First, we have $J\subseteq A$, because $J\trianglelefteq A$. If we take $a\in A$, then $a x\in J$, for all $x\in J$. But, as $1\in J$, then also $a 1\in J$. Therefore, $a\in J$ and we have $A\subseteq J$. That implies $J=A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. If $J\trianglelefteq A$ and there exists $a\in J$ such that $a a^{-1}=a^{-1} a=1$, for some $a^{-1}\in J$, then $J=A$.

\noindent\newline{\bf Proof.} As, for some $a\in J$ there exists $a^{-1}\in J$ such that $a a^{-1}=1$, then, due to $J\leq A$, we have $a a^{-1}\in J$, i.e. $1\in J$. By previous proposition, if $1\in J$, then $J=A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field. If $J\trianglelefteq F$, then $J=F$ or $J=\{0\}$.

\noindent\newline{\bf Proof.} As all $a\in F$ are invertible, i.e. there exists $a^{-1}\in F$, such that $a a^{-1}=1$. So, if we have $a\in J$, then, as $J\trianglelefteq F$, we have $b a\in J$, for all $b\in F$. But, we also have $a^{-1}\in F$, so $a^{-1} a\in J$, i.e. $1\in J$. By a previous proposition we have that $J=F$. The only other ideal is $J=\{0\}$, if $J$ contains only the zero, i.e. the neutral element with respect to addition.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be rings, and $f:A\rightarrow B$ a homomorphism. Then,

\begin{enumerate}
\item $f(0)=0$ and $f(-a)=-f(a)$ for all $a\in A$.
\item $f(A)\leq B$.
\item $\ker{f}\trianglelefteq A$.
\item Let $1\in A$. If $\ran{f}$ is an integral domain, then $f(1)\in\{0,1\}$. If $f(1)=0$, then $f(x)=0$ for every $x\in A$. If $f(1)=1$, the image of every invertible element of $A$ is an invertible element of $\ran{f}$.
\item Any homomorphic image of a commutative ring is a commutative ring. Any homomorphic image of a field is a field.
\item If the domain $A$ of the homomorphism $f$ is a field, and if the range of $f$ has more than one element, then $f$ is injective.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} Altough results follow immediately from group theory, we will redo the proof in additive notation. First, let $0_A\in A$ be a zero in $A$ and $0_B\in B$ zero in $B$. Then, $0_A+0_A=0_A$. But, as $f$ is a well-defined function, $f(0_A+0_A)=f(0_A)$. As $f$ is a homomorphism, $f(0_A)+f(0_A)=f(0_A)$. As $B$ is a ring, there exists $-f(0_A)$ (only a notation for negative, i.e. additive inverse, of $f(0_A)\in B$) such that $f(0_A)=f(0_A)+(-f(0_A))$. But, $f(0_A)+(-f(0_A))=0_B$, so $f(0_A)=0_B$. That's why it is unambiguous if we denote both zeros in $A$ and in $B$ with $0$. Then, take $a\in A$. We have $a+(-a)=0$. As $f$ is a well-defined function, then $f(a+(-a))=f(0)$. As $f$ is a homomorphism, and $f(0)=0$, we have $f(a)+f(-a)=0$. Then, as $B$ is a ring and closed with respect to negatives, there exists $-f(a)\in B$ such that $f(a)+(-f(a))=0$. Therefore, $f(a)+f(-a)+(-f(a))=-f(a)$. But, that is equivalent to $f(-a)=-f(a)$. Therefore, $f$ copies negatives to negatives.

{\it Ad $2$.} As $f(A)=\{b\in B:\ (\exists a\in A)(f(a)=b)\}$, then, $f(A)\subseteq B$. Assume $b_1,b_2\in f(A)$. Then, there exist $a_1,a_2\in A$ such that $f(a_1)=b_1$ and $f(a_2)=b_2$. First, $b_1-b_2=b_1+(-b_2)=f(a_1)+(-f(a_2))=f(a_1)+f(-a_2)=f(a_1+(-a_2))\in f(A)$, so $b_1-b_2\in f(A)$. Then, $b_1 b_2=f(a_1)f(a_2)=f(a_1 a_2)\in f(A)$, so $b_1 b_2\in f(A)$. Therefore, $f(A)\leq B$.

{\it Ad $3$.} As $\ker{f}=\{a\in A:\ f(a)=0\}$, we have $\ker{f}\subseteq A$. But, assume $x,y\in\ker{f}$. As $A$ is a ring, there exists $a\in A$ such that $x-y=a$ (it is closed with respect to subtraction). Then, as $f$ is a well-defined function, $f(x-y)=f(a)$. But, $f(a)=f(x-y)=f(x+(-y))=f(x)+f(-y)=f(x)+(-f(y))=0+(-0)=0$. Thus, $f(x-y)=0$ and we have $x-y\in\ker{f}$. Also, as $A$ is a ring, there exists $a'\in A$ such that $x y=a'$ (it is closed with respect to products). As $f$ is a well-defined function, $f(x y)=f(a')$. But, $f(a')=f(x y)=f(x)f(y)=0\cdot 0=0$, so $x y\in\ker{f}$. Also, if $x\in\ker{f}$ and $a\in A$, then we have $x a=a'$, for some $a'\in A$. As $f$ is a well-defined function, $f(a')=f(x a)=f(x)f(a)=0\cdot f(a)=0$, so $x a\in\ker{f}$. Thus, $\ker{f}\trianglelefteq A$.

{\it Ad $4$.} Let $\ran{f}$ be an integral domain, i.e. for all $f(a),f(b),f(c)\in\ran{f}$, $f(a)\neq 0$, we have $f(a)f(b)=f(a)f(c)$ implies $f(b)=f(c)$. As $1\in A$, then $f(1)\in\ran{f}$. But, as $f$ is a homomorphism, we have $f(1)=f(1\cdot 1)=f(1)f(1)$. Then, taking the negative of $f(1)$, we get $0=f(1)^2-f(1)$. That is, by distributive law, equivalent to $0=f(1)(f(1)-1)$ (notice that $1$ here refers to unity in $\ran{f}$; maybe we should have denoted it as $1_{\ran{f}}$). As $B$ is an integral domain, that implies that $f(1)=0$ or $f(1)-1=0$, i.e. $f(1)=1$. Thus, $f(1)\in\{0,1\}$. Assume $f(1)=0$. As $a\cdot 1=a$, for all $a\in A$, then, $f(a)=f(a\cdot 1)=f(a)f(1)=0$. Therefore, $f(a)=0$, for all $a\in A$. Now, let $f(1)=1$. Assume that for some $a\in A$ there exists $a^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$. Then, $f(a a^{-1})=f(a^{-1} a)=f(1)$. That gives us $f(a)f(a^{-1})=f(a^{-1})f(a)=1$, i.e. $f(a)[f(a)]^{-1}=[f(a)]^{-1} f(a)=1$, i.e. $f(a)$ (if $f(a)\neq 0$) is invertible.

{\it Ad $5$.} Let $A$ be a commutative ring. Then, $a b=b a$, for all $a,b\in A$. Choose $f(a),f(b)\in\ran{f}$. Then, $f(a)f(b)=f(a b)=f(b a)=f(b)f(a)$, for all $f(a),f(b)\in\ran{f}$, therefore $\ran{f}$ is a commutative ring. Let $A$ be a field. Then, $A$ is commutative, $1\in A$ and for all $a\in A$ there exists $a^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$. The property of unity is that $1\cdot a=a\cdot 1=a$, for all $a\in A$. If we take $f(a)\in\ran{f}$, then $f(a)=f(a\cdot 1)=f(a)f(1)$. Therefore, $f(1)=1_{\ran{f}}$, and we can designate it as $1$. If we take $f(a)\in\ran{f}$, then $a\in A$, and there exists $a^{-1}\in A$ such that $a^{-1} a=a a^{-1}=1$. But, as $a^{-1}\in A$, then $f(a^{-1})\in\ran{f}$. We have $f(a a^{-1})=1$, i.e. $f(a)f(a^{-1})=1$. Thus, $f(a^{-1})=[f(a)]^{-1}$, and every $f(a)\in\ran{f}$ (except zero, of course) is invertible. That means that $\ran{f}$ is a field.

{\it Ad $6$.} Assume $A$ is a field and $\left|\ran{f}\right|>1$, i.e. there exists $f(a)\in\ran{f}$ such that $f(a)\neq 0$. Also, there exists $1\in\ran{f}$. But, as $A$ is a field, then $\ran{f}$ is a field. If we take $f(a)=f(b)$, for some $f(a),f(b)\in\ran{f}$, we have $f(a)[f(b)]^{-1}=f(b)[f(b)^{-1}]$. That gives us $f(a b^{-1})=1$. As $f(1)=1$, then, $f(a b^{-1})=f(1)$. That gives us $f(a b^{-1}-1)=0$. That means that $(a b^{-1}-1)\in\ker{f}$. But, as $\ker{f}\trianglelefteq A$, and $A$ is a field, then $\ker{f}=\{0\}$, or $\ker{f}=A$. If it were that $\ker{f}=A$, we would have that $f(a)=0$, for all $a\in A$, meaning $\ran{f}=0$. That would be in contradiction with our assumption that $\left|\ran{f}\right|>1$. So, it must be $\ker{f}=\{0\}$. That implies $a b^{-1}-1=0$, i.e. $a b^{-1}=1$. Multiplying by $b$ on the right gives us $a=b$. Thus, as $f(a)=f(b)$ implied $a=b$, we have that $f$ is injective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m,n\in\Z$. If $m\neq n$, then $m\Z\ncong n\Z$.

\noindent\newline{\bf Proof.} If we assume that there exists an isomorphism, it would have to carry zero to zero and inverse to inverse, i.e. it would have to satisfy following set of equations:

\begin{eqnarray*}
f(0)&=&0,\\
f(-m k)&=&-f(m k),\\
f(m k_1+m k_2)&=&f(m k_1)+f(m k_2),\\
f(m k_1 m k_2)&=&f(m k_1)f(m k_2).
\end{eqnarray*}

\noindent\newline We can assume that $f(m k)=n g(k)$, where $g:\Z\rightarrow\Z$ is a bijection carrying indices. From $f(m k_1+m k_2)=f(m k_1)+f(m k_2)$, we then get $f(m k_1+m k_2)=f(m(k_1+k_2)=n g(k_1+k_2)$. But also, $f(m k_1)+f(m k_2)=n g(k_1)+n g(k_2)$, so it must be that $g(k_1+k_2)=g(k_1)+g(k_2)$. We also have $f(m k_1 m k_2)=f(m(m k_1 k_2))=n g(m k_1 k_2)$. But, we know that $g(m k_1 k_2)=g(\underbrace{k_1 k_2+\cdots+k_1 k_2}_{m\textnormal{ times}})=m g(k_1 k_2)$. Therefore, $f(m k_1 m k_2)=n m g(k_1 k_2)$, but also $f(m k_1)f(m k_2)=n g(k_1) n g(k_2)$, so $m g(k_1 k_2)=n g(k_1)g(k_2)$, i.e. $g(k_1 k_2)=\frac{n}{m}g(k_1)g(k_2)$. Now, as $n\neq m$, then $\frac{n}{m}\neq 1$. But, that fact can be exploited, as $g(k)=g(k\cdot(1\cdot 1))=\frac{n}{m}g(k)g(1\cdot 1)=\frac{n^2}{m^2}g(k)g(1)g(1)$. Now, $f(m k)=f(m(k\cdot 1))=n g(k\cdot 1)=\frac{n^2}{m} g(k) g(1)$. Let $k\neq 0$, then, $g(k)\neq 0$ (it has to be $f(0)=0$, so $f(m\cdot 0)=n g(0)=0$). But, $f(m k)=n g(k)$, so we have $n g(k)=\frac{n^2}{m} g(k) g(1)$. That gives us, as $g(k)\neq 0$, that $g(1)=\frac{m}{n}$. But, $g(1)=g(1\cdot 1)=\frac{n^2}{m^2}g(1)g(1)$. That means that, as $g(1)\neq 0$, that $g(1)=\frac{m^2}{n^2}$. But, $g(1)=\frac{m}{n}$, so we have $\frac{m^2}{n^2}=\frac{m}{n}$, i.e. $\frac{m}{n}=1$, but that can only happen if $m=n$. This is a contradiction to our assumption that $m\neq n$, and then, $f$ cannot be an isomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and $J\trianglelefteq A$. Then, the {\bf radical} of $J$ is the set:

\begin{equation*}
\rad{J}=\left\{a\in A:\ \left(\exists n\in\Z\right)\left(a^n\in J\right)\right\}.
\end{equation*}

\noindent\newline{\bf Proposition.} Let $A$ be a ring and $J\trianglelefteq A$. Let $a\in A$. Then, if $a^n\in J$, for some $n\in\Z$, then $a^m\in J$, for all $m\geq n$, $m\in\Z$.

\noindent\newline{\bf Proof.} Assume $m=n$. Then, there is nothing to prove as already $a^n\in J$. Assume $a^m\in J$ for some $m>n$. Then, as $a^m\in J$ and $a\in A$, we have $a a^m\in J$ (as $J\trianglelefteq A$), i.e. $a^{m+1}\in J$. Thus, by induction we have that $a^m\in J$, for all $m>n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring and $J\trianglelefteq A$. Then, $\rad{J}\trianglelefteq A$.

\noindent\newline{\bf Proof.} First, $\rad{J}\subseteq A$ is obvious from the definition of $\rad{J}$. Let $a,b\in\rad{J}$. Then there exist $m,n\in\Z$ such that $a^m,b^n\in J$. If we take $(a+b)^{n+m}$, by binomial formula (applicable as $A$ is commutative) we have:

\begin{eqnarray*}
\left(a+b\right)^{n+m}&=&\sum_{k=0}^{n+m}{\binom{n+m}{k}a^{n+m-k}b^k}\\\\
&=&\sum_{k=0}^{n}{\binom{n+m}{k}a^{n+m-k}b^k}+\sum_{k=n+1}^{n+m}{\binom{n+m}{k}a^{n+m-k}b^k}.
\end{eqnarray*}

\noindent\newline In the first sum we have $a^{n+m-k}$ where $0\leq k\leq n$. That implies $0\geq -k\geq -n$. Therefore, $n+m-k\geq n+m-n=m$. By previous proposition we have that $a^{n+m-k}\in J$, as $n+m-k\geq m$. But, as $b^k\in A$, then $a^{n+m-k}b^k\in J$. In the second sum we have $b^k$, where $n+1\leq k\leq n+m$. So, $k>n$ and we have, by a previous proposition, that $b^k\in J$. As $a^{n+m-k}\in A$, we have $a^{n+m-k}b^k\in J$. As all members of the sums are in $J$, then their sum is also in $J$, as also $J\leq A$. That gives us that $(a+b)^{n+m}\in J$. From that we have that $a+b\in\rad{J}$. If $a\in\rad{J}$, then $-a^m=(-a)^m$ if $m$ is odd and $-a^{m+1}=(-a)^{m+1}$ if $m$ is even, so $-a\in\rad{J}$. Finally, if $a\in A$ and $b\in\rad{J}$, then $b^m\in J$. As $A$ is commutative, we have $(a b)^m=a^m b^m$. As $b^m\in J$, and $a^m\in A$, then $(a b)^m\in J$. So, $a b\in\rad{J}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $J,K\trianglelefteq A$. If $J\cap K=\{0\}$, then $j k=0$, for every $j\in J$ and $k\in K$.

\noindent\newline{\bf Proof.} Let $j\in J$ and $k\in K$. As $k\in K\subseteq A$, then $j k\in J$. As $j\in J\subseteq A$, then $j k\in K$. From that we have $j k\in J\cap K$. As $J\cap K=\{0\}$, then it can only be that $j k=0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring. For any $a\in A$, $I_a=\{a x+j+k:\ x\in A,\ j\in J,\ k\in K\}\trianglelefteq A$.

\noindent\newline{\bf Proof.} Let $a\in A$. Then, $a x_1+j_1+k_1,a x_2+j_2+k_2\in I_a$. We have $(a x_1+j_1+k_1)-(a x_2+j_2+k_2)=a(x_1-x_2)+(j_1-j_2)+(k_1-k_2)$. As $x_1-x_2\in A$, $j_1-j_2\in J$ and $k_1-k_2\in K$, then $(a x_1+j_1+k_1)-(a x_2+j_2+k_2)\in I_a$. Now, let $b\in A$. Then, $(a x_1+j_1+k_1)b=a x_1 b+j_1 b+k_1 b$. As $x_1 b\in A$, $j_1 b\in J$ and $k_1 b\in K$, we have that $(a x_1+j_1+k_1)b\in I_a$. As $A$ is commutative then also $b(a x_1+j_1+k_1)\in I_a$, so $I_a\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a commutative ring and $a\in A$. Set

\begin{equation*}
\Ann{a}=\left\{x\in A:\ a x=0\right\}
\end{equation*}

\noindent\newline is called the {\bf annihilator} of $a$. Also, set

\begin{equation*}
\Ann{A}=\left\{x\in A:\ \left(\forall a\in A\right)\left(a x=0\right)\right\}
\end{equation*}

\noindent\newline is called the {\bf annihilating ideal} of $A$.

\noindent\newline{\bf Proposition.} Let $A$ be a commutative ring. Then,

\begin{enumerate}
\item $\Ann{a}\trianglelefteq A$, for all $a\in A$.
\item $\Ann{A}\trianglelefteq A$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a\in A$. Obviously, $\Ann{a}\subseteq A$. Then, let $x,y\in\Ann{a}$. Then, $a x=a y=0$. First, by distributive law $a(x-y)=a x-a y=0-0=0$, so $x-y\in\Ann{a}$. By associativity of multiplication, $a(x y)=(a x)y=0 y=0$, so $x y\in\Ann{a}$. Finally, if $b\in A$, then $a(x b)=(a x)b=0$, so $x b\in\Ann{A}$. As $A$ is commutative, then $a(b x)=0$ and $b x\in\Ann{a}$. Thus, $\Ann{a}\trianglelefteq A$.

{\it Ad $2$.} We have $\Ann{A}\subseteq A$. If $x,y\in\Ann{A}$, then $a x=a y=0$, for all $a\in A$. So, by distributive law, $a(x-y)=a x-a y=0+0=0$, for all $a\in A$ and we have $x-y\in\Ann{A}$. Also, by associativity, $a(x y)=(a x)y=0\cdot y=0$, and $x y\in\Ann{A}$. Finally, if $b\in A$, then, by associativity, $a(x b)=(a x)b=0\cdot b=0$ and $x b\in\Ann{A}$. As $A$ is commutative, then also $b x\in\Ann{A}$. That implies $\Ann{A}\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $1\in A$. Then, $\Ann{A}=\{0\}$.

\noindent\newline{\bf Proof.} We can see that $0\in\Ann{A}$ as $a\cdot 0=0$, for all $a\in A$. Now, assume $x\in\Ann{A}$, $x\neq 0$. Then, $a x=0$, for all $a\in A$. But, as $1\in A$, then also $1\cdot x=0$. Thus, we have $x=0$, which is a contradiction. Therefore, $\Ann{A}=\{0\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. Then, $\{0\}\trianglelefteq A$ and $A\trianglelefteq A$.

\noindent\newline{\bf Proof.} As $0-0=0$ and $0\cdot 0$ we have $\{0\}\leq A$. Then, as $a\cdot 0=0\cdot a=0$, for all $a\in A$, we have $\{0\}\trianglelefteq A$. Then, as $A\leq A$ and $a x\in A$ for all $x\in A$ and $a\in A$, we have $A\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be rings and $f:A\rightarrow B$ a homomorphism from $A$ to $B$. If $\ker{f}\subseteq J\trianglelefteq A$, then $f(J)\trianglelefteq f(A)$.

\noindent\newline{\bf Proof.} Assume $\ker{f}\subseteq J$. Obviously, $f(J)\subseteq f(A)$. Now, take $f(x),f(y)\in f(J)$ (note that we have $x,y\in J$). Then, $f(x)f(y)=f(x y)$. As $x,y\in J$ and $J\trianglelefteq A$, then $x y\in J$. So, $f(x y)\in f(J)$ and from that $f(x)f(y)\in f(J)$. Similarly, as $x^{-1}\in J$ then $f(x^{-1})\in f(J)$. But, $f(x^{-1})=[f(x)]^{-1}\in J$. Thus, $f(J)\leq f(A)$. But, if we take $f(a)\in f(A)$ and $f(j)\in J$, then $f(a)f(j)=f(a j)$. But, as $J\trianglelefteq A$ and $a\in A$ and $j\in J$, then $a j\in J$. Similarly we show that $f(j)f(a)\in f(J)$. From that we have $f(J)\trianglelefteq f(A)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and $J\trianglelefteq A$. We say that $J$ is a {\bf maximal ideal} of $A$ if, for all $K\trianglelefteq A$, $J\subset K\subseteq A$ implies $K=A$.

\noindent\newline{\bf Proposition.} Let $A$ and $B$ be rings with $f:A\rightarrow B$ a homomorphism. If $\ran{f}$ is a field, then $\ker{f}$ is a maximal ideal of $A$.

\noindent\newline{\bf Proof.} Assume $\ker{f}\subset J$, where $J\trianglelefteq A$. Then, by a previous proposition, $f(J)\trianglelefteq f(A)=\ran{f}$. As $\ran{f}$ is a field, then, by a previous proposition, $f(J)=\ran{f}$ or $f(J)=\{0\}$. Assume $f(J)=\{0\}$. If we take $j\in J$, then $f(j)\in f(J)=\{0\}$, so it must be $f(j)=0$, i.e. $j\in\ker{f}$. Thus, $J\subseteq\ker{f}$, contrary to our assumption that $\ker{f}\subset J$ and it cannot be that $f(J)=\{0\}$. Assume $f(J)=\ran{f}$. We already know that $J\subseteq A$. Assume $J\subset A$. Then there exists $a\in A-J$ and we have $f(a)\in f(A-J)\subseteq\ran{f}$. But, $f(J)=\ran{f}$, so $f(a)\in f(J)$. But, as $f(J)$ contains only $f(j)$, where $j\in J$, then it must be that $a\in J$, contrary to our assumption that $a\notin J$ and $a\in A$. So it must be $J=A$. That means that $\ker{f}$ is a maximal ideal of $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that, from elementary arithmetic we know that $\Z$ is an integral domain. We will later prove it more rigorously (and the proof will be found in my works on number theory).

\noindent\newline{\bf Proposition.} There are no non-trivial\footnote{Trivial homomorphisms are $f(x)=0$ and $f(x)=x$.} homomorphisms from $\Z$ to $\Z$.

\noindent\newline{\bf Proof.} Assume that $f:\Z\rightarrow\Z$ is a homomorphism. Every element in $\dom{f}$ must be used, and so does $1\in\Z$. Note that $\Z=\cyc{1}$. So, if $f(k)\in\ran{f}$, then, as $k\in\Z$, we have

\begin{equation*}
k=\underbrace{1+1+\cdots+1}_{k\textnormal{ times}}.
\end{equation*}

\noindent\newline So, the image of $k$ is equal to:

\begin{equation*}
f(k)=f(\underbrace{1+1+\cdots+1}_{k\textnormal{ times}})=\underbrace{f(1)+f(1)+\cdots+f(1)}_{k\textnormal{ times}}=k\cdot f(1).
\end{equation*}

\noindent\newline As each $f(k)$ can be represented as a multiple (in group theory power) of $f(1)$, it is obvious that $\ran{f}=\cyc{f(1)}$. Assume $f(1)=k$, where $k\neq 1$. Then, $f(n)=f(1\cdot n)=f(1)f(n)=k f(n)$. From $f(n)=k f(n)=f(n) k$ (as multiplication is commutative) we get that $k$, i.e. $f(1)$ is a unity in $\ran{f}$. As $\ran{f}$ is a subring of $\Z$, with inherited cancellation property, and unity, it is an integral domain. So, by a previous proposition, either $f(1)=1$ or $f(1)=0$. If it were that $f(1)=1$, then $f(\cyc{1})=\cyc{f(1)}=\cyc{1}$. So, if we take $f(x)\in\ran{f}=\cyc{1}$, then $f(x)=k$, i.e. $f(x)=k\cdot 1$. But, also $x=x\cdot 1$, so $f(x)=f(x\cdot 1)=f(x)f(1)=x\cdot 1$. But, that means that $x\cdot 1=k\cdot 1$. From that we have $x=k$, i.e. $f(x)=x$, for all $x\in\Z$. If $f(1)=0$, then $f(\cyc{1})=\cyc{f(1)}=\cyc{0}=0$. Thus, $f(x)=0$, for all $x\in\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a commutative ring and $\pi_a:A\rightarrow A$ with $\pi_a(x)=a x$, for all $a\in A$. Then,

\begin{enumerate}
\item $\pi_a$ is an endomorphism of the additive group $A$.
\item Let $a\neq 0$. Then, $\pi_a$ is injective if and only if $a$ is not a divisor of zero.
\item Let $1\in A$. Then, $\pi_a$ is surjective if and only if $a$ is invertible.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} First we will check that $\pi_a$ is well-defined. If we take $x\in A$, then, as $a\in A$ and $x\in A$, obviously $a x\in A$, so $f(x)=a x$. Then, if $x=y$, and we multiply that by $a$ on the left, we get $a x=a y$, i.e. $f(x)=f(y)$. Therefore, $f$ is indeed a well-defined function. Then, $\pi_a(x+y)=a(x+y)=a x+a y=\pi_a(x)+\pi_a(y)$, so $\pi_a$ is a homomorphism, i.e. endomorphism, for the additive group $A$.

{\it Ad $2$.} {\it Necessity.} Assume $\pi_a(x)=\pi_a(y)$ implies $x=y$. That is, $a x=a y$ implies $x=y$. From the former expression we have $a x-a y=0$, i.e. $a(x-y)=0$, which implies $x=y$, that is $x-y=0$. Therefore, $a$ cannot be a divisor of zero, as it would have to be $a\neq 0$ and $(x-y)\neq 0$. {\it Sufficiency.} Assume $a$ is not a divisor of zero. Then, $a x=0$ implies $a=0$ or $x=0$. Assume $\pi_a(x)=\pi_a(y)$, i.e. $a x=a y$. Then, $a x-a y=0$ and $a(x-y)=0$. As $a$ is not a divisor of zero, we have either $a=0$ or $x-y=0$, i.e. $x=y$. It cannot be the former, so it is the latter and we are done.

{\it Ad $3$.} {\it Necessity.} Assume $\pi_a$ is surjective, that is, for all $y\in A$, there exists $x\in A$ such that $\pi_a(x)=y$. Then, we have $a x=y$. As also $1\in A$, then we can take $y=1$. We would then have that there exists $x\in A$ such that $\pi_a(x)=1$, i.e. $a x=1$. As $A$ is commutative then $a x=x a$. So, $a x=x a=1$ is equivalent to the fact that $a$ is invertible. {\it Sufficiency.} Assume that there exists $a^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$. If we take $y\in A$, then we need to find $x\in A$ such that $\pi_a(x)=y$. That means that $a x=y$. As $a$ is invertible, we can multiply that equation by $a^{-1}$ on the left. So we get $x=a^{-1} y$. To conclude, for all $y\in A$ we have that $\pi_a(a^{-1}y)=y$ and $\pi_a$ is surjective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $\mathcal{A}=\{\pi_a:\ a\in A\}$ be a set with two operations, $[\pi_a+\pi_b](x)=\pi_a(x)+\pi_b(x)$ and $\pi_a\pi_b=\pi_a\circ\pi_b$. Then, $\mathcal{A}$ is a ring.

\noindent\newline{\bf Proof.} {\it Additive group.} Associativity is inherited from $A$, as $\pi_a(x),\pi_b(x),\pi_c(x)\in A$, we have $[\pi_a+(\pi_b+\pi_c)](x)=\pi_a(x)+[\pi_b+\pi_c](x)=\pi_a(x)+(\pi_b(x)+\pi_c(x))=(\pi_a(x)+\pi_b(x))+\pi_c(x)=[\pi_a+\pi_b](x)+\pi_c(x)=[(\pi_a+\pi_b)+\pi_c](x)$. Commutativity is also inherited from $A$. Neutral element is $\pi_0$ as $[\pi_0+\pi_a](x)=\pi_0(x)+\pi_a(x)=0 x+a x=0+a x=a x=\pi_a(x)$. Commutativity fulfills $[\pi_0+\pi_a](x)=[\pi_a+\pi_0](x)$. Finally, inverse element is $\pi_{-a}(x)$ as $[\pi_a+\pi_{-a}](x)=\pi_a(x)+\pi_{-a}(x)=a x+(-a)x=a x-a x=0=0 x=\pi_0(x)$. Due to commutativity it's also $[\pi_{-a}+\pi_a](x)=\pi_0(x)$. {\it Distributive laws.} We have $[\pi_a (\pi_b+\pi_c)](x)=[\pi_a\circ(\pi_b+\pi_c)](x)=\pi_a([\pi_b+\pi_c](x))=a(\pi_b(x)+\pi_c(x))=a\pi_b(x)+a\pi_c(x)=\pi_a(\pi_b(x))+\pi_a(\pi_c(x))=[\pi_a\pi_b](x)+[\pi_a\pi_c](x)$. Similarly, we can show the second distributive law. {\it Multiplicative group.} Associativity holds due to associativity of function composition. Therefore, $\mathcal{A}$ is a ring.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $\phi:A\rightarrow\mathcal{A}$ is given by $\phi(a)=\pi_a$, then $\phi$ is a homomorphism. Additionaly, if $A$ has a unity or $A$ has no divisors of zero, then $\phi$ is an isomorphism.

\noindent\newline{\bf Proof.} First, $\phi$ is well defined as for each $a\in A$ there exists $\pi_a\in\mathcal{A}$, by definition. Also if $a=b$, then, multiplying by $x\in A$ on the right, we have $a x=b x$, i.e. $\pi_a(x)=\pi_b(x)$, for all $x\in A$. Now, for all $a,b\in A$, $\phi(a+b)=\pi_{a+b}=(a+b)x=a x+b x=\pi_{a}+\pi{b}$. Also, $\phi(a b)=\pi_{a b}=(a b)x=a(b x)=\pi_a(b x)=\pi_a(\pi_b(x))=\pi_a\pi_b$. If $\pi_a\in\mathcal{A}$, then we want to find $x\in A$ such that $\phi(x)=\pi_a$. But, as we have $\pi_a\in\mathcal{A}$, for all $a\in A$, then, there exists $a\in A$ such that $x=a$, i.e. $\phi(a)=\pi_a$. Therefore, $\phi$ is a surjective homomorphism. Now, assume $A$ has no divisors of zero and $\phi(a)=\phi(b)$. That is equivalent to $\pi_a=\pi_b$, i.e. $\pi_a(x)-\pi_b(x)=0$. That means that $a x-b x=0$, for all $x\in A$. By distributive law, $(a-b)x=0$. As $A$ has no divisors of zero, then either $a-b=0$ (and from that $a=b$ and we are done) or $x=0$. But, as $(a-b)x=0$ is for all $x\in A$, it is not necessary that $x=0$. If it were that $x=0$ for all $x\in A$, then it would also be $a=0$ and $b=0$. In both cases, $\phi$ is injective. Now, assume $1\in A$. Then, there exists $\pi_1\in\mathcal{A}$ such that $\pi_1(x)=1 x=x$. We can see that $\pi_1(\pi_a(x))=\pi_1(a x)=1(a x)=a x=\pi_a(x)$. Also $\pi_a(\pi_1(x))=\pi_a(x)$. Thus, $\pi_1\in\mathcal{A}$ is unity. Assume $|\ker{\phi}|>1$. Then there exists $a\in\ker{\phi}$, $a\neq 0$, such that $\phi{a}=\pi_0$. That would mean that $\pi_a=\pi_0$, i.e. $a x=0$, for all $x\in A$. But, as also $1\in A$, then $a\cdot 1=0$, i.e. $a=0$, which is a contradiction. Therefore, $\ker{\phi}=\{0\}$ and $\phi$ is injective. As it is also surjective, it is bijective, and as it is also a homomorphism, it is an isomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Quotient rings}
\end{center}

\vskip 0.5cm

\noindent We will just go through results from group theory, but in additive notation. If $A$ is a ring and $J\trianglelefteq A$, then we define, for any $a\in A$, the right coset of $J$ as the set $J+a=\{j+a:\ j\in J\}$. Then, all results from group theory hold, such as $a\in J+b$ if and only if $J+a=J+b$ if and only if $a-b\in J$. Also, $J+a=J$ if and only if $a\in J$. We will then designate $A\slash J=\{J+a:\ a\in A\}$.

\noindent\newline{\bf Theorem.} Let $J\trianglelefteq A$. If $J+a=J+c$ and $J+b=J+d$, for some $a,b,c,d\in A$, then:

\begin{enumerate}
\item $J+(a+b)=J+(c+d)$.
\item $J+(a b)=J+(c d)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} As additive group $A$ is an Abelian group, every subgroup of $A$ is normal. Thus, additive $J$ is a normal subgroup of additive group $A$, and by a previous theorem from group theory we have $J+(a+b)=J+(c+d)$. We will prove it once more to clear things up with additive notation and using the fact that additive $A$ is Abelian (not the fact that additive $J$ is normal; because of that, this proof will be more specific, i.e. less general). As $J+a=J+c$, then $a-c\in J$ and $b-d\in J$. Note that as $A$ is Abelian, we have $a=a+c+(-c)=c+a+(-c)=c+a-c$. Let $j\in J$. Then, $j+(a+b)\in J$. We have $j+(a+b)=j+(c+a-c)+(d+b-d)$. As $a-c\in J$ and $b-d\in J$, there exist $j_1,j_2\in J$ such that $a-c=j_1$ and $b-d=j_2$. Thus, $j+(a+b)=j+(c+j_1)+(d+j_2)=(j+j_1+j_2)+(c+d)$. As $J\trianglelefteq A$, then $j+j_1+j_2\in J$ so there exists $j_3\in J$ such that $j_3=j+j_1+j_2$ and we have that for all $j\in J$ there exists $j_3\in J$ such that $j+(a+b)=j_3+(c+d)$, meaning $J+(a+b)\subseteq J+(c+d)$. Again, let $j\in J$. Then $j+(c+d)\in J+(c+d)$. We have $j+(c+d)=j+(a+c-a)+(b+d-b)$. But, as $a-c\in J$, then there exists $j_1\in J$ such that $a-c=j_1$. Then, $a-c-j_1=0$ and from that $-j_1=c-a$. Similarly, as $b-d\in J$, there exists $j_2\in J$ such that $b-d=j_2$. From that we get $b-d-j_2=0$, that is, $-j_2=d-b$. Therefore, $j+(c+d)=j+(a-j_1)+(b-j_2)=(j-j_1-j_2)+(a+b)$. As $J\trianglelefteq A$, then $j-j_1-j_2\in J$ so there exists $j_3\in J$ such that $j-j_1-j_2=j_3$. So, for all $j\in J$ there exists $j_3\in J$ such that $j+(c+d)=j_3+(a+b)$, meaning $J+(c+d)\subseteq J+(a+b)$. That result, combined with former result, gives us $J+(a+b)=J+(c+d)$.

{\it Ad $2$.} Let $j\in J$. Then, $j+(a b)\in J+(a b)$. We have $j+(a b)=j+c b+a b-c b=j+c b+(a-c)b$. But, as $a-c\in J$, then also $(a-c)b\in J$, as $J\trianglelefteq A$. Therefore, there exists $j_1\in J$ such that $j+(a b)=(j+j_1)+(c b)$. Now, $j+(a b)=(j+j_1)+c d+c b-c d=(j+j_1)+c d+c(b-d)$. As $b-d\in J$, then also $c(b-d)\in J$ due to $J\trianglelefteq A$. So, there exists $j_2\in J$ such that $j+(a b)=(j+j_1+j_2)+(c d)$. As for all $j\in J$ there exists $j_3\in J$ (where $j_3=j+j_1+j_2$) such that $j+(a b)=j_3+(c d)$, we have $J+(a b)\subseteq J+(c d)$. Again, let $j\in J$. Then $j+(c d)\in J+(c d)$. We have $j+(c d)=j+c b+c d-c b=j+(c b)+c(d-b)$. As $b-d\in J$, then there exists $j_1\in J$ such that $j_1=b-d$. That gives us $-j_1=d-b$. Then, as $-j_1\in J$, we have $d-b\in J$. As $J\trianglelefteq A$, we have $c(d-b)\in J$, i.e. there exists $j_2\in J$ such that $j_2=c(d-b)$. We have $j+(c+d)=(j+j_2)+(c b)=(j+j_2)+a b+c b-a b=(j+j_2)+(a b)+(c-a)b$. As $a-c\in J$, then there exists $j_3\in j$ such that $a-c=j_3$. Now, $-j_3=c-a$ so $c-a\in J$, and, as $J\trianglelefteq A$, we have $(c-a)b\in J$, i.e. there exists $j_4\in J$ such that $(c-a)b=j_4$. Finally, $j+(c+d)=(j+j_2+j_4)+(a b)$. As $j+j_2+j_4\in J$, there exists $j_5\in J$ such that, for all $j\in J$, we have $j+(c d)=j_5+(a b)$. Thus, $J+(c d)\subseteq J+(a b)$ and, combining that with former subset relation, we have $J+(a b)=J+(c d)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be a ring and $J\trianglelefteq A$. Then, $A\slash J$ (with coset addition and multiplication defined as above) is a ring.

\noindent\newline{\bf Proof.} The binary operations are well-defined (they are defined for all $J+a\in A\slash J$, and satisfy the property of uniqueness due to the theorem above). {\it Additive associativity.} For all $J+a,J+b,J+c\in A\slash J$, we have $[J+a]+([J+b]+[J+c])=[J+a]+[J+(b+c)]=J+(a+(b+c))=J+((a+b)+c)=[J+(a+b)]+[J+c]=([J+a]+[J+b])+[J+c]$. {\it Zero.} We have $0\in A$ and so $J+0=J\in A\slash J$. If $J+a\in A\slash J$, then $[J+a]+J=[J+a]+[J+0]=J+(a+0)=J+a$. Also, $J+[J+a]=[J+0]+[J+a]=J+(a+0)=J+a$. {\it Negatives.} If $a\in A$, then $-a\in A$. Then also $J+a\in A\slash J$ and $J+(-a)\in A\slash J$. We have $[J+a]+[J+(-a)]=J+(a+(-a))=J+0=J$. Also, $[J+(-a)]+[J+a]=J+(-a+a)=J+0=J$. Therefore, $-(J+a)=J+(-a)$. {\it Commutativity.} Let $a,b\in A$. Then, $J+a,J+b\in A\slash J$ and $[J+a]+[J+b]=J+(a+b)=J+(b+a)=[J+b]+[J+a]$. {\it Distributivity.} Let $a,b,c\in J$. Then, $[J+a]([J+b]+[J+c])=[J+a]\cdot[J+(b+c)]=J+(a(b+c))=J+(a b+a c)=[J+(a b)]+[J+(a c)]=[J+a]\cdot[J+b]+[J+a]\cdot[J+c]$. Also, $([J+a]+[J+b])[J+c]=[J+(a+b)]\cdot[J+c]=J+((a+b)c)=J+(a c+b c)=[J+(a c)]+[J+(b c)]=[J+a]\cdot[J+c]+[J+b]\cdot[J+c]$. {\it Multiplicative associativity.} For all $J+a,J+b,J+c\in A\slash J$, we have $[J+a]([J+b]\cdot[J+c])=[J+a]\cdot[J+(b c)]=J+(a(b c))=J+((a b) c)=[J+(a b)]\cdot[J+c]=([J+a]\cdot[J+b])[J+c]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $J\trianglelefteq A$. Then,

\begin{enumerate}
\item If $1\in A$, then $A\slash J$ is a ring with unity.
\item If $A$ is commutative, then so is $A\slash J$.
\item If $A$ is a field, then so is $A\slash J$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} As $A$ is a ring, then by the previous theorem, so is $A\slash J$. Now, as $1\in A$, we have $J+1\in A\slash J$. Then, for $a\in A$, $[J+a]\cdot[J+1]=J+(a\cdot 1)=J+a$. Also, $[J+1]\cdot[J+a]=J+(1\cdot a)=J+a$. {\it Ad $2$.} If $a b=b a$, for all $a,b\in A$, then $[J+a]\cdot[J+b]=J+(a b)=J+(b a)=[J+b]\cdot[J+a]$. {\it Ad $3$.} If for every $a\in A$, there exists $a^{-1}\in A$ such that $a a^{-1}=a^{-1} a=1$, then, as $a,a^{-1}\in A$, also $J+a,J+(a^{-1})\in A\slash J$. We have $[J+a]\cdot[J+(a^{-1})]=J+(a a^{-1})=J+1$. Also, $[J+(a^{-1})]\cdot[J+a]=J+(a^{-1}a)=J+1$. Therefore, $J+(a^{-1})=(J+a)^{-1}$. If $A$ has a unity then so does $A\slash J$. If $A$ is commutative, then so is $A\slash J$. If $A$ has all elements invertible, then so does $A\slash J$. In other words, if $A$ is a field, then $A\slash J$ is a field.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be a ring and $J\trianglelefteq A$. Then, there exists a homomorphism $f:\homo{A}{A\slash J}{J}$ such that $\ran{f}=A\slash J$.

\noindent\newline{\bf Proof.} Let $f(x)=J+x$. This function is well-defined, as if $x\in A$, then there exists $J+x\in A\slash J$ and we have $f(x)=J+x$. Now, if $x=y$, then obviously $J+x=J+y$, i.e. $f(x)=f(y)$. Furthermore, $f(x+y)=J+(x+y)=(J+x)+(J+y)=f(x)+f(y)$ and $f(x y)=J+(x y)=(J+x)(J+y)=f(x)f(y)$. Therefore, $f$ is a homomorphism from $A$ to $A\slash J$. It is also surjective, as if we take $J+x\in A\slash J$, then, $x\in A$, so we have $f(x)=J+x$. Then, also:

\begin{equation*}
\ker{f}=\{x\in A:\ f(x)=J\}=\{x\in A:\ J+x=J\}=\{x\in A:\ x\in J\}=J.
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (FHT for rings).} Let $A$ be a ring and $f:\homo{A}{B}{J}$ a homomorphism such that $\ran{f}=B$. Then, $A\slash J\cong B$.

\noindent\newline{\bf Proof.} We have that $\ker{f}\trianglelefteq A$, i.e. $J\trianglelefteq A$, thus $A\slash J$ is a well-defined group. Let $\phi:A\slash J\rightarrow B$ be a mapping defined with $\phi(J+x)=f(x)$. If we take $J+x\in A\slash J$, then $x\in A$ and $f(x)$ is well defined by assumption. Also, $J+x=J+y$ implies $x-y\in J$. But, $J=\ker{f}$, so $x-y\in\ker{f}$, i.e. $f(x-y)=0$. As $f$ is a homomorphism, $f(x)-f(y)=0$ and we have $f(x)=f(y)$. Therefore, $\phi$ is well-defined. Now, if $f(x)=f(y)$, we have $f(x)-f(y)=0$ and, as $f$ is a homomorphism, $f(x-y)=0$. That means that $x-y\in\ker{f}=J$ which is equivalent to $J+x=J+y$. Thus, $\phi$ is injective. If we take $y\in B$, then, as $B=\ran{f}$, there exists $x\in A$ such that $f(x)=y$. Therefore, we have $f(x)\in B$, but, as $x\in A$, then $J+x\in A\slash J$, so $\phi(J+x)=f(x)=y$ and $\phi$ is surjective. Finally, $\phi((J+x)+(J+y))=\phi(J+(x+y))=f(x+y)=f(x)+f(y)=\phi(J+x)+\phi(J+y)$. Similarly, $\phi((J+x)(J+y))=\phi(J+(x y))=f(x y)=f(x)f(y)=\phi(J+x)\phi(J+y)$. Thus, $\phi$ is an isomorphism from $A\slash J$ to $B$ and we have $A\slash J\cong B$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be rings and $J\trianglelefteq A$. If $A\slash J\cong B$, then there exists a homomorphism $f:\homo{A}{B}{J}$ such that $\ran{f}=B$.

\noindent\newline{\bf Proof.} From a previous theorem we have that there exists a homomorphism $h:\homo{A}{A\slash J}{J}$ with $h(x)=J+x$. But, as $A\slash J\cong B$, there exists an isomorphism $g:A\slash J\longrightarrow B$. If we take $g\circ h:A\rightarrow B$, we will prove that $f=g\circ h$ is a homomorphism with $\ker{f}=J$. As $g$ and $h$ are well-defined, so is their composition. Now, $f(x+y)=g(h(x+y))=g(J+(x+y))=g((J+x)+(J+y))=g(J+x)+g(J+y)=g(h(x))+g(h(y))=f(x)+f(y)$. Similarly, $f(x y)=g(h(x y))=g(J+(x y))=g((J+x)(J+y))=g(J+x)g(J+y)=g(h(x))g(h(y))=f(x)f(y)$. Therefore, $f$ is a homomorphism from $A$ to $B$. But, $\ker{f}=\{x\in A:\ g(h(x))=e\}=\{x\in A:\ g(J+x)=e\}$. Then, $J+x\in\ker{g}$. But, as $\ker{g}=\{J\}$, then $J+x\in\{J\}$ so it must be $J+x=J$ i.e. $x\in J$. So, $\ker{f}=\{x\in A:\ x\in J\}=J$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $A$ be a commutative ring with unity, $J\trianglelefteq A$, $a\in A$ and $K=\{a x-y:\ (x\in A)\wedge(y\in J)\}$. Then, $K\trianglelefteq A$.

\noindent\newline{\bf Proof.} By definition of $K$, we have $K\subseteq A$. Then, if we take $a x_1-y_1,a x_2-y_2$, we have $(a x_1-y_1)-(a x_2-y_2)=a x_1-y_1-a x_2+y_2=a(x_1-x_2)-(y_1-y_2)$ and $(a x_1-y_1)(a x_2-y_2)=a^2 x_1 x_2-a x_1 y_2-a x_2 y_1+y_1 y_2=a(a x_1 x_2-x_1 y_1-x-2 y_1)-(y_1 y_2)$. Thus, as $K$ is closed with respect to subtraction and multiplication, $K\leq A$. Finally, if $a x-y\in K$ and $b\in A$, we have $b(a x-y)=b a x-b y$. But, as $A$ is commutative, we have $a x b-y b=a(x b)-(y b)\in K$ and that means $K\trianglelefteq A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be a commutative ring with unity and $J\trianglelefteq A$. Then, $J$ is a maximal ideal of $A$ if and only if $A\slash J$ is a field.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $J$ be a maximal ideal of $A$ and $J+a\in A\slash J$. We want to prove that $A\slash J$ is a field, i.e. if $J+a\neq J$ (in field all elements except zero are invertible), there exists $J+x\in A\slash J$ such that $(J+a)(J+x)=(J+x)(J+a)=J+1$. Therefore, assume that $J+a\neq J$, i.e. that is it not true that $J+a=J$ (which is equivalent to $a\in J$). That means that it is not true that $a\in J$, i.e. it is true that $a\notin J$. Assume the inverse $J+x$ of $J+a$ does not exist. Then it follows that it cannot be that $(J+a)(J+x)=J+1$. That expression is equivalent to $J+(a x)=J+1$. After applying $J+(-1)$ on that equality, we get $(J+(a x))+(J+(-1))=J$ (and reverse). That is equivalent to $J+(a x-1)=J$. So, as $(J+a)(J+x)=J+1$ is equivalent to $J+(a x-1)=J$, i.e. $a x-1\in J$, and we assumed that $(J+a)(J+x)=J+1$ is impossible, then it must be that $a x-1\notin J$. Now, let $K$ be defined as in a previous lemma. Then we have $K\trianglelefteq A$. If we take $j\in J$, then, as $j=a0-(-j)$, we have $j=a0-(-j)\in K$, so $J\subseteq K$. But, our condition is that $a\notin J$ and $a=a\cdot 1+0$ (as $0\in J$, taken for $y$ and $1\in A$, taken for $x$). Therefore, $a\in K$. So, there exists $a\in K-J$ and that means that $J\subset K\trianglelefteq A$. But, as $J$ is, by assumption maximal ideal, by definition it must be that $K=A$. Therefore, as $1\in A$, we have $1\in K$. Then, there exist $x\in A$ and $y\in J$ such that $a x-y=1$. That is equivalent to $a x-1=y$, so it must be that $a x-1\in J$, for some $x\in A$. This is contrary to our assumption and it must be that $J+a\neq J$ is invertible. Therefore, $A\slash J$ is a field.

{\it Sufficiency.} Let $A\slash J$ be a field. Then, at least $1\in A$ and $1\neq 0$, so $|A\slash J|>1$. From that we have $|A|:|J|>1$, i.e. $|A|>|J|$. So, we can assume that there exists $K\trianglelefteq A$ such that $J\subset K\subseteq A$. Then, $K\neq\{0\}$. As $J\trianglelefteq A$, $J\subset K\trianglelefteq A$, then $J\trianglelefteq K$, so $K\slash J$ is a well-defined quotient ring. If we take $J+k\in K\slash J$, then, as $k\in K\subseteq A$, we have that $J+k\in A\slash J$, i.e. $K\slash J\subseteq A\slash J$. It is evident that, as there exists $k\in K$ (from $K\neq\{0\}$) such that $k\neq 0$. But then as $J+k\in K\slash J\subseteq A\slash J$, we have $(J+k)^{-1}=J+(k^{-1})\in A\slash J$ and $k^{-1}\in A$. As $K\trianglelefteq A$, we have $k k^{-1}\in K$, i.e. $1\in K$. That implies that, if $a\in A$, then $a 1\in K$, so $A\subseteq K$ and we have $A=K$, i.e. $J$ is a maximal ideal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove the following:

\begin{enumerate}
\item $\mathcal{F}(\R)\slash\{f\in\mathcal{F}(\R):\ f(0)=f(1)=0\}\cong\R\times\R$;
\item $\mathcal{F}(\R)\slash\{f\in\mathcal{F}(\R):\ (\forall x\in\Q)(f(x)=0)\}\cong\mathcal{F}(\Q,\R)$.
\end{enumerate}

\noindent{\bf Solution.} {\it Ad $1$.} Let $J=\{f\in\mathcal{F}(\R):\ f(0)=f(1)=0\}$. Then, $J\subseteq\mathcal{F}(\R)$, by definition. If $f,g\in J$, then, $[f-g](0)=f(0)-g(0)=0-0=0$. Also, $[f-g](1)=f(1)-g(1)=0-0=0$. Therefore, $f-g\in J$. Then, $[f g](0)=f(0)g(0)=0\cdot 0=0$ and $[f g](1)=f(1)g(1)=0\cdot 0=0$. Finally, if $f\in\mathcal{F}(\R)$ and $g\in J$, then $[f g](0)=f(0)g(0)=f(0)\cdot 0=0$ and $[f g](1)=f(1)g(1)=f(1)\cdot 0=0$ (function multiplication is commutative so the same thing is true for $g f$), so $f g\in J$ and $g f\in J$. Therefore, $J\trianglelefteq\mathcal{F}(\R)$. Actually all this was unnecessary as FHT will prove it for us, but hey, it was a more or less fun excercise. Let $\phi:\mathcal{F}(\R)\rightarrow\R\times\R$ be a mapping defined with $\phi(f)=(f(0),f(1))$. Then, $\phi$ is well defined, as every $f$ is a function from $\R$ to $\R$, so it is defined in $0$ and in $1$. Now, if $f=g$, then $f(x)=g(x)$ for all $x\in\R$. Therefore, $f(0)=g(0)$ and $f(1)=g(1)$, or by definition of ordered pair, $(f(0),f(1))=(g(0),g(1))$, i.e. $\phi(f)=\phi(g)$. Now, $\phi(f+g)=([f+g](0),[f+g](1))=(f(0)+g(0),f(1)+g(1))=(f(0),f(1))+(g(0),g(1))=\phi(f)+\phi(g)$. Also, $\phi(f g)=([f g](0),[f g](1))=(f(0)g(0),f(1)g(1))=(f(0),f(1))(g(0),g(1))=\phi(f)\phi(g)$. Now, if we take $(a,b)\in\R\times\R$, then there exists $f\in\R\rightarrow\R$ such that $f(0)=a$ and $f(1)=b$ (as $a,b\in\R$). Thus, $\phi$ is a surjective homomorphism. Also, $\ker{\phi}=\{f:\ \phi(f)=(0,0)\}=\{f:\ (f(0),f(1))=(0,0)\}=J$. By fundamental homomorphism theorem $\mathcal{F}(\R)\slash J\cong\R\times\R$.

{\it Ad $2$.} Let $\phi(f):\mathcal{F}(\R)\rightarrow\mathcal{F}(\Q,\R)$ be defined with $\phi(f)=f\circ r$, where $r:\Q\rightarrow\R$ with $r(x)=f(x)$. We see that $r$ is well defined as, if we take $x\in\Q\subset\R$, then there exists $f(x)\in\R$. Then, composition $f\circ r$ is well-defined as $\cod{r}=\dom{f}$. Then, if $f,g\in\mathcal{F}(\R)$ and $f=g$, we have $f(x)=g(x)$. It is obvious, then, that $f(r(x))=g(r(x))$, i.e. $\phi(f)=\phi(g)$. Now, $\phi(f+g)=[f+g]\circ r$. We see that $[f+g](r(x))=f(r(x))+g(r(x))$, so $[f+g]\circ r=f\circ r+g\circ r=\phi(f)+\phi(g)$. Similarly, $\phi(f+g)=[f g]\circ r$ implies $[f g](r(x))=f(r(x))g(r(x))=\phi(f)\phi(g)$. If $h\in\mathcal{F}(\Q,\R)$, then there exists $h'\in\mathcal{F}(\R)$ such that $h(x)=h'(x)$ (as all $x\in\Q\subset\R$). So, $\phi(h')=h\circ r$ (think about it). Therefore, $\phi$ is a surjective homomorphism. Finally, $\ker{\phi}=\{f\in\mathcal{F}(\R):\ \phi(f)=0\}=\{f\in\mathcal{F}(\R):\ f(r(x))=0\}$. As $f(r(x))=0$, then, as $r:\Q\rightarrow\R$, we have that $f(y)=0$, for all $y\in\Q$. Therefore, $\ker{\phi}=\{f\in\mathcal{F}(\R):\ (\forall x\in\Q)(f(x)=0)\}$. By FHT, $\mathcal{F}(\R)\slash\{f\in\mathcal{F}(\R):\ (\forall x\in\Q)(f(x)=0)\}\cong\mathcal{F}(\Q,\R)$.

\noindent\newline{\bf Remark.} Let $A$ be a ring. Notice that as additive $A$ is Abelian group, we have $n(x+y)=n x+n y$ (same as $(x y)^n=x^n y^n$ for Abelian groups in multiplicative notation), for all $n\in\Z$ and $x,y\in a$.

\noindent\newline{\bf Problem.} Suppose $2x=0$ for every $x\in A$, where $A$ is a commutative ring. Prove that $(x+y)^2=x^2+y^2$ for all $x,y\in A$. Conclude that the function $h(x)=x^2$ is a homomorphism from $A$ to $A$. If $J=\{x\in A:\ x^2=0\}$ and $B=\{x^2:\ x\in A\}$, explain why $J\trianglelefteq A$, $B\leq A$ and $A\slash J\cong B$.

\noindent\newline{\bf Solution.} First, $(x+y)^2=(x+y)(x+y)=x(x+y)+y(x+y)=x^2+x y+y x+y^2=x^2+2 x y+y^2=x^2+y^2$. Then, for all $x\in A$, there exists $x^2\in A$. If $x=y$, then $x^2=x y$, i.e. $x^2=y^2$. Then, $h(x+y)=(x+y)^2=x^2+y^2=h(x)+h(y)$ and $h(x y)=(x y)^2=x^2 y^2=h(x)h(y)$. We have $\ker{h}=\{x:\ h(x)=0\}$, i.e. $\ker{h}=\{x:\ x^2=0\}=J$. But, $h$ needs to be surjective and it is obvious that $B=\ran{f}$. So, by FHT $A\slash J\cong B$.

\noindent\newline{\bf Problem.} Suppose $6x=0$ for every $x\in A$, where $A$ is a commutative ring. Prove that the function $h(x)=3x$ is a homomorphism from $A$ to $A$. If $J=\{x:\ 3x=0\}$ and $B=\{3x:\ x\in A\}$, explain why $J\trianglelefteq A$, $B\leq A$ and $A\slash J\cong B$.

\noindent\newline{\bf Solution.} For all $x\in A$ we have $3x\in A$. If $x=y$, then $x+x+x=y+y+y$, i.e. $3x=3y$. Then, $h(x+y)=3(x+y)=(x+y)+(x+y)+(x+y)=3x+3y=h(x)+h(y)$ and $h(x y)=3x y=3x+0=3x y+6x y=3x(y+2 y)=3 x 3 y=h(x)h(y)$. We have $\ran{f}=\{3 x:\ x\in A\}=B$ and $\ker{f}=\{x:\ h(x)=0\}=\{x:\ 3 x=0\}=J$. So, by FHT, $A\slash J\cong B$.

\noindent\newline{\bf Problem.} If $a$ is an {\bf idempotent} element of commutative ring $A$ (that is, $a^2=a$), prove that the function $\pi_a(x)=a x$ is a homomorphism from $A$ into $A$. Show that $\ker{\pi_a}=\Ann{a}$. Show that $\ran{\pi_a}=\cyc{a}$. Conclude that $A\slash\Ann{a}\cong\cyc{a}$.

\noindent\newline{\bf Solution.} For all $x\in A$, $a x\in A$, as $a\in A$. If $x=y$, then, after multiplying by $a$ on the left we have $a x=a y$, i.e. $\pi_a(x)=\pi_a(y)$, so $\pi_a$ is well-defined. Then, $\pi_a(x+y)=a(x+y)=a x+a y=\pi_a(x)+\pi_a(y)$ and $\pi_a(x y)=a(x y)=a^2 x y=a x a y=\pi_a(x)\pi_a(y)$. Then, $\ran{\pi_a}=\{a x:\ x\in A\}=\cyc{a}$ (principal ideal of $A$ generated by $a$) and $\ker{f}=\{x\in A:\ a x=0\}=\Ann{a}$. So, by FHT, $A\slash\Ann{a}\cong\cyc{a}$.

\noindent\newline{\bf Problem.} Let $A$ be a commutative ring. For each $a\in A$, let $\pi_a$ be the function given by $\pi_a(x)=a x$. Define the following addition and multiplication on $\overline{A}=\{\pi_a:\ a\in A\}$: $\pi_a+\pi_b=\pi_{a+b}$ and $\pi_a\pi_b=\pi_{a b}$ (assume that $\overline{A}$ is a ring). Show that the function $\phi(a)=\pi_a$ is a homomorphism from $A$ onto $\overline{A}$. Show that $A\slash\Ann{A}\cong\overline{A}$.

\noindent\newline{\bf Solution.} For all $a\in A$, $\pi_a$ is well defined, as shown in the previous problem, so there exists $\pi_a\in A$, for all $a\in A$. Also, uniqueness holds as $a=b$ implies $a x=b x$, i.e. $\pi_a(x)=\pi_b(x)$, for all $x\in A$. Then, $\phi(a+b)=\pi_{a+b}=\pi_a+\pi_b=\phi(a)+\phi(b)$ and $\phi(a b)=\pi_{a b}=\pi_a\pi_b=\phi(a)\phi(b)$. We have $\ran{\phi}=\{\pi_a:\ a\in A\}=\overline{A}$ and $\ker{\phi}=\{a\in A:\ \pi_a=\pi_0\}$. But, that means that $\pi_a(x)=\pi_0(x)$, i.e. $a x=0$, for all $x\in A$. So, $\ker{f}=\{a\in A:\ (\forall x\in A)(a x=0)\}=\Ann{A}$ (as $a$ and $x$ here are interchangeable; the statement is true for all $a$ and for all $x$). Thus, $A\slash\Ann{A}\cong\overline{A}$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring and $J\trianglelefteq A$. Then,

\begin{enumerate}
\item Every element of $A\slash J$ has a square root iff for every $x\in A$, there is some $y\in A$ such that $x-y^2\in J$.
\item Every element of $A\slash J$ is its own negative iff $x+x\in J$ for every $x\in A$.
\item $A\slash J$ is a boolean ring iff $x^2-x\in J$ for every $x\in A$.
\item If $A$ is commutative and $J=\{a\in A:\ (\exists n\in\N)(a^n=0)\}$, then $J\trianglelefteq A$ and $A\slash J$ has no nilpotent elements except zero ($J\in A\slash J$).
\item Every element of $A\slash J$ is nilpotent iff for every $x\in A$, there exists $n\in\Z^{+}$ such that $x^n\in J$.
\item $A\slash J$ has a unity element iff there exists an element $a\in A$ such that $a x-x\in J$ and $x a-x\in J$, for every $x\in A$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Necessity.} Assume that for all $J+x\in A\slash J$ there exists $J+y\in A\slash J$ such that $J+x=(J+y)^2$. That implies $J+x=J+y^2$, i.e. $x-y^2\in J$. {\it Sufficiency.} Assume that for every $x\in A$ there exists $y\in A$ such that $x-y^2\in J$. That is equivalent to $J+x=J+y^2$ which is equivalent to $J+x=(J+y)^2$, i.e. for every $J+x\in A\slash J$ there exists $J+y\in A\slash J$ such that $J+x=(J+y)^2$. In other words, every element of $A\slash J$ has a square root.

{\it Ad $2$.} {\it Necessity.} Assume that for all $J+x\in A\slash J$ we have $J+x=-(J+x)$. That is equivalent to $J+x=J+(-x)$, i.e. $x+x\in J$, for all $x\in A$. {\it Sufficiency.} Assume $x+x\in J$, for every $x\in A$. Then, $J+x=J+(-x)$, i.e. $J+x=-(J+x)$, so every element of $A\slash J$ is its own negative.

{\it Ad $3$.} {\it Necessity.} Assume $A\slash J$ is a boolean ring. That means that every element is idempotent, i.e. $(J+x)^2=J+x$, for all $J+x\in A\slash J$. That is equivalent to $J+(x^2)=J+x$, i.e. $x^2-x\in J$. {\it Sufficiency.} Assume $x^2-x\in J$ for all $x\in A$. Then, $J+(x^2)=J+x$, i.e. $(J+x)^2=J+x$. That is, every element of $A\slash J$ is idempotent.

{\it Ad $4$.} Let $J=\{a\in A: (\exists n\in\N)(a^n=0)\}$. We see that $J\subseteq A$. Now, take $a,b\in J$. We have, by a previous proposition that $a+b$ is nilpotent in commutative rings, so $a+b\in J$. If we take $a\in J$, then there exists $n\in\N$ such that $a^n=0$. So, if we take $(-a)^{2 n}=((-a)^2)^n=(a^2)^n=(a^n)^2=0^2=0$, we have that $-a$ is nilpotent and have $-a\in J$. Then, by a previous proposition, if $x\in A$ and $a\in J$, then, as $A$ is commutative, $x a$ is nilpotent and $x a\in J$. So is true for $a x$ (due to commutativity) and we have $a x\in J$. Thus, $J\trianglelefteq A$. Assume there exists $J+a\in A\slash J$ such that $(J+a)^n=J$, for some $n\in\Z^{+}$ and $J+a\neq J$. That means that $J+a^n=J$, i.e. $a^n\in J$. But, then there must exist $m\in\Z^{+}$ such that $(a^n)^m=0$ and that is equivalent to $a^{n m}=0$. But, that also means that $a\in J$, so $J+a=J$ which is a contradiction. Therefore, there are no nilpotent elements in $A\slash J$ except $J$.

{\it Ad $5$.} {\it Necessity.} Assume that for all $J+x\in A\slash J$ (thus $x\in A$) there exists $n\in\Z^{+}$ such that $(J+x)^n=J$. That is equivalent to $J+(x^n)=J$, i.e. $x^n\in J$. {\it Sufficiency.} Assume that for all $x\in A$ there exists $n\in\Z^{+}$ such that $x^n\in J$. That is equivalent to $J+x^n=J$ and again to $(J+x)^n=J$. Thus, $J+x$ is nilpotent.

{\it Ad $6$.} {\it Necessity.} Assume $J+1\in A\slash J$ (and by that $1\in A$). Then, for all $x\in A\slash J$, $(J+1)(J+x)=J+x$ and $(J+x)(J+1)=J+x$. That is equivalent to $J+(1 x)=J+x$ and $J+(x 1)=J+x$. Then, that is equivalent to $1 x-x\in J$ and $x 1-x\in J$. {\it Sufficiency.} Assume that for all $x\in A$ exists $a\in A$ such that $a x-x\in J$ and $x a-x\in J$. That is equivalent to $J+(a x)=J+x$ and $J+(x a)=J+x$. That is equivalent to $(J+a)(J+x)=J+x$ and $(J+x)(J+a)=J+x$. Therefore, $J+a$ is a unity in $A\slash J$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and let $J\trianglelefteq A$. We say that $J$ is a {\bf prime ideal} of $A$ and write $J\primeideal A$ if $a b\in J$ implies $a\in J$ or $b\in J$, for all $a,b\in A$.

\noindent\newline{\bf Theorem.} Let $A$ be a commutative ring with unity. Then, $J\primeideal A$ if and only if $A\slash J$ is an integral domain.

\noindent\newline{\bf Proof.} {\it Necessity.} By a previous proposition we have that $A\slash J$ is a commutative ring with unity. Assume $J\primeideal A$. Then, for all $a,b\in A$, if $a b\in J$, then $a\in J$ or $b\in J$. Now, let $J+a,J+b,J+c\in A\slash J$ and assume that $J+a\neq J$. Then, $(J+a)(J+b)=(J+a)(J+c)$ is equivalent to $J+(a b)=J+(a c)$ which is equivalent to $J+(a b)-(J+(a c))=J$. That is, again, equivalent to $J+(a b-a c)=J$, i.e. $J+(a(b-c))=J$, meaning $a(b-c)\in J$. As $a\in A$, $b-c\in A$, then, as $J\primeideal A$, either $a\in J$ or $b-c\in J$. If $a\in J$, then $J+a=J$, which cannot be as we assumed $J+a\neq J$. So, we are only left with $b-c\in J$ which is equivalent to $J+b=J+c$. As $(J+a)(J+b)=(J+a)(J+c)$, where $J+a\neq J$ implies $J+b=J+c$, $A\slash J$ is an integral domain.

{\it Sufficiency.} Assume that $A\slash J$ is an integral domain. Take $a,b\in A$ such that $a b\in J$. Then, $J+(a b)=J$ and $J+a,J+b\in A\slash J$. As $J+(a b)=(J+a)(J+b)$ we have $(J+a)(J+b)=J$. As $A\slash J$ is an integral domain, it has no divisors of zero (zero in $A\slash J$ is $J$), so it must be $J+a=J$ or $J+b=J$, i.e. $a\in J$ or $b\in J$. Therefore, $J\primeideal A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Every maximal ideal of $A$ is a prime ideal.

\noindent\newline{\bf Proof.} Suppose $J\trianglelefteq A$ is a maximal ideal of $A$. Then, by a previous theorem, $A\slash J$ is a field. But, every field is an integral domain, so $A\slash J$ is also an integral domain. By the previous theorem, $J\primeideal A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring and $J\trianglelefteq A$. We say that $J$ is a {\bf primary ideal} of $A$ if, for all $a, b\in A$ such that $a b\in J$, then either $a\in J$ or $b^n\in J$ for some $n\in\Z^{+}$.

\noindent\newline{\bf Definition.} Let $A$ be a ring and $J\trianglelefteq A$. We say that $J$ is a {\bf semiprime ideal} if, for all $a\in A$ such that $a^n\in J$, for some $n\in\Z^{+}$, it follows that $a\in J$.

\noindent\newline{\bf Proposition.} Let $A$ be a commutative ring with unity and $J\trianglelefteq A$. Then,

\begin{enumerate}
\item $A\slash J$ is a field iff for every $a\in A$, $a\notin J$, there exists $b\in A-J$ such that $a b-1\in J$.
\item Element $J+a\in A\slash J$, $J+a\neq J$ is either invertible or a divisor of zero iff for every $a\in A-J$, there exists $x\in A-J$ such that either $a x\in J$ or $a x-1\in J$.
\item Every zero divisor in $A\slash J$ is nilpotent iff $J$ is a primary ideal.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Necessity.} Assume $A\slash J$ is a field. Then, for all $J+a\in A\slash J$, $J+a\neq J$, there exists $J+b\in A\slash J-\{J\}$ such that $(J+a)(J+b)=(J+b)(J+a)=J+1$. That is equivalent to $J+(a b)=J+1$ and that is equivalent to $a b-1\in J$. {\it Sufficiency.} Assume that for all $a\in A-J$ exists $b\in A-J$ (then $J+b\neq J$, i.e. $J+b\in A\slash J-\{J\}$) such that $a b-1\in J$. That is equivalent to $J+(a b)=J+1$ and that is equivalent to $(J+a)(J+b)=J+1$. As $A$ is a commutative ring, then so is $A\slash J$ and we have $(J+a)(J+b)=(J+b)(J+a)=1$. Therefore, $A\slash J$ is a field.

{\it Ad $2$.} {\it Necessity.} Assume that $J+a\in A\slash J-\{J\}$ is invertible. Then, by result from above, there exists $x\in A$ such that $a x-1\in J$. If $J+a$ is a divisor of zero, then there exists $J+x\in A\slash J-\{J\}$ such that $(J+a)(J+x)=J$. That is equivalent to $J+(a x)=J$, i.e. $a x\in J$. {\it Sufficiency.} Assume that for all $a\in A-J$ there exists $x\in A-J$ such that $a x\in J$ or $a x-1\in J$. If $a x\in J$, then $J+(a x)=J$, i.e. $(J+a)(J+x)=J$. As $J$ is zero in $A\slash J$ and $J+x,J+a\neq J$, then $J+a$ (and $J+x$) is a divisor of zero. If $a x-1\in J$, then by a previous proposition $a$ is invertible.

{\it Ad $3$.} {\it Necessity.} Let $a,b\in A$ such that $a b\in J$. That means that $J+(a b)=J$, i.e $(J+a)(J+b)=J$. If $J+a$ is not a divisor of zero, then $J+a=J$ or $J+b=J$, i.e. $a\in J$ or $b\in J$ (same as $b^1\in J$). If $J+a$ is a divisor of zero, then it is nilpotent by assumption and there exists $n\in\Z^{+}$ such that $(J+a)^n=J$, i.e. $J+(a^n)=J$. That means $a^n\in J$. Therefore, $J$ is a primary ideal. {\it Sufficiency.} Assume that $J$ is a primary ideal and that $J+a\in A\slash J$, $J+a\neq J$ is a divisor of zero. Then, there exists $J+b\in A\slash J$, $J+b\neq J$ such that $(J+a)(J+b)=J$, which is equivalent to $a b\in J$ (see above). As $A$ is commutative, then so is $A\slash J$, so we have $b a\in J$ As $J$ is a primary ideal then either $b\in J$ (impossible as that would mean $J+b=J$, contrary to our assumption), or there exists $n\in\Z^{+}$ such that $a^n\in J$. Then, $J+a^n=J$, i.e. $(J+a)^n=J$, meaning that $J+a$ is nilpotent.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $J\trianglelefteq A$. Then, $J$ is semiprime if and only if $A\slash J$ has no nilpotent elements except zero.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume that $J$ is semiprime ideal of $A$ and that there exists $J+a\in A\slash J-\{J\}$ and $n\in\Z^{+}$ such that $(J+a)^n=J$. That implies $J+(a^n)=J$, i.e. $a^n\in J$. But, as $J$ is a semiprime ideal, then we have $a\in J$, i.e. $J+a=J$, which is a contradiction to assumption that $J+a\neq J$. Therefore, there are no nilpotent elements in $A\slash J$ except zero (which is $J$). {\it Sufficiency.} Let $A\slash J$ have no nilpotent elements except zero, i.e. for all $J+a\in A\slash J-\{J\}$ there does not exist $n\in\Z^{+}$ such that $(J+a)^n=J$. Now, assume $a^m\in J$, for some $m\in\Z^{+}$. Then, $J+(a^m)=J$, i.e. $(J+a)^m=J$. But that would mean that $J+a$ is nilpotent, but the only nilpotent element is zero so it must be $m=1$, i.e. $J+a=J$, which is equivalent to $a\in J$. Therefore, $J$ is a semiprime ideal of $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} An integral domain can have no nonzero nilpotent elements.

\noindent\newline{\bf Proof.} Let $A$ be an integral domain and $a\in A$, $a\neq 0$. We want to prove that there does not exists $n\in\Z^{+}$ such that $a^n=0$, i.e. that it cannot be that $a^n=0$, for any $n$. We cannot have $a=0$, as we assumed $a$ is non-zero. We also cannot have $a^2=0$ as that is equivalent to $a a=0$. As $A$ is an integral domain, then it has no divisors of zero and it must be that $a=0$ or $a=0$, which is impossible. Now, assume that $a^n=0$ is impossible. We will show that $a^{n+1}=0$ is impossible. We have $a^{n+1}=a a^n=0$. As $A$ is an integral domain, it has no divisors of zero, so either $a=0$ or $a^n=0$. But both is impossible so it cannot be that $a^{n+1}=0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every prime ideal in a commutative ring is semiprime.

\noindent\newline{\bf Proof.} Let $A$ be a commutative ring and $J\primeideal A$. Then, by a previous theorem, $A\slash J$ is an integral domain and it has no nonzero nilpotent elements. By a previous proposition, as $A\slash J$ has no nonzero nilpotent elements, $J$ is semiprime.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Formal construction of integers modulo $n$}
\end{center}

\vskip 0.5cm

\noindent{\bf Remark.} Let $n\in\Z$. Notice that $n\Z=\cyc{n}$ and that, as $\cyc{n}$ is Abelian and a subgroup of $\Z$ it is a normal subgroup of $\Z$.

\noindent\newline{\bf Lemma.} Let $n,m,k\in\Z$. Then, $n\Z+k=n\Z+(n m+k)$.

\noindent\newline{\bf Proof.} If we take $n x+k\in n\Z+k$, then we must find $y\in\Z$ such that $n x+k=n y+(n m+k)$. From that we have $n x=n y+n m$ and $n x-n m=n y$. That gives us $y=(x-m)$. So, $n x+k=n(x-m)+(n m+k)$. As $(x-m)\in\Z$, then $n x+k=n(x-m)+(n m+k)\in n\Z+(n m+k)$ giving us $n\Z+k\subseteq n\Z+(n m+k)$. If we take $n x+(n m+k)\in n\Z+(n m+k)$, we have $n x+(n m+k)=n(x+m)+k$ which is in $n\Z+k$. Therefore, $n\Z+(n m+k)\subseteq n\Z+k$ which finally gives us $n\Z+(n m+k)=n\Z+k$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $n\in\Z^{+}$. Then, $[\Z : n\Z]=n$.

\noindent\newline{\bf Proof.} As $n\Z$ is a normal subgroup of $\Z$ we can define a quotient group $\Z\slash n\Z=\{n\Z+k:\ k\in\Z\}$. Let $\mathcal{Z}=\{n\Z+k:\ k\in\{0,\ldots,n-1\}\}$. Let $k,l\in\Z$, $k\neq l$ and $0\leq k,l<n$. Then there exist $n\Z+k, n\Z+l\in\mathcal{Z}$. Suppose $n\Z+k=n\Z+l$. Then, if we take $n x+k\in n\Z+k$, where $x\in\Z$, there exists $y\in\Z$ such that $n x+k=n y+l$. That gives us $n(x-y)=l-k$. From that we have $n|(l-k)$ and then $l\equiv k\pmod n$. But, as $0\leq l,k<n$ it follows by a proposition that $l=k$. That is contrary to our assumption that $k\neq l$ and it has to be $n\Z+k\neq n\Z+l$ and from that we have $|\mathcal{Z}|=n$. Obviously $\mathcal{Z}\subseteq\Z\slash n\Z$. If we take $n\Z+k\in\Z\slash n\Z$, then we can use division with remainder theorem to obtain $q,r\in\Z$ such that $k=q n+r$, where $0\leq r<|n|=n$. Then we have $n\Z+k=n\Z+(n q+r)$. From that, by a previous lemma, we have $n\Z+k=n\Z+r$, where $0\leq r<n$. That means that $n\Z+r\in\mathcal{Z}$ and we have $\Z\slash n\Z\subseteq\mathcal{Z}$. That implies $\Z\slash n\Z=\mathcal{Z}$, but also $\left|\Z\slash n\Z\right|=\left|\mathcal{Z}\right|=n$. From that we have $[\Z : n\Z]=\left|\Z\slash n\Z\right|=n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $n\in\Z^{+}$. Then, $\Z_n\cong\Z\slash n\Z$.

\noindent\newline{\bf Proof.} Let $f:\Z_n\rightarrow\Z\slash n\Z$ with $f([a]_n)=n\Z+a$. We can see that $f$ is defined for all $[a]_n\in\Z_n$. Now, if $[a]_n=[b]_n$, we have $a\equiv b\pmod n$. From that we have $n|(a-b)$, i.e. there exists $q\in\Z$ such that $a-b=n q$, that is $a=n q+b$. So, $n\Z+a=n\Z+(n q+b)=n\Z+b$ (by a previous lemma). Therefore, $f$ satisfies the property of uniqueness and is well-defined. {\it Injectivity.} Suppose $f([a]_n)=f([b]_n)$. Then, $n\Z+a=n\Z+b$. That means that if we take $n x+a\in n\Z+a$, where $x\in\Z$, there exists $y\in\Z$ such that $n y+b=n x+a$. That gives us $n(y-x)=a-b$, which is equivalent to $n|(a-b)$, i.e. $a\equiv b\pmod n$. But, then $a\sim_n b$ and from that $[a]_n=[b]_n$. {\it Surjectivity.} If we take $n\Z+a\in\Z\slash n\Z$, we can see that, as $a\in\Z$, we can always find $[a]_n\in\Z_n$ so that $f([a]_n)=n\Z+a$. Finally, $f([a]_n+[b]_n)=f([a+b]_n)=n\Z+(a+b)$. By definition of coset addition, $n\Z+(a+b)=[n\Z+a]+[n\Z+b]=f([a]_n)+f([b]_n)$. Therefore, $f$ is an isomorphism from $Z_n$ to $\Z\slash n\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $n\in\Z^{\ast}$ and $a,b\in\Z$. Then, $n\Z+a=n\Z+b$ if and only if $a\equiv b\pmod n$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $n\Z+a=n\Z+b$. Then, if we take $n x+a\in n\Z+a$, there exists $y\in\Z$ such that $n y+b=n x+a$. That is equivalent to $n(y-x)=a-b$ which implies $a\equiv b\pmod n$. {\it Sufficiency.} Let $a\equiv b\pmod n$. That means that there exists $q\in\Z$ such that $a-b=n q$, i.e. $a=n q+b$. Then, $n\Z+a=n\Z+(n q+b)$ and, by a previous lemma, $n\Z+(n q+b)=n\Z+b$.

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Proposition.} Let $a,b\in\Z$. Then, $(n\Z+a)\cdot(n\Z+b)=n\Z+(a b)$ is a well-defined binary operation on $\Z\slash n\Z$.

\noindent\newline{\bf Proof.} Assume $n\Z+a=n\Z+c$ and $n\Z+b=n\Z+d$. Then, by a previous lemma, $a\equiv c\pmod n$ and $b\equiv d\pmod n$. From a property of congruence we have $a b\equiv c d\pmod n$ and that implies, by a previous lemma, $n\Z+(a b)=n\Z+(c d)$. Therefore, binary operation satisfies property of uniqueness (and is well-defined as it is also defined for all $n\Z+a\in\Z\slash n\Z$, by definition).

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Remark.} From now on we will use notation $\overline{a}=n\Z+a$. E. g., from now on we have $\overline{a}+\overline{b}=\overline{a+b}$ and $\overline{a}\overline{b}=\overline{a b}$. Also, from a previous proposition we have $\overline{a}=\overline{b}$ if and only if $a\equiv b\pmod n$.

\noindent\newline{\bf Proposition.} Let $\left(\Z\slash n\Z\right)^{\ast}=\left\{\overline{a}\in\Z\slash n\Z:\ \gcd{\left(a,n\right)}=1\right\}$. Then, $\left(\Z\slash n\Z\right)^{\ast}$ with multiplication as defined in the previous proposition is an Abelian group.

\noindent\newline{\bf Proof.} From a previous proposition we have that multiplication is well-defined on $\Z\slash n\Z$. It carries on uniqueness, but we have to check whether it is closed. Let $\overline{a},\overline{b}\in\left(\Z\slash n\Z\right)^{\ast}$. Then, $\gcd{(a,n)}=1$ and $\gcd{(b,n)}=1$. It is obvious that $\gcd{(a b,n)}=1$ (proof in my work on number theory). From that we have $\overline{a b}\in\left(\Z\slash n\Z\right)^{\ast}$. {\it Associativity.} We have $\overline{a}\left(\overline{b}\overline{c}\right)=\overline{a}\overline{b c}=\overline{a(b c)}=\overline{(a b)c}=\overline{a b}\overline{c}=\left(\overline{a}\overline{b}\right)\overline{c}$. {\it Neutral element.} As $\gcd{1,n}=1$, then $\overline{1}\in\left(\Z\slash n\Z\right)^{\ast}$. Furthermore, $\overline{1}\overline{a}=\overline{1\cdot a}=\overline{a}=\overline{a\cdot 1}=\overline{a}\overline{1}$. {\it Inverse element.} From a proposition in my number theory script there exists $b\in\Z$ such that $a b\equiv 1\pmod n$ if and only if $\gcd{(a,n)}=1$. From that $\overline{b}\in\left(\Z\slash n\Z\right)^{\ast}$ and $\overline{b}\overline{a}=\overline{a}\overline{b}=\overline{1}$. {\it Commutativity.} Let $\overline{a},\overline{b}\in\left(\Z\slash n\Z\right)^{\ast}$. Then, $\overline{a}\overline{b}=\overline{a b}=\overline{b a}=\overline{b}\overline{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $\overline{a}\in\left(\Z\slash n\Z\right)^{\ast}$ and $k\in\Z^{+}_0$. Then, $\left(\overline{a}\right)^k=\overline{a^k}$.

\noindent\newline{\bf Proof.} Let $n=1$. Then, $\left(\overline{a}\right)^1=\overline{a}$. Assume that $\left(\overline{a}\right)^n=\overline{a^n}$. Then, $\left(\overline{a}\right)^{n+1}=\left(\overline{a}\right)^n\left(\overline{a}\right)$. By assumption, we have $\left(\overline{a}\right)^{n}\left(\overline{a}\right)=\overline{a^n}\overline{a}$. By definition, $\overline{a^n}\overline{a}=\overline{a^{n} a}=\overline{a^{n+1}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} It is obvious that $\left|\left(\Z\slash n\Z\right)^{\ast}\right|=\varphi(n)$, where $\varphi$ is Euler's totient function. From that we have $\left|\left(\Z\slash p\Z\right)^{\ast}\right|=p-1$. Also, remember that, if $G$ is a group and $a\in G$, then $a^{|G|}=e$. We shall remind ourselves of that through the following lemma, as it will be of great importance to prove next two great theorems.

\noindent\newline{\bf Lemma.} Let $G$ be a group and $a\in G$. Then, $a^{|G|}=e$.

\noindent\newline{\bf Proof.} By a corollary of Lagrange's theorem, we have that $\ord{a}$ divides order of $G$. Therefore, $|G|=k\ord{a}$, for some $k\in\N$. Then,

\begin{equation*}
a^{|G|}=a^{k\ord{a}}=\left(a^{\ord{a}}\right)^k=e^k=e.
\end{equation*}

\begin{flushright}
$\square$
\end{flushright}

\noindent{\bf Euler's theorem.} Let $a\in\Z$ and $m\in\N$ where $\gcd{(a,m)}=1$. Then,

\begin{equation*}
a^{\varphi(m)}\equiv 1\pmod m.
\end{equation*}

\noindent\newline{\bf Proof.} As $\left(\Z\slash m\Z\right)^{\ast}$ contains all $\overline{a}$ such that $\gcd{(a,m)}=1$, then, by definition of Euler's totient function, it follows that $\left|\left(\Z\slash m\Z\right)^{\ast}\right|=\varphi(m)$. From a previous lemma, for all $\overline{a}\in\left(\Z\slash m\Z\right)^{\ast}$, we have (remember that neutral element in $\left(\Z\slash m\Z\right)^{\ast}$ is $\overline{1}$):

\begin{equation*}
\left(\overline{a}\right)^{\varphi(m)}=\overline{1}.
\end{equation*}

\noindent\newline That implies, using a previous proposition:

\begin{equation*}
\left(\overline{a}\right)^{\varphi(m)}=\overline{a^{\varphi(m)}}=\overline{1}.
\end{equation*}

\noindent\newline Finally, by a previous proposition, $\overline{a}=\overline{b}$ if and only if $a\equiv b\pmod m$, so from $\overline{a^{\varphi(m)}}=\overline{1}$ we get $a^{\varphi(m)}\equiv 1\pmod m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Fermat's little theorem.} For all $a\in\Z$, $m\in\N$ and $p\in P$, $a^{p-1}\equiv 1\pmod m$.

\noindent\newline{\bf Proof.} It follows directly from Euler's theorem and from $\varphi(p)=p-1$ that $a^{\varphi(p)}\equiv 1\pmod m$, i.e. $a^{p-1}\equiv 1\pmod m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $m,n\in\Z^{+}$ such that $\gcd{(m,n)}=1$. Then,

\begin{equation*}
\zmod{(m n)}\cong\zmod{m}\times\zmod{n}.
\end{equation*}

\noindent\newline{\bf Proof.} Let mapping $f:\Z\rightarrow\Z\slash m\Z\times\Z\slash n\Z$ be defined with $f(x)=(m\Z+x,n\Z+x)$. First, we will show that $f$ is a well-defined function. If we take $x\in\Z$, then there exist $m\Z+x\in\zmod{m}$ and $n\Z+x\in\zmod{n}$. Then, there also exists $(m\Z+x,n\Z+y)\in\zmod{m}\times\zmod{n}$ and we have $f(x)=(m\Z+x,n\Z+y)$. Now, assume $x=y$. We then have that $m\Z+x=m\Z+y$ and $n\Z+x=n\Z+y$. From definition of ordered pair, that is equivalent to $(m\Z+x,n\Z+x)=(m\Z+y,n\Z+y)$, i.e. $f(x)=f(y)$. Therefore, $f$ is well-defined. Now, using the fact that coset addition and multiplication in $\zmod{m}$ and $\zmod{n}$ are well-defined, and using the definition of direct product of groups, we have:

\begin{eqnarray*}
f(x+y)&=&\left(m\Z+(x+y),n\Z+(x+y)\right)\\
&=&\left((m\Z+x)+(m\Z+y),(n\Z+x)+(n\Z+y)\right)\\
&=&(m\Z+x,n\Z+x)+(m\Z+y,n\Z+y)=f(x)+f(y).
\end{eqnarray*}

\noindent\newline We also have that:

\begin{eqnarray*}
f(x y)&=&\left(m\Z+(x y),n\Z+(x y)\right)\\
&=&\left((m\Z+x)(m\Z+y),(n\Z+x)(n\Z+y)\right)\\
&=&(m\Z+x,n\Z+x)(m\Z+y,n\Z+y)=f(x)f(y).
\end{eqnarray*}

\noindent\newline That implies that $f$ is a homomorphism. Let us remind ourselves that zero in $\zmod{m}$ is $m\Z+0=m\Z$, as $(m\Z+0)+(m\Z+x)=m\Z+(0+x)=m\Z+x$ (commutativity provides the other condition). So, the zero in $\zmod{m}\times\zmod{n}$ is $\left(m\Z,n\Z\right)$. Let us observe the kernel of $f$:

\begin{equation*}
\ker{f}=\{x\in\Z:\ f(x)=(m\Z,n\Z)\}=\{x\in\Z:\ (m\Z+x,n\Z+x)=(m\Z,n\Z)\}.
\end{equation*}

\noindent\newline Notice that if $m\Z+x=m\Z$, then $x\in m\Z$. Also, if $n\Z+x=n\Z$, then $x\in n\Z$. That actually means that if $x\in m\Z$ and $x\in n\Z$, then $x=m z_1$ and $x=n z_2$, for some $z_1,z_2\in\Z$. In other words $m|x$ and $n|x$. That tells us that $x$ is a common multiple of $m$ and $n$. Therefore $x=k\lcm{m,n}$, for some $k\in\Z$. So, we may write:

\begin{eqnarray*}
\ker{f}&=&\{x\in\Z:\ (\exists k\in\Z)(x=k\lcm{m,n})\}\\
&=&\{k\lcm{m,n}\in\Z:\ k\in\Z\}=\lcm{m,n}\Z.
\end{eqnarray*}

\noindent\newline As $\gcd{(m,n)}=1$, then $\lcm{m,n}=m n$ and we have $\ker{f}=(m n)\Z$. To use FHT in the way we want, we need to prove that $\ran{f}=\zmod{m}\times\zmod{n}$. If we take $(m\Z+y,n\Z+y)\in\zmod{m}\times\zmod{n}$, does there exist $x\in\Z$ such that $f(x)=(m\Z+y,n\Z+y)$? That is equivalent to $(m\Z+x,n\Z+x)=(m\Z+y,n\Z+y)$. Let us find the value of $x\in\Z$. From definition of ordered pair equality, we have $m\Z+x=m\Z+y$ and $n\Z+x=n\Z+y$. As $m\in\Z$, then $n\Z+m\in\zmod{n}$. Also, as $n\in\Z$, then $m\Z+n\in\zmod{m}$. As $\gcd{(m,n)}=1$, $n\Z+m$ has an inverse $(n\Z+m)^{-1}\in\zmodinv{n}$. Similarly, $m\Z+n$ has an inverse $(m\Z+n)^{-1}\in\zmodinv{m}$. Then there exist $m',n'\in\Z$ such that $(m\Z+n)^{-1}=m\Z+n'$ and $(n\Z+m)^{-1}=n\Z+m'$. As $m,m',n,n'\in\Z$ and $y\in\Z$, then $m m'y\in\Z$ and $n n'y\in\Z$ but also $m m'y+n n'y\in\Z$. So, there exist $n\Z+(m m'y+n n'y)\in\zmod{n}$ and $m\Z+(m m'y+n n'y)\in\zmod{m}$. Keep in mind that $n\Z+n t=n\Z$ (because $n t\in n\Z$), for all $t\in\Z$ and that $(n\Z+m)(n\Z+m')=(n\Z+m)(n\Z+m)^{-1}=n\Z+1$. We have:

\begin{eqnarray*}
n\Z+(m m'y+n n'y)&=&(n\Z+(m m')y)+(n\Z+n(n'y))\\
&=&(n\Z+(m m'))(n\Z+y)+n\Z\\
&=&(n\Z+m)(n\Z+m')(n\Z+y)+n\Z\\
&=&(n\Z+1)(n\Z+y)+n\Z\\
&=&(n\Z+y)+n\Z=n\Z+y.
\end{eqnarray*}

\noindent\newline On the other hand,

\begin{eqnarray*}
m\Z+(m m'y+n n'y)&=&(m\Z+m(m'y))+(m\Z+(n n')y)\\
&=&m\Z+(m\Z+(n n'))(m\Z+y)\\
&=&m\Z+(m\Z+n)(m\Z+n')(m\Z+y)\\
&=&m\Z+(m\Z+1)(m\Z+y)\\
&=&m\Z+(m\Z+y)=m\Z+y.
\end{eqnarray*}

\noindent We see that if we take $x=m m'y+n n'y$ (where $n'$ and $m'$ are obtained as inverses in $\zmodinv{m}$ and $\zmodinv{n}$, respectively) we have that $n\Z+x=n\Z+y$ and $m\Z+x=m\Z+y$, i.e. $(m\Z+x,n\Z+x)=(m\Z+y,n\Z+y)$ or more clearly $f(x)=(m\Z+y,n\Z+y)$. Therefore, $f$ is surjective, i.e. $\ran{f}=\zmod{m}\times\zmod{n}$. Then, by fundamental homomorphism theorem, we have $\zmod{(m n)}\cong\zmod{m}\times\zmod{n}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $m,n\in\N$ and $\gcd{(m,n)}=1$. Then,

\begin{equation*}
\left(\zmod{m}\times\zmod{n}\right)^{\ast}=\zmodinv{m}\times\zmodinv{n}.
\end{equation*}

\noindent\newline{\bf Proof.} Follows directly from previous theorem and a proposition from previous chapter.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $m,n\in\Z^{+}$ and $\gcd{(m,n)}=1$. Then, $\varphi(m n)=\varphi(m)\varphi(n)$.

\noindent\newline{\bf Proof.} Due to the fact that $\left(\zmod{m}\times\zmod{n}\right)^{\ast}=\zmodinv{m}\times\zmodinv{n}$, as follows from the previous corollary, then also:

\begin{equation*}
\left|\left(\zmod{m}\times\zmod{n}\right)^{\ast}\right|=\left|\zmodinv{m}\times\zmodinv{n}\right|.
\end{equation*}

\noindent\newline Furthermore, as $\gcd{(m,n)}=1$, by the previous theorem we have:

\begin{equation*}
\zmod{(m n)}\cong\zmod{m}\times\zmod{n}.
\end{equation*}

\noindent\newline A previous proposition implies

\begin{equation*}
\zmodinv{(m n)}\cong\left(\zmod{m}\times\zmod{n}\right)^{\ast}.
\end{equation*}

\noindent\newline Then it follows that

\begin{equation*}
\left|\zmodinv{(m n)}\right|=\left|\left(\zmod{m}\times\zmod{n}\right)^{\ast}\right|.
\end{equation*}

\noindent\newline So, we have:

\begin{equation*}
\left|\zmodinv{(m n)}\right|=\left|\zmodinv{m}\times\zmodinv{n}\right|=\left|\zmodinv{m}\right|\cdot\left|\zmodinv{n}\right|.
\end{equation*}

\noindent\newline Finally, by a previous proposition,

\begin{equation*}
\varphi(m n)=\left|\zmodinv{(m n)}\right|=\left|\zmodinv{m}\right|\cdot\left|\zmodinv{n}\right|=\varphi(m)\varphi(n).
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $m_1,m_2,\ldots,m_k\in\Z^{+}$ such that $\gcd{(m_i,m_j)}=1$, for all $i\neq j$, $i,j\in\{1,\ldots,k\}$, for some $k\in\Z^{+}-\{1\}$. Then,

\begin{equation*}
\zmod{m_1 m_2\cdots m_k}\cong\zmod{m_1}\times\zmod{m_2}\times\cdots\times\zmod{m_k}.
\end{equation*}

\noindent\newline{\bf Proof.} In the theorem we have already proved the case when $k=2$. For $k=1$ it is trivial. Assume that the statement is true for some $k\in\Z$. Then, we need to prove that the assumption is true for $k+1$. Using a proposition in direct product chapter which statest that if $G$, $K$ and $H$ are groups, then $G\cong K$ implies $G\times H\cong G\times K$, and by taking:

\begin{eqnarray*}
G&=&\zmod{m_1 m_2\cdots m_k},\\
K&=&\zmod{m_1}\times\cdots\times\zmod{m_k},\\
H&=&\zmod{m_{k+1}},
\end{eqnarray*}

\noindent\newline we get (notice that $G\cong K$ is true because it is the statement of the assumption of induction):

\begin{equation*}
\zmod{m_1\cdots m_k}\times\zmod{m_{k+1}}\cong\left(\zmod{m_1}\times\cdots\times\zmod{m_k}\right)\times\zmod{m_{k+1}}.
\end{equation*}

\noindent\newline Due to associativity of group operation, the direct product is also associative and the brackets can be dropped on the right-hand side. Now, observing the left-hand side, as $\gcd{(m_{k+1},m_i)}=1$, for all $i\in\{1,\ldots,k\}$, then $\gcd{(m_1\cdots m_k,m_{k+1})}=1$. So, by previous theorem we have:

\begin{equation*}
\zmod{m_1 m_2\cdots m_k m_{k+1}}\cong\zmod{m_1\cdots m_k}\times\zmod{m_{k+1}},
\end{equation*}

\noindent\newline Now, due to the transitivity of isomorphism, from that, and the previous expression, we simply obtain:

\begin{equation*}
\zmod{m_1 m_2\cdots m_k m_{k+1}}\cong\zmod{m_1}\times\cdots\times\zmod{m_k}\times\zmod{m_{k+1}}.
\end{equation*}

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $m\in\Z^{+}$. Then, by fundamental theorem of arithmetic, $m=p_1^{t_1}\cdots p_k^{t^k}$, for some distinct primes $p_1,\ldots,p_k\in P$, integers $t_1,\ldots,t_k,k\in\Z^{+}$ and:

\begin{equation*}
\zmod{m}\cong\zmod{\left(p_1^{t_1}\right)}\times\cdots\times\zmod{\left(p_k^{t_k}\right)}.
\end{equation*}

\noindent\newline{\bf Proof.} Due to the fact that $p_i$ are all distinct primes, which further implies that $\gcd{(p_i^{t_i},p_j^{t_j})}=1$, the previos corollary can be directly applied and the expression easily obtained.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Basically, the previos theorem and it's corollaries are the {\bf Chinese remainder theorem}. It actually states that if:

\begin{equation*}
(a_1,\ldots,a_k)\in\zmod{m_1}\times\cdots\zmod{m_k},
\end{equation*}

\noindent\newline then there exists $x_0\in\zmod{m_1\cdots m_k}$ such that $f(x_0)=(a_1,\ldots,a_k)$, where $f$ is an isomorphism defined by $f(x)=(m_1\Z+x,\ldots,m_k\Z+x)$. So, we would have $a_i=m_i\Z+x_0$, i.e. $a_i-x_0\in m_i\Z$, which is then equivalent to saying $x_0\equiv a_i\pmod{m_i}$, for all $i\in\{1,\ldots,k\}$. In other words, $x_0$ is the solution (unique modulo $m_1\cdots m_k$) to the system of congruences $x\equiv a_1\pmod{m_1},\ldots,x\equiv a_k\pmod{m_k}$.

\noindent\newline{\bf Proposition.} Let $m,n\in\N$. If $m|n$, then $\Z\slash m\Z$ is a homomorphic image of $\Z\slash n\Z$.

\noindent\newline{\bf Proof.} Let $m|n$, i.e. $n=m q$, for some $q\in\N$. Let $f:\Z\slash n\Z\rightarrow\Z\slash m\Z$ with $f(n\Z+x)=m\Z+x$. First, if $n\Z+x=n\Z+y$, then, $x\equiv y\pmod n$. But, $n=m q$, so $x\equiv y\pmod m q$. From that we have $x-y=m q q'$, where $q'\in\Z$. That is, $x-y=m(q q')$, so $m|x-y$ and $x\equiv y\pmod m$. Therefore, $m\Z+x=m\Z+y$, i.e. $f(n\Z+x)=f(n\Z+y)$. Also, $f$ is defined for all $n\Z+x$, as that implies $x\in\Z$ and that implies that $m\Z+x\in\Z\slash m\Z$. Now, $f((n\Z+x_1)+(n\Z+x_2))=f(n\Z+(x_1+x_2))=m\Z+(x_1+x_2)=(m\Z+x_1)+(m\Z+x_2)=f(n\Z+x_1)+f(n\Z+x_2)$. Also, $f((n\Z+x_1)(n\Z+x_2))=f(n\Z+x_1 x_2)=m\Z+x_1 x_2=(m\Z+x_1)(m\Z+x_2)=f(n\Z+x_1)f(n\Z+x_2)$, for all $x_1,x_2\in\Z$. Therefore, $f$ is a homomorphism from $\Z\slash n\Z$ onto $\Z\slash m\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m,n\in\N$. If $m|n$, then $n\Z\trianglelefteq m\Z\trianglelefteq\Z$.

\noindent\newline{\bf Proof.} As $n\Z=\{n k:\ k\in\Z\}$, then $n\Z\subseteq\Z$. If $n k, n l\in n\Z$, then $n k-n l=n(k-l)\in n\Z$. Also $(n k)(n l)=n(k n l)\in n\Z$. If $z\in\Z$, then $z(n k)=(n k)z=n(k z)\in n\Z$. Thus, $n\Z\trianglelefteq\Z$, but also $m\Z\trianglelefteq\Z$, due to same reasons. As $m|n$, then $n=m q$, for some $q\in\Z$. Thus, $n\Z=(m q)\Z$. If we take $n k\in n\Z$, then $n k=m q k=m(q k)\in m\Z$. Therefore, $n\Z\subseteq m\Z$. As both $n\Z$ and $m\Z$ are rings, then $n\Z\leq m\Z$. If we take $m k\in m\Z$ and $n l\in n\Z$, then, $(n l)(m k)=(m k)(n l)=n(l m k)\in n\Z$. Therefore, $n\Z\trianglelefteq m\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Note that, due to the third isomorphism theorem ($(G\slash H)\slash (K\slash H)\cong G\slash K$), we can prove that $\Z\slash m\Z$ is a homomorphic image of $\Z\slash n\Z$ from previous proposition. We have $(\Z\slash m\Z)\slash(m\Z\slash n\Z)\cong\Z\slash n\Z$. From corollary of FHT we have that there exists a homomorphism $f:\homo{\zmod{m}}{\zmod{n}}{m\Z\slash n\Z}$.

\noindent\newline{\bf Proposition.} Let $n\in\Z^{+}-2\Z$. Then there exists an injective homomorphism from $\zmod{n}$ to $\zmod{2n}$.

\noindent\newline{\bf Proof.} We will define $f:\zmod{n}\rightarrow\zmod{2n}$ with $f\left(n\Z+x\right)=2n\Z+\left((n+1)x\right)$. First we will prove that $f$ is a function. If we take $n\Z+x\in\Z\slash n\Z$, then we have that $x\in\Z$, but also $(n+1)x\in\Z$. So, there exists $2n\Z+((n+1)x)\in\Z\slash 2n\Z$ and we have $f\left(n\Z+x\right)=2n\Z+((n+1)x)$. Next, $n\Z+x=n\Z+y$ is equivalent to $x\equiv y\pmod n$, i.e. there exists $q\in\Z$ such that $x-y=n q$, that is $x=n q+y$. Now, for all $z\in\Z$ we have $2n z+(n+1)x\in n\Z+(n+1)x$. Then:

\begin{eqnarray*}
2n z+(n+1)x&=&2n z+(n+1)(n q+y)\\
&=&2n z+n^2 q+n y+n q+y\\
&=&n(2 z+n q+q)+n y+y\\
&=&n(2 z+q(n+1))+(n+1)y.
\end{eqnarray*}

\noindent\newline Important observation here is that, as $n$ is odd, then $n+1$ is even, i.e. there exists $s\in\Z$ such that $n+1=2s$. So,

\begin{equation*}
2n z+(n+1)x=n(2z+2q s)+(n+1)y=2n(z+q s)+(n+1)y.
\end{equation*}

\noindent\newline Take $z'=z+q s$. Obviously $z'\in\Z$ and we have that for all $z\in\Z$ there exists $z'\in\Z$ such that $2n z+(n+1)x=2n z'+(n+1)y$, meaning $2n\Z+(n+1)x\subseteq 2n\Z+(n+1)y$. Now, for all $z\in\Z$ we have $2n z+(n+1)y\in n\Z+(n+1)x$. From $x-y=n q$ we get $y=x-n q$, i.e. $y=x+n(-q)$. Thus, we have:

\begin{eqnarray*}
2n z+(n+1)y&=&2n z+(n+1)(x+n(-q))\\
&=&2n z+n^2(-q)+n x+n(-q)+x\\
&=&n(2 z+n(-q)+(-q))+n x+x\\
&=&n(2 z+(-q)(n+1))+(n+1)x.
\end{eqnarray*}

\noindent\newline That again implies, as $2|n+1$ that

\begin{equation*}
2n z+(n+1)y=2n z'+(n+1)x,
\end{equation*}

\noindent\newline where $z'=z-q\frac{n+1}{2}$. Thus, we reach that $2n\Z+(n+1)y\subseteq 2n\Z+(n+1)x$. Combining this result with the former one, we get $2n\Z+(n+1)x=2n\Z+(n+1)y$. Therefore, from $n\Z+x=n\Z+y$ we get $2n\Z+(n+1)x=2n\Z+(n+1)y$, i.e. $n\Z+x=n\Z+y$ implies $f(n\Z+x)=f(n\Z+y)$. Therefore, $f$ is well-defined. We will prove only injectivity now, as surjectivity obviously does not hold (see motivational example). Assume $f(n\Z+x)=f(n\Z+y)$. By definition of $f$ that is equivalent to $2n\Z+(n+1)x=2n\Z+(n+1)y$. That is in turn equivalent to $(n+1)x\equiv(n+1)y\pmod 2n$. As $n$ is odd, $n+1$ is even, i.e. $n+1=2m$, for some $m\in\Z$. We then have $2m x\equiv 2m y\pmod 2n$. Then there exists $q\in\Z$ such that $2m x-2 m y=2 n q$. That gives us $m x-m y=n q$, i.e. $m(x-y)=n q$. Notice that from $n+1=2m$ we have that $m|n+1$. If $m=1$, then we have $x-y=n q$, i.e. $x\equiv y\pmod n$. For $m\neq 1$, assume that $\gcd{(n,m)}\neq 1$. Then there exists $g\in\Z$, $g\neq\pm1$, such that $g|n$ and $g|m$, i.e. $n=n'g$ and $m=m'g$. Then, $n+1=2m$ implies $n'g+1=2m'g$. From that we have $1=g(2m'-n')$. But, then $g|1$ and it can only be that $g=\pm 1$, contrary to our assumption. Therefore, $|g|=\gcd{(m,n)}=1$. Then, $m(x-y)=n q$ implies, by Euclid's lemma, that $n|(x-y)$, i.e. $x\equiv y\pmod n$ which is equivalent to $n\Z+x=n\Z+y$. As $f(n\Z+x)=f(n\Z+y)$ implied $n\Z+x=n\Z+y$, we conclude that $f$ is injective. Finally we will show that $f$ is a homomorphism. Somewhat trivial for addition, we have:

\begin{eqnarray*}
f\left((n\Z+x)+(n\Z+y)\right)&=&f\left(n\Z+(x+y)\right)\\
&=&2n\Z+\left((n+1)(x+y)\right)=2n\Z+\left((n+1)x+(n+1)y\right)\\
&=&\left(2n\Z+(n+1)x\right)+\left(2n\Z+(n+1)y\right)\\
&=&f\left(n\Z+x\right)+f\left(n\Z+y\right).
\end{eqnarray*}

\noindent\newline To prove that $f$ is homomorphism with regard to multiplication, we will need to use the fact\footnote{Proved in my works on number theory.} that, if $n$ is odd, then $(n+1)^2\equiv(n+1)\pmod 2n$. We have:

\begin{equation*}
f\left((n\Z+x)(n\Z+y)\right)=f\left(n\Z+(x y)\right)=2n\Z+\left((n+1)(x y)\right).
\end{equation*}

\noindent\newline As $(n+1)\in\Z$, and obviously $x y\in\Z$, we have:

\begin{equation*}
2n\Z+\left((n+1)(x y)\right)=\left(2n\Z+(n+1)\right)\left(2n\Z+x y\right).
\end{equation*}

\noindent\newline As $(n+1)^2\equiv(n+1)\pmod 2n$, for odd $n$, then $2n\Z+(n+1)=2n\Z+\left((n+1)^2\right)$. Therefore,

\begin{eqnarray*}
f\left((n\Z+x)(n\Z+y)\right)&=&\left(2n\Z+(n+1)\right)\left(2n\Z+x y\right)\\
&=&\left(2n\Z+(n+1)^2\right)\left(2n\Z+x y\right)\\
&=&2n\Z+\left((n+1)^2(x y)\right)\\
&=&2n\Z+\left((n+1)x\cdot(n+1)y\right)\\
&=&\left(2n\Z+\left((n+1)x\right)\right)\left(2n\Z+\left((n+1)y\right)\right)\\
&=&f\left(n\Z+x\right)f\left(n\Z+y\right).
\end{eqnarray*}

\noindent\newline Therefore, we have proved that $f$ is an injective homomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $a_i\in\Z$, for $i\in\{0,\ldots,n\}$ and $x\in\Z$. Then, if $p=a_n x^n+\cdots+a_1 x+a_0=0$ and $f:\hom{\Z}{\zmod{m}}{m\Z}$ is the projective homomorphism, i.e. defined with $f(x)=m\Z+x$, then $f(p)=f(0)$ implies $f(p)=0$ (as $f(0)=0$). Also, $f(p)=m\Z+(a_n x^n+\cdots+a_1 x+a_0)=[m\Z+(a_n x^n)]+\cdots+[m\Z+(a_1 x)]+[m\Z+a_0]=[m\Z+a_n]\cdot[m\Z+x]^n+\cdots+[m\Z+a_1]\cdot[m\Z+x]+[m\Z+a_0]=m\Z$. Therefore, if a polynomial equation is satisfied in $\Z$ it must be satisfied in $\zmod{m}$. In a much better notation that can be written as $\overline{a_n}\cdot\overline{x}^n+\cdots+\overline{a_1}\cdot\overline{x}+\overline{a_0}=\overline{0}$ if there is no confusion around $m$. For an equation to have solutions in $\Z$ it is therefore necessary for it to have solutions in $\zmod{m}$.

\noindent\newline{\bf Problem.} Prove that the equation $x^2-7y^2-24=0$ has no integer solutions.

\noindent\newline{\bf Solution.} Let us observe that equation in $\zmod{7}$. We have $\overline{x^2}-\overline{7}\overline{y}^2-\overline{24}=\overline{0}$. Reducing representatives modulo $7$, and as $\overline{7}=\overline{0}$, we have $\overline{x}^2-\overline{0}\cdot\overline{y}^2-\overline{3}=\overline{0}$. Then, as $\overline{0}\cdot\overline{y}^2=\overline{0\cdot y^2}=\overline{0}$, and as $\overline{x^2}-\overline{0}=\overline{x^2-0}=\overline{x^2}$, we have $\overline{x}^2-\overline{3}=\overline{0}$. We can add to that equation $\overline{3}$ and obtain $\overline{x}^2=\overline{3}$. Now, obviously $\overline{0}^2=\overline{0}$ and $\overline{1}^2=\overline{1^2}=\overline{1}=\overline{36}=\overline{6}$ so they cannot be solutions. Neither can be $\overline{2}$ as $\overline{2}^2=\overline{4}=\overline{25}=\overline{5}^2$. Furthermore, $\overline{3}^2=\overline{9}=\overline{2}=\overline{16}=\overline{4}^2$. Thus, we have exhausted all elements of $\zmod{7}$ and there are no solutions, so there cannot be any in $\Z$.

\noindent\newline{\bf Remark.} Notice this easy but useful fact. Let $m\in\Z^{\ast}$. Then, $\overline{a}^{2 n}=\overline{m-a}^{2 n}$, for all $\overline{a}\in\zmod{m}$ and $n\in\N$. This is true because $\overline{-a}=\overline{m-a}$, i.e. $-\overline{a}=\overline{m-a}$. Therefore, multiplying that equality by itself even number of times gives us $(-1)^{2n}\overline{a}^{2n}=\overline{m-a}^{2n}$. Therefore, we will only need to check first $\left\lceil\frac{m}{2}\right\rceil+1$ squares.

\noindent\newline{\bf Problem.} Prove that $x^2+(x+1)^2+(x+2)^2=y^2$ has no integer solutions.

\noindent\newline{\bf Solution.} We have $x^2+(x+1)^2+(x+2)^2=x^2+x^2+2x+1+x^2+4x+4=3x^2+6x+5$. So, we must prove that $3x^2+6x+5=y^2$ has no integer solutions. Luckily for us, $\overline{3}=\overline{6}=\overline{0}$ in $\zmod{6}$, so we will apply exactly that homomorphism. We then have $\overline{3}\overline{x}^2+\overline{6}\overline{x}+\overline{5}=\overline{y}^2$. Then, as $\overline{3}=\overline{6}=\overline{0}$ we have $\overline{5}=\overline{y}^2$. Now, we only need to check from $\overline{0}$ to $\overline{3}$. We have $\overline{0}^2=\overline{6}^2=\overline{0}$, $\overline{1}^2=\overline{5}^2=\overline{1}$, $\overline{2}^2=\overline{4}^2=\overline{4}$ and $\overline{3}^2=\overline{9}=\overline{3}$. As no square in $\zmod{6}$ equals $\overline{5}$, then there are no solutions in $\zmod{6}$ and also in $\Z$.

\noindent\newline{\bf Problem.} Let $n\in\Z$. Prove that $x^2+10y^2=n$ has no integer solutions if the last digit of $n$ is $2$, $3$, $7$, or $8$.

\noindent\newline{\bf Solution.} We will not only get the last digit of $n$ in $\zmod{10}$ but also remove $10y^2$ and get $\overline{x}^2=\overline{n}$. All the squares of integers from $0$ to $9$ have $0$ ($0^2$), $1$ ($1^2$ or $9^2$), $4$ ($8^2$ or $2^2$), $5$ ($5^2$), $6$ ($4^2$ and $6^2$) or $9$ ($7^2$ or $3^2$) for their last digits. Therefore, if $n\in\{2,3,7,8\}$ there cannot be a solution in $\zmod{10}$ and also not in $\Z$.

\noindent\newline{\bf Problem.} Prove that the sequence $3,8,13,18,23,\ldots$ does not include the square of any integer.

\noindent\newline{\bf Solution.} We can see that the difference between two neighbour members is $5$ and that this is an arithmetic progression. Therefore, the formula is $a_n=5 n+3$, for $n\in\Z^{+}_0$. So, we must show that $x^2\neq 5n+3$, for any $n\in\Z^{+}_0$. Let us assume that it exists. Then, $x^2=5 n+3$. In $\zmod{5}$ we have $\overline{x}^2=\overline{3}$. We have $\overline{0}^2=\overline{5}^2=\overline{0}$, $\overline{1}^2=\overline{4}^2=\overline{1}$, $\overline{2}^2=\overline{3}^2=\overline{4}$. Therefore, there does not exist the square in $\zmod{5}$ that equals $\overline{3}$ and so it cannot exist in $\Z$. Thus, the sequence above does not include the square of any integer.

\noindent\newline{\bf Problem.} Prove that the sequence $2,10,18,26,\ldots$ does not include the cube of any integer.

\noindent\newline{\bf Solution.} We have the arithmetic sequence $a_n=8n+2$, where $n\in\Z^{+}_0$. We observe $x^3=8n+2$. In $\zmod{4}$, we have $\overline{x}^3=\overline{2}$. It cannot be $\overline{0}$, or $\overline{1}$. Now, $\overline{2}^3=\overline{8}=\overline{0}$ and $\overline{3}^3=\overline{27}=\overline{3}$. Therefore, there do not exist integers such that $x^3=8n+2$.

\noindent\newline{\bf Problem.} Prove that the sequence $3,11,19,27,...$ does not include the sum of two squares of integers.

\noindent\newline{\bf Solution.} We have $a_n=8n+3$, for $n\in\Z^{+}_0$. Then, let $x,y\in\Z$. Assume $x^2+y^2=3n+8$. Let us observe this in $\zmod{4}$. We have $\overline{x}^2+\overline{y}^2=\overline{3}$. We will observe ordered pairs $(\overline{x},\overline{y})$. Obviously it cannot be for $x,y\in\{0,1\}$. Now, take $x=2$. We have $\overline{4}+\overline{y}^2=\overline{3}$, i.e. $\overline{y}^2=\overline{3}$. It is obvious that it cannot be in $\zmod{4}$ (which contains only squares $\overline{0}$ and $\overline{1}$). If $x=3$, then, $\overline{1}+\overline{y}^2=\overline{3}$. That is equivalent to $\overline{y}^2=\overline{2}$. Thus we have exhausted all possibilities and conclude that there do not exist integers such that $x^2+y^2=8n+3$.

\noindent\newline{\bf Problem.} Prove that if $n$ is a product of two consecutive integers, its units digit must be $0$, $2$ or $6$.

\noindent\newline{\bf Solution.} We have $n=x(x+1)$, i.e. $n=x^2+x$. In $\zmod{10}$ we observe that $\overline{0}^2+\overline{0}=\overline{9}^2+\overline{9}=\overline{4}^2+\overline{4}=\overline{0}=\overline{5}^2+\overline{5}$. Then, $\overline{1}^2+\overline{1}=\overline{3}^2+\overline{3}=\overline{6}^2+\overline{6}=\overline{8}^2+\overline{8}=\overline{2}$, $\overline{7}^2+\overline{7}=\overline{6}$.

\noindent\newline{\bf Problem.} Prove that if $n$ is the product of three consecutive integers, its units digit must be $0$, $4$ or $6$.

\noindent\newline{\bf Solution.} We have $n=x(x+1)(x+2)$. In $\zmod{10}$, $\overline{0\cdot 1\cdot 2}=\overline{3\cdot 4\cdot 5}=\overline{4\cdot 5\cdot 6}=\overline{5\cdot 6\cdot 7}=\overline{0}$, $\overline{1\cdot 2\cdot 3}=\overline{6\cdot 7\cdot 8}=\overline{6}$, $\overline{2\cdot 3\cdot 4}=\overline{7\cdot 8\cdot 9}=\overline{4}$. The following three contain $10$ which is $0$ modulo $10$, so their product is zero. That exhausts all the cases.

\newpage

\begin{center}
{\bf Integral domains}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be a ring. If there exists $n\in\Z^{+}$ such that $n\cdot a=0$, and if $m<n$, then $m\cdot a=0$ implies $m=0$, for all $m\in\Z^{+}_0$, then we say that $n$ is the {\bf additive order}\footnote{This is analogous to the order of group elements in group theory, so all results here follow for additive groups.} of $a$. If $1\in A$, the additive order $n$ of $1$ is called {\bf characteristic} of $A$ (or we say that ring $A$ has characteristic $n$) and we write $\rchar{A}=n$; if there is no $n\in\Z^{+}$ such that $n\cdot 1=0$ then $A$ has characteristic $0$ and we write $\rchar{A}=0$.

\noindent\newline{\bf Theorem.} All the nonzero elements in an integral domain have the additive order equal to its characteristic.

\noindent\newline{\bf Proof.} Let $A$ be an integral domain and $a\in A$, where $a\neq 0$. We have $a=a 1$ and $1\cdot n=\underbrace{1+1+\cdots+1}_{n\textnormal{ times}}$. Now, $n\cdot a=\underbrace{a 1+a 1+\cdots+a 1}_{n\textnormal{ times}}$. By distributive law, that is equivalent to $n\cdot a=a\left(\underbrace{1+1+\cdots+1}_{n\textnormal{ times}}\right)=a(n\cdot 1)$. Now, $n\cdot 1\in A$, $a\in A$ and $n\cdot a\in A$. Assume the additive order of $a$ is $n$. Then, $n\cdot a=0$ and $0=a(n\cdot 1)$. As $A$ is an integral domain it has no divisors of zero. But, as $a\neq 0$, it must be that $n\cdot 1=0$. Therefore, the additive order of $1$ divides additive order of $a$. Now, assume additive order of $1$ is $n$. Then, $n\cdot 1=0$ and we have $(n\cdot 1)a=0a=0$, i.e $n\cdot a=0$. Therefore, the additive order of $a$ divides additive order of $1$ (group theory). From that we have that additive order of $a$ is the same as additive order of $1$, i.e. characteristic of $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From the previous theorem, we have that if $A$ has characteristic $n$, then $(n k)\cdot a=0$, for all $a\in A$ and $k\in\Z$.

\noindent\newline{\bf Theorem.} In an integral domain with nonzero characteristic, the characteristic is a prime number.

\noindent\newline{\bf Proof.} Assume $n=k m$, for some $k,m\in\Z^{+}$ and that characteristic of integral domain $A$ is $n$. Then, $n\cdot 1=(k m)\cdot 1=\underbrace{1+1+\cdots+1}_{k m\textnormal{ times}}=\underbrace{k\cdot 1+\cdots+k\cdot 1}_{m\textnormal{ times}}=m\cdot(k\cdot 1)$. Then, $m\cdot(k\cdot 1)=0$ implies that characteristic of $k\cdot 1$ is $m$. But, by a previous theorem, that means that $m=n$. Then, it must be that $k=1$. Therefore, $n$ is divisible only by $1$ and itself, so it must be $n\in P$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} In any integral domain $A$ of characteristic $p$, $(a+b)^p=a^p+b^p$, for all $a,b\in A$.

\noindent\newline{\bf Proof.} We know\footnote{Proof in my works on number theory.} that $p\equiv\binom{p}{n}$ for all $n\in\Z^{+}$, $n<p$. Thus, from binomial formula, $(a+b)^p$ contains $\binom{p}{n}$ as a coefficient (with $0<n<p$) along every member except $a^p$ and $b^p$ (where actually we have $\binom{p}{p}$ and $\binom{p}{0}$). Therefore, all members become zero and only $a^p$ and $b^p$ remain, as $\binom{p}{p}=\binom{p}{0}=1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $n\in\Z^{+}-\{1,2\}$ and let $D=\{0,1,d_1,\ldots,d_{n-2}\}$ be a finite integral domain. Then, $\pi_d:D\rightarrow D$ defined with $\pi_{d}(x)=d x$ is a bijection for all $d\in D-\{0\}$.

\noindent\newline{\bf Proof.} Let $d\in D-\{0\}$. First we will prove that $f$ is well-defined. If we take $x\in D$, then, as $D$ is closed with respect to multiplication, we have $d x\in D$ and then $\pi_d(x)=d x$. If $x=y$, then, multiplying by $d$ on the left (or right, remember that an integral domain is commutative), we have $d x=d y$, i.e. $\pi_d(x)=\pi_d(y)$, so uniqueness is satisfied. Now, if $\pi_d(x)=\pi_d(y)$, i.e. $d x=d y$, then as $d\neq 0$ and as $D$ is an integral domain, we have that $x=y$, so $\pi_d$ is injective. The key observation is in the fact that $D$ is finite. By a previous proposition, if $\dom{f}$ and $\cod{f}$ are finite and $\left|\dom{f}\right|=\left|\cod{f}\right|$, then function $f$ is injective if and only if it is surjective. As $\dom{\pi_d}=D=\cod{\pi_d}$, which is finite, then also $\left|\dom{\pi_d}\right|=|D|=\left|\cod{\pi_d}\right|$. We have shown that $\pi_d$ is an injective function, so it also must be surjective. In other words, $\pi_d:D\rightarrow D$ is bijective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Any finite integral domain is a field.

\noindent\newline{\bf Proof.} If $D=\{0,1\}$, then $1\in D-\{0\}$ is its own inverse and $D$ is a field. Let $n\in\Z^{+}-\{1,2\}$. Let $D=\{0,1,d_1,\ldots,d_{n-2}\}$ be an integral domain and $d\in D-\{0\}$. Then, by a previous lemma $\pi_d:D\rightarrow D$ is a bijection and it has an inverse $\pi_d^{-1}$. Let us observe the preimage:

\begin{equation*}
\pi_d^{-1}(D)=\{x\in D:\ (\exists y\in D)(\pi_d(x)=y)\}.
\end{equation*}

\noindent\newline As $\pi_d$ is a bijection, so is $\pi_d^{-1}(D)$ and it must be $\ran{\pi_d^{-1}}=D$, i.e. $\pi_d^{-1}(D)=D$. As $1\in D$, then there exists $x\in\pi_d^{-1}(D)$ such that $\pi_d(x)=1$, i.e. $d x=1$. In other words, $d$ is invertible. Our choice of $d$ was arbitrary, so every $d\in D-\{0\}$ is invertible and, from that, $D$ is a field.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition\footnote{Results follow from group theory, but I will prove them again, nontheless.}.} Let $D$ be a finite integral domain and $d\in D-\{0\}$. Then,

\begin{enumerate}
\item If $n\cdot d=0$, where $n\in\Z-\{0\}$, then $\rchar{D}|n$.
\item If $\rchar{D}=0$, $n\in\Z-\{0\}$, and $n\cdot a=0$, then $a=0$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $n\in\Z-\{0\}$ and $n\cdot d=0$. By division with remainder we have $n=q\rchar{D}+r$, where $q,r\in\Z$ and $0\leq r<\left|\rchar{D}\right|=\rchar{D}$. Then, $n\cdot d=(q\rchar{D}+r)\cdot d=q(\rchar{D}\cdot d)+r\cdot d=0$. As $\rchar{D}\cdot d=0$, then we have $q(\rchar{D}\cdot d)+r\cdot d=r\cdot d$ and from that $r\cdot d=0$. But, as $r<\rchar{D}$, it can only be that $r=0$. So, $n=q\rchar{D}+0=q\rchar{D}$, i.e. $\rchar{D}|n$.

{\it Ad $2$.} Assume $n\in\Z^{+}$, $n\cdot a=0$ and $a\neq 0$. We have $n\cdot a=n\cdot(1a)=(n\cdot 1)a=0$. As $D$ is an integral domain, it must be that either $a=0$ or $n\cdot 1=0$. But, by our assumption $a\neq 0$, so it must be $n\cdot 1=0$. By definition of a characteristic, if $\rchar{D}=0$, there does not exist $n\in\Z^{+}$ such that $n\cdot 1=0$, so we have a contradiction and it must be that $a=0$.

By definition, as $\rchar{D}=0$, there does not exist $n\in\Z^{+}$ such that $n\cdot a=0$. So, the only other option is that $a=0$. If $n<0$, then we can always take $n\cdot a=(-n)\cdot(-a)=0$, so we have the same reasoning.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $D$ be a finite integral domain. Solve:

\begin{enumerate}
\item If $\rchar{D}=3$, and $5\cdot a=0$, for some $a\in D$, then $a=0$.
\item If there is a nonzero element $a$ in $D$ such that $256\cdot a=0$, then $\rchar{D}=2$.
\item If there are distinct nonzero elements $a$ and $b$ in $D$ such that $125\cdot a=125\cdot b$, then $\rchar{D}=5$.
\item If there are nonzero elements $a$ and $b$ in $D$ such that $(a+b)^2=a^2+b^2$, then $\rchar{A}=2$.
\item If there are nonzero elements $a$ and $b$ in $D$ such that $10a=0$ and $14b=0$, then $\rchar{A}=2$.
\end{enumerate}

\noindent{\bf Solution.} Notice that in all exercises $D$ is nontrivial, as we assume existence of nonzero elements.

\begin{enumerate}
\item {\it If $\rchar{D}=3$, and $5\cdot a=0$, for some $a\in D$, then $a=0$.} Assume $a\neq 0$. By previous proposition, as $5\cdot a=0$, then $\rchar{D}|5$. But $3\nmid 5$, so it must be $a=0$.

\item {\it If there is a nonzero element $a$ in $D$ such that $256\cdot a=0$, then $\rchar{D}=2$.} We have $a\neq 0$ and $256\cdot a=0$. Then, by previous proposition, $\rchar{D}|256$, i.e. $\rchar{D}|2^8$. But, we also know that $\rchar{D}\in P$, so it can only be that $\rchar{D}=2$.

\item {\it If there are distinct nonzero elements $a$ and $b$ in $D$ such that $125\cdot a=125\cdot b$, then $\rchar{D}=5$.} We have $125\cdot a=125\cdot b$ and $a\neq b$, $a\neq 0$ and $b\neq 0$. Then, $125\cdot a-125\cdot b=0$. From this we have $125\cdot(1 a)-125\cdot(1 b)=0$, i.e. $(125\cdot 1)a-(125\cdot 1)b=0$. By, distributive law, as $(125\cdot 1)\in A$, then $(125\cdot 1)(a-b)=0$. That is equivalent to $125\cdot(1(a-b))=0$, that is $125\cdot(a-b)=0$. If it were that $a-b=0$, then we would have $a=b$, but we assumed $a\neq b$. So, by previous proposition $\rchar{D}|125$, i.e. $\rchar{D}|5^3$. This implies, as $\rchar{D}\in P$, that $\rchar{D}=5$.

\item {\it If there are nonzero elements $a$ and $b$ in $D$ such that $(a+b)^2=a^2+b^2$, then $\rchar{D}=2$.} We have $(a+b)^2=a^2+b^2$. That is equivalent to $a^2+2a b+b^2=a^2+b^2$, i.e. $2a b=0$. If it were that $a b=0$, then we would have, as $D$ is an integral domain and has no divisors of zero, that $a\neq 0$ or $b\neq 0$. But, by assumption $a\neq 0$ and $b\neq 0$. That means that $a b\neq 0$. So, as $2a b=0$, and $a b\in D$, then, $\rchar{A}|2$ and the only possibility, as $\rchar{D}\in P$, is $\rchar{D}=2$.

\item {\it If there are nonzero elements $a$ and $b$ in $D$ such that $10a=0$ and $14b=0$, then $\rchar{D}=2$.} We have $\rchar{D}|10$ and $\rchar{D}|14$. The only common divisor of $10$ and $14$ is $2$, so it must be $\rchar{D}=2$.

\end{enumerate}

\noindent{\bf Theorem.} Let $D$ be a finite integral domain. Then, $\rchar{D}$ divides $|D|$.

\noindent\newline{\bf Proof.} Let $d\in D$, $d\neq 0$. Then, $\rchar{D}\cdot d=0$. As additive group $D$ is an Abelian group, then the additive order of $d$ divides $\rchar{D}$. But, as $\rchar{D}\in P$, it can only be that the additive order of $d$ is $1$ or $\rchar{D}$. It cannot be $1$ as then we would have $d=0$, so additive order of $d$ is $\rchar{D}$. As additive order of $d$ divides $|D|$, then $\rchar{D}$ divides order of $D$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $D$ be a finite integral domain with $|D|=p$, where $p\in P$. Then, $\rchar{D}=p$.

\noindent\newline{\bf Proof.} From the previous theorem we have $\rchar{D}$ divides order of $|D|$, so the only option is $\rchar{D}=1$ or $\rchar{D}=p$. It cannot be that $\rchar{D}=1$ as that would imply $1\cdot 1=0$, i.e. $1=0$, and we would have $D=\{0\}$, i.e. $|D|=1$. So it must be that $\rchar{D}=p$.

\begin{flushright}
$\square$\\
\end{flushright} 

\noindent{\bf Corollary.} Let $D$ be a finite integral domain with $|D|=p^m$, where $p\in P$ and $m\in\Z^{+}$, then $\rchar{D}=p$.

\noindent\newline{\bf Proof.} We have that $\rchar{D}$ divides $|D|$ and $\rchar{D}\in P$. That is satisfied only by $p$, so $\rchar{D}=p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $D$ be a finite integral domain. If $|D|=81$, then by previous corollary it is obvious that $\rchar{D}=3$.

\noindent\newline{\bf Proposition.} Let $D$ be a finite integral domain. If additive group $D$ is cyclic, then $|D|\in P$.

\noindent\newline{\bf Proof.} If additive $D$ (assume $|D|=m$, where $m\in\Z^{+}$) is cyclic, then there exists $d\in D$ such that if $a\in D$, we have $a=n\cdot d$, for some $n\in\Z^{+}$. We know that $m\cdot d=0$. Therefore, $\rchar{D}$ divides $m$ and is equal to some $p\in P$. Then, as $\rchar{D}\cdot d=0$, it must be that additive order of $d$ divides $\rchar{D}$. So, we have $m|p$. Therefore, as $m|p$ and $p|m$ it must be that $m=p$, i.e. $|D|=p\in P$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a finite commutative ring with unity. Then, every $a\in A-\{0\}$ is either a divisor of zero or invertible.

\noindent\newline{\bf Proof.} If $A=\{0,1\}$, then it is obvious that $1\in A$ is invertible. Let $n\in\Z^{+}-\{1,2\}$ and $A=\{0,1,a_1,\ldots,a_{n-2}\}$. Then, $|A|=n$. Let $a\in A-\{0\}$. Let $\pi_a:A\rightarrow A$ be defined with $\pi_a(x)=a x$. Then, $\pi_a$ is a well-defined as $a x\in A$ for all $x\in A$ (due to $A$ being closed with respect to multiplication) and as $x=y$ implies $a x=a y$ (due to uniqueness of binary operation of multiplication). We have that either $\pi_a$ is injective or is not injective. If $\pi_a$ is injective, then, as $A$ is finite, it is also surjective, and by that, bijective. Therefore, $\ran{f}=A$, so $1\in\ran{f}$ and, as $\pi_a$ is surjective, there exists $x\in A$ such that $\pi_a(x)=1$, i.e. $a x=1$. As it is commutative, $x a=1$, i.e. $a$ is invertible. Assume $\pi_a$ is not injective. Then, for some $x,y\in A$, we have that $a x=a y$ and $x\neq y$. From $a x=a y$ it follows that $a x-a y=0$, i.e. $a(x-y)=0$. But, as $x\neq y$, then $x-y\neq 0$. Therefore, as also $a\neq 0$, we have that $a\in A-\{0\}$ (and also $x-y\in A-\{0\}$) is a divisor of zero.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $A$ be a ring. If $a\in A-\{0\}$ is not a divisor of zero then $a^m$ is not a divisor of zero and $a^m\neq 0$, for all $m\in\Z^{+}$.

\noindent\newline{\bf Proof.} If $m=1$, then obviously $a^1=a$ is not a divisor of zero and $a^1=a\neq 0$. Assume $a^m$ is not a divisor of zero and $a^m\neq 0$. We will prove that $a^{m+1}$ is not a divisor of zero and $a^{m+1}\neq 0$. Let $b\in A$ such that $a^{m+1} b=0$. That is equivalent to $a(a^m b)=0$. As $a\neq 0$ and $a$ is not a divisor of zero, then $a^m b=0$. But, as $a^m$ is not a divisor of zero and $a^m\neq 0$, by assumption, then $b=0$. Therefore, $a^{m+1} b=0$ implies $b=0$ (or $a^{m+1}=0$). Assume $a^{m+1}=0$. Then, $a a^m=0$. But, that is impossible as $a\neq 0$ and $a^m\neq 0$ and they are not divisors of zero. So, $a^{m+1}$ is not a divisor of zero and $a^{m+1}\neq 0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be a finite commutative ring with unity. If $a\in A-\{0\}$ is not a divisor of zero, then there exists $m\in\Z^{+}$ such that $a^m=1$.

\noindent\newline{\bf Proof.} Let $a\in A-\{0\}$ and $\pi_a:\Z^{+}\rightarrow A$ be a mapping defined with $\pi_a(x)=a^x$. If $x\in\Z^{+}$ then $a^x\in A$. If $x=y$, then $x-y=0$. As $a^0=1$, then $a^{x-y}=1$. Multiplying by $a^y$ gives us $a^x=a^y$. Obviously $\pi_a$ is a well-defined function. But, $\phi_a$ cannot be an injection as $\Z^{+}$ is infinite and $A$ is finite. Therefore, there exist $x,y\in\Z^{+}$ such that, $a^x=a^y$ and $x\neq y$. Assume $x<y$. Then, $a^x-a^y=0$ is equivalent to $a^x(1-a^{y-x})=0$. As $a$ is not a divisor of zero, then also $a^x$ is not a divisor of zero and $a^x\neq 0$, by a previous lemma. That implies that $1-a^{y-x}=0$. But, that is equivalent to $1=a^{y-x}$. Taking $m=y-x$ proves the theorem.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $A$ be a finite commutative ring with unity. Then, if $a\in A-\{0\}$ is invertible, there exists $m\in\Z^{+}$ such that $a^m=a^{-1}$.

\noindent\newline{\bf Proof.} As $a$ is invertible, there exists $a^{-1}\in A-\{0\}$ such that $a a^{-1}=1$. Assume $a$ is a divisor of zero. Then for some $b\in A$, where $b\neq 0$, we have $a b=0$. But, as $a^{-1}$ is invertible, we have $a^{-1} a b=a^{-1} 0$, i.e. $b=0$, which is a contradiction. Therefore, $a$ is not a divisor of zero, and so is not $a^{-1}$. By a previous theorem, there exists $n\in\Z^{+}$ such that $a^n=1$. Then, we have $a^{n-1}=a^{-1}$. If $n>1$, then taking $m=n$ proves the corollary. If it were that $n=1$, then we would have $a^0=a^{-1}$, i.e. $a^{-1}=a=1$. But, as $1^m=1$, for all $m\in\Z^{+}$, we have $a^m=a^{-1}$, for all $m\in\Z^{+}$, so in this case, any positive integer will do for $m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $D$ be an integral domain. Then:

\begin{enumerate}
\item Relation $\sim$ defined on $D\times D-\{0\}$ as $(a,b)\sim(c,d)$ if and only if $a d=b c$, for all $a,b\in D$, is an equivalence relation.
\item Let $[a,b]=\{(c,d)\in D\times D-\{0\}:\ a d=b c\}$. Then, $D^{\ast}=\{[a,b]:\ (a,b)\in D\times D-\{0\}\}$ with $[a,b]+[c,d]=[a d+b c,b d]$ as addition and $[a,b]\cdot[c,d]=[a c,b d]$ as multiplication\footnote{Notice that the $+$ sign on the left-hand side of definition of addition denotes addition in $D^{\ast}$ and on the right-hand side we use addition and multiplication from $D$; similarly, in the second definition, $\cdot$ on the left-hand side denotes multiplication in $D^{\ast}$ and on the right-hand side multiplication in $D$.} is a field. Zero is $[0,1]$ and unity $[1,1]$.
\item Let $D'=\{[a,1]:\ a\in D\}$. Then, $D'\leq D^{\ast}$ and $D\cong D'$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Reflexivity.} We have $(a,b)\sim(a,b)$ because $a b=b a$ and $D$ is commutative. {\it Symmetry.} Let $(a,b)\sim(c,d)$. Then, $a d=b c$, but as $D$ is commutative, also $c b=d a$, therefore $(c,d)\sim(a,b)$. {\it Transitivity.} Let $(a,b)\sim(c,d)$ and $(c,d)\sim(e,f)$. Then, $a d=b c$ and $c f=d e$. Multiplying first equation by $f$ gives us $a d f=b c f$. Substituting $c f$ for $d e$ gives us $a d f=b d e$. As $D$ is commutative and associativity holds, that is equivalent to $(a f)d=(b e)d$. From $d\neq 0$ (because $(c,d)\in D\times D-\{0\}$), as $D$ is an integral domain, then $a f=b e$, i.e. $(a,b)\sim(e,f)$. Thus, $\sim$ is an equivalence relation.

{\it Ad $2$.} As $\sim$ is an equivalence relation, then it is obvious that $[a,b]=\{(c,d)\in D\times D-\{0\}:\ (a,b)\sim(c,d)\}=\{(c,d)\in D\times D-\{0\}:\ a d=b c\}$ are its equivalence classes. Let $D^{\ast}=\{[a,b]:\ (a,b)\in D\times D-\{0\}\}$. We will show simultaneously that $(D^{\ast},+)$ and $(D^{\ast},\cdot)$ are Abelian groups. {\it Associativity.} We have $[a,b]([c,d]\cdot[e,f])=[a,b]([c f+d e,d f])=[(d f)a+b(c f+d e),b(d f)]=[a d f+b c f+b d e,b d f]=[(b d)e+f(a d+b c),(b d)f]=[a d+b c,b d]\cdot[e,f]=([a,b]\cdot[c,d])\cdot[e,f]$. Similarly, $[a,b]\cdot([c,d]\cdot[e,f])=[a,b]\cdot[c e,d f]=[a(c e),b(d f)]=[(a c)e,(b d)f]=[a c,b d]\cdot[e,f]=([a,b]\cdot[c,d])\cdot[e,f]$. {\it Neutral elements.} Zero is $[0,1]$ as $[0,1]+[a,b]=[1 a+0 b,1 b]=[a,b]$ and $[a,b]+[0,1]=[a 1+0 b,b 1]=[a,b]$. Unity is $[1,1]$ because $[1,1]\cdot[a,b]=[a,b]\cdot[1,1]=[a 1,b 1]=[a,b]$. {\it Inverse elements.} Inverse of $[a,b]$ in $(D^{\ast},+)$ is $[-a,b]$ (that is, $-[a,b]=[-a,b]$) because $[a,b]+[-a,b]=[a b-a b,b^2]=[0,b^2]$. But, $[0,b^2]\sim[0,1]$ because $0 1=b^2 0$, i.e. $0=0$. Inverse of $[a,b]$ in $(D^{\ast},\cdot)$ is $[b,a]$ because $[a,b]\cdot[b,a]=[a b,b a]$. It is obvious that $[a b,b a]\sim[1,1]$ because $a b1=b a 1$, i.e. $a b=a b$. {\it Commutativity.} Both groups are commutative as $[a,b]+[c,d]=[a d+b c,b d]=[c b+a d,d b]=[c,d]+[a,b]$ and $[a,b]\cdot[c,d]=[a c,b d]=[c a,d b]=[c,d]\cdot[a,b]$. {\it Distributive law.} We have $[a,b]\cdot([c,d]+[e,f])=[a,b]\cdot[c f+d e,d f]=[a(c f+d e),b(d f)]=[a c f+a d e,b d f]$. From the other side, $[a,b]\cdot[c,d]+[a,b]\cdot[e,f]=[a c,b d]+[a e,b f]=[a c b f+a e b d,b^2 d f]=[b(a c f)+b(a d e),b(b d f)]$. We know that $[a c f+a d e,b d f]\sim[b(a c f+a d e),b(b d f)]$ because $(a c f+a d e)\cdot b(b d f)=(b d f)\cdot b(a c f+a d e)$, i.e. $(a c f+a d e)(b d f)=(a c f+a d e)(b d f)$. Second distributive law holds because of commutativity. Therefore, as $(D^{\ast},+)$ and $(D^{\ast},\cdot)$ are Abelian groups, and multiplication is distributive over addition, then $(D^{\ast},+,\cdot)$ is a field.

{\it Ad $3$.} If $[a,1]\in D'$, then $(a,1)\in D\times D-\{0\}$, so $[a,1]\in D^{\ast}$ and $D'\subseteq D^{\ast}$. Let $[a,1],[b,1]\in D'$. Then, $[a,1]-[b,1]=[a,1]+[-b,1]=[a+(-b),1]\in D'$ and $[a,1]\cdot[b,1]=[a b,1]\in D'$, so $D'\leq D^{\ast}$, i.e. $D'$ is a subring of $D^{\ast}$. Let $\phi:D\rightarrow D'$ be a mapping defined with $\phi(x)=[x,1]$. Then, if we take $x\in D$, there exists $[x,1]\in D'$ and $\phi(x)=[x,1]$. Also, if $x=y$, i.e. $x 1=y 1$, then $(x,1)\sim(y,1)$ and $[x,1]=[y,1]$, that is, $\phi(x)=\phi(y)$. Therefore, $\phi$ is well-defined. Now, if we take $[x,1]\in D'$, obviously there exists $x\in D$ such that $\phi(x)=[x,1]$. If $\phi(x)=\phi(y)$, that is, $[x,1]=[y,1]$, then $(x,1)\sim(y,1)$, i.e. $x 1=1 y$ and $x=y$. Therefore, $\phi$ is bijective. Let $x,y\in D$. Then, $\phi(x+y)=[x+y,1]=[x 1+1 y,1\cdot 1]=[x,1]+[y,1]=\phi(x)+\phi(y)$ and $\phi(x y)=[x y,1]=[x y,1\cdot 1]=[x,1]\cdot[y,1]=\phi(x)\cdot\phi(y)$. In conclusion, $D\cong D'$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $D$ be an integral domain, $d\in D$ and $p\in P$. Then,

\begin{enumerate}
\item If $\rchar{D}=p$ and $m\in\Z^{+}$ such that $m\cdot d=0$, where $p\nmid m$, then $d=0$.
\item If $p\cdot d=0$ and $d\neq 0$, then $\rchar{D}=p$.
\item If $d\neq 0$, $p^m\cdot d=0$, for some $m\in\Z^{+}$, then $\rchar{D}=p$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Assume $d\neq 0$. Then, by a previous proposition $\rchar{D}|m$, i.e. $p|m$, which is a contradiction to assumption that $p\nmid m$, so it must be that $d=0$.

{\it Ad $2$.} Let $d\neq 0$, $p\cdot d=0$. Assume $\rchar{D}=q$, where $q\in P$ (as characteristic is a prime number). Then we have $q\cdot d=0$, but also $q\cdot d-p\cdot d=0$, i.e. $(q-p)d=0$. As $d\neq 0$, by assumption, and as $D$ is an integral domain, then $(q-p)=0$, i.e. $q=p$, which means $\rchar{D}=p$.

{\it Ad $3$.} Let $d\neq 0$, $p^m\cdot d=0$. We have $p\cdot(p^{m-1}\cdot d)=0$. Obviously $p^{m-1}\in\Z^{+}$ because $m\in\Z^{+}$. Therefore, $p^{m-1}\cdot d\in D$, that is to say, there exists $a\in D$ such that $p^{m-1}\cdot d=a$. So, from $p\cdot(p^{m-1}\cdot d)=0$, we have $p\cdot a=0$, where $a\neq 0$. Thus, by previous problem, we have $\rchar{D}=p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $D$ be an integral domain. If $|D|=p$, then $D\cong\zmod{p}$.

\noindent\newline{\bf Proof.} As $|D|=p$ then additive $D$ is cyclic and isomorphic to $\zmod{p}$. We will show that the generator of additive $D$ is its unity. From a previous theorem (corollary of the theorem, to be more precise), we have that $\rchar{D}=p$. Let us observe $D'=\{k\cdot 1:\ k\in\{0,\ldots,p-1\}\subset\Z\}$. Obviously $|D'|\leq p$. Assume $|D'|<p$. Then there exist $k_1\cdot 1,k_2\cdot 1\in D'$, with $k_1,k_2\in\{0,\ldots,p-1\}$ such that $k_1\cdot 1=k_2\cdot 1$ and $k_1\neq k_2$. That is equivalent to $k_1\cdot 1-k_2\cdot 1=0$. That implies $(k_1-k_2)\cdot 1=0$. But, as $0\leq k_1,k_2<p$, then $0\leq|k_1-k_2|<p$. Let $q=k_1-k_2$. Assume $q\geq0$. If $q\leq0$ then we can take $k_2\cdot 1-k_1\cdot 1=0$ (by adding negative of $k_1\cdot 1$ instead of $k_2\cdot 1$). We then have $q\cdot 1=0$. That implies that $q|\rchar{D}$, i.e. $q|p$. But that means that either $q=1$ or $q=p$. If $q=1$, then $1=0$, making $D$ trivial, i.e. $D=\{0\}$ and $|D|=1$, and $1\notin P$, which is a contradiction. Therefore, it must be that $q=p$, but $q<p$, so it is again a contradiction and we have that it must be $q=0$ (it satisfies $0\cdot 1=0$) which implies $k_1=k_2$, again a contradiction. Therefore, order of $D'$ cannot be less than $p$ and it can only be that $|D'|=p$. From that we have $|D'|=|D|$. But, if we take $k\cdot 1\in D'$, for any $k\in\Z$, then $k\cdot 1\in D$, giving us $D'\subseteq D$. We have $|D'|=|D|$ and $D'\subseteq D$, so it must be $D=D'$. Generator of additive $D'$ is obviously $1$, and so generator of $D$ is also $1$ (its unity).

Now we can take $f:D\rightarrow\zmod{p}$ with $f(n\cdot 1)=p\Z+n$. Let $x\in D$. As $D=D'$ then there exists $n\cdot 1\in D'$ such that $x=n\cdot 1$. As $n\in\Z$ then there exists $p\Z+n\in\zmod{p}$ making $f(x)=f(n\cdot 1)=p\Z+n$. If $n\cdot 1=m\cdot 1$, then it must be that, due to the same reasoning as above, that $n=m$. So they are also congruent modulo $p$, i.e. $n-m=0\cdot p$, so $p\Z+n=p\Z+m$, i.e. $f(n\cdot 1)=f(m\cdot 1)$. Thus, $f$ is well-defined. Now, if we take $p\Z+n\in\zmod{p}$ it is obvious that $n\in\Z$. Now, by division with remainder theorem, there exist $q,r\in\Z$ such that $n=q p+r$, where $0\leq r<|p|=p$. Also, $n=q p+r$ is equivalent to $n-r=q p$, i.e. $n\equiv r\pmod p$. That is equivalent to $p\Z+n=p\Z+r$. Therefore, as $r\in\{0,\ldots,p-1\}$, there exists $r\cdot 1\in D'$ and, as $D'=D$, there exists $x\in D$ such that $x=r\cdot 1$. Thus, $f(x)=f(1\cdot r)=p\Z+r=p\Z+n$ and $f$ is surjective. If we take $f(n\cdot 1)=f(m\cdot 1)$, then $p\Z+n=p\Z+m$, giving us $n\equiv m\pmod p$. But, as $n\cdot 1,m\cdot 1\in D'$, then $0\leq n,m<p$. That, combined with $n\equiv m\pmod p$ implies $n=m$ and $n\cdot 1=m\cdot 1$, making $f$ injective and bijective. Now, $f(1\cdot n+1\cdot m)=f(1\cdot(n+m))=p\Z+(n+m)=(p\Z+n)+(p\Z+m)=f(1\cdot n)+f(1\cdot m)$ and $f((1\cdot n)(1\cdot m))=f(1\cdot (n m))=p\Z+(n m)=(p\Z+n)(p\Z+m)=f(1\cdot n)f(1\cdot m)$ and $f$ is an isomorphism from $D$ to $\zmod{p}$, which means that $D\cong\zmod{p}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $D$ be an integral domain with $\rchar{D}=p$, for some $p\in P$, and let $m\in\Z^{+}$. Then:

\begin{enumerate}
\item $(a+b)^{p^m}=a^{p^m}+b^{p^m}$, for all $a,b\in D$.
\item $(a_1+a_2+\cdots+a_n)^{p^m}=a_1^{p^m}+a_2^{p^m}+\cdots+a_n^{p^m}$, where $n\in\Z^{+}$ and $a_1,a_2,\ldots,a_n\in D$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $m=1$. Then, $(a+b)^p=a^p+b^p$. Assume that $(a+b)^{p^m}=a^{p^m}+b^{p^m}$ for some $m\in\Z^{+}$. Then, $(a+b)^{p^{m+1}}=(a+b)^{p^m p}=\left((a+b)^{p^m}\right)^p=\left(a^{p^m}+b^{p^m}\right)^p=\left(a^{p^m}\right)^p+\left(b^{p^m}\right)^p=a^{p^{m+1}}+b^{p^{m+1}}$. Therefore, the equality is true for all $m\in\Z^{+}$.

{\it Ad $2$.} Let $n=1$. Then, $(a_1)^{p^m}=a_1^{p^m}$. Assume the statement is true for some $n\in\Z^{+}$. Then, $(a_1+\cdots+a_n+a_{n+1})^{p^m}$ can be grouped to have two members, i.e. $((a_1+\cdots+a_n)+a_{n+1})^{p^m}$. By previous problem, that equals $(a_1+\cdots+a_n)^{p^m}+a_{n+1}^{p^m}$. By assumption, that is equal to $a_1^{p^m}+\cdots+a_n^{p^m}+a_{n+1}^{p^m}$. That way, we have proved the formula by induction on $n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be integral domains such that $A\subseteq B$ and let $p\in P$. Then, $\rchar{A}=p$ if and only if $\rchar{B}=p$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $\rchar{A}=p$. Then, for all $a\in A-\{0\}$, we have that $p\cdot a=0$. But, as $a\in B$, due to $A\subseteq B$, we have $p\cdot a=0$. By previous problem, $\rchar{B}|p$, so it must be that $\rchar{B}=p$. {\it Sufficiency.} Let $\rchar{B}=p$. Take $a\in A-\{0\}$. Then, $a\in B$, so $p\cdot a=0$. That means that $\rchar{A}|p$ and it must be $\rchar{A}=p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every finite field\footnote{Due to a previous theorem, $F$ is a finite field if and only if $F$ is a finite integral domain.} has nonzero characteristic.

\noindent\newline{\bf Proof.} Let $F$ be a field with $|F|=n$ and $\rchar{F}=0$. Then, $n\cdot 1=0$, but, as $\rchar{F}=0$, there cannot, by definition, exist $n\in\Z^{+}$ such that $n\cdot 1=0$. Therefore, it must be $\rchar{F}\neq 0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a finite field with $\rchar{F}=p$. The function $f:F\rightarrow F$ defined with $f(a)=a^p$ is called a {\bf Froebenius automorphism}.

\noindent\newline{\bf Lemma.} Let $D$ be an integral domain and $p\in P$. If $\rchar{D}=p$, then $f:D\rightarrow D$ defined with $f(d)=d^p$ is a homomorphism from $D$ to $D$.

\noindent\newline{\bf Proof.} Let $\rchar{D}=p$, $f:D\rightarrow D$ with $f(d)=d^p$. If we take $d\in D$, then due to $D$ being closed with respect to multiplication, $d^p\in D$, so $f(d)=d^p$. If $x=y$, then obviously $x^p=y^p$ (if we multiplied the former equality $p$ times with itself), i.e. $f(x)=f(y)$. Thus, $D$ is well-defined. So, $f(x+y)=(x+y)^p$. By a previous theorem, that is equivalent to $f(x+y)=x^p+y^p=f(x)+f(y)$. Finally, $f(x y)=(x y)^p=x^p y^p=f(x)f(y)$, thus $f$ is a homomorphism from $D$ to $D$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Froebenius automorphism is an automorphism.

\noindent\newline{\bf Proof.} Let $F$ be a finite field. Then, by a previous proposition it has a nonzero characteristic, which must be, by a previous theorem a prime number. Thus, $\rchar{F}=p$, for some $p\in P$. Then, as every field is also an integral domain, from the previous lemma it follows that $f:F\rightarrow F$ defined with $f(a)=a^p$ is a homomorphism from $F$ to $F$.

{\it Injectivity.} We won't pay much attention to what $\ker{f}$ is, but how it relates to $F$. We know that $\ker{f}\trianglelefteq F$. So, assume $f$ is not injective, i.e. $\ker{f}\neq\{0\}$. Then, there exists $a\in\ker{f}$ such that $a\neq 0$. But, all $a\in F-\{0\}$ are invertible, so there exists $a^{-1}\in F$ such that $a a^{-1}=1$. As $\ker{f}$ is an ideal of $F$, for all $x\in F$ and $y\in\ker{f}$ we must have $x y\in\ker{f}$ and $y x\in\ker{f}$. Therefore, as $a\in\ker{f}$, and $a^{-1}\in F$, then $a a^{-1}$ and $a^{-1} a$ are in $\ker{f}$. That is, $1\in\ker{f}$. But, then, if $a\in F$, it also must be that $1\cdot a\in\ker{f}$, i.e. $a\in\ker{f}$. Therefore, $F\subseteq\ker{f}$ which implies $F=\ker{f}$. That would mean that $f(x)=0$ for all $x\in F$, but at least $f(1)=1^p=1$. Therefore, there does not exist $a\in\ker{f}-\{0\}$ and it must be that $\ker{f}=\{0\}$. So, if we take $f(x),f(y)\in\ran{f}$, then, $f(x)=f(y)$ implies $f(x)-f(y)=0$. But, as $f$ is a homomorphism, that is equivalent to $f(x-y)=0$. That means that $x-y\in\ker{f}$ and, as $\ker{f}=\{0\}$, it can only be that $x-y=0$, i.e. $x=y$. Therefore, $f$ is injective. {\it Surjectivity.} As $F$ is finite and $|F|=|\dom{f}|=|\cod{f}|$, and as $F$ is an injective function, then $F$ is also surjective. That means that $f$ is a bijective homomorphism, i.e. an isomorphism. As it is an isomorphism from $F$ to $F$ it is an automorphism on $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $p\in P$. In a finite field of characteristic $p$, every element has a $p$-th root.

\noindent\newline{\bf Proof.} Let $F$ be a finite field and $\rchar{F}=p$. Let $y\in F$. We want to prove that there exists $x\in F$ such that $x^p=y$. Let us observe Froebenius automorphism on $F$ (that we can as we're dealing with a finite field with characteristic $p$). We have $f:F\rightarrow F$ with $f(x)=x^p$ and we know that $f$ is an automorphism (but also a surjection). Then, if we take $y\in F$, we have $y\in\ran{f}=F$, and there exists $x\in F$ such that $f(x)=y$, i.e. $x^p=y$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf The integers}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $D$ be an integral domain and $<$ a relation on $D$ such that for all $a,b,c\in D$:

\begin{enumerate}
\item $a<b$, $b<a$ or $a=b$;
\item $a<b$ and $b<c$ implies $a<c$;
\item $a<b$ implies $a+c<b+c$;
\item $0<c$ and $a<b$ imply $a c<b c$.
\end{enumerate}

\noindent Then, $D$ with relation $<$, is called an {\bf ordered integral domain}. Also, we say that $<$ is an {\bf order relation} on $D$.

\noindent\newline{\bf Remark.} We will just note that $a\leq b$ means $a<b$ or $a=b$.

\noindent\newline{\bf Definition.} Let $D$ be an ordered integral domain and $d\in D$. Then, if $d>0$, we say that $d$ is {\bf positive}, and if $d<0$ we say that $d$ is {\bf negative}.

\noindent\newline{\bf Proposition.} Let $D$ be an ordered integral domain and $a,b\in D$. Then, $a<b$ if and only if $-b<-a$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $a<b$. Then we can add $-a$ on both sides to get $a-a<b-a$, i.e. $0<b-a$. Then, we can add $-b$ on both sides and get $0-b<b-a-b$, which is equivalent to $-b<-a$. {\it Sufficiency.} Let $-b<-a$. Then we add $a$ and $b$ on both sides and get $-b+a+b<-a+a+b$, i.e. $a<b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} From the previous proposition we have that if $a$ is positive, then $-a$ is negative, and if $a$ is negative, then $-a$ is positive. That follows from $0<a$ implies $-a<-0$, i.e. $-a<0$, and from $a<0$ implies $-0<-a$, that is, $0<-a$.

\noindent\newline{\bf Proposition.} The square of every non-zero element in an ordered integral domain is positive. Also, unity is always positive.

\noindent\newline{\bf Proof.} Let $D$ be an ordered integral domain and $d\in D-\{0\}$. Assume $0<d$. Then, multiplying that equality by $d$ (that we can because $0<d$) gives us $0 d<d d$, which is equivalent to $0<d^2$. Assume $d<0$. Then, by previous proposition, $0<-d$, and multiplying that by $-d$ (possible because $0<-d$) gives us $-d0<(-d)(-d)$, i.e. $0<d^2$. So, $0<d^2$, for all $d\in D-\{0\}$, but so it is true for $1\in D-\{0\}$, i.e. $0<1^2$. But, $1^2=1$, so $0<1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $D$ be an ordered integral domain. Then, for every $n\in\Z$ we have $n\cdot 1<(n+1)\cdot 1$.

\noindent\newline{\bf Proof.} From the previous proposition we have that $0<1$. As $n\cdot 1\in D$, then adding it to both sides gives us $n\cdot 1<1+n\cdot 1$. That is, $n\cdot 1<1\cdot 1+n\cdot 1$ which is equivalent\footnote{Be careful to notice that in $1\cdot 1$, first $1$ is integer and second $1$ is unity in $D$. We kind of use ambiguous notation, but I will either clarify them more in a finished version, or make it unambiguous by using $1_D$ to denote unity in $D$ (and also $0_D$ to denote zero in $D$).} to $n\cdot 1<(1+n)\cdot 1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $D$ be an ordered integral domain. Then we denote the set of all positive elements in $D$ as:

\begin{equation*}
D^{+}=\{d\in D:\ d>0\}.
\end{equation*}

\noindent\newline{\bf Definition.} Let $D$ be an ordered integral domain. If there exists $d\in D^{+}$ such that $d\leq x$, for all $x\in D^{+}$, we say that $D$ has a {\bf well-ordering property}. Also, an ordered integral domain with well-ordering property is called an {\bf integral system}.

\noindent\newline{\bf Proposition.} In any integral system, there is no element between\footnote{No element greater than zero and less than unity, to be more precise.} zero and unity.

\noindent\newline{\bf Proof.} Let $D$ be an integral system. Assume that there exists $x\in D$ such that $0<x<1$. Then the set $A=\{x\in D:\ 0<x<1\}$ is non-empty and, due to well-ordering of $D$, there exists $a\in A$ such that $a\leq x$ for all $x\in A$. But, also $0<a<1$, i.e. $0<a$ and $a<1$. As $0<a$, we can multiply both equalities to get $0<a^2$ and $a^2<a$, i.e. $0<a^2<a$. But, $a^2<a$ is a contradiction to well-ordering on $D$ and $A$ must be empty.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Characteristic of an ordered integral domain is zero.

\noindent\newline{\bf Proof.} Assume that there exists $n\in\Z^{+}$ such that $n\cdot 1=0$. But, due to a previous proposition, we have $0<1<2\cdot 1<\ldots<n\cdot 1$, then, $0<n\cdot 1$, so that is a contradiction and there cannot exist $n\in\Z^{+}$ such that $n\cdot 1=0$ and by definition, characteristic of an ordered integral domain is zero.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every integral system is isomorphic to $\Z$.

\noindent\newline{\bf Proof.} Let $D$ be an integral system. Let $D'=\{x\in D:\ (\exists n\in\Z)(x=n\cdot 1)\}$. First, we will show that $D=D'$. If we take $x\in D'$, then obviously $x\in D$ and we have $D'\subseteq D$. Now, let us take $x\in D$. Assume that $x\neq n\cdot 1$, for any $n\in\Z$. Then, as by a previous proposition, for all $n\cdot 1,(n+1)\cdot 1\in D'$ we have $n\cdot 1<(n+1)\cdot 1$, then there must exist $m\in\Z$ such that $m\cdot 1<x<(m+1)\cdot 1$, i.e. $m\cdot 1<x$ and $x<(m+1)\cdot 1$. Latter inequality can be written as $x<m\cdot 1+1$. Now, adding $-m\cdot 1$ on both sides on both inequalities, gives us $m\cdot 1-m\cdot 1<x-m\cdot 1$ and $x-m\cdot 1<m\cdot 1+1-m\cdot 1$, respectively. But, those two inequalities are equivalent to $0<x-m\cdot 1$ and $x-m\cdot 1<1$, i.e. $0<x-m\cdot 1<1$. As $x-m\cdot 1\in D$, and $D$ has a well-ordering property, there cannot be an element between zero and unity. Therefore there does not exist $x\in D$ such that $x\notin D'$ and we have $D\subseteq D'$, combined with a previous result that is $D=D'$.

Now, an obvious isomorphism is $f:D\rightarrow\Z$ defined with $f(n\cdot 1)=n$. That function is well-defined due to reasoning above and because $n\cdot 1=m\cdot 1$ implies $n\cdot 1-m\cdot 1=0$ and $(n-m)\cdot 1=0$. As $\rchar{D}=0$ and $1\neq 0$ in integral domain, then it must be $n-m=0$, that is $n=m$. Thus, $f$ satisfies property of uniqueness. If we take $n\in\Z$ then obviously $n\cdot 1\in D$ due to $D$ being closed with respect to addition (here successive addition, $n$ times). Finally, if $f(n\cdot 1)=f(m\cdot 1)$, then $n=m$. But, that obviously implies $n\cdot 1=m\cdot 1$ and $f$ is a bijection. Now, $f(n\cdot 1+m\cdot 1)=f((n+m)\cdot 1)=n+m=f(n\cdot 1)+f(m\cdot 1)$ and $f((n\cdot 1)(m\cdot 1))=f((n m)\cdot 1)=n m=f(n\cdot 1)f(m\cdot 1)$ and $f$ is an isomorphism from $D$ to $\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $S\subseteq\Z^{+}$. If:

\begin{enumerate}
\item $1\in S$,
\item $s\in S$ implies $s+1\in S$, for all $s\in S$,
\end{enumerate}

\noindent then, $S=\Z^{+}$.

\noindent\newline{\bf Proof.} Let $T=\Z^{+}-S$. Assume $T\neq\emptyset$. Then, as $T\subseteq\Z^{+}$, and $\Z^{+}$ has a well-ordering property, there exists $m\in T$ such that $m\leq t$ for all $t\in T$. Also, as $1\in S$, then $1\notin T$ and $m\neq 1$, i.e. $1<m$. From that we have $0<m-1$, so $m-1\in\Z^{+}$. Assume $m-1\notin S$. Then, it must be that $m-1\in T$ (as $T=\Z^{+}-S$, i.e. it contains all elements of $\Z^{+}$ that are not in $S$). As $m-1\in T$ and $m\in T$, and also $m-1<m$, it is a contradiction to $m$ being the least element. Therefore, $m-1\in S$. But, as $m-1\in S$, by assumption we also have $m-1+1\in S$, i.e. $m\in S$, which is a contradiction to $m\in T$ (i.e. $m\in\Z^{+}$ and $m\notin S$). Therefore, it must be that $T=\emptyset$ and from that we have $S=\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (principle of mathematical induction).} Assume the following:

\begin{enumerate}
\item Statement $S_1$ is true.
\item For all $k\in\Z^{+}$, statement $S_k$ is true implies statement $S_{k+1}$ is true.
\end{enumerate}

\noindent Then, statement $S_n$ is true for all $n\in\Z^{+}$.

\noindent\newline{\bf Proof.} Let $T$ be a set of all integers for which statement is true. Then, $1\in T$. Also, if $t\in T$, then also $t+1\in T$, as truth of statement $S_t$ implies truth of $S_{t+1}$. Then, by previous theorem $T=\Z^{+}$, i.e. statement $S_n$ is true for all $n\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $D$ be an ordered integral domain. Prove the following, for all $a$, $b$ and $c$ in $D$:

\begin{enumerate}
\item If $a\leq b$ and $b\leq c$, then $a\leq c$;
\item If $a\leq b$, then $a+c\leq b+c$;
\item If $a\leq b$ and $c\geq 0$, then $a c\leq b c$;
\item If $a<b$ and $c<0$, then $b c<a c$;
\item If $a+c<b+c$, then $a<b$;
\item If $a c<b c$ and $c>0$ then $a<b$;
\item If $a<b$ and $c<d$, then $a+c<b+d$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $a\leq b$ and $b\leq c$, then $a\leq c$.} Let $a\leq b$ and $b\leq c$. Then, $a<b$ or $a=b$ and $b<c$ or $b=c$. Assume $a<b$. If $b<c$ then, by definition, $a<c$. If $b=c$, then by substitution, $a<c$. Assume $a=b$. If $b<c$, then by substitution $a<c$. If $b=c$, then $a=c$. Therefore, if $a\leq b$ and $b\leq c$ then $a<c$ or $a=c$, i.e. $a\leq c$.

\item {\it If $a\leq b$, then $a+c\leq b+c$.} Let $a\leq b$. That means $a<b$ or $a=b$. Assume $a<b$. Then, by definition $a+c<b+c$. If $a=b$, then by adding $c$ we get $a+c=b+c$. Therefore, if $a\leq b$, then $a+c<b+c$ or $a+c=b+c$, that is $a+c\leq b+c$.

\item {\it If $a\leq b$ and $c\geq 0$, then $a c\leq b c$.} Let $a\leq b$ and $0\leq c$, i.e. $a<b$ or $a=b$ and $0<c$ or $0=c$. Assume $a<b$. If $0<c$, by definition, $a c<b c$. If $c=0$, then $a\cdot 0=0$ and $b\cdot 0=0$, so $a0=b0$, i.e. $a c=b c$. Assume $a=b$. Then, multiplying by $c$ gives us $a c=b c$. So, $a\leq b$ and $0\leq c$ implies $a c<b c$ or $a c=b c$, which means $a c\leq b c$.

\item {\it If $a<b$ and $c<0$, then $b c<a c$.} If $c<0$, then $0<-c$, by a previous proposition. That implies $a(-c)<b(-c)$, i.e. $-a c<-b c$. But, due to a previous proposition, that implies $b c<a c$.

\item {\it If $a+c<b+c$, then $a<b$.} Let $a+c<b+c$. Using $-c$ in the definition gives us $(a+c)-c<(b+c)-c$, which means $a+(c-c)<b+(c-c)$. That is equivalent to $a+0<b+0$, i.e. $a<b$.

\item {\it If $a c<b c$ and $c>0$ then $a<b$.} Let $a c<b c$ and $0<c$. Assume $a\geq b$. Then, as $c>0$ we have $a c\geq b c$. But, that is in contradiction with $a c<b c$. Therefore, it must be $a<b$.

\item {\it If $a<b$ and $c<d$, then $a+c<b+d$.} We have $a<b$ and $c<d$, i.e. $a-b<0$ and $0<d-c$, respectively. That implies $a-b<d-c$, and after adding $b+c$ we get $(a-b)+(b+c)<(d-c)+(b+c)$, which is due to associativity and commutativity equivalent to $a+c<b+d$.
\end{enumerate}

\noindent{\bf Problem.} Let $D$ be an ordered integral domain. Prove the following, for all $a$, $b$ and $c$ in $D$:

\begin{enumerate}
\item $a^2+b^2\geq 2a b$;
\item $a^2+b^2\geq a b$ and $a^2+b^2\geq -a b$;
\item $a^2+b^2+c^2\geq a b+b c+a c$;
\item $a^2+b^2>a b$, if $a^2+b^2\neq 0$;
\item $a+b<a b+1$, if $a,b>1$;
\item $a b+a c+b c+1<a+b+c+a b c$, if $a,b,c>1$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $a^2+b^2\geq 2a b$.} The square of any non-zero element in $D$ is positive. Assume $(a-b)\in D-\{0\}$. Then, $(a-b)^2>0$. That is equivalent to $a^2-2a b+b^2>0$. After adding $2a b$ on both sides, by definition, we have $a^2-2a b+b^2+2a b>0+2a b$, that is, $a^2+b^2>2a b$. Now, if $(a-b)=0$, then $(a-b)^2=0(a-b)$, i.e. $(a-b)^2=0$. From that we have $a^2-2a b+b^2=0$, which is $a^2+b^2=2a b$. Therefore, $a^2+b^2>2a b$ or $a^2+b^2=2a b$, so $a^2+b^2\geq 2a b$.

\item {\it $a^2+b^2\geq a b$ and $a^2+b^2\geq -a b$.} Assume $a b\geq 0$. Then, $a b+a b\geq a b$ and we have $2a b\geq a b$. Therefore, as $a^2+b^2\geq 2a b$ and $2a b\geq a b$, we have $a^2+b^2\geq a b$. Assume $a b<0$. As $a^2\geq 0$ and $b^2\geq 0$, then $a^2+b^2\geq 0$, by a previous problem. If $a^2+b^2>0$, then, as $0>a b$, we have $a^2+b^2>a b$. If $a^2+b^2=0$, then by substitution $a^2+b^2>a b$. So, $a^2+b^2\geq a b$. Assume $a b\geq 0$. Then, $0\geq -a b$ and we have $a b\geq -a b$. Assume $a b>0$. Then, $0>-a b$ and we have $a b>-a b$. Therefore $a b\geq -a b$, so $a^2+b^2\geq a b$ and $a b\geq -a b$ gets us $a^2+b^2\geq -a b$.

\item {\it $a^2+b^2+c^2\geq a b+b c+a c$.} We know that $a^2+b^2\geq 2a b$, $b^2+c^2\geq 2a b$ and $a^2+c^2\geq 2a c$. By previous problem, we can add those three inequalities to get $a^2+b^2+b^2+c^2+a^2+c^2\geq 2a b+2b c+2a c$, i.e. $2a^2+2b^2+2c^2\geq 2a b+2b c+2a c$. By distributive law that is equivalent to $2(a^2+b^2+c^2)\geq 2(a b+b c+a c)$. That is, again equivalent to $2\cdot (1(a^2+b^2+c^2))\geq 2\cdot (1(a b+b c+a c))$. Then, that is $(2\cdot 1)(a^2+b^2+c^2)\geq(2\cdot 1)(a b+b c+a c)$. As $2\cdot 1\in D$, and $0<1<2\cdot 1$, then $2\cdot 1>0$ implies that $a^2+b^2+c^2\geq a b+b c+a c$.

\item {\it $a^2+b^2>a b$, if $a^2+b^2\neq 0$.} Proof by contraposition. Let $a^2+b^2\leq a b$. But, by a previous proposition, we have $a^2+b^2\geq a b$. So it can only be $a^2+b^2=a b$. Then, $(a-b)^2\geq 0$ implies $a^2-2a b+b^2\geq 0$. That is, $a^2-2a^2-2b^2+b^2\geq 0$, by substitution, i.e. $-a^2-b^2\geq 0$. From that we have $-(a^2+b^2)\geq 0$. That can only be if $a^2+b^2=0$.

\item {\it $a+b<a b+1$, if $a,b>1$.} From $a>1$ and $b>1$ we have $a-1>0$ and $b-1>0$. So, we can multiply $a-1>0$ by $b-1$ to get $(a-1)(b-1)>0$. That is, $a b-a-b+1>0$. From that we have $a b+1>a+b$.

\item {\it $a b+a c+b c+1<a+b+c+a b c$, if $a,b,c>1$.} From $a>1$, $b>1$ and $c>1$ we have $a-1>0$, $b-1>0$ and $c-1>0$. We can multiply $a-1>0$ by $b-1$ and then by $c-1$ to get $(a-1)(b-1)(c-1)>0$. That is equivalent to $(a b-a-b+1)(c-1)>0$, that is $a b c-a b-a c+a-b c+b+c-1>0$ That means $a b c+a+b+c>a c+b c+a b+1$.
\end{enumerate}

\noindent{\bf Definition.} Let $D$ be an ordered integral domain. Then, for all $d\in D$ we define {\bf absolute value of $d$} as:

\begin{equation*}
|d|=\left\{\begin{array}{c}
d,\ d\geq 0,\\
-d,\ d<0.
\end{array}\right.
\end{equation*}

\noindent\newline{\bf Problem.} Let $D$ be an ordered integral domain and $a,b\in D$. Prove:

\begin{enumerate}
\item $|-a|=|a|$;
\item $a\leq|a|$;
\item $a\geq-|a|$;
\item If $b>0$, $|a|\leq b$ iff $-b\leq a\leq b$;
\item $|a+b|\leq|a|+|b|$;
\item $|a-b|\leq|a|+|b|$;
\item $|a b|=|a|\cdot|b|$;
\item $|a|-|b|\leq|a-b|$;
\item $||a|-|b||\leq|a-b|$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $|-a|=|a|$.} Assume $a\geq 0$. Then, $-a\leq 0$, so $|-a|=-(-a)=a$. Also, as $a\geq 0$, by definition $|a|=a$. Therefore $a\geq 0$ implies $|-a|=|a|$. Assume $a<0$. Then, $-a>0$ so $|-a|=-a$. Also, as $a<0$, then by definition $|a|=-a$ and we have $|a|=-a=|-a|$. In conclusion, $|-a|=|a|$.

\item {\it $a\leq|a|$.} Assume $a\geq 0$. Then $|a|=a$. Assume $a<0$. Then, $|a|=-a$. But, as $a<0$, then $0<-a$, and we have $0<|a|$ by substitution and $a<|a|$. In conclusion, we have $a=|a|$ or $a<|a|$ so $a\leq |a|$.

\item {\it $a\geq-|a|$.} Assume $a\geq 0$. Then, $|a|=a$, and, if we add $-(a+|a|)$ on both sides, we get $-|a|=-a$. But, we know that $a\geq 0$, and then $0\geq -a$, so $a\geq -a$. By substituting $-|a|$ for $-a$ we get $a\geq-|a|$. If $a<0$ then $|a|=-a$, and after adding $a-|a|$ on both sides, we have $a=-|a|$. Therefore, $a\geq-|a|$.

\item {\it If $b>0$, $|a|\leq b$ iff $-b\leq a\leq b$.} Let $b>0$. {\it Necessity.} Let $|a|\leq b$. Assume $a\geq 0$. Then, $|a|=a$ and we have $a\leq b$. But, also $b>0$, and, as $a\geq 0$, then, if $a>0$, we have $a+b>0$. If $a=0$, then $0+b>0$, i.e. $a+b>0$. From that we have $a>-b$. We can loosen our claim by saying $a\geq-b$. Therefore, if $a\geq 0$ we have $-b\leq a$ and $a\leq b$, which means $-b\leq a\leq b$. If $a<0$, then $|a|=-a$ and we have $-a\leq b$. That is equivalent to $-b\leq a$. Therefore, we have $-b\leq a$. But, as $a<0$ and $0<b$, we have $a<b$ and we can loosen that by saying $a\leq b$. Therefore, if $a<0$ we again obtain $-b\leq a\leq b$. {\it Sufficiency.} Let $-b\leq a$ and $a\leq b$. Assume $a\geq 0$. Then, $|a|=a$ and we have $|a|\leq b$. Assume $a<0$. Then we have $|a|=-a$. But, as $-b\leq a$ we have $-a\leq b$, from which we get $|a|\leq b$. Therefore, $|a|\leq b$.

\item {\it $|a+b|\leq|a|+|b|$.} Assume $a,b\geq 0$. Then, $a+b\geq 0$ and we have $|a+b|=a+b$. Also, as $a\geq 0$ we have $|a|=a$ and, as $b\geq 0$, we have $|b|=b$. So, $a+b=|a|+|b|$. Therefore, $|a+b|=|a|+|b|$. Assume $a\geq 0$ and $b<0$. Assume $a\geq -b$. Then, $a+b\geq 0$ and we have $|a+b|=a+b$. On the other hand, we have $|a|=a$ and $|b|=-b$. So, $|a|+|b|=a+(-b)$. As $b<0$ we have $0<-b$ so $b<-b$. After adding $a$ on both sides we get $a+b<a-b$, i.e. $|a+b|<|a|+|b|$. Assume $a<-b$. Then, $a+b<0$ and we have $|a+b|=-a-b$. Also, we have $|a|+|b|=a+(-b)$. But, as $a\geq 0$ and $0\geq -a$, then $a\geq -a$, so we have $a-b\geq-a-b$, which means $|a|+|b|\geq|a+b|$, i.e. $|a+b|\leq|a|+|b|$. The same proof goes for $a<0$ and $b\geq 0$, the difference is in notation. Assume $a,b<0$. Then, $a+b<0$ and we have $|a+b|=-(a+b)=-a-b$. We also have $|a|=-a$ and $|b|=-b$, so $|a|+|b|=-a-b$. And, from that we have $|a+b|=|a|+|b|$. In conclusion, we had $|a+b|=|a|+|b|$ or $|a+b|<|a|+|b|$ or $|a+b|\leq|a|+|b|$, which is actually $|a+b|\leq|a|+|b|$ by definition.

\item {\it $|a-b|\leq|a|+|b|$.} We have $|a+(-b)|\leq|a|+|-b|$, by previous problem. But, $|-b|=|b|$, so $|a+(-b)|\leq|a|+|b|$, i.e. $|a-b|\leq|a|+|b|$.

\item {\it $|a b|=|a|\cdot|b|$.} Assume $a,b\geq 0$. Then, $a b\geq 0$ and we have $|a b|=a b$. But, also $|a|=a$ and $|b|=b$, so, $|a b|=|a|\cdot|b|$. Assume $a\geq 0$ and $b<0$. Then, $-b>0$. If $a>0$, then $-a b>0$. Therefore, $a b<0$, and we have $|a b|=-a b$. On the other hand, $|a|=a$ because $a\geq 0$, and $|b|=-b$ as $b<0$. So, $|a|\cdot|b|=-a b$ and we have $|a b|=|a|\cdot|b|$. If $a=0$, then $|0 b|=|0|=0$ and $|0|\cdot|b|=0b=0$ and that is $|a b|=|a|\cdot|b|$. Assume $a,b<0$. Then, $-a>0$ and $-b>0$ so, $-a(-b)>0$, i.e. $a b>0$. From that we have $|a b|=a b$. Also, $|a|=-a$ and $|b|=-b$, so $|a|\cdot|b|=-a(-b)=a b$. Therefore, $|a b|=|a|\cdot|b|$ in this case, and in all cases (we only didn't check $a<0$ and $b\geq 0$, but that is analogous to the $a\geq 0$ and $b<0$ with a difference in notation).

\item {\it $|a|-|b|\leq|a-b|$.} Assume $a,b\geq 0$. Assume $a\geq b$. Then, $a-b\geq 0$ and we have $|a-b|=a-b$. Also, $|a|=a$ and $|b|=b$, so $|a|-|b|=a-b$. Thus, $|a-b|=|a|-|b|$. Assume $a<b$. Then, $a-b<0$ and $|a-b|=-(a-b)=b-a$. But, $|a|=a$ and $|b|=b$, so $|a|-|b|=a-b$. We know that $a-b<0$, so $0<b-a$, i.e. $a-b<b-a$ and we have $|a|-|b|<|a-b|$. Assume $a\geq 0$ and $b<0$. Then, $a\geq 0$ and $-b>0$, so $a-b>0$, i.e. $|a-b|=a-b$. Also, $|a|=a$ and $|b|=-b$ and we have $|a|-|b|=a+b$. We know that $b<0$ and $0<-b$ so $b<-b$ and, after adding $a$ on both sides, $a+b<a-b$. Then, $|a|-|b|<|a-b|$. Assume $a<0$ and $b\geq 0$. Then, $-a>0$ and $b-a>0$. From that we have $a-b<0$ and $|a-b|=b-a$. Also, $|a|=-a$ and $|b|=b$, and $|a|-|b|=-a-b$. Now, as $-b<b$, we have $-a-b<b-a$, i.e. $|a|-|b|<|a-b|$. Assume $a,b\leq 0$. Then, $-b\geq 0$. Assume $a\geq b$. Then, $a-b\geq 0$ and we have $|a-b|=a-b$. Also, $|a|=-a$ and $|b|=-b$ and we have $|a|-|b|=-a+b$. As $a-b\geq 0$, then $0\geq b-a$ and we have $a-b\geq b-a$, i.e. $|a-b|\geq|a|-|b|$, which is $|a|-|b|\leq|a-b|$. Assume $a<b$. Then, $a-b<0$ and we have $|a-b|=b-a$. Also, $|a|=-a$ and $|b|=-b$ and $|a|-|b|=-a+b$. In this case, $|a|-|b|=|a-b|$. That exhausts all possibilities, and in each case we have $|a|-|b|=|a-b|$ or $|a|-|b|<|a-b|$. By definition, that is $|a|-|b|\leq|a-b|$.

\item {\it $||a|-|b||\leq|a-b|$.} Assume $|a|\geq|b|$. Then, $|a|-|b|\geq 0$ and we have $||a|-|b||=|a|-|b|$ and by previous problem, $|a|-|b|\leq|a-b|$, so $||a|-|b||\leq|a-b|$, by substitution. Assume $|a|<|b|$. Then, $|a|-|b|<0$ and we have $||a|-|b||=|b|-|a|$. But, by previous problem, $|b|-|a|\leq|b-a|$. But, also $|b|-|a|\leq|-(a-b)|$. By a previous problem, $|-(a-b)|=|a-b|$, so $|b|-|a|\leq|a-b|$. Thus, we have $|b|-|a|\leq|a-b|$, i.e. $||a|-|b||\leq|a-b|$.

\end{enumerate}

\noindent{\bf Problem.} Let $A$ be a ring, $a,b\in A$ and $m,n\in\Z^{+}$. If $1\cdot a=a$ and $n\cdot a+a=(n+1)\cdot a$, prove:

\begin{enumerate}
\item $n\cdot(a+b)=n\cdot a+n\cdot b$;
\item $(n+m)\cdot a=n\cdot a+m\cdot a$;
\item $(n\cdot a) b=a(n\cdot b)=n\cdot(a b)$;
\item $m\cdot(n\cdot a)=(m n)\cdot a$;
\item $n\cdot a=(n\cdot 1_A)a$;
\item $(n\cdot a)(m\cdot b)=(n m)\cdot(a b)$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it $n\cdot(a+b)=n\cdot a+n\cdot b$.} Let $n=1$. Then, by assumption, $1\cdot(a+b)=a+b=1\cdot a+1\cdot b$. Then, if the formula is true for $n$, we have, for $n+1$, by definition $(n+1)\cdot(a+b)=n\cdot(a+b)+(a+b)$, and when using the assumption and basis, $n\cdot(a+b)+(a+b)=n\cdot a+n\cdot b+a+b=n\cdot a+a+n\cdot b+b=(n+1)\cdot a+(n+1)\cdot b$.

\item {\it $(n+m)\cdot a=n\cdot a+m\cdot a$.} Let $m=1$. Then, by definition, $(n+1)\cdot a=n\cdot a+a=n\cdot a+1\cdot a$. Assuming formula is true for $m$, we have $(n+(m+1))\cdot a=((n+m)+1)\cdot a=(n+m)\cdot a+a$. Using the assumption, we have $(n+m)\cdot a+a=n\cdot a+m\cdot a+a=n\cdot a+(m+1)\cdot a$.

\item {\it $(n\cdot a)b=a(n\cdot b)=n\cdot(a b)$.} Let $n=1$. Then, $(1\cdot a)b=a b=a(1\cdot b)=1\cdot(a b)$. If formulae are true for $n$, then $((n+1)\cdot a)b=(n\cdot a+a)b=(n\cdot a)b+a b$. From this we have, using the assumption of induction, $(n\cdot a)b+a b=a(n\cdot b)+a b=a(n\cdot b+b)=a((n+1)\cdot b)$. Also, $(n\cdot a)b+a b=n\cdot(a b)+a b=(n+1)\cdot a b$, by definition.

\item {\it $m\cdot(n\cdot a)=(m n)\cdot a$.} Let $m=1$. Then, $1\cdot(n\cdot a)=n\cdot a=(n\cdot 1)\cdot a$. If formula is true for $m$, then for $m+1$ we have $(m+1)\cdot(n\cdot a)=m\cdot(n\cdot a)+n\cdot a$, by definition. Using the assumption we have $m\cdot(n\cdot a)+n\cdot a=(m n)\cdot a+n\cdot a$. By previous problem, that is $(m n)\cdot a+n\cdot a=(m n+n)\cdot a=((m+1)n)\cdot a$.

\item {\it $n\cdot a=(n\cdot 1_A)a$.} By using the third problem, we have $n\cdot a=(n\cdot a)1_A=n\cdot(a 1_A)=n\cdot(1_A a)=(n\cdot 1_A) a$.

\item {\it $(n\cdot a)(m\cdot b)=(n m)\cdot(a b)$.} Let $m=1$. Then, $(n\cdot a)(1\cdot b)=(n\cdot a)b=n\cdot(a b)=(n\cdot 1)\cdot(a b)$. Assume formula is true for $m$. Then, for $m+1$, we have $(n\cdot a)((m+1)\cdot b)=(n\cdot a)(m\cdot b+b)$, by definition. Furthermore, by distributive law, $(n\cdot a)(m\cdot b+b)=(n\cdot a)(m\cdot b)+(n\cdot a)b$. By using assumption of induction and formula from the third problem, we have $(n m)\cdot(a b)+n\cdot(a b)$. By the second problem, we have $(n m)\cdot(a b)+n\cdot(a b)=(n m+n)\cdot(a b)=(n(m+1))\cdot(a b)$.

\end{enumerate}

\noindent{\bf Theorem.} Let $K\subseteq\Z^{+}$. Let:

\begin{enumerate}
\item $1\in K$.
\item For all $k\in K$, if $l\in K$, for all $l\in\Z^{+}\cap\{1,\ldots,k\}$, then $k\in K$.
\end{enumerate}

\noindent Then, $K=\Z^{+}$.

\noindent\newline{\bf Proof.} Let $k\in K$. Let $1\in K$ and let $K$ have a property that, if $l\in K$, for all $l<k$, then $k\in K$. Let $K'=\Z^{+}-K$. Assume that $K'\neq\emptyset$. Then, by the well-ordering property it has a least element $c$. As $1\in K$, then $1\notin K'$ so we must have $c>1$. Now, let us observe $c-d$, for all $d\in\{1,\ldots,c-1\}$. Then, $0<d<c$, so $c-d\in\Z^{+}$. Assume that $c-d\notin K$. If $c-d\notin K$, then $c-d\in K'$ and, as $c-d<c$, $c$ cannot be the least element. Therefore, $c-d\notin K'$ so it must be that $c-d\in K$. Thus, if we take $l=c-d$ and $k=c$, we have that $l\in K$ for all $l<k$. Therefore, by second property, $k\in K$, i.e. $c\in K$. But, then it cannot be that $c\in K'$ and $K'$ must be empty, implying $K'=\emptyset$, i.e. $\Z^{+}=K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (principle of strong induction).} If:

\begin{enumerate}
\item Statement $S_1$ is true.
\item If statement $S_i$ is true for every $i<k$, for some $k\in\Z^{+}$, then statement $S_k$ is true.
\end{enumerate}

\noindent Then, $S_n$ is true for all $n\in\Z^{+}$.

\noindent\newline{\bf Proof.} Let $S_1$ be true. Let $K$ be the set of all integers for which the statement is true. Then, $1\in K$. But, if statement is true for all $i<k$, where $k\in\Z^{+}$, then $S_k$ is true. Therefore, if $i\in K$, for all $i<k$, then $k\in K$. So, by previous theorem, $\Z^{+}=K$, that is $S_n$ is true for all $n\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Every ideal of $\Z$ is principal.

\noindent\newline{\bf Proof.} Let $J\trianglelefteq\Z$. We must show that for all $y\in J$ there exists $a\in J$ such that $y=x a$ for some $x\in J$. $J\cap\Z^{+}=\emptyset$. Now, if there are no negative elements, at least $0\in J$, i.e. $J=\{0\}$ so we can say $J=\cyc{0}$. If there are negative elements in $J$, then it cannot be that $J\cap\Z^{+}=\emptyset$, because if $x\in J$ and $x<0$, then also $-x\in J$, as $J$ is closed with respect to negatives and, as $-x\in\Z^{+}$ and $-x\in J$, we have $J\cap\Z^{+}\neq\emptyset$. Therefore, by the well-ordering property, as $J\cap\Z^{+}\subseteq\Z^{+}$ and $J\cap\Z^{+}$ is non-empty, there exists $a\in J\cap\Z^{+}$ such that $a<x$, for all $x\in J\cap\Z^{+}$. Take any $y\in J$. By division with remainder theorem, there exist $q,r\in\Z$ such that $y=a q+r$, where $0\leq r<|a|$. Note that $|a|=a$, as $a\in\Z^{+}$. As $q\in\Z$ and $a\in J$, then as $J\trianglelefteq A$, we have $a q\in J$. Also, as $y\in J$ and $a q\in J$, then $y-a q\in J$. Therefore, $r\in J$. Assume, $0<r$, so $r\in\Z^{+}$ and $r\in J\cap\Z^{+}$. But, we said that $a$ is the least element and we have $r<a$, which is a contradiction. Therefore, it must be that $r=0$ and we have $y=a q$. Therefore, $J=\cyc{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m\in\Z$. Then, $m|1$ if and only if $m$ is invertible.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $m|1$. That means that there exists $q\in\Z$ such that $1=m q$. That is, of course, due to $\Z$ being commutative, equivalent to $1=q m$. Denoting $q$ as $m^{-1}$ clarifies that $m$ is invertible, as then $1=m m^{-1}=m^{-1} m$. {\it Sufficiency.} Assume $m$ is invertible. Then there exists $m^{-1}\in\Z$ such that $m m^{-1}=m^{-1}m=1$. But, that implies that $m|1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} In $\Z$, the only invertible elements are $1$ and $-1$.

\noindent\newline{\bf Proof.} Assume $m\in\Z$ is invertible, and that $m\neq\pm 1$. Then, by previous proposition, $m|1$, so there exists $q\in\Z$ such that $m q=1$. Let us observe the nature of $m$ and $q$. We know that $m,q\neq 0$ as then it would be $m q=0$. We also know that either $m>0$ and $q>0$ or $m<0$ and $q<0$, because $1>0$, i.e. $m q>0$. In the first case, either $q=1$ or $q>1$. If $q=1$, then $m q=m$. But, as $m q=1$, then $m=1$, which is a contradiction. If $q>1$, then $m q>m$, but as $m q=1$, we have $1>m$. But, as $m>0$ and $m\neq 0$, this is impossible. In the second case, if $m<0$ and $q<0$, then, as $q\neq 0$, it must be either $q=-1$ or $q<-1$. If $q=-1$, then $m q=-m$, i.e. $1=-m$, so $m=-1$, which is a contradiction to assumption that $m\notin\{1,-1\}$. If $q<-1$, then, as $-m>0$, we have $(-m)q<-1(-m)$, i.e. $-m q<-m$. As $m q=1$, then $-m q=-1$ and we have $-1<-m$, which is equivalent to $m>1$, which is a contradiction to assumption that $m>0$. Therefore, if $m$ is invertible, then $m\in\{1,-1\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If $r,s\in\Z$ such that $r|s$ and $s|r$, we say that $r$ and $s$ are {\bf associates} in $\Z$.

\noindent\newline{\bf Proposition.} Let $r,s\in\Z$. If $r$ and $s$ are associates in $\Z$, then $r=\pm s$.

\noindent\newline{\bf Proof.} Let $r|s$ and $s|r$. Then there exist $q_1,q_2\in\Z$ such that $s=q_1 r$ and $r=q_2 s$. Multiplying the first equality with $q_2$ gives us $s q_2=q_1 r q_2$. That is equivalent to $r=q_1 r q_2$. As $\Z$ is an integral domain, that is equivalent to $1=q_1 q_2$. As $\Z$ is commutative, then $1=q_1 q_2=q_2 q_1$, i.e. $q_1$ and $q_2$ are invertible. So, $q_1,q_w=\pm 1$. Therefore, $r=\pm s$ and $s=\pm r$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m\in\Z-\{0\}$. Then $\cyc{m}$ is a prime ideal of $\Z$ if and only if $m$ is prime.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $\cyc{m}$ is a prime ideal of $\Z$ and $m$ is not a prime. Then, there exist $m_1,m_2\in\Z-\{-1,1\}$ such that $m=m_1 m_2$. Now, as $m_1 m_2=m\in\cyc{m}$ we have that $m_1\in\cyc{m}$ or $m_2\in\cyc{m}$. Assume $m_1\in\cyc{m}$. But, that means that there exists $k\in\Z$ such that $m_1=k m$, meaning $m|m_1$. So, from $m_1|m$ and $m|m_1$, we conclude that $m_1=\pm m$. That would imply $m=\pm m m_2$, and as $\Z$ is an integral domain, $1=\pm m_2$, which is a contradiction. Therefore, $m$ is a prime number. {\it Sufficiency.} Let $m$ be a prime number. Then, $\cyc{m}=\{x m:\ x\in\Z\}$. Take $y_1 y_2\in\cyc{m}$. Then, there exists $k\in\Z$ such that $y_1 y_2=m k$. Now, we have that $m|(y_1 y_2)$. Assume $m\nmid y_1$ and $m\nmid y_2$. Then, as $m$ is a prime number, it's only divisors are $\pm 1$ and $\pm m$. If it were $\gcd{m,y_1}=m$, we would have $m|y_1$, a contradiction. So, $\gcd{m,y_1}=1$ and by Euclid's lemma $m|y_2$, which is a contradiction. Therefore, $m|y_1$ or $m|y_2$. Without loss of generality, assume $m|y_1$. Then, there exists $q\in\Z$ such that $y_1=q m$. But, that means that $y_1\in\cyc{m}$. If we assumed $m|y_2$, we would, in the same way, get $y_2\in\cyc{m}$. Therefore, $y_1 y_2\in\cyc{m}$ implies $y_1\in\cyc{m}$ or $y_2\in\cyc{m}$, so $\cyc{m}\primeideal\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every prime ideal of $\Z$ is maximal.

\noindent\newline{\bf Proof.} Assume $J\primeideal\Z$ and that there exists $K\trianglelefteq\Z$ such that $J\subset K$ and $K\neq\Z$. As $J,K\trianglelefteq\Z$, they are principal and prime, i.e. $J=\cyc{p_1}$ and $K=\cyc{p_2}$, for some $p_1,p_2\in P$. As $J\subset K$, then $p_1\in J$ implies $p_1\in K$, so there exists $k\in\Z$ such that $p_1=k p_2$. But, that would imply $p_2|p_1$, which is possible only if $p_2=p_1$ or $p_2=1$. But, $p_2=p_1$ would imply $\cyc{p_1}=\cyc{p_2}$, i.e. $J=K$, which is a contradiction to $J\subset K$. Then, $p_2=1$ would imply $K=\cyc{p_2}=\cyc{1}=\Z$, which is a contradiction to $K\neq\Z$. Therefore, $J$ is maximal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m\in\Z$. Then, $\cyc{m}=m\Z$.

\noindent\newline{\bf Proof.} Let $x\in\cyc{m}$. Then, there exists $k\in\Z$ such that $x=k m$. But, $m k\in m\Z$, so $x\in m\Z$ and $\cyc{m}\subseteq m\Z$. Now, if we take $x\in m\Z$, then there exists $k\in\Z$ such that $x=m k$. But, $m k\in\cyc{m}$, so $m\Z\subseteq\cyc{m}$ and from that we have $m\Z=\cyc{m}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p\in P$. Then, $\zmod{p}$ is a field.

\noindent\newline{\bf Proof.} Let $p\in P$. Then, $\zmod{p}=\Z\slash\cyc{p}$, by a previous proposition. Also, by a previous proposition, as $p\in P$, $\cyc{p}$ is a prime ideal. Therefore, by a previous proposition, it is a maximal ideal, and by a previous theorem, $\Z\slash\cyc{p}$ is a field.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m,n\in\Z$. Then, $\cyc{m}\cap\cyc{n}=\cyc{\lcm{m,n}}$.

\noindent\newline{\bf Proof.} Let $x\in\cyc{m}\cap\cyc{n}$. Then there exist $k_1,k_2\in\Z$ such that $x=m k_1$ and $x=n k_2$. From that we have that $m|x$ and $n|x$, so $x$ is a common multiple of $m$ and $n$. Therefore, $x=k\lcm{m,n}$, for some $k\in\Z$ and $x\in\cyc{\lcm{m,n}}$. That implies $\cyc{m}\cap\cyc{n}\subseteq\cyc{\lcm{m,n}}$. Now, let $x\in\cyc{\lcm{m,n}}$. Then there exists $k\in\Z$ such that $x=k\lcm{m,n}$. So, as $m|\lcm{m,n}$ and $n|\lcm{m,n}$, also $m|x$ and $n|x$, and there exist $k_1,k_2\in\Z$ such that $x=m k_1$ and $x=n k_2$, which implies $x\in\cyc{m}$ and $x\in\cyc{n}$. That is, $x\in\cyc{m}\cap\cyc{n}$ and $\cyc{\lcm{m,n}}\subseteq\cyc{m}\cap\cyc{n}$. Therefore, $\cyc{m}\cap\cyc{n}=\cyc{\lcm{m,n}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every homomorphic image of $\Z$ is isomorphic to $\zmod{n}$, for some $n\in\Z$.

\noindent\newline{\bf Proof.} Let $A$ be a homomorphic image of $\Z$. Then, there exists a homomorphism $f:\Z\rightarrow A$. By FHT, $A\cong\Z\slash\ker{f}$. But, $\ker{f}\trianglelefteq\Z$, and by a previous proposition, $\ker{f}$ is a principal ideal, i.e. there exists $n\in\Z$ such that $\ker{f}=\cyc{n}$. But, then $A\cong\Z\slash\ker{f}$ is equivalent to $A\cong\Z\slash\cyc{n}$, which is in turn eqivalent to $A\cong\zmod{n}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group and $a,b\in G$. Then, $\{n\in\Z:\ a b^n=b^n a\}\trianglelefteq\Z$.

\noindent\newline{\bf Proof.} Let $S=\{n\in\Z:\ a b^n=b^n a\}$. By definition, $S\subseteq\Z$. Let $m,n\in S$. Then, $a b^m=b^m a$ and $a b^n=b^n a$. Multiplying first equality on the right by $b^n$ gives us $a b^m b^n=b^m a b^n$. But, $a b^n=b^n a$, so $a b^{m+n}=b^m b^n a$, i.e. $a b^{m+n}=b^{m+n} a$, so $m+n\in S$. Now, multiplying first equality on the right by $b^{-m}\in G$ gives us $a b^m b^{-m}=b^m a b^{-m}$. We multiply that equality again on the left by $b^{-m}$ and get $b^{-m} a=b^{-m} b^n a b^{-m}$, which is $b^{-m} a=a b^{-m}$, so $-m\in S$. Finally, if $k\in\Z^{+}$, we will prove that $a b^{m k}=b^{m k} a$. For $k=1$, we have $a b^{m 1}=a b^m=b^m a=b^{m 1} a$, so $1\in S$. Then, assume the statement holds for $k$, i.e. $a b^{m k}=b^{m k} a$. We will prove that it holds for $k+1$. We have $a b^{m(k+1)}=a b^{m k+m}=a b^{m k} b^m$. By assumption of induction, $a b^{m k} b^m=b^{m k} a b^m=b^{m k} b^{m}a=b^{m k+m}a=b^{m(k+1)} a$. Therefore, the equality is true for all $k\in\Z^{+}$. If $k<0$, then we can take $k=-k'$, so that $k'>0$. We have $a b^{m k}=a b^{-m k'}=a (b^{-m})^{k'}$. But, as also $a b^{-m}=b^{-m}a$, as proved just a moment ago, then $a(b^{-m})^{k'}=(b^{-m})^{k'}a=b^{-m k'}a=b^{m k}a$. If $k=0$, then $a b^0=a e=e a=b^0 a$. Therefore, $a b^{m k}=b^{m k} a$ is true for all $k\in\Z$, so $m k\in S$. Now, as $n\in S\subseteq\Z$, it is also true that $a b^{m n}=b^{m n} a$ (which can be obtained in the same way as above). In conclusion, $S\trianglelefteq\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $G$ be a group, $H\leq G$ and $a\in G$. Then, $\{n\in\Z:\ a^n\in H\}\trianglelefteq\Z$.

\noindent\newline{\bf Proof.} Let $S=\{n\in\Z:\ a^n\in H\}$. By definition, $S\subseteq\Z$. Let $m,n\in\Z$. Then, $a^m,a^n\in H$. But, as $H\leq G$, we also have $a^m a^n\in H$, i.e. $a^{m+n}\in H$, and $a^{-m}\in H$. So, $m+n\in\Z$ and $-m\in\Z$. Let $k\in\Z$. Then, $a^{m k}=(a^m)^k$. But, $a^m\in H$ and so $(a^m)^k\in H$ and we have $k\in S$, for all $k\in\Z$ (and so also for $n$). Therefore, $S\trianglelefteq\Z$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m,n\in\Z$. Then, $\cyc{m}+\cyc{n}=\cyc{\gcd{(m,n)}}$.

\noindent\newline{\bf Proof.} Let $m k+n l\in\cyc{m}+\cyc{n}$ and $g=\gcd{(m,n)}$. Then, there exists $g'\in\Z$ such that $m k+n l=g'$. As $g|m$ and $g|n$, we have that there exist $m',n'\in\Z$ such that $m=m'g$ and $n=n'g$. Therefore, $g'=m k+n l=m'g k+n'g l$, i.e. $g'=g(m'k+n'l)$. If we take $x=m'k+n'l$ then $g'=\gcd{(m,n)} x$, where $x\in\Z$. Therefore, $g'\in\cyc{\gcd{(m,n)}}$ and $\cyc{m}+\cyc{n}\subseteq\cyc{\gcd{(m,n)}}$. Take $k\gcd{(m,n)}\in\cyc{\gcd{(m,n)}}$. By Bezout's lemma, there exist $x,y\in\Z$ such that $\gcd{(m,n)}=m x+n y$. Multiplying that equality by $k$ we get $k\gcd{(m,n)}=m(x k)+n(y k)$. So, $m(x k)\in\cyc{m}$ and $n(y k)\in\cyc{n}$, therefore $m(x k)+n(y k)\in\cyc{m}+\cyc{n}$, and that is $k\gcd{(m,n)}\in\cyc{m}+\cyc{n}$, from which we get $\cyc{\gcd{(m,n)}}\subseteq\cyc{m}+\cyc{n}$ and conclude that $\cyc{m}+\cyc{n}=\cyc{\gcd{(m,n)}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Notes on classification of integral domains}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be an integral domain and let $a,b\in A$. If there exists $q\in A$ such that $b=q a$, we say that $a$ {\bf divides} $b$ and symbolize that by writing $a|b$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain. Then, for all $a\in A-\{0\}$:

\begin{enumerate}
\item $a\mid 0$;
\item $0\nmid a$.
\end{enumerate}

\noindent{\bf Proof.} Let $a\in A-\{0\}$. {\it Ad $1$.} We have $0=a0$, so $a|0$. {\it Ad $2$.} Assume $0\mid a$. Then, there exists $q\in A$ such that $a=0 q$. But, that implies $a=0$, which is a contradiction to $a\neq 0$. Therefore, $0\nmid a$ for all $a\in A-\{0\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be an integral domain and $a\in A-\{0\}$. If $a|1$ then we say that $a$ is a {\bf unit}. We denote by $\unitset{A}$ as the set of all units in $A$, that is:

\begin{equation*}
\unitset{A}=\{a\in A-\{0\}:\ a|1\}.
\end{equation*}

\noindent\newline{\bf Theorem.} Every finite integral domain is a field.

\noindent\newline{\bf Proof.} Just notice that, if $A=\{0,1\}$ is an integral doman, then $A$ is a trivial field, every $a\in A-\{0\}$, which is only unity, has an inverse, and is commutative with respect to multiplication. Now, let $A=\{0,1,a_1,\ldots,a_m\}$ be a finite integral domain and $m\in\Z^{+}$. Then, $|A|=m+2$. Let $f_i:A-\{0\}\rightarrow A-\{0\}$ be a mapping defined with $f_i(x)=a_i x$, for all $i\in\{1,\ldots,m\}$ (we do not define it for $0$ and $1$ - zero is not inverible anyway, and $1$ is always invertible, it is its own inverse). Then, it is obvious that $f_i$ is a function, because $a_i x\in A-\{0\}$, for all $a_i,x\in A-\{0\}$ (as $A$ is an integral domain, it has no zero divisors, so $a_i x\neq 0$). Also, if $x=y$, then, multiplying by $a_i$, we have $a_i x=a_i y$, i.e. $f_i(x)=f_i(y)$. Thus, uniqueness is also satisfied. Then, if $a_i x=a_i y$, as $a_i\neq 0$ we have $x=y$, so $f_i$ is injective. Therefore, $\ran{f_i}=\{a_i x:\ x\in A-\{0\}\}=\{a_1 1, a_i a_1, a_i a_2,\ldots,a_i a_m\}$. Assume that $a_i a_j=a_i a_k$, for some $j,k\in\{1,\ldots,m\}$, $j\neq k$, and also allowing $a_j=1$. That would imply, as $A$ is an integral domain, that $a_j=a_k$, for some $j\neq k$, and that would be a contradiction that $\{1,a_1,\ldots,a_m\}$ are distinct. Thus, $|\ran{f_i}|=|A-\{0\}|$. As $\ran{f_i}\subseteq A-\{0\}$, that, and the latter, implies $A-\{0\}=\ran{f_i}$, i.e. $f_i$ is surjective. Then, it is also bijective. So, if we $a_i\in A$, and if we take $1\in A-\{0\}$, there must exist $a_j\in A$ such that $f_i(a_j)=1$, i.e. $a_i a_j=1$. Thus, every $a_i\in A-\{0\}$, for $i\in\{1,\ldots,m\}$ has an inverse (as does $1\in A$) and $A$ is a field.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} As we now can classify all finite integral domains as field, in the further classification (and only in this chapter), we will conduct proofs for fields and infinite integral domains separately.

\noindent\newline{\bf Definition.} Let $A$ be an integral domain and $p\in A-\{0\}$. We say that $p$ is a {\bf prime} in $A$ if $\cyc{p}$ is a prime ideal.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain and let $p$ be a prime in $A$. Then, if $p|a b$, for some $a,b\in A-\{0\}$, then $p|a$ or $p|b$.

\noindent\newline{\bf Proof.} Let $p|a b$. Then, $a b=p q$, for some $q\in A$. That implies that $a b\in\cyc{p}$, and as $p$ is prime, $\cyc{p}$ is a prime ideal. Thus, $a\in\cyc{p}$ or $b\in\cyc{p}$. Equivalently, there exists $r\in A$ such that $a=p r$ or $b=p r$. In other words, $p|a$ or $p|b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent\newline{\bf Definition.} Let $A$ be an integral domain and $a\in A-\unitset{A}$, $a\neq 0$. If $a=p q$, for some $p,q\in A$, implies $p\in\unitset{A}$ or $q\in\unitset{A}$, we say that $a$ is {\bf irreducible} in $A$. If that is not the case, we say that $a$ is {\bf reducible} in $A$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain and $p\in A$ a prime in $A$. Then $p$ is irreducible.

\noindent\newline{\bf Proof.} Let $p\in A$ be a prime and assume that $p=q r$, for some $q,r\in A$. Then, as $p$ is a prime, $\cyc{p}$ is a prime ideal. From that we have $\cyc{p}=\cyc{q r}$, so $q r\in\cyc{p}$. That implies $q\in\cyc{p}$ or $r\in\cyc{p}$. If $q\in\cyc{p}$, then $q=p s$, for some $s\in A$. From that we have $q r=p r s$, and as $q r=p$, that implies $p=p r s$. As $p\neq 0$ and $A$ is an integral domain, $p\cdot 1=p(r s)$ implies $r s=1$, i.e. $r\in\unitset{A}$. In other words, $p$ is irreducible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be an integral domain and $p,q\in A$. If $p=u q$, for some $u\in\unitset{A}$, then we say that $p$ is {\bf associate} of $q$ in $A$.

\noindent\newline{\bf Definition.} Let $A$ be an integral domain. We say that $A$ is an {\bf unique factorization domain} (or UFD for short) if:

\begin{enumerate}

\item There exist irreducibles $p_1,\ldots,p_m\in A$, for some $m\in\Z^{+}$, and a unit $u\in\unitset{A}$ such that $a=u p_1 p_2\cdots p_m$, for all $a\in A-\unitset{A}$, $a\neq 0$.

\item From $a=u p_1 p_2\cdots p_m$ and $a=v q_1 q_2\cdots q_n$, where $p_1,\ldots,p_m,q_1,\ldots,q_n$ are irreducibles in $A$ with $m,n\in\Z^{+}$ and $u,v\in\unitset{A}$, follows that $m=n$, $u=v$ and that there exists a permutation $\rho$ on $\{1,\ldots,m\}$ such that $p_{i}=q_{\rho(i)}$, for all $i\in\{1,\ldots,m\}$.

\end{enumerate}

\noindent{\bf Proposition.} Every field is a UFD.

\noindent\newline{\bf Proof.} Let $F$ be a field. Take $a\in F-\{0\}$. Then, as $F$ is a field, there exists $a^{-1}\in F-\{0\}$ such that $a a^{-1}=a^{-1} a=1$. From that we have that $a|1$, i.e. $F-\{0\}\subseteq\unitset{F}$. Also, by definition, $\unitset{F}\subseteq F-\{0\}$, so we have $F-\{0\}=\unitset{F}$. That implies that $(F-\{0\})-\unitset{F}=\emptyset$, so the conditions for UFD vacuously hold (as there are no elements on which to check them).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a UFD and $p\in A$. Then, $p$ is prime if and only if it is irreducible.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $p$ be a prime in $A$. Then, as UFD $A$ is by definition an integral domain, by previous proposition, which states that if $p$ is prime in integral domain, it is irreducible, we have that $p$ is irreducible in $A$.

{\it Sufficiency.} Let $p$ be irreducible. Let $a b\in\cyc{p}$. We must prove that $a\in\cyc{p}$ or $b\in\cyc{p}$. As $a b\in\cyc{p}$, and $\cyc{p}$ is principal ideal, we have $a b=p q$, for some $q\in A$. But, as $A$ is a UFD, then, $q=u q_1\cdots q_m$, $a=v a_1\cdots a_k$ and $b=w b_1\cdots b_l$, where $q_i$, $a_j$ and $b_n$ are irreducible for all $i\in\{1,\ldots,m\}$, $j\in\{1,\ldots,k\}$ and $n\in\{1,\ldots,l\}$, and $u,v,w\in\unitset{A}$. Then, $(v w)a_1\cdots a_k b_1\cdots b_l=u p q_1\cdots q_m$ and we may write that same expression as $u x_1\cdots x_{k+l}=v y_1\cdots y_{m+1}$, where $x_i=a_i$ for all $i\in\{1,\ldots,k\}$, $x_{k+i}=b_i$, for all $i\in\{1,\ldots,l\}$, $y_1=p$ and $y_{i+1}=q_i$, for all $i\in\{1,\ldots,m\}$. As $A$ is a UFD, it also follows that $k+l=1+m$, $v w=u$, and that there exists a permutation $\rho$ on $\{1,\ldots,m+1\}$ such that $x_i=y_{\rho(i)}$. As $\rho$ is a permutation, it is a bijection and has an inverse such that $x_{\rho^{-1}(i)}=y_{i}$. Specifically, as $p=y_1$ (without loss of generality), we have $p=x_{\rho^{-1}(1)}$, and equivalently, $p=x_{\rho^{-1}(1)}$. Without loss of generality, we can take $p=y_1=x_{\rho^{-1}(1)}=a_1$. So, $p=a_1$, which implies $a_1\in\cyc{p}$. As $a_2\cdots a_k\in A$, and $\cyc{p}\trianglelefteq A$ (principal ideal is an ideal), we have $a_1 a_2\cdots a_k\in\cyc{p}$, i.e. $a\in\cyc{p}$. Thus, $\cyc{p}$ is a prime ideal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be an integral domain and $a,b\in A-\{0\}$. If there exists $d\in A$ such that $d|a$ and $d|b$, we say that $d$ is a {\bf common divisor} of $a$ and $b$.

\noindent\newline{\bf Definition.} Let $A$ be an integral domain and $a,b\in A$. Let $G$ be a set such that $g\in G$ if and only if $g$ is a common divisor of $a$ and $b$ such that $g'|a$ and $g'|b$ implies $g'|g$, for all $g'\in A$. If there exists $g\in G$ such that $u\nmid g$, for all $u\in\unitset{A}-\{1\}$, we say that $g$ is a {\bf greatest common divisor} of $a$ and $b$ and symbolize that by writing $g=\gcd{(a,b)}$.

\noindent\newline{\bf Proposition.} Let $A$ be a UFD and $a,b\in(A-\{0\})-\unitset{A}$. Then, there exists a unique $g\in A$ such that $g=\gcd{(a,b)}$.

\noindent\newline{\bf Proof.} {\it Existence.} Let $p_1,\ldots,p_k$ be irreducibles in $A$ such that $p_i|a$ and $p_i|b$, for all $i\in\{1,\ldots,k\}$. Then, $a=u p_1^{r_1}\cdots p_k^{r_k}$ and $b=v p_1^{s_1}\cdots p_k^{s_k}$, where $r_1,\ldots,r_k\in\Z^{+}_0$ and $s_1,\ldots,s_l\in\Z^{+}_0$, for some $k\in\Z^{+}$ and $u,v\in\unitset{A}$. Take $m_i=\min{\{r_i,s_i\}}$, for all $i\in\{1,\ldots,k\}$. Then, it is obvious that $p_i^{m_i}|p_i^{r_i}$ and $p_i^{m_i}|p_i^{s_i}$, for all $i\in\{1,\ldots,k\}$. That implies that $p_i^{m_i}|a$ and $p_i^{m_i}|b$, for all $i\in\{1,\ldots,k\}$. If we take $g=p_1^{m_1}\cdots p_k^{m_k}$, then $g|a$ and $g|b$. Also $u\nmid g$, for all $u\in\unitset{A}-\{1\}$. Then, assume $g'|a$ and $g'|b$. That means that $g'=w p_1^{h_1}\cdots p_k^{h_k}$, where it must be that $p_i^{h_i}|p_i^{r_i}$ and $p_i^{h_i}|p_i^{s_i}$. That implies that $h_i\leq m_i$, for all $i\in\{1,\ldots,k\}$. From that again follows that $p_i^{h_i}|p_i^{m_i}$, for all $i\in\{1,\ldots,k\}$. From that we have $p_i^{m_i}=z_i p_i^{h_i}$, where $z_i=p_i^{c_i}$, for some $c_i\in\Z^{+}_0$, and then we have $g=w^{-1}(z_1\cdots z_2)(w(p_1^{h_1}\cdots p_k^{h_k}))=w^{-1}Z g'$, where $Z=z_1,\ldots,z_k$. That implies that $g'|g$ and it must be that $g=\gcd{(a,b)}$.

{\it Uniqueness.} Assume that there exists $g'\in A$ such that $g'=\gcd{(a,b)}$. That implies that $u\nmid g'$, for all $u\in\unitset{A}-\{1\}$, $g'|a$, $g'|b$, and $g''|a$ with $g''|b$ implies $g''|g'$ for all $g''\in A$. But, as $g|a$ and $g|b$ then it must be that $g|g'$. Also, as $g=\gcd{(a,b)}$, we have $g'|g$, due to $g'|a$ and $g'|b$. Therefore, from $g'|g$, we have that there exists $u\in A$ such that $g=u g'$. From $g|g'$, we have that there exists $v\in A$ such that $g'=v g$. Then, multiplying the former equality with $v$ gives us $g v=u v g'$, i.e. $g'=u v g'$. As $A$ is a UFD, and from that by definition an integral domain, and as $g'\neq 0$, then $1 g'=(u v)g'$ implies $1=u v$, i.e. $u,v\in\unitset{A}$. But, as $u'\nmid g$, for all $u'\in\unitset{A}-\{1\}$ and $v'\nmid g'$, for all $v'\in\unitset{A}-\{1\}$, by defition, then it must be that $u=v=1$ and $g=g'$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be an integral domain. If every ideal of $A$ is principal, we say that $A$ is a {\bf principal ideal domain} (or PID for short).

\noindent\newline{\bf Lemma.} Let $A$ be an integral domain and $a,b\in A-\{0\}$ such that $a=b q$ for some $q\in A$. Then, $q\neq 0$.

\noindent\newline{\bf Proof.} Assume $q=0$. Then, $a=b\cdot 0=0$, which is a contradiction to $a\neq 0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $A$ be an integral domain and $a,b\in A-\{0\}$. If $a|b$ and $b|a$ then $a=b$.

\noindent\newline{\bf Proof.} As $a|b$, there exists $p\in A$ such that $b=p a$. As $b|a$, there exists $q\in A$ such that $a=q b$. From $b=p a$, after multiplying equality by $q$, we have $b q=p a q$. From that and $a=q b$, we have $a=p a q$, that is $a=p q a$. As $A$ is an integral domain, by cancellation law, that implies $1=p q$. As $a,b\neq 0$, then also $p,q\neq 0$ by previous lemma, we can apply cancellation law. From $1\cdot 1=p q$ (also note that $A$ is not trivial by definition of an integral domain so $1\neq 0$) we have $p=1$. Then, $1=1\cdot q$, which implies $q=1$. Thus, $a=b\cdot 1$ and $b=a\cdot 1$, i.e. $a=b$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be a principal ideal domain. Then, for all $a,b\in A-\{0\}$:

\begin{enumerate}
\item There exists a unique $g\in A$ such that $g=\gcd{(a,b)}$.
\item If $g=\gcd{(a,b)}$, there exist $p,q\in A$ such that $\gcd{(a,b)}=a p+b q$ ({\it Bezout's lemma}).
\end{enumerate}

\noindent{\bf Proof.} Let $a,b\in A$ and $J=\{a x+b y:\ x,y\in A\}$. Let $a x_1+b y_1, a x_2+b y_2\in J$. Obviously, $J\subseteq A$. Then, $(a x_1+b y_1)+(a x_2+b y_2)=a(x_1+x_2)+b(y_1+y_2)\in J$ and $(a x_1+b y_1)(a x_2+b y_2)=a^2 x_1 x_2+a x_1 b y_2+b y_1 a x_2+b^2 y_1 y_2=a(a x_1 x_2+x_1 b y_2)+b(y_1 a x_2+b y_1 y_2)\in J$. Also, if $c\in A$, then $c(a x_1+b y_1)=(a x_1+b y_1) c=a(x_1 c)+b(y_1 c)\in J$. Thus, $J\trianglelefteq A$, and that implies that $J$ is a principal ideal of $A$. In other words, $J$ is generated by some $g\in A$ which means $J=\cyc{g}$. As $a=a\cdot 1+b\cdot 0\in\cyc{g}$ and $b=a\cdot 0+b\cdot 1\in\cyc{g}$, then there exist $s,t\in A$ such that $a=g s$ and $b=g t$. That implies $g|a$ and $g|b$. Also, as $g\in\cyc{g}=J$, there exist $p,q\in A$ such that $g=a p+b q$.

Assume that there exists $g'\in A$ such that $g'|a$ and $g'|b$. Then there exist $s',t'\in A$ such that $a=g's'$ and $b=g't'$. From $g=a p+b q$ we have $g=g's'p+g't'q$, i.e. $g=g'(s'p+t'q)$. From that follows $g'|g$ and by definition $g=\gcd{(a,b)}$. Assume that $h=\gcd{(a,b)}$. Then, $h|a$ and $h|b$, so, as $g=\gcd{(a,b)}$, we have $g|h$. Also, as $h$ is a greatest common divisor of $a$ and $b$, from $g|a$ and $g|b$ we have $g|h$. From $g|h$ and $h|g$ we have, by a previous lemma, $g=h$. So, greatest common divisor is unique.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Rings of polynomials}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be a commutative ring with unity, $a_0,a_1,\ldots,a_n\in A$ and $x$ an arbitrary symbol. Every expression of the form:

\begin{equation*}
a_0+a_1 x+a_2 x^2+\cdots+a_n x^n
\end{equation*}

\noindent\newline is called a {\bf polynomial in $x$ with coefficients in $A$}, or more simply, a {\bf polynomial in $x$ over $A$}. The expressions $a_k x^k$, for $k\in\{1,\ldots,n\}$ are called the {\bf terms} of the polynomial. The coefficient $a_0$ is called the {\bf zero term} of the polynomial.

\noindent\newline{\bf Remark.} The set of all polynomials in $x$ with coefficients in $A$ is defined as:

\begin{equation*}
A[x]=\{a_0+a_1 x+\cdots+a_n x^n:\ (\forall i\in\{0,\ldots,n\})(a_i\in A)\wedge(n\in\Z^{+}_0)\}.
\end{equation*}

\noindent\newline Then, we denote $a(x)=a_0+a_1 x+\cdots+a_n x^n$. If $a_i\in A$, for all $i\in\{0,\ldots,n\}$, then $a(x)\in A[x]$.

\noindent\newline{\bf Definition.} We say that $a(x)=0$ is a {\bf zero polynomial}. We define the {\bf degree} of a non-zero polynomial as:

\begin{equation*}
\deg{a(x)}=\max\{k\in\Z^{+}_0:\ a_k\neq 0\}.
\end{equation*}

\noindent\newline Then, the {\bf zero degree polynomial} is $a(x)=a_0$. Also, the {\bf leading coefficient} is $a_{\deg{a(x)}}$.

\noindent\newline{\bf Remark.} Let $A$ be a ring and $a(x),b(x)\in A[x]$. Let $m=\deg{a(x)}$ and $n=\deg{b(x)}$. Then, $a(x)=b(x)$ if and only if $m=n$ and $a_i=b_i$, for all $i\in\{0,\ldots,m\}$. For example, in $\zmod{5}[x]$, it is not true that $x^8+1=x^3+1$, because $\deg{(x^8+1)}=8\neq 3=\deg{(x^3+1)}$.

\noindent\newline{\bf Definition.} Let $n\in\Z^{+}_0$ and $a(x),b(x)\in A[x]$ such that\footnote{Notice here that we do not assume they have the same degree; rather we allow coefficients to be zero, to avoid complications with formulae.}:

\begin{eqnarray*}
a(x)&=&a_0+a_1 x+\cdots+a_n x^n,\\
b(x)&=&b_0+b_1 x+\cdots+b_n x^n.
\end{eqnarray*}

\noindent\newline Then we define polynomial addition and multiplication, respectively:

\begin{eqnarray*}
a(x)+b(x)&=&(a_0+a_1 x+\cdots+a_n x^n)+(b_0+b_1 x+\cdots+b_n x^n)\\
&=&(a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2+\cdots+(a_n+b_n)x^n\\
a(x)b(x)&=&(a_0+a_1 x+\cdots+a_n x^n)(b_0+b_1 x+\cdots+b_n x^n)\\
&=&(a_0 b_0)+(a_0 b_1+a_1 b_0)x+(a_0 b_2+a_1 b_1+a_2 b_0)x^2+\cdots+(a_n b_n)x^{2 n}.
\end{eqnarray*}

\noindent\newline From now on we will consider $A[x]$ together with these operations. The coefficients of the resulting polynomials, i.e. $c(x)=a(x)+b(x)$ and $d(x)=a(x)b(x)$ can be obtained, by observing formulae from above. For, addition, we simply have $c_k=a_k+b_k$, for all $k\in\{0,\ldots,n\}$. For multiplication\footnote{Compare with recursive formula for Catalan's numbers in the beginning of the script.} it is important to note that for $d_k$, all indices sum up to $k$. So, for all $k\in\{0,\ldots,2n\}$:

\begin{equation*}
d_k=\sum_{i+j=k}{a_{i} b_{j}}.
\end{equation*}

\noindent\newline Finally, notice that:

\begin{eqnarray*}
\deg{(a(x)+b(x))}\leq\max{\{\deg{a(x)},\deg{b(x)}\}}\\
\deg{(a(x)b(x))}\leq\deg{a(x)}+\deg{b(x)}.
\end{eqnarray*}

\noindent\newline{\bf Lemma.} Let $A$ be a ring and $a(x),b(x),c(x)\in A[x]$. Then,

\begin{enumerate}
\item $[a(x)+b(x)]+c(x)=a(x)+[b(x)+c(x)]$.
\item $a(x)[b(x)c(x)]=[a(x)b(x)]c(x)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $d(x)=b(x)+c(x)$. Then, coefficients for $d(x)$ are given for all $i\in\{0,\ldots,n\}$ with $d_i=a_i+b_i$. Let $e(x)=a(x)+[b(x)+c(x)]$, i.e. $e(x)=a(x)+d(x)$. So, coefficients for $e(x)$ are $e_i=a_i+d_i=a_i+(b_i+c_i)=a_i+b_i+c_i$, as $A$ is associative. On the other hand, if we took $d(x)=a(x)+b(x)$ and $e(x)=[a(x)+b(x)]+c(x)$, that is $e(x)=d(x)+c(x)$, we would have $e_i=(a_i+b_i)+c_i=a_i+b_i+c_i$, as $A$ is associative. Therefore, both $e(x)$ have equal degrees and coefficients, so $a(x)+[b(x)+c(x)]=[a(x)+b(x)]+c(x)$.

{\it Ad $2$.} Let $d(x)=b(x)c(x)$. Coefficients for $d(x)$ are given with $d_k=\sum_{i+j=k}{b_i c_j}$, where $k\in\{0,\ldots,2n\}$. Let $e(x)=a(x)[b(x)c(x)]$, that is, $e(x)=a(x)d(x)$. From that we have coefficients for $e(x)$, given with, $e_l=\sum_{k+m=l}{a_m d_k}=\sum_{k+m=l}{a_m\sum_{i+j=k}{b_i c_j}}=\sum_{k+m=l}{\sum_{i+j=k}{a_m b_i c_j}}$, where $l\in\{0,\ldots,3n\}$. But, as $i+j=k$ and $k+m=l$, then $k+m=l$, i.e. $i+j+m=l$. So, we may write that as $e_l=\sum_{i+j+m=l}{a_m (b_i c_j)}$. On the other hand, if we take $d(x)=a(x)b(x)$, we have $d_k=\sum_{i+j=k}{a_i b_j}$. Then, $e_l=\sum_{k+m=l}{\left(\sum_{i+j=k}{a_i b_j}\right)c_m}=\sum_{k+m=l}{(a_i b_j)c_m}$. As $A$ is a ring, then it is associative, and the expressions for $e_l$ are equal.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $A$ be a ring and $a(x),b(x),c(x)\in A[x]$. Then,

\begin{enumerate}
\item $a(x)+b(x)=b(x)+a(x)$.
\item $a(x)[b(x)+c(x)]=[a(x)b(x)]+[a(x)c(x)]$.
\item $[a(x)+b(x)]c(x)=[a(x)c(x)]+[b(x)c(x)]$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Coefficients for $[a(x)+b(x)]$ are $a_i+b_i$, which is equal to $b_i+a_i$, because additive $A$ is commutative. These are the coefficients for $[b(x)+a(x)]$. {\it Ad $2$.} Let $d(x)=b(x)+c(x)$. Then, coefficients for $d(x)$ are given with $d_i=b_i+c_i$, where $i\in\{0,\ldots,n\}$. Now, take $e(x)=a(x)[b(x)+c(x)]$, so $e(x)=a(x)d(x)$. We can see that coefficients for $e(x)$ are given by:

\begin{eqnarray*}
e_k&=&\sum_{i+j=k}{a_j d_i}=\sum_{i+j=k}{a_j(b_i+c_i)}\\\\
&=&\sum_{i+j=k}{a_j b_i+a_j c_i}=\sum_{i+j=k}{a_j b_i}+\sum_{i+j=k}{a_j c_i}.
\end{eqnarray*}

\noindent\newline It is easy to see that the given sum is equal to the coefficients for $[a(x)b(x)]+[a(x)c(x)]$. {\it Ad $3$.} Proof analogous to the proof of the previous result.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $A$ is a ring, then $A[x]$ is a ring.

\noindent\newline{\bf Proof.} Let $A$ be a ring. Then, by previous two lemmas, associativity and distribudive laws hold for $A[x]$. Also additive $A[x]$ is commutative. Let $a(x)\in A[x]$. Then, zero is $o(x)=0$, where $0\in A$, because $0+a_i=a_i+0=a_i$, for all $i\in\{0,\ldots,n\}$. Negative of $a(x)$ is $-a(x)=-a_0+\cdots+(-a_n)x^n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. Then,

\begin{enumerate}
\item If $A$ is commutative, then $A[x]$ is commutative.
\item If $1\in A$, then $A[x]$ is a ring with unity.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a b=b a$, for all $a,b\in A$. Then, let $c(x)=a(x)b(x)$. Coefficients for $c(x)$ are $c_k=\sum_{i+j=k}{a_i b_j}$. As $A$ is commutative, $c_k=\sum_{i+j=k}{b_j a_i}$, but those are the coefficients for $b(x)a(x)$. So, $a(x)b(x)=b(x)a(x)$. {\it Ad $2$.} Let $1\in A$. Then, in $A[x]$, unity is $u(x)=1$ because $u(x)a(x)=1(a_0+\cdots+a_n x^n)=1 a_0+\cdots+1 a_n x^n=a(x)$. One can see that the same holds for $a(x)u(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $A$ is an integral domain, then $A[x]$ is an integral domain and:

\begin{equation*}
\deg{a(x)b(x)}=\deg{a(x)}+\deg{b(x)}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $A$ be an integral domain. Then it is also a commutative ring with unity. By a previous proposition, $A[x]$ is a commutative ring with unity. Now, $A$ has no divisors of zero. Therefore, we must show that, if $a(x)b(x)=0$, then either $a(x)=0$ or $b(x)=0$. Conversely, if $a(x),b(x)\neq 0$, then $a(x)b(x)\neq 0$. As $a(x)$ and $b(x)$ are non-zero polynomials, then their leading coefficient is not zero. In other words, if $\deg{a(x)}=m$ and $\deg{b(x)}=n$, then $a_n\neq 0$ and $b_n\neq 0$. Therefore, as $A$ is an integral domain, it must be that $a_n b_m\neq 0$. Assume $n>m$. Then, we can take $a(x)b(x)=(a_0+\cdots+a_m x^m)(b_0+\cdots+b_n x^n)$. Then, the coefficient for $x^{n+m}$ is $a_m b_n$, because in $\{0,\ldots,m\}$ and $\{0,\ldots,n\}$ no other power can add up to $n+m$. Therefore, leading coefficient of $c(x)$ is $a_m b_n$ and $\deg{a(x)b(x)}=m+n=\deg{a(x)}+\deg{b(x)}$. That also implies that $a(x)b(x)$ is a non-zero polynomimal because it has at least one non-zero coefficient and that is its leading coefficient. So, $A[x]$ is an integral domain.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} If $A$ is an integral domain, then $A[x]$ is called {\bf domain of polynomials}. Notice that, if $F$ is a field, then $F[x]$ is not necessarily a field, but as $F$ is also an integral domain, then $F[x]$ is at least an integral domain, by the previous theorem. 

\noindent\newline{\bf Remark.} In the proof of the following theorem, we will follow the elementary procedure of dividing two polynomials and it will be proved by strong induction over difference of degrees of two polynomials.

\noindent\newline{\bf Theorem (Division algorithm for polynomials).} Let $F$ be a field and $a(x),b(x)\in F[x]$. Then, there exist unique $q(x),r(x)\in F[x]$ such that $a(x)=b(x)q(x)+r(x)$, where $\deg{r(x)}<\deg{b(x)}$.

\noindent\newline{\bf Proof.} {\it Existence.} Let $b(x)\in F[x]$. We will show that for all $a(x)$ there exist $q(x),r(x)\in F[x]$ such that $a(x)=b(x)q(x)+r(x)$ with $\deg{r(x)}<\deg{b(x)}$. If $\deg{a(x)}<\deg{b(x)}$, then $a(x)=b(x)\cdot 0+a(x)$ (we have $q(x)=0$ and $r(x)=a(x)$) and obviously $\deg{r(x)}=\deg{a(x)}<\deg{b(x)}$.

Let $m=\deg{a(x)}\geq\deg{b(x)}=n$. We will prove this case by strong induction on $m$. If $m=n$, i.e. $\deg{a(x)}=\deg{b(x)}$ and, if $a(x)=a_0+\cdots+a_m x^m$ (where $m=\deg{a(x)}=\deg{b(x)}$, so $a_m\neq 0$) and $b(x)=b_0+\cdots+b_m x^m$, then we can take, as $F$ is a field, $b_m^{-1}\in F$ so that $q(x)=b_m^{-1}a_m$ (notice that $q(x)\in F[x]$). Then, we have:

\begin{eqnarray*}
b(x)q(x)&=&(b_0+b_1 x+\cdots+b_{m-1}x^{m-1}+b_m x^m)(b_m^{-1}a_m)\\
&=&b_0 b_m^{-1}a_m+b_1 b_m^{-1}a_m x+\cdots+b_{m-1}b_m^{-1}a_m x^{m-1}+a_m x^m.
\end{eqnarray*}

\noindent\newline It is easy to see that all we need to do is subtract new coefficients and add ones from $a(x)$ to get the residue:

\begin{equation*}
r(x)=(a_0-b_0 b_m^{-1}a_m)+(a_1-b_1 b_m^{-1}a_m)x+\cdots+(a_{m-1}-b_{m-1} b_m^{-1}a_m)x^{m-1}.
\end{equation*}

\noindent\newline So, it is evident, I hope, that $a(x)=b(x)q(x)+r(x)$, and that $\deg{r(x)}\leq m-1<m=\deg{b(x)}$, which proves the basis of induction (when $d=0$).

Now, assume that the statement is true for all $a(x)\in F[x]$ with degree $k$ such that $n\leq k<m$. We will show that it is true for all $a(x)$ of degree $m$. Let $a(x)=a_0+\cdots+a_m x^m$, where $m=\deg{a(x)}$ and $b(x)=b_0+\cdots+b_n x^n$, where $n=\deg{b(x)}$. Now, as $b_n\in F$, then $b_n^{-1}\in F$. As $m\geq n$ (from $m=\deg{a(x)}\geq\deg{b(x)}=n$), we have that $m-n\geq 0$. Thus, if we take $q_1(x)=a_m b_n^{-1} x^{m-n}$, as $m-n\geq 0$, we have $q_1(x)\in F[x]$. We are taking $x^{m-n}$ to remove the leading term of $b(x)$ and make it into the leading term of $a(x)$. Now, let us see what happens when we take the product of $b(x)$ and $q_1(x)$:

\begin{eqnarray*}
b(x)q_1(x)&=&(b_0+b_1 x+\cdots+b_{n-1} x^{n-1}+b_n x^n)(a_m b_n^{-1} x^{m-n})\\
&=&b_0 a_m b_n^{-1} x^{m-n}+b_1 a_m b_n^{-1} x^{m-n+1}+\cdots+b_{n-1}a_m b_n^{-1} x^{m-1}+a_m x^m.
\end{eqnarray*}

\noindent\newline Notice that then $a(x)-b(x)q_1(x)$ is equal to:

\begin{equation*}
a_0+\cdots+a_{m-n-1} x^{m-n-1}+(a_{m-n}-b_0 a_m b_n^{-1})x^{m-n}+\cdots+(a_{m-1}-b_{n-1}a_m b_n^{-1}) x^{m-1}.
\end{equation*}

\noindent\newline That implies that $\deg{\left(a(x)-b(x)q_1(x)\right)}\leq m-1$. But, as $m-1<m$ we can apply assumption of induction on $a(x)-b(x)q_1(x)$ (by dividing with $b(x)$. We have that there exist $q_2(x),r(x)\in F[x]$ such that $a(x)-b(x)q_1(x)=b(x)q_2(x)+r(x)$, where $\deg{r(x)}<\deg{b(x)}$. From that we have $a(x)=b(x)[q_1(x)+q_2(x)]+r(x)$. Taking $q(x)=q_1(x)+q_2(x)$ proves the existence part of the theorem.

{\it Uniqueness.} Let $a(x)=b(x)q_1(x)+r_1(x)$ and $a(x)=b(x)q_2(x)+r_2(x)$, where $\deg{r_1(x)},\deg{r_2(x)}<\deg{b(x)}$. That implies $b(x)q_1(x)+r_1(x)=b(x)q_2(x)+r_2(x)$, which is equivalent to $b(x)[q_1(x)-q_2(x)]=[r_2(x)-r_1(x)]$. Assume $\deg{\left(q_1(x)-q_2(x)\right)}\geq 0$. Then, as $F[x]$ is an integral domain,

\begin{equation*}
\deg{\left(b(x)[q_1(x)-q_2(x)]\right)}=\deg{b(x)}+\deg{\left(q_1(x)-q_2(x)\right)}.
\end{equation*}

\noindent\newline But, that would mean that $\deg{b(x)}+\deg{q_1(x)-q_2(x)}=\deg{r_2(x)-r_1(x)}<\deg{b(x)}$ from which we would have $\deg{q_1(x)-q_2(x)}<0$, which is a contradiction. Therefore, $\deg{q_1(x)-q_2(x)}$ is undefined and it follows that $q_1(x)-q_2(x)=0$, i.e. $q_1(x)=q_2(x)=q(x)$. Then, $b(x)q(x)+r_1(x)=b(x)q(x)+r_2(x)$ implies $r_1(x)=r_2(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $a(x)=2x^2+3x+1$ and $b(x)=x^3+5x^2+x$. Compute\footnote{For (b), (c) and (d) we consider $a(x)=\overline{2}x^2+\overline{3}x+\overline{1}$ and $b(x)=x^3+\overline{5}x^2+x$.} $a(x)+b(x)$, $a(x)-b(x)$ and $a(x)b(x)$ in (a) $\Z[x]$, (b) $\zmod{5}[x]$, (c) $\zmod{6}[x]$ and (d) $\zmod{7}[x]$.

\noindent{\bf Solution.} (a) In $\Z[x]$, we have:

\begin{eqnarray*}
a(x)+b(x)&=&(2x^2+3x+1)+(x^3+5x^2+x)=x^3+7x^2+4x+1,\\
a(x)-b(x)&=&(2x^2+3x+1)-(x^3+5x^2+x)=-x^3-3x^2+2x+1,\\
a(x)b(x)&=&(2x^2+3x+1)(x^3+5x^2+x)=2x^5+13x^4+18x^3+8x^2+x.
\end{eqnarray*}

\noindent\newline We will use this to compute polynomials in (b), (c) and (d). Note that if we add two polynomials in, e.g. $\zmod{5}[x]$, we first add their representatives, but then take residue modulo $5$ to get the cosets. Therefore, for (b):

\begin{eqnarray*}
a(x)+b(x)&=&x^3+\overline{2}x^2+\overline{4}x+\overline{1},\\
a(x)-b(x)&=&\overline{4}x^3+\overline{2}x^2+\overline{2}x+\overline{1},\\
a(x)b(x)&=&\overline{2}x^5+\overline{3}x^4+\overline{3}x^3+\overline{3}x^2+x.
\end{eqnarray*}

\noindent\newline For (c):

\begin{eqnarray*}
a(x)+b(x)&=&x^3+\overline{1}x^2+\overline{4}x+\overline{1}\\
&=&x^3+x^2+\overline{4}x+\overline{1},\\
a(x)-b(x)&=&\overline{5}x^3+\overline{3}x^2+\overline{2}x+\overline{1},\\
a(x)b(x)&=&\overline{2}x^5+\overline{1}x^4+\overline{0}x^3+\overline{2}x^2+x\\
&=&\overline{2}x^5+x^4+\overline{2}x^2+x.
\end{eqnarray*}

\noindent\newline Finally, for (d):

\begin{eqnarray*}
a(x)+b(x)&=&x^3+\overline{0}x^2+\overline{4}x+\overline{1}\\
&=&x^3+\overline{4}x+\overline{1},\\
a(x)-b(x)&=&\overline{6}x^3+\overline{4}x^2+\overline{2}x+\overline{1},\\
a(x)b(x)&=&\overline{2}x^5+\overline{6}x^4+\overline{4}x^3+\overline{1}x^2+x\\
&=&\overline{2}x^5+\overline{6}x^4+\overline{4}x^3+x^2+x.
\end{eqnarray*}

\noindent\newline{\bf Problem.} Find the quotient and remainder when we divide: (a) $x^3+x^2+x+1$ by $x^2+3x+2$ in $\Z[x]$ and (b) $x^3+x^2+x+\overline{1}$ by $x^2+\overline{3}x+\overline{2}$ in $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} For (a):

\begin{eqnarray*}
&&(x^3+x^2+x+1):(x^2+3x+2)=x-2\\
&-&\underline{(x^3+3x^2+2x)}\\
&&-2x^2-x+1\\
&-&\underline{(-2x^2-6x-4)}\\
&&5x+5
\end{eqnarray*}

\noindent\newline We have $q(x)=x-2$ and $r(x)=5(x-1)$, so $x^3+x^2+x+1=(x^2+3x+2)(x-2)+5(x+1)$. Now, for (b) we need to take care of multiplication and addition in $\zmod{5}$:

\begin{eqnarray*}
&&(x^3+x^2+x+\overline{1}):(x^2+\overline{3}x+\overline{2})=x+\overline{3}\\
&-&\underline{(x^3+\overline{3}x^2+\overline{2}x)}\\
&&\overline{3}x^2+\overline{4}x+\overline{1}\\
&-&\underline{(\overline{3}x^2+\overline{4}x+\overline{1})}\\
&&\overline{0}
\end{eqnarray*}

\noindent\newline That gives us $q(x)=x+\overline{3}$ and $r(x)=\overline{0}$, so $x^3+x^2+x+\overline{1}=(x^2+\overline{3}x+\overline{2})(x+\overline{3})$.

\noindent\newline{\bf Problem.} Find the quotient and remainder when $x^3+2$ is divided by $2x^2+3x+4$ in (a) $\Z[x]$ and when $x^3+\overline{2}$ is divided by $\overline{2}x^2+\overline{3}x+\overline{4}$ in (b) $\zmod{3}[x]$ and (c) $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} Notice that we cannot use division with remainder theorem in $\Z[x]$ because there does not exist $m\in\Z$ such that $2x^2m x=x^3$, i.e. $2m=1$. For (b), in $\zmod{3}[x]$, first we have that $\overline{2}x^2+\overline{3}x+\overline{4}=\overline{2}x^2+\overline{1}$ and then:

\begin{eqnarray*}
&&(x^3+\overline{2}):(\overline{2}x^2+\overline{1})=\overline{2}x\\
&-&\underline{(x^3+\overline{2}x)}\\
&&-\overline{2}x+\overline{2}\\
\end{eqnarray*}

\noindent\newline Therefore, $q(x)=\overline{2}x$ and $r(x)=\overline{2}-\overline{2}x=\overline{2}+x$, i.e. $x^3+\overline{2}=(\overline{2}x^2+\overline{1})(\overline{2}x)+(\overline{2}+x)$. For (c), or $\zmod{5}[x]$, we have:

\begin{eqnarray*}
&&(x^3+\overline{2}):(\overline{2}x^2+\overline{3}x+\overline{4})=\overline{3}x-\overline{2}\\
&-&\underline{(x^3+\overline{4}x^2+\overline{2}x)}\\
&&-\overline{4}x^2-\overline{2}x+\overline{2}\\
&-&\underline{(-\overline{4}x^2-x-\overline{3})}\\
&&-x
\end{eqnarray*}

\noindent\newline That is, $q(x)=\overline{3}x-\overline{2}=\overline{3}(x+\overline{1})$ and $r(x)=-x=\overline{4}x$, so $x^3+\overline{2}=(\overline{2}x^2+\overline{3}x+\overline{4})\overline{3}(x+\overline{1})+\overline{4}$.

\noindent\newline{\bf Definition.} Let $A$ be a ring and $a(x),b(x)\in A[x]$. If there exists $q(x)\in A[x]$ such that $a(x)=b(x)q(x)$, we say that $b(x)$ is a {\bf factor} of $a(x)$ and write $b(x)|a(x)$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring and $m\in\Z^{+}-2\Z$. Then, in $A[x]$:

\begin{enumerate}
\item $x+1|x^m+1$.
\item $x+1|x^m+x^{m-1}+\cdots+x+1$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Proof by induction. Let $m=1$. Then, $x+1=1\cdot(x+1)$, so $x+1|x+1$. Assume the following is true for some odd $m$. Then we will prove it is true for $m+2$. Using the division algorithm we get:

\begin{eqnarray*}
&&(x^{m+2}+1):(x+1)=x^m x-x^m\\
&-&\underline{(x^{m+2}+x^{m+1})}\\
&&-x^{m+1}+1\\
&-&\underline{(-x^{m+1}-x^m)}\\
&&1+x^m
\end{eqnarray*}

\noindent\newline We can check if that is true: $(x+1)(x^{m+1}-x^m)+(x^m+1)=(x+1)x^{m+1}-(x+1)x^m+x^m+1=x^{m+2}+x^{m+1}-x^{m+1}-x^m+x^m+1=x^{m+2}+1$. Indeed, $(x^{m+2}+1)=(x+1)(x^{m+1}-x^m)+(1+x^m)$. But, by assumption of induction, we have that there exists $q(x)\in A[x]$ such that $(x^m+1)=(x+1)q(x)$, so $(x^{m+2}+1)=(x+1)(x^{m+1}-x^m)+(x+1)q(x)$. That implies $(x^{m+2}+1)=(x+1)[x^{m+1}-x^m+q(x)]$. Thus, as $x^{m+1}-x^m+q(x)\in A[x]$ and that implies $x+1|x^{m+2}+1$, the statement is true for any odd $m$.

{\it Ad $2$.} Proof by induction. Let $m=1$. Then, $x+1|x+1$ is again obvious. Assume that $x+1|x^m+x^{m-1}+\cdots+x+1$ for some odd $m$. We will prove it is true for $m+2$. We have $x^{m+2}+x^{m+1}+\cdots+x+1=(x+1)x^{m+1}+(x^m+x^{m-1}+\cdots+x+1)$. But, by assumption of induction, there exists $q(x)\in A[x]$ such that $(x^m+x^{m-1}+\cdots+x+1)=(x+1)q(x)$. Therefore, $x^{m+2}+x^{m+1}+\cdots+x+1=(x+1)x^{m+1}+(x+1)q(x)=(x+1)[x^{m+1}+q(x)]$. As $x^{m+1}+q(x)\in A[x]$, we have that $x+1|x^{m+2}+x^{m+2}+\cdots+x+1$, and the statement is true for all odd $m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m\in\Z^{+}-\{1\}$ and $n\in\Z^{+}$. In $\zmod{m}[x]$, $x+\overline{m-1}|x^n+\overline{m-1}$.

\noindent\newline{\bf Proof.} Let $m\in\Z^{+}-\{1\}$. We will prove the statement by induction on $n$. If $n=1$, then $x+\overline{m-1}|x+\overline{m-1}$. Assume that the statement is true for some $n$ in $\zmod{m}[x]$, i.e. there exists $q(x)\in\zmod{m}[x]$ such that $x^n+\overline{m-1}=(x+\overline{m-1})q(x)$. Then, for $n+1$, we have:

\begin{eqnarray*}
&&(x^{n+1}+\overline{m-1}):(x+\overline{m-1})=x^n\\
&-&\underline{(x^{n+1}+\overline{m-1}x^n)}\\
&&-\overline{m-1}x^n+\overline{m-1}
\end{eqnarray*}

\noindent\newline Thus, $x^{n+1}+\overline{m-1}=(x+\overline{m-1})x^n-\overline{m-1}x^n+\overline{m-1}$ and it seems we cannot use the assumption. But, we can write $-\overline{m-1}=\overline{-(m-1)}=\overline{1-m}=\overline{1}-\overline{m}$. In $\zmod{m}$, $\overline{m}=\overline{0}$, so $-\overline{m-1}=\overline{1}-\overline{0}=\overline{1}$. Therefore, $-\overline{m-1}x^n+\overline{m-1}=x^n+\overline{m-1}$ and also $x^{n+1}+\overline{m-1}=(x+\overline{m-1})x^n+(x^n+\overline{m-1})$. So, by assumption of induction that is equivalent to $x^{n+1}+\overline{m-1}=(x+\overline{m-1})x^n+(x+\overline{m-1})q(x)$, i.e. $x^{n+1}+\overline{m-1}=(x+\overline{m-1})(x^n+q(x))$. As $x^n,q(x)\in\zmod{m}[x]$, then $x^n+q(x)\in\zmod{m}[x]$, due to $\zmod{m}[x]$ being a ring and therefore closed with respect to additives. In conclusion, $x+\overline{m-1}|x^{n+1}+\overline{m-1}$, for all $m\in\Z^{+}-\{1\}$ and $n\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Show that there is no $m\in\Z$ such that $3x^2+4x+m|6x^4+50$ in $\Z[x]$.

\noindent\newline{\bf Solution.} Assume that there exists $q(x)\in\Z[x]$ such that $6x^4+50=q(x)(3x^2+4x+m)$. Obviously, it must be $\deg{q(x)}=2$, so $q(x)=q_0+q_1 x+q_2 x^2$, where $q_0,q_1,q_2\in\Z$. Now, $6x^4+50=(q_0+q_1 x+q_2 x^2)(3x^2+4x+m)$, i.e. $50+6x^4=q_0 m+(4q_0+q_1 m)x+(3q_0+4q_1+q_2 m)x^2+(3 q_1+4 q_2)x^3+3q_2 x^4$. From that we have $50=q_0 m$ and $6=3q_2$ (from which we get $q_2=2$), but also $4q_0+q_1 m=0$, $3q_0+4 q_1+q_2 m=0$ and $3 q_1+4 q_2=0$. From the last equation we get $3 q_1+4\cdot 2=0$, i.e. $3 q_1+8=0$. But, that would mean that $3 q_1=-8$, but there does not exist such $q_1\in\Z$, as it would imply that $3|8$. Therefore, there does not exist $m$ such that $3x^2+4x+m$ is a factor of $6x^4+50$ in $\Z[x]$.

\noindent\newline{\bf Problem.} For what values of $m\in\Z^{+}$ is $x^2+\overline{1}$ a factor of $x^5+\overline{5}x+\overline{6}$ in $\zmod{m}[x]$?

\noindent\newline{\bf Solution.} We will act by division algorithm as if $\zmod{m}[x]$ were a field, and then observe when we were taking inverses to broaden our range for $m$ (if that happens):

\begin{eqnarray*}
&&(x^5+\overline{5}x+\overline{6}):(x^2+\overline{1})=x^3-x\\
&-&\underline{(x^5+x^3)}\\
&&-x^3+\overline{5}{x}+\overline{6}\\
&-&\underline{(-x^3-x)}\\
&&\overline{6}x+\overline{6}
\end{eqnarray*}

\noindent\newline We have $x^5+\overline{5}x+\overline{6}=(x^2+\overline{1})(x^3-x)+(\overline{6}x+\overline{6})$. In order to $x^2+\overline{1}$ be a factor of $x^5+\overline{5}x+\overline{6}$ we need to have $\overline{6}x+\overline{6}=\overline{0}$ (the remainder must be zero, in the sense of a zero polynomial). Remember, we are not looking for $x$ that will be equal to zero (this is not an equation, as $x$ is only a placeholder). Therefore, we must have $\overline{6}=\overline{0}$ (when considering $\overline{6}x=\overline{0}x$) and $\overline{6}=\overline{0}$ (when considering $\overline{6}=\overline{0}$). Of course, that means that we must have $6\equiv0\mod m$, i.e. $m|6$. So, the values of $m$ are $2$, $3$ or $6$ (we do not consider $m=1$ as then we have a trivial ring, i.e. $\{0\}$) in which $x^2+\overline{1}$ will be a factor of $x^5+\overline{5}x+\overline{6}$ in $\zmod{m}[x]$.

\noindent\newline{\bf Proposition.} There are $m^{n}(m-1)$ polynomials of degree $n\in\Z^{+}_0$ in $\zmod{m}[x]$, where $m\in\Z^{+}-\{1\}$.

\noindent\newline{\bf Proof.} We will observe $q(x)\in\zmod{m}[x]$. We have $q(x)=q_0+q_1 x+\cdots+q_{n-1}x^{n-1}+q_n x^n$. Now, each for each $q_i$, $i\in\{0,\ldots,n\}$ we have $m$ choices, except for $q_n$ which cannot be zero (then degree would not be $n$, but $n-1$ or less), bringing us to $m-1$ choices. So, as there are $n$ of $q_i$ that can be zero, and only first cannot be zero. That implies that in $\zmod{m}[x]$ there are $(m-1)\underbrace{m\cdot m\cdots m}_{n\textnormal{ times}}=(m-1)m^{n}$ polynomials of degree $n$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $A$ be an integral domain. Then:

\begin{enumerate}
\item If $(x+1)^2=x^2+1$ in $A[x]$, then $\rchar{A}=2$;
\item If $(x+1)^4=x^4+1$ in $A[x]$, then $\rchar{A}=2$;
\item If $(x+1)^6=x^6+2x^3+1$ in $A[x]$, then $\rchar{A}=3$.
\end{enumerate}

\noindent{\bf Solution.}

\begin{enumerate}
\item {\it If $(x+1)^2=x^2+1$ in $A[x]$, then $\rchar{A}=2$.} We know that $(x+1)^2=x^2+2x+1$. But, that implies $x^2+1=x^2+2x+1$, i.e. $2x=0$. Note here that $2\notin A$, but $2\in\Z$, as an integral multiple to symbolize $x+x$ (which is actually $1x+1x$). Also, we should not think of $x$ as an unknown, but as a placeholder, so we have $2x=0x$, i.e. $2\cdot(1x)=0x$, which is $(2\cdot 1)x=0x$. From that it follows that $2\cdot 1=0$ (as we said polynomials are equal if their coefficients are equal). As $1+1=0$ in $A$, then obviously $\rchar{A}=2$.

\item {\it If $(x+1)^4=x^4+1$ in $A[x]$, then $\rchar{A}=2$.} We have $(x+1)^4=x^4+4x^3+6x^2+4x+1$. But, $x^4+1=x^4+4x^3+6x^2+4x+1$, that is $4x^3+6x^2+4x=0$. We can factor out $2x$ and get $2x(2x^2+3x+2)=0$. As $A[x]$ is an integral domain, then $2x=0$ or $2x^2+3x+2=0$. If $2x=0$, then, following the same reasoning as above, $\rchar{A}=2$. If $2x^2+3x+2=0$, then $2x^2+3x+2=0x^2+0x+0$ implies $2\cdot 1=0$, $3\cdot 1=0$ and $2\cdot 1=0$. Now, as $2\cdot 1=0$, we have $\rchar{A}=2$. But, then $0=3\cdot 1=2\cdot 1+1=0+1=1$, giving us contradiction to the fact that $A$ is an integral domain (it cannot be a trivial ring, which happens if $1=0$).

\item {\it If $(x+1)^6=x^6+2x^3+1$ in $A[x]$, then $\rchar{A}=3$.} We know that $(x+1)^6=x^6+6x^5+15x^4+20x^3+15x^2+6x+1$. Then, $x^6+2x^3+1=x^6+6x^5+15x^4+20x^3+15x^2+6x+1$, so $6x^5+15x^4+18x^3+15x^2+6x=0$. We can factor out $3x$ to get $3x(2x^4+5x^3+6x^2+5x+2x)=0$. As $A$ is an integral domain, then, so is $A[x]$ and we have either $3x=0$ or $2x^4+5x^3+6x^2+5x+2x=0$. If $3x=0$, then, as $3x=x+x+x=1x+1x+1x=3\cdot(1x)=(3\cdot 1)x$, we have $3\cdot 1=0$, and $\rchar{A}=3$. If $2x^4+5x^3+6x^2+5x+2x=0$, then we would have $2\cdot 1=0$ and $5\cdot 1=0$ which is impossible because $2\cdot 1=0$ would imply $0=5\cdot 1=2\cdot 1+2\cdot 1+1=0+0+1=1$, that is, it would imply that $A$ is a trivial ring, and, by definition, integral domain is a non-trivial ring.
\end{enumerate}

\noindent{\bf Problem.} Show that $F[x]$ can never be a field (even if $F$ is a field).

\noindent\newline{\bf Solution.} If $F[x]$ were a field, then each element of $F[x]$ would have to be invertible. So, as $x\in F[x]$, we would have $p(x)\in F[x]$ such that $x p(x)=1$. We know that $\deg{x}=1$ and $\deg{1}=0$. But, then, if $\deg{p(x)}=m$, we would have, as $F[x]$ is at least an integral domain, $\deg{x}+\deg{p(x)}=\deg{1}$, i.e. $m+1=0$. But, that would mean that $m=-1$, and a degree cannot be negative. If it were undefined, still we would have zero polynomial and we could not have unity as a result.

\noindent\newline{\bf Remark.} Note that, e.g. in $\zmod{8}[x]$, we can think of divisors of zero $(\overline{4}x+\overline{4})(\overline{2}x+\overline{6})=\overline{8}x^2+\overline{24}x+\overline{8}x+\overline{24}=\overline{0}$. Also, we can think of an inverse by observing $(\overline{4}x+\overline{3})^2=\overline{16}x^2+2\cdot\overline{12}x+\overline{9}$. As $2\cdot\overline{12}=\overline{12}+\overline{12}=\overline{24}=\overline{0}$ and $\overline{9}=\overline{1}$, we have $(\overline{4}x+\overline{3})(\overline{4}x+\overline{3})=\overline{1}$. As $\zmod{8}[x]$ is commutative, $(\overline{4}x+\overline{3})^{-1}=\overline{4}x+\overline{3}$, meaning there do exist inverse polynomials, but not in general and never can every polynomial have an inverse in a field, by a previous proposition.

\noindent\newline{\bf Problem.} Show that $A[x]$ can never have all elements different than zero and unity be divisors of zero (even if $A$ is not an integral domain).

\noindent\newline{\bf Solution.} Assume that for all $p(x)\in A[x]$, $p(x)\neq 0,1$, there exists $q(x)\in A[x]$, $q(x)\neq 0,1$, such that $p(x)q(x)=0$. That would also mean that for $x$ there exists some $q(x)\in A[x]$, $q(x)\neq 0,1$, such that $x q(x)=0$. Assume $q(x)=q_0+\cdots+q_n x^n$. Then, $x q(x)=q_0 x+\cdots+q_n x^{n-1}=0$. But, that would imply $q_0=q_1=\ldots=q_n=0$, i.e. $q(x)=0$, which is against our assumption that $q(x)\neq 0$.

\noindent\newline{\bf Problem.} Show that in every $A[x]$, there are elements different from zero and unity that are not idempotent and that there are elements different from zero and unity that are not nilpotent.

\noindent\newline{\bf Solution.} Assume that for all $q(x)\in A[x]-\{0,1\}$ there exists $m\in\Z^{+}$ such that $q(x)^m=0$. That would mean, if $q(x)=x$ that $x^m=0$, i.e. $1 x^m=0 x^m$, giving us $1=0$ and implying that $A[x]$ is a trivial ring.

\noindent\newline{\bf Problem.} Show that if $A$ is not an integral domain, neither is $A[x]$.

\noindent\newline{\bf Solution.} If $A$ is not an integral domain, then there exist $a,b\in A-\{0\}$ such that $a b=0$. But, there are constant polynomials $a,b\in A[x]$ and we would have $a b=0$, i.e. $A[x]$ would have divisors of zero, and could not be an integral domain.

\noindent\newline{\bf Problem.} Give examples of divisors of zero, of degrees $0$, $1$ and $2$ in $\zmod{4}[x]$.

\noindent\newline{\bf Solution.} For degree $0$, we have $\overline{2}\in\zmod{4}[x]$ and $(\overline{2})(\overline{2})=\overline{4}=\overline{0}$. Similarly, $(\overline{2}x)(\overline{2}x)=\overline{4}x^2=\overline{0}$ and $(\overline{2}x^2)(\overline{2}x^2)=\overline{4}x^4=\overline{0}$.

\noindent\newline{\bf Problem.} In $\zmod{10}[x]$, $(\overline{2}x+\overline{2})(\overline{2}x+\overline{2})=(\overline{2}x+\overline{2})(\overline{5}x^3+\overline{2}x+\overline{2})$, yet $(\overline{2}x+\overline{2})$ cannot be canceled in this equation. Explain why this is possible in $\zmod{10}[x]$, but not in $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} The reason $(\overline{2}x+\overline{2})$ cannot be cancelled is because that would imply $\overline{2}x+\overline{2}=\overline{5}x^3+\overline{2}x+\overline{2}$, i.e. $\overline{5}x^3=\overline{0}$. That means $\overline{5}=\overline{0}$, which is not true in $\zmod{10}[x]$. This is due to the fact that $\zmod{10}$ is not an integral domain, as we have $\overline{2}\overline{5}=\overline{0}$. But, note that this could not happen in $\zmod{5}[x]$, as $\zmod{5}$ is a field (because $5\in P$), so $\zmod{5}[x]$ is an integral domain and cancellation law holds.

\noindent\newline{\bf Problem.} Give examples (a) in $\zmod{4}[x]$, (b) in $\zmod{6}[x]$ and (c) in $\zmod{9}[x]$ of polynomials $a(x)$ and $b(x)$ such that $\deg{a(x)b(x)}<\deg{a(x)}+\deg{b(x)}$.

\noindent\newline{\bf Solution.} (a) For, $\zmod{4}[x]$, we have $a(x)=\overline{2}x+\overline{1}$, $\deg{a(x)}=1$ and $b(x)=\overline{2}x^2+\overline{3}$, $\deg{b(x)}=2$. Then, $a(x)b(x)=(\overline{2}x+\overline{1})(\overline{2}x^2+\overline{3})=\overline{4}x^3+\overline{2}x^2+\overline{3}+\overline{6}x=\overline{2}x^2+\overline{2}x+\overline{3}$, so $\deg{a(x)b(x)}=2<2+1=\deg{a(x)}+\deg{b(x)}$. (b) In $\zmod{6}[x]$, take $a(x)=\overline{2}x^2$ and $b(x)=\overline{3}x+\overline{1}$. Then, $\deg{a(x)}=2$ and $\deg{b(x)}=1$. So, $a(x)b(x)=\overline{6}x^3+\overline{2}x^2=\overline{2}x^2$. We see that $\deg{a(x)b(x)}=2<2+1=\deg{a(x)}+\deg{b(x)}$. (c) In $\zmod{9}[x]$, we have $a(x)=\overline{3}x^2+\overline{1}$ and $b(x)=\overline{3}x^4+\overline{2}$, so $a(x)b(x)=\overline{9}x^6+\overline{3}x^4+\overline{6}x^2+\overline{2}=\overline{3}x^4+\overline{6}x^2+\overline{2}$. So, $\deg{a(x)b(x)}=4<2+4=\deg{a(x)}+\deg{b(x)}$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring that is not an integral domain. Then, there exist $a(x),b(x)\in A[x]$ such that $\deg{a(x)b(x)}<\deg{a(x)}+\deg{b(x)}$.

\noindent\newline{\bf Proof.} Let $A$ be a ring such that $A$ is not an integral domain. Then, there exist $a,b\in A$, such that $a b=0$ and $a\neq 0$ and $b\neq 0$. But, then $a x+1,b\in A[x]$. We have $\deg{a(x)}+\deg{b(x)}=1+0=1$ and $a(x)b(x)=(a x+1)b=a b x+b=0x+b=b$, so $\deg{a(x)b(x)}=0<1\deg{a(x)}+\deg{b(x)}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $A$ is an integral domain, then the only invertible non-zero elements in $A[x]$ are constant polynomials.

\noindent\newline{\bf Proof.} Let $a(x),b(x)\in A[x]$ with $\deg{a(x)}=m$ and $\deg{a(x)}=n$, where $m,n\in\Z^{+}_0$. Assume $a(x)b(x)=1$. Then, as $A$ is an integral domain, also $A[x]$ is an integral domain and we have $\deg{a(x)b(x)}=\deg{a(x)}+\deg{b(x)}=m+n$ and $\deg{1}=0$. That would imply $0=m+n$. But, as $m,n\in\Z^{+}_0$, it can on only be that $m=n=0$, i.e. $a(x)$ and $b(x)$ are constant polynomials.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Show that in $\zmod{4}[x]$ there are invertible polynomials of all degrees.

\noindent\newline{\bf Solution.} We can take our inspiration from $(\overline{2}x+\overline{3})(\overline{2}x+\overline{3})=\overline{4}x^2+(\overline{6}+\overline{6})x+\overline{9}=\overline{12}x+\overline{1}=\overline{1}$. Let $m\in\Z^{+}$ and $a(x)=\overline{a_m}x^m+\overline{a_0}$, $b(x)=\overline{b_m}x^m+\overline{b_0}$ with $\overline{a_m},\overline{b_m}\neq\overline{0}$. Then, we want $a(x)b(x)=\overline{a_m b_m}x^{2 m}+\overline{a_m b_0+b_m a_0}x^m+\overline{a_0 b_0}=\overline{1}$. But, that means that $\overline{a_m b_m}=\overline{0}$, and as $\overline{a_m},\overline{b_m}\neq\overline{0}$, it must be $\overline{a_m}=\overline{b_m}=2$. Also, it must be that $\overline{a_0 b_0}=\overline{1}$. But, in $\zmod{4}$, the only invertible elements are $\overline{1}$ and $\overline{3}$ (as they are relatively prime to and less than $4$). Also $\overline{1\cdot 1}=\overline{1}$ and $\overline{3\cdot 3}=\overline{9}=\overline{1}$. If it were that $\overline{a_0}=\overline{b_0}=1$, we would also need $\overline{a_m b_0+b_m a_0}=\overline{0}$, i.e. $\overline{2\cdot 1+2\cdot 1}=\overline{4}=\overline{0}$, so that would work. If it were that $\overline{a_0}=\overline{b_0}=3$, then we would need $\overline{3\cdot 2+3\cdot 2}=\overline{12}=\overline{0}$. Therefore, $\left[\overline{2}x^m+\overline{1}\right]^{-1}=\overline{2}x^m+\overline{1}$ and $\left[\overline{2}x^m+\overline{3}\right]^{-1}=\overline{2}x^m+\overline{3}$.

\noindent\newline{\bf Problem.} Give all the ways of factoring $x^2$ into two polynomials of degree $1$ in $\zmod{9}[x]$ and $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} We know that $\deg{x^2}=2$, so if $a(x)=\overline{a_1}x+\overline{a_0}$ and $b(x)=\overline{b_1}x+\overline{b_0}$, we insist that $\deg{a(x)}=1$ and $\deg{b(x)}=1$, implying $\overline{a_1},\overline{b_1}\neq\overline{0}$. Let us observe $a(x)b(x)=\overline{a_1 b_1}x^2+\overline{a_1 b_0+b_1 a_0}x+\overline{a_0 b_0}$. Then we must have $\overline{a_1 b_1}=\overline{1}$, and the invertible elements in $\zmod{9}$ are $\overline{1}$, $\overline{2}$, $\overline{4}$, $\overline{5}$, $\overline{7}$ and $\overline{8}$. We notice that that gives us four cases: (a) $\overline{a_1}=\overline{b_1}=\overline{1}$, (b) $\overline{a_1}=\overline{2}$ and $\overline{b_1}=\overline{5}$, (c) $\overline{a_1}=\overline{4}$ and $\overline{b_1}=\overline{7}$ and (d) $\overline{a_1}=\overline{b_1}=\overline{8}$. Notice that in all cases we must have $\overline{a_0 b_1+a_1 b_0}=\overline{a_0 b_0}=\overline{0}$, the candidates for which can only be $\overline{0}$ (which we will examine later), $\overline{3}$ and $\overline{6}$. We will give an example how to work out the first case and then list all the other cases. (a) We have $\overline{a_1}=\overline{b_1}=\overline{1}$. Then, $\overline{a_0 b_1+a_1 b_0}=\overline{a_0\cdot 1+1\cdot b_0}=\overline{a_0+b_0}$. If $\overline{a_0}=\overline{3}$, then we can take $\overline{b_0}\in\{\overline{3},\overline{6}\}$. Also, if $\overline{a_0}=\overline{6}$, then we have $\overline{b_0}\in\{\overline{3},\overline{6}\}$. But can it be that $\overline{a_0}=\overline{b_0}$? Then we would have $\overline{3+3}=\overline{6}\neq\overline{0}$ or $\overline{6+6}=\overline{12}=\overline{3}\neq\overline{0}$. So, we have the only option:

\begin{equation*}
x^2=\left(x+\overline{3}\right)\left(x+\overline{6}\right).
\end{equation*}

\noindent\newline It would be tedious to write the same process over and over again, so I will list all other possibilities:

\begin{eqnarray*}
x^2&=&\left(\overline{2}x+\overline{3}\right)\left(\overline{5}x+\overline{6}\right)\\
&=&\left(\overline{2}x+\overline{6}\right)\left(\overline{5}x+\overline{3}\right)\\
&=&\left(\overline{4}x+\overline{3}\right)\left(\overline{7}x+\overline{6}\right)\\
&=&\left(\overline{4}x+\overline{6}\right)\left(\overline{7}x+\overline{3}\right)\\
&=&\left(\overline{8}x+\overline{3}\right)\left(\overline{8}x+\overline{6}\right).
\end{eqnarray*}

\noindent\newline Now, if we also consider $\overline{a_0}=\overline{b_0}=\overline{0}$, we would have:

\begin{equation*}
x^2=x\cdot x=\left(\overline{2}x\right)\left(\overline{5}x\right)=\left(\overline{4}x\right)\left(\overline{7}x\right)=\left(\overline{8}x\right)\left(\overline{8}x\right).
\end{equation*}

\noindent\newline Now, in $\zmod{5}[x]$, there are no zero divisors, so there is no way we can get rid of $\overline{a_0 b_0}$ except by setting $\overline{a_0}=\overline{b_0}=\overline{0}$. Then, $\overline{a_0 b_1+b_0 a_1}=\overline{0\cdot b_1+0\cdot a_1}=\overline{0+0}=\overline{0}$. And, the invertible elements are, $\overline{1}$, $\overline{2}$ and $\overline{3}$, $\overline{4}$. So, we only have:

\begin{equation*}
x^2=x\cdot x=\left(\overline{2}x\right)\left(\overline{3}x\right)=\left(\overline{4}x\right)\left(\overline{4}x\right).
\end{equation*}

\noindent\newline{\bf Problem.} (a) Find all the square roots of $x^2+x+\overline{4}\in\zmod{5}[x]$. (b) Show that in $\zmod{8}[x]$, there are infinitely many square roots of $\overline{1}$.

\noindent\newline{\bf Solution.} (a) We want to find $a(x)\in\zmod{5}[x]$ such that $[a(x)]^2=x^2+x+\overline{4}$. As $\zmod{5}$ is a field, due to $5$ being prime, then it must be $\deg{\left(x^2+x+\overline{4}\right)}=2=\deg{\left([a(x)]^2\right)}=\deg{\left(a(x)a(x)\right)}=\deg{a(x)}+\deg{a(x)}=2\deg{a(x)}$, i.e. $\deg{a(x)}=1$. So, $a(x)=\overline{a_1}x+\overline{a_0}$. We have $\left(\overline{a_1}x+\overline{a_0}\right)^2=\overline{a_1^2} x^2+\overline{2a_1 a_0}x+\overline{a_0^2}=x^2+x+\overline{4}$. So, it must be that $\overline{a_0^2}=\overline{4}$. Surely, we can have $\overline{a_0}\in\{\overline{2},\overline{3}\}$. Similarly, we have $\overline{a_1^2}=\overline{1}$ and that implies $\overline{a_1}\in\{\overline{1},\overline{4}\}$. Finally, we have $\overline{2a_1 a_0}=\overline{1}$, i.e. $\overline{2}\overline{a_1 a_0}=\overline{1}$. Multiplying that equality by $\overline{3}$ gives us $\overline{a_1 a_0}=\overline{3}$. Assume $\overline{a_0}=\overline{2}$. Assume $\overline{a_1}=\overline{1}$. That cannot be, due to the latter condition. Assume $\overline{a_1}=\overline{4}$. That can be, as $\overline{a_0 a_1}=\overline{2\cdot 4}=\overline{8}=\overline{3}$. So, we have:

\begin{equation*}
\left(\overline{4}x+\overline{2}\right)^2=\overline{16}x^2+\overline{2\cdot 4\cdot 2}x+\overline{4}=x^2+x+\overline{4}.
\end{equation*}

\noindent\newline Now, assume $\overline{a_0}=\overline{3}$. Let $\overline{a_1}=\overline{1}$. That can be as $\overline{1\cdot 3}=\overline{3}$ and we have:

\begin{equation*}
\left(x+\overline{3}\right)^2=x^2+\overline{6}x+\overline{9}=x^2+x+\overline{4}.
\end{equation*}

\noindent\newline Finally, assume $\overline{a_1}=\overline{4}$. That cannot be as $\overline{3\cdot 4}=\overline{2}$. So, the only square roots of $x^2+x+\overline{4}$ are the forementioned two. (b) Let $a(x)=\overline{a_m}x^m+\overline{a_0}$, where $\overline{a_m},\overline{a_0}\in\zmod{8}$ and $m\in\Z^{+}$. We have $\left(\overline{a_m}x^m+\overline{a_0}\right)^2=\overline{a_m^2}x^{2 m}+\overline{2a_m a_0}x+\overline{a_0^2}=\overline{1}$. Now, we must have $\overline{a_m^2}=\overline{0}$, but $\overline{a_m}\neq 0$, then $\overline{2a_m a_0}=0$ and $\overline{a_0^2}=\overline{1}$. It can easily be checked that $\overline{a_0}\in\{\overline{1},\overline{5},\overline{7}\}$ and $\overline{a_m}=\overline{4}$. Now, we only need $\overline{2\cdot 4 a_0}=\overline{0}$, i.e. $\overline{8 a_0}=\overline{0}$. But, that is always true, so our choice of $\overline{a_0}$ is arbitrary. We have:

\begin{eqnarray*}
\left(\overline{4}x^m+\overline{1}\right)^2&=&\overline{1},\\
\left(\overline{4}x^m+\overline{5}\right)^2&=&\overline{1},\\
\left(\overline{4}x^m+\overline{7}\right)^2&=&\overline{1}.
\end{eqnarray*}

\noindent\newline As this is true for all $m\in\Z^{+}$, there are infinitely many square roots of $\overline{1}\in\zmod{8}[x]$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain. If $\rchar{A}=p$, then $\rchar{A[x]}=p$.

\noindent\newline{\bf Proof.} As $\rchar{A}=p$, then $p\cdot 1=0$. Now, take $u(x)\in A[x]$ where $u(x)=1$, i.e. $u(x)$ is unity in $A[x]$. Then, $p\cdot u(x)=p\cdot 1=0$. If it were that $q\cdot u(x)=0$, for some $0\leq q<p$, we would have $q\cdot u(x)=q\cdot 1=0$, and that would contradict the fact that $p=\rchar{A}$ (it has to be the least number satisfying that condition).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Give an example of an infinite integral domain with finite characteristic.

\noindent\newline{\bf Solution.} We know that $\zmod{p}$, for $p\in P$, is an integral domain and that $p\cdot\overline{1}=\overline{0}$. Then, $\zmod{p}[x]$ is also an integral domain and we have $p\cdot\overline{1}=\overline{0}$, where $\overline{1},\overline{0}\in\zmod{p}[x]$ are its unity and zero, respectively (corresponding to unity and zeto in $\zmod{p}$). But, $\zmod{p}[x]$ is infinite because every $p(x)\in\zmod{p}[x]$ is of the form $p(x)=\overline{a_0}+\overline{a_1}x+\cdots+\overline{a_m}x^m$. As $m\in\Z^{+}$, there are infinitely many polynomials (when considering their degrees).

\noindent\newline{\bf Proposition.} Let $A$ be a ring with unity. Then, $x-1|x^m-1$, for all $m\in\Z^{+}$.

\noindent\newline{\bf Proof.} Let $m=1$. Then, $(x-1)\cdot 1=x-1=x^1-1$. Assume the statement is true for some $m$, i.e. there exists $q(x)\in A[x]$ such that $(x-1)q(x)=x^m-1$. Now, let us prove it is true for $m+1$. We have $x^{m+1}-1=x^m x-x+x-1=x(x^m-1)+(x-1)$. But, by assumption, $x^m-1=(x-1)q(x)$, so we have $x^{m+1}-1=x(x-1)q(x)+(x-1)=(x-1)(x q(x)+1)$. Therefore, $x-1|x^{m+1}-1$ so the statement is true for all $m\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be an integral domain where $1_A\in A$ denotes unity in $A$. If $\rchar{A}=p$, for some $p\in P$, then $x+(p-1)\cdot 1_A$ is a factor of $x^m+(p-1)\cdot 1_A$, for all $m\in\Z^{+}$.

\noindent\newline{\bf Proof.} We have $x+(p-1)\cdot 1_A=x+p\cdot 1_A-1\cdot 1_A$. As $\rchar{A}=p$, then $p\cdot 1_A=0$, so $x+(p-1)\cdot 1_A=x-1_A$. Similarly, $x^m+(p-1)\cdot 1_A=x^m-1_A$, and the rest of the proof follows from the previous proposition.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be an integral domain, $a\in A$ and let $\rchar{A}=p$, for some $p\in P$. Then\footnote{One should conduct a proof with some care here, as using the previous result that $(a+c)^p=a^p+c^p$ for all $a,c\in A$ would be invalid. That is because of the definition of $x$ as a placeholder, not as an element of $A$, so the equality must imply that degrees and coefficients are equal. Exempli gratia, in $\zmod{2}$, $a^2+1=a^4+1$, but, in $\zmod{2}[x]$, $x^2+1\neq x^4+1$.}, in $A[x]$, $(a x+c)^p=a^p x^p+c^p$, for all $c\in A$.

\noindent\newline{\bf Proof.} Using the binomial formula, we have:

\begin{equation*}
(a x+c)^p=\sum_{k=0}^{p}{\binom{p}{k}(a x)^k c^{p-k}}.
\end{equation*}

\noindent\newline Now, we remember that $p|\binom{p}{k}$ for all $k\in\{1,\ldots,p-1\}$. So, there exists $z_k\in\Z$ such that $\binom{p}{k}=z_k p$, for all $k\in\{1,\ldots,p-1\}$. Also notice that $\binom{p}{0}=1$ and $\binom{p}{p}=1$. Thus, we have:

\begin{equation*}
(a x+c)^p=(a x)^p+\sum_{k=1}^{p-1}{(z_k p)\cdot((a x)^k c^{p-k})}+c^p.
\end{equation*}

\noindent\newline As $\rchar{A}=p$, then $\rchar{A[x]}=p$, so $(z_k p)\cdot((a x)^k c^{p-k})=p\cdot(z_k\cdot((a x)^k c^{p-k}))=0$. Therefore, we are only left with $(a x+c)^p=(a x)^p+c^p=a^p x^p+c^p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be an integral domain and $\rchar{A}=p$, for some $p\in P$. Let $m\in\Z^{+}_0$ and $a_i\in A$, for $i\in\{0,\ldots,m\}$. Then:

\begin{equation*}
(a_m x^m+a_{m-1}x^{m-1}+\cdots+a_1 x+a_0)^p=a_m^p x^{m p}+a_{m-1}^p x^{(m-1)p}+\cdots+a_1^p x^p+a_0^p.
\end{equation*}

\noindent\newline{\bf Proof.} Let $m=0$. Then $(a_0)^p=a_0^p$. If $m=1$, then by previous proposition, $(a_1 x+a_0)^p=a_1^p x^p+a_0^p$. Assume the statement is true for some $m\in\Z^{+}_0$. We will prove that it implies truth for $m+1$. We have:

\begin{equation*}
(a_{m+1} x^{m+1}+\cdots+a_1 x+a_0)^p=(a_{m+1} x^{m+1}+(a_m x^m+\cdots+a_1 x+a_0))^p.
\end{equation*}

\noindent\newline It is easy to see, by observing previous proposition, that:

\begin{equation*}
(a_{m+1} x^{m+1}+(a_m x^m+\cdots+a_1 x+a_0))^p=a_{m+1}^p x^{(m+1)p}+(a_m x^m+\cdots+a_1 x+a_0)^p.
\end{equation*}

\noindent\newline Using the assumption of induction we get:

\begin{equation*}
a_{m+1}^p x^{(m+1)p}+(a_m x^m+\cdots+a_1 x+a_0)^p=a_{m+1}^p x^{(m+1)p}+a_m^p x^{m p}+\cdots+a_1^p x^p+a_0^p.
\end{equation*}

\noindent\newline Therefore, by principle of mathematical induction, the statement is true for all $m\in\Z^{+}_0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. If $B\leq A$, then $B[x]\leq A[x]$.

\noindent\newline{\bf Proof.} Let $b(x)\in B[x]$. Then $b(x)=b_m x^m+\cdots+b_1 x+b_0$, where $b_i\in B$, for all $i\in\{0,\ldots,m\}$. But, as $B\leq A$, then also $b_i\in A$. So, $b(x)\in A[x]$ and $B[x]\subseteq A[x]$. Let $b(x),b'(x)\in B[x]$. Then, if $b(x)=b_m x^m+\cdots+b_1 x+b_0$ and $b'(x)=b'_m x^m+\cdots+b'_1 x+b'_0$ (allowing some $b'_i$ or $b_i$ to be zero), we have $b(x)-b'(x)=(b_m-b'_m)x^m+\cdots+(b_1-b'_1)x+(b_0-b'_0)$. It is obvious that $b_i-b'_i\in B$ because $B\leq A$, so also $b(x)-b'(x)\in B[x]$. Finally, one can see that $b(x)b'(x)\in B[x]$ because the coefficients in $b(x)b'(x)$ consist of factors of coefficients of $b(x)$ and $b'(x)$, and as $B\leq A$, that implies that the coefficients remain in $B$. From that we have $b(x)b'(x)\in B[x]$ and $B[x]\leq A[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring. If $B\trianglelefteq A$, then $B[x]\trianglelefteq A[x]$.

\noindent\newline{\bf Proof.} By the previous proposition, $B[x]\leq A[x]$. If $a(x)\in A[x]$ and $b(x)\in B[x]$, then the coefficients in $a(x)b(x)$ (or $b(x)a(x)$) consists of factors of coefficients $a_i\in A$ and $b_i\in B$. But, as $B\trianglelefteq A$, those coefficients are again in $B$, so $a(x)b(x)\in B[x]$ (and similarly $b(x)a(x)\in B[x]$).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $S=\{a^m x^m+\cdots+a_1 x+a_0\in A[x]:\ a_i=0\textnormal{ for odd }i)\}$. Then, $S\leq A[x]$.

\noindent\newline{\bf Proof.} It is obvious from definition of $S$ that $S\subseteq A[x]$. Take $a(x),b(x)\in S$ with $a(x)=a_m x^m+\cdots+a_1 x+a_0$ and $b(x)=b_m x^m+\cdots+b_1 x+b_0$ (allowing some $a_i$ or $b_i$ to be zero, so they are not necessarily of the same degree). It is also obvious that the difference $a_i-b_i$ will be zero for odd $i$, so $a(x)-b(x)\in S$. Then, let us odd coefficients in $a(x)b(x)$:

\begin{equation*}
c_k=\sum_{i+j=k}{a_i b_j}.
\end{equation*}

\noindent\newline If $k$ is odd, then either $i$ is odd or $j$ is odd (they cannot be both even or both odd). Therefore, either $a_i=0$ or $b_j=0$ so always $a_i b_j=0$ and the sum of zeros is again zero. Therefore, $a(x)b(x)\in S$ and $S\leq A[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that the proposition above does not hold for polynomials if all even-indexed coefficents are equal to zero, because in multiplication we would allow both of them to be odd-indexed, and not necessarily equal to zero. Due to the same fact $S$ is not necessarily an ideal of $A[x]$.

\noindent\newline{\bf Proposition.} Let $A$ be a ring. Let $J=\{a_m x^m+\cdots+a_1 x+a_0\in A[x]:\ a_0=0\}$. Then, $J\trianglelefteq A[x]$. If $A$ is an integral domain, then $J$ is a prime ideal of $A[x]$.

\noindent\newline{\bf Proof.} Obviously $J\subseteq A$. Let $a(x),b(x)\in J$, i.e. $a(x)=a_m x^m+\cdots+a_1 x$ and $b(x)=b_m x^m+\cdots+b_1 x$ (allowing some coefficients to be zero). Then, constant coefficient of $a(x)-b(x)$ is obviously equal to zero and the constant coefficient of $a(x)b(x)$ is just $a_0 b_0$, so it is again zero. If we take some other $c(x)\in A[x]$, then constant coefficient of $c(x)a(x)$ is $c_0 a_0$, i.e. $c_0 0$ so it is zero. Same goes for $a(x)c(x)$ so we have $J\trianglelefteq A[x]$. But, if we take $a(x)b(x)\in J$, that means that $a_0 b_0=0$. But, as $A$ is an integral domain, either $a_0=0$ or $b_0=0$. So, it must be that either $a(x)\in J$ or $b(x)\in J$, meaning $J$ is a prime ideal of $A[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be a ring and $J=\{a_m x^m+\cdots+a_1 x+a_0\in A[x]:\ a_m+\cdots+a_1+a_0=0\}$. Then, $J\trianglelefteq A[x]$. If $A$ is an integral domain, then $J$ is a prime ideal of $A[x]$.

\noindent\newline{\bf Proof.} From definition of $J$, we have that $J\subseteq A$. If we take $a(x),b(x)\in J$ with $a(x)=a_m x^m+\cdots+a_1 x+a_0$ and $b(x)=b_m x^m+\cdots+b_1 x+b_0$ (granting some $b_i$ might be equal to zero, which does not change the definition of the set), we have $a_m+\cdots+a_1+a_0=0$ and $b_m+\cdots+b_1+b_0=0$. If we take $c_i$ as the $i$-th coefficient of the difference $a(x)-b(x)$, then $c_i=a_i-b_i$. But, $c_m+\cdots+c_1+c_0=(a_m-b_m)+\cdots+(a_1-b_1)+(a_0-b_0)=(a_m+\cdots+a_1+a_0)-(b_m+\cdots+b_1+b_0)=0-0=0$, so $a(x)-b(x)\in J$. Now, we will move ahead to simplify the proof. Notice that the sum of coefficients can be obtained by substituting $x$ with $1$. So, the sum of coefficients of $c(x)=a(x)b(x)$ is equal to $c(1)$ and that is of course $c(1)=a(1)b(1)$. Therefore, as $a(1)=0$ and $b(1)=0$ it must be that $c(1)=0\cdot 0=0$. Similarly, if $a(x)\in J$ and $b(x)\in A[x]$, then $a(1)b(1)=0\cdot b(1)=0$, so $a(x)b(x)\in J$ (same goes for $b(x)a(x)$). Thus, $J\trianglelefteq A[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ be an integral domain. Then\footnote{Do not confuse the notation for principal ideal with that of a cyclic group; if $G$ were a group and $x\in G$, then $\cyc{x}=\{x^m:\ m\in\Z\}$. But here, as $x\in A[x]$ (regard it as $1\cdot x$ for easier understanding), then $\cyc{x}=\{x a(x):\ a(x)\in A[x]\}$.}, $A[x]\slash\cyc{x}\cong A$.

\noindent\newline{\bf Proof.} Let $f:A[x]\rightarrow A$ be a mapping defined with $h(a^m+\cdots+a_1 x+a_0)=a_0$, for all $a^m+\cdots+a_1 x+a_0\in A[x]$. It is easy to see from definition of $f$ that it is a well-defined function. If we take $a(x)\in A[x]$, then it has a constant term, so there exists $a_0\in A$ so that $f(a(x))=a_0$. If we take $a(x),b(x)\in A[x]$, and $a(x)=b(x)$, then their degrees are equal and their respective coefficients are equal, and so are their constant terms, i.e. $a_0=b_0$, so that implies $f(a(x))=f(b(x))$. Now that we have argued that $f$ is a well-defined function, we need to show that it is surjective. If we take $a_0\in A$, then simply $a_0\in A[x]$, so $f(a_0)=a_0$. Now, let $a(x),b(x)\in A[x]$ with $a(x)=a_m x^m+\cdots+a_1 x+a_0$ and $b(x)=b_m x^m+\cdots+b_1 x+b_0$ (allowing some $b_i$ to be equal to zero). Then, $f(a(x)+b(x))=f((a_m+b_m)x^m+\cdots+(a_1+b_1)x+(a_0+b_0))=a_0+b_0=f(a(x))+f(b(x))$ and $f((a_m x^m+\cdots+a_1 x+a_0)(b_m x^m+\cdots+b_1 x+b_0))=f(c_{2 m}x^{2 m}+\cdots+c_1 x+c_0)$, where $c_k=\sum_{i+j=k}{a_i b_j}$, for all $k\in\{0,\ldots,2m\}$. Then, $c_0=a_0 b_0$, so $f(a(x)b(x))=a_0 b_0=f(a(x))f(b(x))$. Therefore, as $f$ is a surjective homomorphism, we can apply the fundamental homomorphism theorem for rings. We have $A[x]\slash\ker{f}\cong A$. The only thing left is to investigete the kernel of $f$. We have $\ker{f}=\{a_m x^m+\cdots+a_1 x+a_0\in A[x]:\ a_0=0\}=\{a_m x^m+\cdots+a_1 x\in A[x]\}=\{x(a_m x^{m-1}+\cdots+a_1)\in A[x]\}$. That reminds us exactly of $\cyc{x}$. If we take $a(x)\in\cyc{x}$, then $a(x)=x b(x)$, for some $b(x)\in A[x]$, i.e. constant term of $a(x)$ is equal to zero, so $a(x)\in\ker{f}$, i.e. $\cyc{x}\subseteq\ker{f}$. If we take $a(x)\in\ker{f}$, then it is equal to $x b(x)$, where $b(x)\in A[x]$. But, that precisely means that $x b(x)\in\cyc{x}$, i.e. $a(x)\in\cyc{x}$. The result of that is $\ker{f}\subseteq\cyc{x}$ implying $\ker{f}=\cyc{x}$. Thus we have $A[x]\slash\cyc{x}\cong A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $A$ be an integral domain and $g:A[x]\rightarrow A$ send every polynomial to the sum of its coefficients. Prove that $g$ is a surjective homomorphism, and describe its kernel.

\noindent\newline{\bf Solution.} Let $g$ be defined with $g(a_m x^m+\cdots+a_1 x+a_0)=a_m+\cdots+a_1+a_0$. Then, $g$ is obviously defined for all $a(x)\in A[x]$ and $a_m x^m+\cdots+a_1 x+a_0=b_m x^m+\cdots+b_1 x+b_0$ implies $a_i=b_i$, so $a_m+\cdots+a_1+a_0=b_m+\cdots+b_1+b_0$ (can be proven inductively), for all $a_m x^m+\cdots+a_1 x+a_0\in A[x]$ and $b_m x^m+\cdots+b_1 x+b_0\in A[x]$ (allowing some $b_i$ to be equal to zero). Then, if we take $a\in A$, there exists $a\in A[x]$ such that $g(a)=a$. Thus, $g$ is surjective. Finally, if $a_m x^m+\cdots+a_1 x+a_0,b_m x^m+\cdots+b_1 x+b_0\in A[x]$ (allowing some $b_i$ to be equal to zero), then, $g((a_m x^m+\cdots+a_1 x+a_0)+(b_m x^m+\cdots+b_1 x+b_0))=g((a_m+b_m)x^m+\cdots+(a_1+b_1)x+(a_0+b_0))=(a_m+b_m)+\cdots+(a_1+b_1)+(a_0+b_0)=(a_m+\cdots+a_1+a_0)+(b_m+\cdots+b_1+b_0)=g(a_m x^m+\cdots+a_1 x+a_0)+g(b_m x^m+\cdots+b_1 x+b_0)$. Also, $g((a_m x^m+\cdots+a_1 x+a_0)(b_m x^m+\cdots+b_1 x+b_0))=g(c_{2 m} x^{2 m}+\cdots+c_1 x+c_0)=c_{2 m}+\cdots+c_1+c_0$, where $c_k=\sum_{i+j=k}{a_i b_j}$, for all $k\in\{0,\ldots,2m\}$. We realize that $a(x)b(x)=c(x)$ implies $a(1)b(1)=c(1)$. But, $c_{2 m}+\cdots+c_1+c_0=c(1)$, so $c_{2 m}+\cdots+c_1+c_0=a(1)b(1)$, i.e. $c_{2 m}+\cdots+c_1+c_0=(a_m+\cdots+a_1+a_0)(b_m+\cdots+b_1+b_0)$. Therefore, $g((a_m x^m+\cdots+a_1 x+a_0)(b_m x^m+\cdots+b_1 x+b_0))=(a_m+\cdots+a_1+a_0)(b_m+\cdots+b_1+b_0)=g(a(x))g(b(x))$. Therefore, $g$ is an injective homomorphism and we can apply fundamental homomorphism theorem to obtain $A[x]\slash\ker{g}\cong A$. It is obvious to see that $\ker{g}=\{a_m x^m+\cdots+a_1 x+a_0\in A[x]:\ a_m+\cdots+a_1+a_0=0\}$, which is equal to the ideal $J$ of $A[x]$ described in a previous proposition.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain and $c\in A$. Let $h:A[x]\rightarrow A[x]$ be defined by $h(a(x))=a(c x)$. Then, $h$ is an automorphism if and only if $c$ is invertible.

\noindent\newline{\bf Proof.} First we will prove that $h$ is a homomorphism. Take $a(x)\in A[x]$ with $a(x)=a_m x^m+\cdots+a_1 x+a_0$. Then, $a(c x)=a_m (c x)^m+\cdots+a_1(c x)+a_0=a_m c^m x^m+\cdots+a_1 c x+a_0$. As $a_i c^i\in A$, then $a(c x)\in A[x]$. Also, if $b(x)\in A[x]$ with $b(x)=b_m x^m+\cdots+b_1 x+b_0$ (allowing some $b_i$ to be equal to zero) and $a(x)=b(x)$, then $a_i=b_i$, for all $i\in\{0,\ldots,m\}$. That implies $a_i c^i=b_i c^i$, i.e. $a_m c^m x^m+\cdots+a_1 c x+a_0=b_m c^m x^m+\cdots+b_1 c x+b_0$, i.e. $a(c x)=b(c x)$. That is equivalent to $h(a(x))=h(b(x))$ implying that $h$ is well-defined. Then, $h(a(x)+b(x))=h((a_m x^m+\cdots+a_1 x+a_0)+(b_m x^m+\cdots+b_1 x+b_0))=h((a_m+b_m)x^m+\cdots+(a_1+b_1)x+(a_0+b_0))=(a_m+b_m)c^m x^m+\cdots+(a_1+b_1)c x+(a_0+b_0)=(a_m c^m x^m+\cdots+a_1 c x+a_0)+(b_m c^m x^m+\cdots+b_1 c x+b_0)=h(a(x))+h(b(x))$. From that follows that $h$ is a homomorphism.

{\it Necessity.} Assume $h$ is automorphism. We must show that $c$ is invertible. Let $a(x)=x$. Then, as $a(x)\in A[x]$ and $h$ is surjective, there exists $b(x)\in A[x]$ such that $h(b(x))=x$, i.e. $b(c x)=x$. As polynomial $b(c x)$ is equal to $x$ it also must be of degree $1$ and have all other coefficients (except $b_1$) zero. That means that $b(c x)=b_1(c x)$. As coefficients for $b_1(c x)$ and $x$ (read $1 x$) must be equal, then $b_1 c=1$. As $A[x]$ is commutative then also $c b_1=1$ and it follows that $c$ is invertible.

{\it Sufficiency.} Assume $c$ is invertible. Then, if $h(a(x))=h(b(x))$, we have $a(c x)=b(c x)$. That means that coefficients of $a(c x)$ and $b(c x)$ are equal and the two polynomials are of equal degrees. So we have $c^i a_i=c^i b_i$. But, as $c$ is invertible, we can multiply that by $c^{-i}$ and get $a_i=b_i$, meaning $a(x)=b(x)$, i.e. $h$ is injective. Also, if we take $a(x)\in A[x]$, then we must prove that there exists $b(x)\in A[x]$ such that $h(b(x))=a(x)$. But, as $h(b(x))=b(c x)$, we have $b(c x)=a(x)$. It is obvious that it must be $b(x)=a(c^{-1} x)$. Then, we have $b(c x)=a(c^{-1} c x)=a(x)$. Therefore, $h$ is surjective, and $h$ is an automorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be rings and let $h:\homo{A}{B}{K}$ be a homomorphism. Let\footnote{We say that $\overline{h}$ is induced by $h$.} $\overline{h}:A[x]\rightarrow B[x]$ be defined as

\begin{equation*}
\overline{h}(a_0+a_1 x+\cdots+a_n x^n)=h(a_0)+h(a_1)x+\cdots+h(a_m)x^m.
\end{equation*}

\noindent\newline Then:

\begin{enumerate}
\item $\overline{h}$ is homomorphism.
\item $\overline{h}$ is surjective if and only if $h$ is surjective.
\item $\overline{h}$ is injective if and only if $h$ is injective.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} If we take $a(x)\in A[x]$, then $a(x)=a_0+a_1 x+\cdots+a_m x^m$. As $h$ is a well-defined function by assumption, then there exist $h(a_i)$ such that $\overline{h}(a(x))=h(a_0)+h(a_1)x+\cdots+h(a_m)x^m$, so $\overline{h}$ is well-defined. Also, if $a(x)=b(x)$, then $a_i=b_i$ and so, as $h$ has a property of uniqueness, $h(a_i)=h(b_i)$. Therefore, the coefficients of $\overline{h}(a(x))$ and $\overline{h}(b(x))$ are equal and $\overline{h}(a(x))=\overline{h}(b(x))$. That implies $\overline{h}$ satisfies the property of uniqueness. Now, $\overline{h}((a_0+a_1 x+\cdots+a_m x^m)+(b_0+b_1 x+\cdots+b_m x^m))=\overline{h}((a_0+b_0)+(a_1+b_1)x+\cdots+(a_m+b_m)x^m)=h(a_0+b_0)+h(a_1+b_1)x+\cdots+h(a_m+b_m)x^m$. But, as $h$ is a homomorphism, we have $h(a_0+b_0)+h(a_1+b_1)x+\cdots+h(a_m+b_m)x^m=h(a_0)+h(b_0)+h(a_1)x+h(b_1)x+\cdots+h(a_m)x^m+h(b_m)x^m=\overline{h}(a_0+a_1 x+\cdots+a_m x^m)+\overline{h}(b_0+b_1 x+\cdots+b_m x^m)$. Also, $\overline{h}((a_0+a_1 x+\cdots+a_m x^m)(b_0+b_1 x+\cdots+b_m x^m))=\overline{h}(c_0+c_1 x+\cdots+c_{2 m}x^{2 m})=h(c_0)+h(c_1)x+\cdots+h(c_{2m})x^{2m}$, where $c_k=\sum_{i+j=k}{a_i b_j}$, for all $k\in\{0,\ldots,2m\}$. As $h$ is a homomorphism, then $h(c_k)=h\left(\sum_{i+j=k}{a_i b_j}\right)=\sum_{i+j=k}{h(a_i b_j)}=\sum_{i+j=k}{h(a_i)h(b_j)}$. But, these are coefficients of $\overline{h}(a(x))$ and $\overline{h}(b(x))$, so $\overline{h}(a(x)b(x))=\overline{h}(c(x))=\overline{h}(a(x))\overline{h}(b(x))$ and that implies that $\overline{h}$ is a homomorphism.

{\it Ad $2$.} {\it Necessity.} Assume $\overline{h}$ is surjective and take $b\in B$. We must show that there exists $a\in A$ such that $h(a)=b$. As $b\in B$, then also $b\in B[x]$. As $\overline{h}$ is surjective, there exists $c(x)\in A[x]$ such that $\overline{h}(c(x))=b$. The coefficients of $\overline{h}(c(x))$ are all $h(c_i)$ by definition, and as $\overline{h}(c(x))$ equals a zero-degree polynomial $b$, all it's coefficients, except the constant term, equal zero. So, it only remains that $h(c_0)=b$, proving that $h$ is surjective. {\it Sufficiency.} Assume $h$ is surjective and take $b(x)\in B[x]$. We must show that there exists $a(x)\in A[x]$ such that $\overline{h}(a(x))=b(x)$. Let $b(x)=b_0+b_1 x+\cdots+b_m x^m$. Then, $b_i\in B$, for all $i\in\{0,\ldots,m\}$, and as $h$ is surjective, there exist $a_i\in A$ such that $h(a_i)=b_i$. Therefore, $a(x)=a_0+b_1 x+\cdots+a_m x^m$ is the polynomial we sought and $\overline{h}$ is surjective.

{\it Ad $3$.} {\it Necessity.} Assume $\overline{h}$ is injective and let $a,b\in A$ such that $a\neq b$. But, also $a,b\in A[x]$ and, as $a\neq b$, then $\overline{h}(a)\neq\overline{h}(b)$. But, $\overline{h}(a)$ is of degree zero, and so iz $\overline{h}(b)$. Therefore, it must be that the constant terms are different, i.e. $h(a)\neq h(b)$, meanining $h$ is injective. {\it Sufficiency.} Assume $h$ is injective and let $a(x),b(x)\in A[x]$. Then, assume $\overline{h}(a(x))=\overline{h}(b(x))$. That implies that those two polynomials are of the same degree and that their coefficients are equal. But, their coefficients are $h(a_i)$ and $h(b_i)$, respectively, so it must be $h(a_i)=h(b_i)$. As $h$ is injective, that implies $a_i=b_i$, i.e. $a(x)=b(x)$, so $\overline{h}$ is injective also.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $A$ and $B$ be rings and let $\overline{h}$ be induced by homomorphism $h:\homo{A}{B}{K}$. Let $a(x),b(x)\in A[x]$. If $a(x)$ is a factor of $b(x)$, then $\overline{h}(a(x))$ is a factor of $\overline{h}(b(x))$.

\noindent\newline{\bf Proof.} Let $a(x)$ be a factor of $b(x)$, i.e. there exists $q(x)\in A[x]$ such that $b(x)=q(x)a(x)$. Then, as $q(x)a(x)\in A[x]$ and $\overline{h}$ is well-defined, we have:

\begin{equation*}
\overline{h}(b(x))=\overline{h}(q(x)a(x)).
\end{equation*}

\noindent\newline But, as $\overline{h}$ is a homomorphism, that is equivalent to $\overline{h}(b(x))=\overline{h}(q(x))\overline{h}(a(x))$, i.e. $\overline{h}(a(x))$ is a factor of $\overline{h}(b(x))$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $m\in\Z^{+}$. If $h:\Z\rightarrow\zmod{m}$ is the projective homomorphism, let $\overline{h}:\Z[x]\rightarrow\zmod{m}[x]$ be the homomorphism induced by $h$. Then:

\begin{enumerate}
\item $\overline{h}(a(x))=\overline{0}$ if and only if $m$ divides every coefficient of $a(x)$.
\item If $m\in P$ and $a(x)b(x)\in\ker{\overline{h}}$, then $a(x)\in\ker{\overline{h}}$ or $b(x)\in\ker{\overline{h}}$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a(x)\in\Z[x]$ so that $a(x)=a_m x^m+\cdots+a_1 x+a_0$, for some $m\in\Z^{+}_0$ and $a_i\in\Z$, for all $i\in\{0,\ldots,m\}$. {\it Necessity.} Assume $\overline{h}(a(x))=\overline{0}$. Then, $\overline{h}(a(x))=h(a_m)x^m+\cdots+h(a_1)x+h(a_0)$. But, as $h$ is a projective homomorphism, and as $\overline{h}(a(x))=\overline{0}$, we have $\overline{h}(a(x))=\overline{a_m}x^m+\cdots+\overline{a_1}x+\overline{a_0}=\overline{0}$. That implies, as all corresponding coefficients on both sides must be equal (along with degrees of $\overline{h}(a(x))$ and $\overline{0}$) that $\overline{a_i}=\overline{0}$, for all $i\in\{0,\ldots,m\}$. But, that means that $a_i\equiv 0\pmod m$, i.e. $m|a_i$. {\it Sufficiency.} Let $m|a_i$ for all $i\in\{0,\ldots,m\}$. Then, $a_i\equiv 0\pmod m$, i.e. $\overline{a_i}=\overline{0}$, for all $i\in\{0,\ldots,m\}$. As $\overline{h}(a(x))=h(a_m)x^m+\cdots+h(a_1)x+h(a_0)=\overline{a_m}x^m+\cdots+\overline{a_1}x+\overline{a_0}$, that implies that $\overline{h}(a(x))=\overline{0}$.

{\it Ad $2$.} Let $m\in P$ and $a(x),b(x)\in\Z[x]$ such that $a(x)b(x)\in\ker{\overline{h}}$. As $m\in P$, then $\zmod{m}$ is a field, by a previous proposition. Also, it is an integral domain, so $\zmod{m}[x]$ is an integral domain. By a previous proposition, as $h$ is surjective (projective homomorphism is always surjective) $\overline{h}:\Z[x]\rightarrow\zmod{m}[x]$ is surjective. Then, by FHT, $\Z[x]\slash\ker{\overline{h}}\cong\zmod{m}[x]$ and it follows that $\Z[x]\slash\ker{\overline{h}}$ is an integral domain (as it is isomorphic to integral domain $\zmod{m}[x]$. By a previous proposition, $\ker{\overline{h}}$ is a prime ideal of $\Z[x]$, which implies that $a(x)\in\ker{\overline{h}}$ or $b(x)\in\ker{\overline{h}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that, as $A[x]$ is an integral domain (if $A$ is integral domain), then we can construct a field of quotients denoted by $A(x,y)$. Then, $A(x,y)$ consists of all the fractions $\frac{a(x)}{b(x)}$, where $b(x)\neq 0$, and $a(x),b(x)\in A[x]$.

\noindent\newline{\bf Proposition.} Let $A$ be an integral domain. Then, $\rchar{A(x)}=\rchar{A}$.

\noindent\newline{\bf Proof.} Let $\rchar{A}=p$. That implies $p\cdot 1=0$. But, also $1\in A(x)$, so $p\cdot 1=0$. As $p\in P$, then $\rchar{A(x)}=p$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent\newline{\bf Remark.} Notice that previous proposition implies that for every $p\in P$, there is an infinite field of characteristic $p$.

\noindent\newline{\bf Proposition.} If $A$ and $B$ are integral domains and $h:A\rightarrow B$ is an isomorphism, then $\overline{h}:A(x)\rightarrow B(x)$ is an isomorphism defined as:

\begin{equation*}
\tilde{h}\left(\frac{a_m x^m+\cdots+a_1 x+a_0}{b_n x^n+\cdots+b_1 x+b_0}\right)=\frac{h(a_m)x^m+\cdots+h(a_1)x+h(a_0)}{h(b_n)x^n+\cdots+h(b_1)x+h(b_0)}=\frac{\overline{h}(a(x))}{\overline{h}(b(x))}.
\end{equation*}

\noindent\newline{\bf Proof.} Assume $h$ is an isomorphism and that $\tilde{h}$ is defined as above. Then, if we take $\frac{a(x)}{b(x)}\in A(x)$, as $h$ is well-defined, there exists $\frac{a'(x)}{b'(x)}$ in $B(x)$ (whose coefficients are $h(a_i)$ and $h(b_i)$) such that $\tilde{h}\left(\frac{a(x)}{b(x)}\right)=\frac{a'(x)}{b'(x)}$. If $\frac{a(x)}{b(x)}=\frac{a'(x)}{b'(x)}$, then, by definition, $a(x)b'(x)=a'(x)b(x)$. As $a(x)b'(x),a'(x)b(x)\in A[x]$, then, as $\overline{h}$ is well-defined, $\overline{h}(a(x)b'(x))=\overline{h}(a'(x)b(x))$. As $\overline{h}$ is a homomorphism, then $\overline{h}(a(x))\overline{h}(b'(x))=\overline{h}(a'(x))\overline{h}(b(x))$. That implies $\frac{\overline{h}(a(x))}{\overline{h}(b(x))}=\frac{\overline{h}(a'(x))}{\overline{h}(b'(x))}$, i.e. $\tilde{h}\left(\frac{a(x)}{b(x)}\right)=\tilde{h}\left(\frac{a'(x)}{b'(x)}\right)$, so $\tilde{h}$ is well-defined. Then, take $\frac{a'(x)}{b'(x)}\in B(x)$. It is easy to see, that, as $h(a_i)$ and $h(b_i)$ are coefficients in $a'(x)$ and $'b(x)$, that $\frac{\overline{h}(a(x))}{\overline{h}(b(x))}\in A(x)$, granting coefficients of $a(x)$ and $b(x)$ are $a_i,b_i\in A(x)$, then equals $\frac{a'(x)}{b'(x)}$. Also, if $\tilde{h}\left(\frac{a(x)}{b(x)}\right)=\tilde{h}\left(\frac{(a'(x)}{b(x)}\right)$, then $\frac{\overline{h}(a(x))}{\overline{h}(b(x))}=\frac{\overline{h}(a'(x))}{\overline{h}(b'(x))}$, i.e. $\overline{h}(a(x))\overline{h}(b'(x))=\overline{h}(a'(x))\overline{h}(b(x))$. That means that the coefficients for $a(x)b'(x)=a'(x)b(x)$ satisfy $\sum_{i+j=k}{h(a_i)h(b'_j)}=\sum_{i+j=k}{h(a'_i)h(b_j)}$. As $h$ is an isomorphism, we have $h\left(\sum_{i+j=k}{a_i b'_j}\right)=h\left(\sum_{i+j=k}{a'_i b_j}\right)$. As $h$ is injective, that implies $\sum_{i+j=k}{a_i b'_j}=\sum_{i+j=k}{a'_i b_j}$, i.e. $a(x)b'(x)=a'(x)b(x)$. That implies $\frac{a(x)}{b(x)}=\frac{a'(x)}{b'(x)}$, so $\tilde{h}$ is also injective. Also,

\begin{eqnarray*}
\tilde{h}\left(\frac{a(x)}{b(x)}+\frac{a'(x)}{b'(x)}\right)&=&\tilde{h}\left(\frac{a(x)b'(x)+a'(x)b(x)}{b(x)b'(x)}\right)=\frac{\overline{h}(a(x)b'(x)+a'(x)b(x))}{\overline{h}(b(x)b'(x))}\\\\
&=&\frac{\overline{h}(a(x))\overline{h}(b'(x))+\overline{h}(a'(x))\overline{h}(b(x))}{\overline{h}(b(x))\overline{h}(b'(x))}\\\\
&=&\frac{\overline{h}(a(x)}{\overline{h}(b(x))}+\frac{\overline{h}(a'(x))}{\overline{h}(b'(x))}=\tilde{h}\left(\frac{a(x)}{b(x)}\right)+\tilde{h}\left(\frac{a'(x)}{b'(x)}\right).
\end{eqnarray*}

\noindent\newline Finally,

\begin{eqnarray*}
\tilde{h}\left(\frac{a(x)}{b(x)}\cdot\frac{a'(x)}{b'(x)}\right)&=&\tilde{h}\left(\frac{a(x)a'(x)}{b(x)b'(x)}\right)=\frac{\overline{h}(a(x)a'(x))}{\overline{h}(b(x)b'(x))}\\\\
&=&\frac{\overline{h}(a(x))\overline{h}(a'(x))}{\overline{h}(b(x))\overline{h}(b'(x))}=\frac{\overline{h}(a(x))}{\overline{h}(b(x))}\cdot\frac{\overline{h}(a'(x))}{\overline{h}(b'(x))}\\\\
&=&\tilde{h}\left(\frac{a(x)}{b(x)}\right)\tilde{h}\left(\frac{a'(x)}{b'(x)}\right).
\end{eqnarray*}

\noindent\newline In conclusion, $\tilde{h}$ is an isomorphism from $A(x)$ to $B(x)$. In other words, if $A\cong B$, then $A(x)\cong B(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be a ring. Then we define {\bf multivariate polynomial} inductively. If polynomial ring $A[x_1]$ is denoted by $A_1$, then $A_1[x_2]$ is a polynomial in two variables denoted by $A[x_1,x_2]$. If $A[x_1,\ldots,x_i]$, where $i\in\Z^{+}$, is denoted by $A_i$, then $A_{i+1}[x_{i+1}]$ is a polynomial ring in $i+1$ letters denoted by $A[x_1,\ldots,x_{i+1}]$. The degree of a multivariate polynomial is the highest sum of powers of individual factors appearing in one of its terms having a non-zero coefficient.

\noindent\newline{\bf Problem.} List all the polynomials of degree less than $3$ in $\zmod{3}[x,y]$.

\noindent\newline{\bf Solution.} We have zero polynomial $0$. Then, for zero degree polynomials, there are $1$ and $2$. For degree one polynomials, we have $x$, $x+\overline{1}$, $x+\overline{2}$, $\overline{2}x$, $\overline{2}x+\overline{1}$, $\overline{2}x+\overline{2}$, $y$, $y+\overline{1}$, $y+\overline{2}$, $\overline{2}y$, $\overline{2}y+\overline{1}$, $\overline{2}y+\overline{2}$. For degree $2$ polynomials, we have $\overline{a_{2,0}}x^2+\overline{a_{0,2}}y^2+\overline{a_{1,1}}x y+\overline{a_{1,0}}x+\overline{a_{0,1}}y+\overline{a_{0,0}}$, where at least one of $a_{2,0}$, $a_{0,2}$ and $a_{1,1}$ is not equal to zero. Then, for degree $3$ we have $\overline{a_{3,0}}x^3+\overline{a_{0,3}}y^3+\overline{a_{2,1}}x^2 y+\overline{a_{1,2}}x y^2+\overline{a_{2,0}}x^2+\overline{a_{0,2}}y^2+\overline{a_{1,1}}x y+\overline{a_{1,0}}x+\overline{a_{0,1}}y+\overline{a_{0,0}}$, where at least one of $a_{3,0}$, $a_{0,3}$, $a_{2,1}$ and $a_{1,2}$ is not equal to zero. Of course, all $a_{i,j}\in\Z$, so $\overline{a_{i,j}}\in\zmod{3}$.

\noindent\newline{\bf Proposition.} If $A$ is an integral domain, then $A[x_1,\ldots,x_m]$ is an integral domain, for all $m\in\Z^{+}$.

\noindent\newline{\bf Proof.} Proof by induction. We have already proved the case for $m=1$ in a previous proposition. Then, assume that if $A$ is an integral domain, then $A[x_1,\ldots,x_m]$ is an integral domain. We will prove that it is true for $m+1$. We know that $A[x_1,\ldots,x_{m+1}]$ actually equals $\left(A[x_1,\ldots,x_m]\right)[x_{m+1}]$ by definition. But, as $A$ is an integral domain, then, so is $A[x_1,\ldots,x_m]$, by assumption. Then, as $A[x_1,\ldots,x_m]$ is an integral domain, also $\left(A[x_1,\ldots,x_m]\right)[x_{m+1}]$ is an integral domain.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} We will denote an arbitrary polynomial $a(x,y)\in A[x,y]$ by $\sum_{i,j\in\Z^{+}_0}{a_{i j}x^i y^j}$. Then, if $a(x,y),b(x,y)\in A[x,y]$ such that $a(x,y)=\sum_{i,j\in\Z^{+}_0}{a_{i j}x^i y^j}$ and $b(x,y)=\sum_{i,j\in\Z^{+}_0}{b_{i j}x^i y^j}$. We define addition as $a(x,y)+b(x,y)=\sum_{i,j\in\Z^{+}_0}{(a_{i j}+b_{i j})x^i y^j}$. Let us observe the two polynomials:

\begin{eqnarray*}
a(x,y)&=&a_{0,0}+a_{1,0}x+a_{0,1}y+a_{1,1}x y+a_{2,0}x^2+a_{0,2}y^2+a_{2,1}x^2 y,\\
b(x,y)&=&b_{0,0}+b_{1,0}x+b_{0,1}y+b_{1,1}x y+b_{2,0}x^2+b_{2,1}x^2 y+b_{2,2}x^2 y^2.
\end{eqnarray*}

\noindent\newline Then, their product is:

\begin{eqnarray*}
a(x,y)b(x,y)&=&\left(a_{0,0} b_{0,0}\right)+\left(b_{0,0}a_{1,0}+b_{1,0}a_{0,0}\right)x+\left(b_{0,0}a_{0,1}+b_{0,1}a_{0,0}\right)y\\
&+&\left(b_{0,0}a_{1,1}+a_{0,0}b_{1,1}+a_{0,1}b_{1,0}+b_{0,1}a_{1,0}\right)x y\\
&+&\left(a_{2,0}b_{0,0}+a_{0,0}b_{2,0}+a_{1,0}b_{1,0}\right)x^2+\left(a_{0,2}b_{0,0}+a_{0,1}b_{0,1}\right)y^2\\
&+&\left(a_{2,1}b_{0,0}+b_{2,1}a_{0,0}+a_{1,1}b_{1,0}+b_{1,1}+a_{1,0}+b_{2,0}a_{0,1}+a_{2,0}b_{0,1}\right)x^2 y\\
&+&\left(a_{0,0}b_{2,2}+a_{2,1}b_{0,1}+b_{2,1}a_{0,1}+b_{2,0}a_{0,2}+a_{1,1}b_{1,1}\right)x^2 y^2.
\end{eqnarray*}

\noindent\newline Then, the sum of indices of $a$ and $b$, observed as vectors, corresponds to the exponents of $x$ and $y$, observed as vectors, respectively. Therefore, we can write the formula for the product of two polynomials in two letters as:

\begin{equation*}
a(x,y)b(x,y)=\sum_{\substack{i+k=u\\j+l=v}}{a_{i j}b_{k l}x^u y^v}
\end{equation*}

\noindent\newline{\bf Proposition.} If $A$ is an integral domain, then $\deg{a(x,y)b(x,y)}=\deg{a(x,y)}+\deg{b(x,y)}$.

\noindent\newline{\bf Proof.} Assume $A$ is an integral domain. Then, let $\deg{a(x,y)}=m$ and $\deg{b(x,y)}=n$. That means that the largest sum of exponents (in some term) is $m$ in $a(x,y)$ and $n$ in $b(x,y)$. So, there exist terms $a_{i,j} x^i y^j$, where $a_{i,j}\in A$, and $b_{k,l} x^k y^l$, where $b_{k,l}\in A$, such that $a_{i,j},b_{k,l}\neq 0$ and $i+j=m$ and $k+l=n$. Then, the largest sum of exponents in $a(x,y)b(x,y)$ is $m+n$ appearing in $a_{i,j}b_{k,l}x^{i+k}y^{j+l}$. As $A$ is an integral domain and $a_{i,j},b_{k,l}\neq 0$, then also $a_{i,j}b_{k,l}\neq 0$, so the degree of $a(x,y)b(x,y)$ is $m+n$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Factoring polynomials}
\end{center}

\vskip 0.5cm

\noindent{\bf Theorem.} Let $F$ be a field. Then, every ideal of $F[x]$ is principal.

\noindent\newline{\bf Proof.} Let $J\trianglelefteq F[x]$. We know that at least $0\in J$. Assume $J=\{0\}$. Then, $J=\cyc{0}$, so $J$ is principal. Assume $|J|>1$, i.e. there exists $a(x)\in J$ such that $a(x)\neq 0$. Let $D=\{\deg{b(x)}:\ b(x)\in J-\{0\}\}$. Notice that we put $\deg{0}\notin D$, as $0$ has an undefined degree. Well, at least $a(x)\in J$, then we have $\deg{a(x)}\in D$. So, $D\neq\emptyset$ and as $D\subseteq\Z^{+}_0$, by well-ordering principle, there exists $\deg{a(x)}\in D$ such that $\deg{a(x)}\leq\deg{b(x)}$, for all $b(x)\in J$. Let $b(x)\in J$. Then, by division with remainder for polynomials, there exist $q(x),r(x)\in J$ (we know that they are in $J$ because $J$ is itself a field and both $a(x),b(x)\in J$) such that $0\leq\deg{r(x)}<\deg{a(x)}$ and $b(x)=a(x)q(x)+r(x)$. But, $\deg{r(x)}<\deg{a(x)}$ is in contradiction with $\deg{a(x)}\leq\deg{c(x)}$, for all $c(x)\in J$. Therefore, it can only be that $r(x)=0$, i.e. its degree is undefined (and it can be as $0\in J$). Therefore, $b(x)=a(x)q(x)$ implies that every $b(x)\in J$ can be written as a polynomial multiple of $a(x)$, i.e. $J=\cyc{a(x)}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field and let $a(x),b(x)\in F[x]$ such that $a(x),b(x)\neq 0$. We say that $a(x)$ and $b(x)$ are {\bf associates} if $a(x)|b(x)$ and $b(x)|a(x)$.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $a(x),b(x)\in F[x]-\{0\}$. Then, $a(x)$ and $b(x)$ are associates if and only if there exist $c,d\in F-\{0\}$, such that $a(x)=c b(x)$ and $b(x)=d a(x)$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $a(x)$ and $b(x)$ be associates. That implies there exist $p(x),q(x)\in A[x]$ such that $a(x)=b(x)p(x)$ and $b(x)=a(x)q(x)$. That means that $a(x)=b(x)p(x)=(a(x)q(x))p(x)$, i.e. $a(x)\cdot 1=a(x)(q(x)p(x))$. As $a(x)\neq 0$ and $A[x]$ is an integral domain, that implies $1=q(x)p(x)$. But, that is possible only if $\deg{p(x)}=\deg{q(x)}=0$, so $p(x)=c$ and $q(x)=d$, where $c,d\in F$ and $c,d\neq 0$. That means $a(x)=c b(x)$ and $b(x)=d a(x)$.

{\it Sufficiency.} Let there exist $c,d\in F-\{0\}$ such that $a(x)=c b(x)$ and $b(x)=d a(x)$. That means that, as $c,d\in F[x]$ also, that $b(x)|a(x)$ and $a(x)|b(x)$, i.e. $a(x)$ and $b(x)$ are associates.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field. Then, $a(x)\in F[x]-\{0\}$ is called {\bf monic} if its leading coefficient is equal to $1$, i.e. it is of the form $a(x)=x^m+a_{m-1}x^{m-1}+\cdots+a_1 x+a_0$, where $m\in\Z^{+}_0$ and $a_{m-1},\ldots,a_1,a_0\in F[x]$. Monic polynomial $a(x)\in F[x]-\{0\}$ is called a {\bf monic associate} of a polynomial $b(x)\in F[x]-\{0\}$ if there exists $c\in F-\{0\}$ such that $b(x)=c a(x)$.

\noindent\newline{\bf Proposition.} Let $F$ be a field. Then, every $a(x)\in F[x]-\{0\}$ has a unique monic associate.

\noindent\newline{\bf Proof.} {\it Existence.} Let $a(x)\in F[x]-\{0\}$ such that $a(x)=a_m x^m+\cdots+a_1 x+a_0$. Then, as $F$ is a field, there exists $a_m^{-1}$ such that $a_m a_m^{-1}=a_m^{-1} a_m=1$. Therefore, $a_m^{-1} a(x)=a_m^{-1} a_m x^m+\cdots+a_m^{-1} a_1 x+a_m^{-1} a_0=x^m+\cdots+(a_m^{-1} a_1)x+(a_m^{-1} a_0)$ is a monic associate of $a(x)$. {\it Uniqueness.} Let $a(x)\in F[x]-\{0\}$ and assume $b(x),c(x)\in F[x]-\{0\}$ are both monic associates of $a(x)$. That means that there exist $b',c'\in F-\{0\}$ such that $a(x)=b'b(x)$ and $a(x)=c'c(x)$ and that $b(x)$ and $c(x)$ are monic. We have $a(x)=b'b(x)=c'c(x)$. But, from $b'b(x)=c'c(x)$, we may conclude that they have equal degrees and that all their coefficients are equal. Therefore, if $b_m$ and $c_m$ are the leading coefficients of $b(x)$ and $c(x)$, respectively, we have $b'b_m=c'c_m$. But, as they are monic, that means that $b_m=c_m=1$, so we have $b'1=c'1$, i.e. $b'=c'$. Therefore, all other coefficients of $b'b(x)$ and $c'c(x)$ are $b'b_i$ and $c'c_i$ and we have $b'b_i=c'c_i$. As $b'=c'$, we have $b'b_i=b'c_i$. Multiplying that by $[b']^{-1}$ (which we can as $F$ is a field), we have $[b']^{-1}b'b_i=[b']^{-1}b'c_i$, i.e. $1b_i=1c_i$, which means $b_i=c_i$. Thus, $b(x)=c(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field and let $a(x),b(x)\in F[x]$. We say that $d(x)\in F[x]$ is a {\bf greatest common divisor} of $a(x)$ and $b(x)$ if:

\begin{enumerate}
\item $d(x)|a(x)$ and $d(x)|b(x)$;
\item $d(x)$ is monic;
\item $c(x)|a(x)$ and $c(x)|b(x)$ implies $c(x)|d(x)$, for all $c(x)\in F[x]$.
\end{enumerate}

\noindent We then write $\gcd{\left(a(x),b(x)\right)}=d(x)$.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $a(x),b(x)\in F[x]$. Then there exists a unique $\gcd{(a(x),b(x))}\in F[x]$ such that $\gcd{(a(x),b(x))}=a(x)p(x)+b(x)q(x)$, for some $p(x),q(x)\in F[x]$.

\noindent\newline{\bf Proof.} {\it Existence.} Let $J=\{a(x)p(x)+b(x)q(x):\ p(x),q(x)\in F[x]\}$. Obviously $J\subseteq F[x]$, by its definition. Take $a(x)p_1(x)+b(x)q_1(x),a(x)p_2(x)+b(x)q_2(x)\in J$. It's easy to see that $a(x)p_1(x)+b(x)q_1(x)+a(x)p_2(x)+b(x)q_2(x)=a(x)[p_1(x)+p_2(x)]+b(x)[q_1(x)+q_2(x)]\in J$. Also, $[a(x)p_1(x)+b(x)q_1(x)][a(x)p_2(x)+b(x)q_2(x)]=a(x)p_1(x)a(x)p_2(x)+a(x)p_1(x)b(x)q_2(x)+b(x)q_1(x)a(x)p_2(x)+b(x)q_1(x)b(x)q_2(x)=a(x)[a(x)p_1(x)p_2(x)+p_1(x)b(x)q_2(x)]+b(x)[q_1(x)a(x)p_2(x)+q_1(x)b(x)q_2(x)]\in F[x]$. Finally, if $c(x)\in F[x]$, then $[a(x)p(x)+b(x)q(x)]c(x)=c(x)[a(x)p(x)+b(x)q(x)]=c(x)a(x)p(x)+c(x)b(x)q(x)=a(x)[c(x)p(x)]+b(x)[c(x)q(x)]\in F[x]$. That implies that $J\trianglelefteq F[x]$ and by previous proposition $J$ is a principal ideal generated by some $d'(x)\in F[x]$. If $d'(x)$ is not monic, then we can take $d(x)\in F[x]$ as $d(x)=d_L^{-1} d'(x)$, where $d_L\in F$ is the leading coefficient of $d'(x)$. Then, $J$ is also generated by $d(x)$, because $a(x)\in J$ implies $a(x)=d'(x)r(x)$, for some $r(x)\in J$, but also $a(x)=d_L^{-1}d(x)r(x)$, i.e. $a(x)=d(x)[d_L^{-1}r(x)]$. Now, as we have $d(x)\in J$, then $d(x)=a(x)p(x)+b(x)q(x)$, for some $p(x),q(x)\in F[x]$. Also, as $a(x)1+b(x)0\in J$, i.e. $a(x)\in J$, then $a(x)=d(x)r_a(x)$, for some $r_a(x)\in F[x]$ and we have $d(x)|a(x)$. Similarly, $a(x)0+b(x)1\in J$, that is $b(x)\in J$, so $b(x)=d(x)r_b(x)$ for some $r_b(x)\in F[x]$ and that means $d(x)|b(x)$. Now, let $c(x)|a(x)$ and $c(x)|b(x)$. Then there exist $p_1(x),q_1(x)\in F[x]$ such that $a(x)=c(x)p_1(x)$ and $b(x)=c(x)q_1(x)$. That means that $a(x)p(x)+b(x)q(x)=d(x)$ is equivalent to $c(x)p_1(x)p(x)+c(x)q_1(x)q(x)=d(x)$, i.e. $c(x)[p_1(x)p(x)+q_1(x)q(x)]=d(x)$. That implies that $c(x)|d(x)$. Therefore, by definition, $d(x)=\gcd{(a(x),b(x))}$. {\it Uniqueness.} Assume $c(x)=\gcd{(a(x),b(x))}$ and $d(x)=\gcd{(a(x),b(x))}$. Then, $c(x)|a(x)$ and $c(x)|b(x)$. But, as $d(x)|a(x)$ and $d(x)|b(x)$ also, then $d(x)|c(x)$. But, at the same time, $c(x)|a(x)$ and $c(x)|b(x)$ implies $c(x)|d(x)$. Therefore, $c(x)=d(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field and $p(x)\in F[x]$ a non-constant polynomial. If there do not exist $q(x),r(x)\in F[x]$ such that $\deg{q(x)}\neq 0$ and $\deg{r(x)}\neq 0$ and $p(x)=q(x)r(x)$, we say that $p(x)$ is {\bf irreducible} over $F$.

\noindent\newline{\bf Lemma.} Let $F$ be a field and $p(x)\in F[x]$. Then, the only divisors of $p(x)$ are $c$ and $d p(x)$, where $c,d\in F$.

\noindent\newline{\bf Proof.} It is obvious that $c|p(x)$ as $p(x)=\left(c\right)\left(c^{-1} p(x)\right)$. Also, $d p(x)|p(x)$ because $p(x)=\left(d p(x)\right)\left(d^{-1}\right)$. Both polynomials satisfy the conditions for irreducibility because $\deg{c}=0$ and $\deg{d}=0$. If we assumed that some other $a(x)\in F[x]$, $0<\deg{a(x)}<\deg{p(x)}$, divides $p(x)$, there would have to exist $q(x)\in F[x]$ such that $p(x)=a(x)q(x)$. But, as $F[x]$ is an integral domain, we have $\deg{p(x)}=\deg{a(x)}+\deg{q(x)}$, i.e. $\deg{p(x)}-\deg{a(x)}=\deg{q(x)}$. As $\deg{a(x)}>0$, we have $\deg{q(x)}=\deg{p(x)}-\deg{a(x)}<\deg{p(x)}$ and $0<\deg{p(x)}-\deg{a(x)}=\deg{q(x)}$ because $\deg{a(x)}<\deg{p(x)}$. Then it is impossible as $q(x)$ cannot be equal to a constant polynomial, i.e. both $a(x)$ and $q(x)$ would be non-constant polynomials.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Euclid's lemma for polynomials).} Let $F$ be a field and $p(x)\in F[x]$ a polynomial irreducible over $F$. If $p(x)|a(x)b(x)$, where $a(x),b(x)\in F[x]$, then $p(x)|a(x)$ or $p(x)|b(x)$.

\noindent\newline{\bf Proof.} If $p(x)|a(x)$, we are done. Now, assume that $p(x)\nmid a(x)$. By previous lemma, the only divisors of $p(x)$ are $c$ and $d p(x)$, where $c,d\in F$. Therefore, as for greatest common divisor we only consider monic polynomials, we have $\gcd{\left(a(x),p(x)\right)}\in\{1,p(x)\}$. If it were that $\gcd{\left(a(x),p(x)\right)}=p(x)$, we would have that $p(x)|a(x)$. The only possibility is that $\gcd{\left(a(x),p(x)\right)}=1$. That means that there exist $c(x),d(x)\in F[x]$ such that $a(x)c(x)+p(x)d(x)=1$. As $p(x)|a(x)b(x)$, there exists $q(x)\in F[x]$ such that $a(x)b(x)=p(x)q(x)$. From $a(x)c(x)+p(x)d(x)=1$ we get $a(x)c(x)=1-p(x)d(x)$. So, we multiply $a(x)b(x)=p(x)q(x)$ with $c(x)$ to get $a(x)c(x)b(x)=p(x)q(x)c(x)$. Then, we substitute expression for $a(x)c(x)$ and get $[1-p(x)d(x)]b(x)=p(x)q(x)c(x)$. That gives us $b(x)-p(x)d(x)b(x)=p(x)q(x)c(x)$, i.e. $b(x)=p(x)q(x)c(x)+p(x)d(x)b(x)$. Extracting a common factor gives us $b(x)=p(x)[q(x)c(x)+d(x)b(x)]$. That implies that $p(x)|b(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $F$ be a field and $p(x)\in F[x]$ a polynomial irreducible over $F$. Let $a_1(x),\ldots,a_m(x)\in F[x]$, where $m\in\Z^{+}$, such that $p(x)|a_1(x)\cdots a_m(x)$, then $p(x)|a_i(x)$, for some $i\in\{1,\ldots,m\}$.

\noindent\newline{\bf Proof.} Proof by induction. Assume $m=1$. Then, $p(x)|a_1(x)$ implies $p(x)|a_1(x)$. Assume that $p(x)|a_1(x)\cdots a_m(x)$, for some $m\in\Z^{+}$, implies $p(x)|a_i(x)$, for some $i\in\{1,\ldots,m\}$. If $p(x)|a_1(x)\cdots a_{m+1}(x)$, then $p(x)|\left(a_1(x)\cdots a_m(x)\right)\left(a_{m+1}(x)\right)$. By previous theorem, $p(x)|a_1(x)\cdots a_m(x)$ or $p(x)|a_{m+1}(x)$. If it is the first case, by assumption of induction, we have that $p(x)|a_i(x)$, for some $i\in\{1,\ldots,m\}$. If it is the second case, we already have $p(x)|a_{m+1}(x)$. In conclusion, $p(x)|a_i(x)$, where $i\in\{1,\ldots,m\}\cup\{m+1\}=\{1,\ldots,m+1\}$, proving the corollary is true for all $m\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $F$ be a field and $p(x)\in F[x]$ a polynomial irreducible over $F$. Let $q_1(x),\ldots,q_m(x)$, for some $m\in\Z^{+}$, be monic irreducible polynomials. If it is the case that $p(x)|q_1(x)\cdots q_m(x)$, then $p(x)=q_i(x)$, for some $i\in\{1,\ldots,m\}$.

\noindent\newline{\bf Proof.} By previous corollary we have that $p(x)|q_i(x)$, for some $i\in\{1,\ldots,m\}$. As $q_i(x)$ is irreducible, it's only divisors are $c$ and $d q_i(x)$, where $c\in F[x]$. But, as $q_i(x)$ is monic, it means that $c=d=1$. So, as $p(x)$ is a divisor of $q_i(x)$ it has to be $p(x)\in\{1,q_i(x)\}$. If it were that $p(x)=1$, by definition, it would not be irreducible\footnote{The definition demands that it is a non-constant polynomial - in a similar fashion definition of a prime number demands that a prime number is not equal to $1$.}. Therefore, it must be that $p(x)=q_i(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $F$ be a field and $p(x)\in F[x]$ a polynomial such that $\deg{p(x)}=1$. Then, $p(x)$ is irreducible over $F$.

\noindent\newline{\bf Proof.} Assume $p(x)$ is not irreducible over $F$, i.e. there exist polynomials $a(x),b(x)\in F[x]$ such that $\deg{a(x)}\neq 0$, $\deg{b(x)}\neq 0$ and $p(x)=a(x)b(x)$. That implies $\deg{p(x)}=\deg{a(x)}+\deg{b(x)}$, i.e. $1=\deg{a(x)}+\deg{b(x)}$. But then, if $\deg{a(x)}=1$ it must be that $\deg{b(x)}=0$, which cannot be. Similarly, if $\deg{b(x)}=1$, it must be $\deg{a(x)}=0$, again a contradiction. Therefore there do not exist such polynomials and it must be that $p(x)$ is irreducible over $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Unique factorization of polynomials).} Every polynomial $a(x)$ of positive degree in $F[x]$ can be uniquely written as a product $a(x)=k p_1(x)\cdots p_m(x)$, where $k\in F$ and $p_1(x),\ldots,p_m(x)\in F[x]$ are monic irreducible polynomials.

\noindent\newline{\bf Proof.} {\it Factorization.} Let $m=\deg{a(x)}$. If $m=1$, by previous lemma we have $a(x)=a(x)$. If $a(x)=a_1 x+a_0$ it can be made monic by $a(x)=a_1\left(x+a_0 a_1^{-1}\right)$. Assume that for all $n\in\Z^{+}$, $n<m$ the polynomial $a(x)$ can be written as $a(x)=k p_1(x)\cdots p_n(x)$, where $p_i(x)$ are monic irreducible polynomials. Now, we will prove that the statement is true for $m$. If $a(x)$ of degree $m$ is irreducible, we are done because $a(x)=a_m\left(a_m^{-1}a(x)\right)$ (where $a_m$ is the leading coefficient of $a(x)$). Assume $a(x)$ is not irreducible. Then, by definition, it can be written as a product $a(x)=p(x)q(x)$, where $p(x),q(x)\in F[x]$ such that $\deg{p(x)}\neq 0$ and $\deg{q(x)}\neq 0$. That means that $\deg{p(x)},\deg{q(x)}<\deg{a(x)}=m$, so by assumption of induction, $p(x)$ and $q(x)$ can be written as a products of irreducible monic polynomials, i.e. $p(x)=k p_1(x)\cdots p_{m_1}(x)$ and $q(x)=l q_1(x)\cdots q_{m_2}(x)$, where $m_1,m_2\in\Z^{+}$. Therefore, $a(x)=(k l)p_1(x)\cdots p_{m_1}(x)q_1(x)\cdots q_{m_2}(x)$, meaning it can be written as a product of irreducible monic polynomials with a constant factor.

{\it Uniqueness.} Assume that $a(x)=k p_1(x)\cdots p_{m_1}(x)$ and $a(x)=l q_1(x)\cdots q_{m_2}(x)$ with $m_1\leq m_2$. That implies $k p_1(x)\cdots p_{m_1}(x)=l q_1(x)\cdots q_{m_2}(x)$. By multiplying that expression with $l^{-1}\in F$, we have $(k l^{-1})p_1(x)\cdots p_{m_1}(x)=q_1(x)\cdots q_{m_2}(x)$. As all $q_i$ are not constant polynomials, then it must be that $k l^{-1}=1$, i.e. $k=l$. By Euclid's lemma (and its corollaries), from $p_i|q_1(x)\cdots q_{m_2}(x)$ it follows that, as $q_j$ are monic and irreducible, that $p_i=q_j$ for some $j$. In that fashion we get $p_i=q_{f(i)}$, where $f:\{1,\ldots,m_1\}\rightarrow\{1,\ldots,m_2\}$ is a function. Now, let $S=\{1,\ldots,m_2\}-f\left(\{1,\ldots,m_1\}\right)$. That implies that $\prod_{i\in S}{q_i(x)}=1$, meaning all other $q_i(x)$ can be disregarded as they are equal to $1$. Therefore factorization of $a(x)$ is unique.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $A$ be an integral domain and $a\in A$. If there exist $p_1,\ldots,p_m\in A$ such that $a=p_1\cdots p_m$ and if $p_1\cdots p_m=q_1\cdots q_n$, for some $q_1\cdots q_n\in A$, implies that $m=n$ and that there exists a bijection $f:\{1,\ldots,m\}\rightarrow\{1,\ldots,m\}$ such that $p_i=q_{f(i)}$, for all $i\in\{1,\ldots,m\}$, then we say that $A$ is a {\bf unique factorization domain} (or {\bf UFD} for short).

\noindent\newline{\bf Problem.} Factor $x^4-4$ into irreducible factors over $\Q$, over $\R$ and over $\C$.

\noindent\newline{\bf Solution.} We have $x^4-4=(x^2-2)(x^2+2)$ and $x^2\pm 2$ is irreducible over $\Q$, but $x^2-2=(x-\sqrt{2})(x+\sqrt{2})$, which cannot be further reduced in $\R$, so $x^4-4=(x-\sqrt{2})(x+\sqrt{2})(x^2+2)$. Finally, over $\C$, we have $x^2+2=(x-i\sqrt{2})(x+i\sqrt{2})$, so $x^4-4=(x-\sqrt{2})(x+\sqrt{2})(x-i\sqrt{2})(x+i\sqrt{2})$.

\noindent\newline{\bf Problem.} Factor $x^2-16$ into irreducible factors over $\Q$, over $\R$ and over $\C$.

\noindent\newline{\bf Solution.} Trivially, $x^2-16=(x-4)(x+4)$ over $\Q$, $\R$ and $\C$.

\noindent\newline{\bf Problem.} Find all the irreducible polynomials of degree $\leq 4$ in $\zmod{2}[x]$.

\noindent\newline{\bf Solution.} Let $a(x)=\overline{a_4}x^4+\overline{a_3}x^3+\overline{a_2}x^2+\overline{a_1}x+\overline{a_0}$. We know that, if $\overline{a_0}=\overline{0}$, then $a(x)$ is reducible in $\zmod{2}[x]$ because then we would have $a(x)=\overline{a_4}x^4+\overline{a_3}x^3+\overline{a_2}x^2+\overline{a_1}x=x\left(\overline{a_4}x^3+\overline{a_3}x^2+\overline{a_2}x+\overline{a_1}\right)$. Therefore, we can restrict ourselves to polynomials with $\overline{a_0}=\overline{1}$. For first degree polynomials, all are irreducible, so we have $x$ (an exception to our observation) and $x+\overline{1}$. For degree $2$, we observe $x^2+x+\overline{1}$ and $x^2+\overline{1}$. As those are second degree polynomials, their factorization must be that of two first degree polynomials ($x$ and $x+\overline{1}$). Of all the trivial cases, we can only observe $(x+\overline{1})(x+\overline{1})=x^2+\overline{1}x+\overline{1}x+\overline{1}=x^2+2\cdot\overline{1}x+\overline{1}=x^2+\overline{1}$. So, $x^2+\overline{1}$ is reducible and we are only left with $x^2+x+\overline{1}$ as the only irreducible polynomial of second degree in $\zmod{2}[x]$. For third degree polynomials, we have $x^3+x^2+x+\overline{1}$, $x^3+x^2+\overline{1}$, $x^3+x+\overline{1}$ and $x^3+\overline{1}$. If a third degree polynomial can be factored, then it has to contain a polynomial of a first degree (because $3=2+1=1+1+1$). Therefore, we can observe only $\overline{a_3} x^3+\overline{a_2} x^2+\overline{a_1} x+\overline{1}=\left(\overline{b_1} x+\overline{b_0}\right)\left(\overline{c_2}x^2+\overline{c_1}x+\overline{c_0}\right)$. It definitely must be that $\overline{b_0 c_0}=\overline{1}$ which further implies $\overline{b_0}=\overline{c_0}=\overline{1}$. Then, it is also obvious that it must be $\overline{b_1}=\overline{c_2}=\overline{1}$. So, we have only two cases for $\overline{c_1}\in\{\overline{0},\overline{1}\}$. If $\overline{c_1}=\overline{0}$, then we have $(x+\overline{1})(x^2+\overline{1})=x^3+\overline{1}x+\overline{1}x^2+\overline{1}=x^2+x^2+x+\overline{1}$. If $\overline{c_1}=\overline{1}$, then $(x+\overline{1})(x^2+x+\overline{1})=x^3+x^2+\overline{1}x+\overline{1}x^2+\overline{1}x+\overline{1}=x^3+\overline{1}$. Therefore, the only irreducible polynomials of degree $3$ are $x^3+x^2+\overline{1}$ and $x^3+x+\overline{1}$. Finally, for degree $4$, we have $x^4+x^3+x^2+x+\overline{1}$, $x^4+x^3+x+\overline{1}$, $x^4+x^2+x+\overline{1}$, $x^4+x^3+x^2+\overline{1}$, $x^4+x^3+\overline{1}$, $x^4+x^2+\overline{1}$, $x^4+x+\overline{1}$ and $x^4+\overline{1}$. In this way, we will observe if we can factor $a(x)$ as $(x+\overline{1})(x^3+\overline{c_2}x^2+\overline{c_1}x+\overline{1})$ or $(x^2+\overline{b_1}x+\overline{1})(x^2+\overline{c_1}x+\overline{1})$. It is obvious that leading and constant terms have to be $\overline{1}$. In the first case, if $\overline{c_2}=\overline{c_1}=\overline{0}$, we have $(x+\overline{1})(x^3+\overline{1})=x^4+x^3+x+\overline{1}$. If $\overline{c_2}=\overline{c_1}=\overline{1}$, we have $(x+\overline{1})(x^3+x^2+x+\overline{1})=x^4+x^3+x^2+x+x^3+x^2+x+\overline{1}=x^4+\overline{1}$. If $\overline{c_2}=\overline{0}$ and $\overline{c_1}=\overline{1}$, we have $(x+\overline{1})(x^3+x+\overline{1})=x^4+x^2+x+x^3+x+\overline{1}=x^4+x^3+x^2+\overline{1}$. If $\overline{c_2}=\overline{1}$ and $\overline{c_1}=\overline{0}$, then $(x+\overline{1})(x^3+x^2+\overline{1})=x^4+x^3+x+x^3+x^2+\overline{1}=x^4+x^2+x+\overline{1}$. Finally, in the second case, we observe $(x^2+\overline{b_1}x+\overline{1})(x^2+\overline{c_1}x+\overline{1})$. Let $\overline{b_1}=\overline{c_1}=\overline{0}$. Then, $(x^2+1)^2=x^4+2x^2+\overline{1}^2=x^4+\overline{1}$. Let $\overline{b_1}=\overline{c_1}=\overline{1}$. Then, $(x^2+x+\overline{1})^2=(x^2+x)^2+2(x^2+x)+\overline{1}^2=x^4+2x^3+x^2+\overline{1}=x^4+x^2+\overline{1}$. If $\overline{b_1}=\overline{0}$ and $\overline{c_1}=\overline{1}$, we have $(x^2+\overline{1})(x^2+x+\overline{1})=x^4+x^3+x^2+x^2+x+\overline{1}=x^4+x^3+x+\overline{1}$. So, the only irreducible polynomials of degree $4$ in $\zmod{2}[x]$ are $x^4+x^3+x^2+x+\overline{1}$, $x^4+x^3+\overline{1}$ and $x^4+x+\overline{1}$. Notice that we could have made things much simplier by using the theorems from the next chapter. But, here we illustrated some methods for checking irreducibility. Also, notice the fine example that $x^4+x^2+\overline{1}=(x^2+x+\overline{1})^2$, while $\overline{1}^4+\overline{1}^2+\overline{1}=\overline{1}$, so by the theorem in the next chapter, and as $x-\overline{1}=x+\overline{1}$, we have that $x+\overline{1}\nmid x^4+x^2+\overline{1}$, but it is reducible.

\noindent\newline{\bf Problem.} Show that $x^2+\overline{2}$ is irreducible in $\zmod{5}[x]$. Then factor $x^4-\overline{4}$ into irreducible factors in $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} The only possibility, as $x^2+\overline{2}$ is of degree $1$ is that $x^2+\overline{2}=\overline{k}(x+\overline{a})(x+\overline{b})=\overline{k}(x^2+\overline{b}x+\overline{a}x+\overline{a b})$. It follows that $\overline{k}=\overline{1}$, so we are left with $x^2+\overline{2}=x^2+\overline{a+b}x+\overline{a b}$. From that we have $\overline{2}=\overline{a b}$, i.e. $\overline{2}=\overline{a}\overline{b}$. We also have $\overline{a+b}=\overline{0}$, i.e. $\overline{a}+\overline{b}=\overline{0}$. That implies $\overline{a}=-\overline{b}$. So, from $\overline{2}=\overline{a}\overline{b}$, we have $\overline{2}=-\overline{a}^2$. That is equivalent to $\overline{a}^2=-\overline{2}$, that is $\overline{a}^2=\overline{3}$. From $\overline{1}^2=\overline{4}^2=\overline{1}$ and $\overline{3}^2=\overline{2}^2=\overline{4}$, we conclude that there does not exist such $\overline{a}$ that $\overline{a}^2=\overline{3}$ and our assumption that $x^2+\overline{2}$ is reducible does not hold.

In $\zmod{5}[x]$, we have $x^4-\overline{4}=(x^2-\overline{2})(x^2+\overline{2})=(x^2+\overline{3})(x^2+\overline{2})$. Both $x^2+\overline{3}$ and $x^2+\overline{2}$ are irreducible.

\noindent\newline{\bf Problem.} Factor $\overline{2}x^3+\overline{4}x+\overline{1}$ in $\zmod{5}[x]$.

\noindent\newline{\bf Solution.} As the polynomial in the problem is of degree three, then, if it can be factored, it has to have at least one polynomial of degree one as a factor. Therefore, $\overline{2}x^3+\overline{4}x+\overline{1}=\overline{k}(x+\overline{a_0})(x^2+\overline{b_1}x+\overline{b_0})=\overline{k}x^3+\overline{k(a_0+b_1)}x^2+\overline{k(b_0+a_0 b_1)}x+\overline{k a_0 b_0}$. From that we have $\overline{k}=\overline{2}$ and then:

\begin{eqnarray*}
\overline{2 a_0+2 b_1}&=&\overline{0},\\
\overline{2 b_0+2 a_0 b_1}&=&\overline{4},\\
\overline{2 a_0 b_0}&=&\overline{1}.
\end{eqnarray*}

\noindent\newline After multiplying all equalities by $\overline{2}^{-1}=\overline{3}$ we get:

\begin{eqnarray*}
\overline{a_0+b_1}&=&\overline{0},\\
\overline{b_0+a_0 b_1}&=&\overline{2},\\
\overline{a_0 b_0}&=&\overline{3}.
\end{eqnarray*}

\noindent\newline From first equality we have $\overline{b_1}=-\overline{a_0}$. From the last equality, we get $\overline{b_0}=\overline{3 a_0^{-1}}$. Substituting those two expressions into second equality, we get $\overline{3 a_0^{-1}-a_0^2}=\overline{2}$. Multiplying by $\overline{a_0}$, we get $\overline{3}=\overline{a_0}^3+\overline{2}\overline{a_0}$, i.e. $\overline{3}=\overline{a_0}\left(\overline{a_0}^2+\overline{2}\right)$. The only two possibilities are $\overline{a_0}\in\{\overline{1},\overline{3}\}$. If $\overline{a_0}=\overline{1}$, then $\overline{b_0}=\overline{3}$ and $\overline{3+b_1}=\overline{2}$, so $\overline{b_1}=\overline{4}$. That complies with $\overline{a_0+b_1}=\overline{0}=\overline{5}$. We can check that by multiplying the expressions in brackets: $(x+\overline{1})(x^2+\overline{4}x+\overline{3})=x^3+\overline{4}x^2+\overline{3}x+x^2+\overline{4}x+\overline{3}=x^3+\overline{5}x^2+\overline{2}x+\overline{3}$. Finally $\overline{2}(x+\overline{1})(x^2+\overline{4}x+\overline{3})=\overline{2}x^3+\overline{4}x+\overline{1}$, exactly the polynomial we wanted to factor. We can write:

\begin{equation*}
\overline{2}x^3+\overline{4}x+\overline{1}=\overline{2}(x+\overline{1})(x^2+\overline{4}x+\overline{3}). 
\end{equation*}

\noindent\newline Let us check the case for $\overline{a_0}=\overline{3}$. We would have $\overline{b_0}=\overline{1}$ and $\overline{b_1}=-\overline{a_0}=\overline{2}$. Also, from $\overline{1+3\cdot 2}=\overline{7}=\overline{2}$, we have a guarantee that these solutions will work. Let us check: $\overline{2}(x+\overline{3})(x^2+\overline{2}x+\overline{1})=\overline{2}x^3+\overline{4}x^2+\overline{2}x+\overline{6}x^2+\overline{12}x+\overline{6}=\overline{2}x^3+\overline{10}x^2+\overline{14}x+\overline{6}=\overline{2}x^3+\overline{4}x+\overline{1}$, proving that this factorization is correct. Now, from these two factorizations we have (disregarding the factor of $\overline{2}$, which gets cancelled out):

\begin{equation*}
(x+\overline{1})(x^2+\overline{4}x+\overline{3})=(x+\overline{3})(x^2+\overline{2}x+\overline{1}).
\end{equation*}

\noindent\newline It is obvious that $\gcd{(x+\overline{1},x+\overline{3})}=1$, so by Euclid's lemma for polynomials, we have that $x+\overline{1}|x^2+\overline{2}x+\overline{1}$. Therefore, $x^2+\overline{2}x+\overline{1}=(x+\overline{1})(x+\overline{c_0})=x^2+\overline{c_0+1}+\overline{c_0}$. That implies $\overline{c_0}=1$, which is in accordance with $\overline{c_0+1}=\overline{1+1}=\overline{2}$. So, we have $x^2+\overline{2}x+\overline{1}=(x+\overline{1})^2$ and then:

\begin{equation*}
(x+\overline{1})(x^2+\overline{4}x+\overline{3})=(x+\overline{3})(x+\overline{1})^2.
\end{equation*}

\noindent\newline Last check, $(x+\overline{3})(x+\overline{1})=x^2+\overline{4}x+\overline{3}$. In conclusion:

\begin{equation*}
\overline{2}x^3+\overline{4}x+\overline{1}=\overline{2}\left(x+\overline{3}\right)\left(x+\overline{1}\right)^2.
\end{equation*}

\noindent\newline{\bf Remark.} We could have solved the problem more efficiently using the theorem from the following chapter, but we wanted do display some techniques without using the fact that $(x-c)|p(x)$ iff $p(x)=0$.

\noindent\newline{\bf Problem.} In $\zmod{6}[x]$, factor each of the following into two polynomials of degree $1$: $x$, $x+\overline{2}$, $x+\overline{3}$. Why is this possible?

\noindent\newline{\bf Solution.} We have $x=(\overline{3}x+\overline{2})(\overline{2}x+\overline{3})$, $x+\overline{2}=(\overline{3}x+\overline{2})(\overline{2}x+\overline{1})$ and $x+\overline{3}=(\overline{3}x+\overline{5})(\overline{2}x+\overline{3})$. That is possible because $\zmod{6}$ is not an integral domain and so is not $\zmod{6}[x]$. Therefore, there exist elements that produce zero when multiplied which makes it possible for the leading term to become zero.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $a(x),b(x)\in F[x]$. Then,

\begin{enumerate}
\item If $a(x)$ and $b(x)$ are distinct monic polynomials, they cannot be associates.
\item Any two distinct irreducible polynomials are relatively prime.
\item If $a(x)$ is irreducible, any associate of $a(x)$ is irreducible.
\item If $a(x)\neq 0$, $a(x)$ cannot be an associate of $0$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a(x)=x^m+a_{m-1}x^{m-1}+\cdots+a_1 x+a_0$ and $b(x)=x^n+b_{n-1}x^{n-1}+\cdots+b_1 x+b_0$. Assume that $a(x)=k b(x)$, for some $k\in F$. If $m\neq n$, then also $a(x)\neq k b(x)$, i.e. they cannot be associates. Then assume that $m=n$ and we have $a(x)=k x^n+(k b_{n-1})x^{n-1}+(k b_1)x+(k b_0)$. Then, we would have the coefficients of leading terms correspond, i.e. $k=1$. But, then $a(x)=b(x)$ which is a contradiction. {\it Ad $2$.} If they were not relatively prime, their greatest common divisor would be some polynomial $p(x)$, which would mean $a(x)=p(x)a_1(x)$ and $b(x)=p(x)b_1(x)$, which is a contradiction to the fact that they are irreducible. {\it Ad $3$.} Let $a(x)$ be irreducible and $b(x)$ an associate of $a(x)$. That means that $a(x)=k b(x)$, for some $k\in F$. If $b(x)$ were not irreducible, there would exist $p(x),q(x)\in F[x]$ such that $b(x)=p(x)q(x)$, but that would mean that $a(x)=k p(x)q(x)$, a contradiction to the condition of irreducibility of $a(x)$. {\it Ad $4$.} Let $a(x)\neq 0$. Assume $a(x)$ is an associate of $0$. Then, $a(x)=k 0$, for some $k\in F$. But then $a(x)=k 0=0$, a contradiction to assumption that $a(x)\neq 0$. 

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p\in P$. In $\zmod{p}[x]$, every nonzero polynomial has exactly $p-1$ associates.

\noindent\newline{\bf Proof.} Let $a(X)\in\zmod{p}[x]$ be a non-zero polynomial. As $\zmod{p}=\{0,1,\ldots,p-1\}$, the number of associates of $a(x)$ is $|S|$, where $S=\{\overline{k} a(x):\ \overline{k}\in\zmod{p}-\{0\}\}$. We have excluded zero, as it can never be an associate of $a(x)$ (nor any of its multiples, which are actually, again, zeros). It is obvious that $|S|\leq p-1$, because $|\zmod{p}-\{0\}|=p-1$. Assume $\overline{k_1} a(x)=\overline{k_2} a(x)$, for some $\overline{k_1},\overline{k_2}\in\zmod{p}[x]$ such that $\overline{k_1}\neq\overline{k_2}$. From $\overline{k_1} a(x)=\overline{k_2} a(x)$, as $\zmod{p}[x]$ is an integral domain (because $\zmod{p}$ is a field), by cancellation law (as $a(x)\neq 0$) we have $\overline{k_1}=\overline{k_2}$, a contradiction to our assumption. Therefore, all $\overline{k}a(x)\in S$, for $\overline{k}\in\zmod{p}-\{0\}$ are mutually different for different $\overline{k}\in\zmod{p}-\{0\}$. So, there are as many elements in $S$ as there are in $\zmod{p}-\{0\}$, and that implies $|S|=p-1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} $x^2+\overline{1}$ is reducible in $\zmod{p}[x]$ if and only if there exist $a,b\in\Z^{+}$ such that $p=a+b$ and $a b\equiv 1\pmod p$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $x^2+\overline{1}$ is reducible in $\zmod{p}[x]$. Then we can write $x^2+1=\overline{k}(x+\overline{a_0})(x+\overline{b_0})$, for some $\overline{k},\overline{a_0},\overline{b_0}\in\zmod{p}$. We can assume $0\leq k,a_0,b_0<p$. Then, $\overline{k}=\overline{1}$ and it must be that $\overline{a_0}+\overline{b_0}=\overline{0}$ and $\overline{a_0}\cdot\overline{b_0}=\overline{1}$. From the latter condition, we have $a_0 b_0\equiv 1\pmod p$ and from the former, $a_0+b_0\equiv 0\pmod p$. That implies $a_0+b_0=q p$, for some $q\in\Z$. But, as $a_0,b_0<p$, we have $a_0+b_0<2 p$, so it must be that $q=1$, i.e. $a_0+b_0=p$. {\it Sufficiency.} Let $p\in P$ and $a,b\in\Z^{+}$ such that $p=a+b$ (from which we get $\overline{a+b}=\overline{0}$, due to $a+b-0=p$) and $a b\equiv 1\mod p$. Then, $(x+\overline{a})(x+\overline{b})=x^2+\overline{a+b}x+\overline{a b}=x^2+\overline{1}$, so $x^2+\overline{1}$ is reducible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p\in P$. The number of irreducible quadratics in $\zmod{p}[x]$ is:

\begin{equation*}
(p-1)\left(p^2-\left[\binom{p}{1}+\binom{p}{2}\right]\right).
\end{equation*}

\noindent\newline{\bf Proof.} Let $m\in\Z^{+}$. Then, $P_m=\left\{q(x)\in\zmod{p}[x]:\ \deg{q(x)}=m\right\}$. Then, as for each $\overline{q_i}$ in $q(x)=\overline{q_m} x^m+\cdots+\overline{q_1}x+\overline{q_0}$ we can choose $p$ elements (except for $\overline{q_m}$ which cannot be zero), we have that $|P_m|=(p-1)p^m$. That is the number of all polynomials of degree $m$ with coefficients in $\zmod{p}$. Thus, $|P_2|=(p-1)p^2$. Let $I_m=\{q(x)\in P_m:\ (\forall a(x),b(x)\in\zmod{p}[x])(q(x)\neq a(x)b(x))\}$. We know that all degree $1$ polynomials are irreducible and they are of the form $q(x)=\overline{q_1}x+\overline{q_0}$, where $\overline{q_0},\overline{q_1}\in\zmod{p}$ and $\overline{q_1}\neq\overline{0}$. It is obvious that then $I_1=P_1$ and $|I_1|=|P_1|=(p-1)p$. Now, if a polynomial is reducible in $P_2$, then it can be written as a product of two polynomials in $I_1=P_1$. Then, the number of ways in which we can choose two polynomials from $I_1$ (with the possibility of repeating the same, i.e. $(x+\overline{1})^2$) is equal to the number of ways in which we can distribute $2$ balls in $|I_1|$ boxes\footnote{Consider that each box represents a polynomial and that underline is the ball. Then, for example in $\zmod{3}[x]$, we could have $x,x+\overline{1},\underline{x+\overline{2}},\overline{2}x,\underline{\overline{2}x+\overline{1}},\overline{2}x+\overline{2}$ which would represent $(x+\overline{2})(\overline{2}x+\overline{1})=\overline{2}x^2+\overline{2}x+\overline{2}$.}. Considering only monic polynomials as the "building blocks", which we can put in $I^M_m$ (representing irreducible monic polynomials of degree $m$), the number of ways in which we can choose $2$ polynomials in $I^M_1$ (with repetition) is:

\begin{equation*}
\binom{|I^M_1|+2-1}{2}=\binom{|I^M_1|+1}{2}.
\end{equation*}

\noindent\newline The only thing left to do is to determine $|I^M_1|$. But, that is quite easy, as we only observe $x+\overline{q_0}$, and for that we have $p$ choices. So, finally, the number of reducible polynomials in $P_2$, as we can put all coefficients except for zero to get non-monic polynomials (explaining $p-1$ as a coefficient of the binomial coefficient), is:

\begin{eqnarray*}
|I_2|&=&|P_2|-(p-1)\binom{|I^M_1|+1}{2}\\\\
&=&(p-1)p^2-(p-1)\binom{p+1}{2}.
\end{eqnarray*}

\noindent\newline That can be simplified as:

\begin{eqnarray*}
|I_2|&=&(p-1)p^2-(p-1)\binom{p+1}{1+1}\\\\
&=&(p-1)p^2-(p-1)\left[\binom{p}{1}+\binom{p}{2}\right].
\end{eqnarray*}

\noindent\newline The formula above actually gives us quite good combinatorial glimpse at how this works. We can either choose $1$ polynomial from $I_1$ and then raise it to the second power or choose two different polynomials from $I_1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Reasoning the same as above, we can conclude that the number of irreducible cubics in $\zmod{p}[x]$ is:

\begin{equation*}
|I_3|=|P_3|-(p-1)\left[|I^M_2|\cdot|I^M_1|+\left(\binom{|I^M_1|}{3}\right)\right].
\end{equation*}

\noindent\newline{\bf Proposition.} Let $F$ be a field and $J\trianglelefteq F[x]$. Then,

\begin{enumerate}
\item Any two generators of $J$ are associates.
\item $J$ has a unique monic generator $m(x)\in F[x]$.
\item Let $a(x)\in F[x]$. Then, $a(x)\in\cyc{m(x)}$ if and only if $m(x)|a(x)$.
\item If $p(x)\in F[x]$ is irreducible, then $\cyc{p(x)}$ is a maximal ideal of $F[x]$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $a(x),b(x)\in J-\{0\}$ such that $J=\cyc{a(x)}=\cyc{b(x)}$. As $b(x)\in J=\cyc{a(x)}$, then there exists $p(x)\in F[x]$ such that $b(x)=a(x)p(x)$. As $a(x)\in J=\cyc{b(x)}$, then there exists $q(x)\in F[x]$ such that $a(x)=b(x)q(x)$. Multiplying that by $p(x)$ we get $a(x)p(x)=b(x)p(x)q(x)$ and that is equivalent to $b(x)=b(x)p(x)q(x)$. As $F[x]$ is an integral domain (because $F$ is a field) and $b(x)\neq 0$, then $b(x)=b(x)p(x)q(x)$ implies $1=p(x)q(x)$. From that follows that $p(x)=p_0$ and $q(x)=q_0$, where $p_0,q_0\in F$. That means that $a(x)=q_0 b(x)$ and $b(x)=p_0 a(x)$ (with $q_0^{-1}=p_0$). To conclude, $a(x)$ and $b(x)$ are associates. {\it Ad $2$.} Assume that $J=\cyc{m(x)}$ where $m(x)$ is monic. Assume that also $J=\cyc{p(x)}$ where $p(x)$ is monic and assume that they are distinct. Then, by previous result, $p(x)$ and $m(x)$ are associates. From a previous problem, there do not exist distinct monic associates, so it must be that $m(x)=p(x)$. {\it Ad $3$.} Let $a(x)\in F[x]$. {\it Necessity.} Assume $a(x)\in\cyc{m(x)}$. Then, there exists $p(x)\in F[x]$ such that $a(x)=m(x)p(x)$. From that we conclude that $m(x)|a(x)$. {\it Sufficiency.} Let $m(x)|a(x)$. Then by definition there exists $q(x)\in F[x]$ such that $a(x)=q(x)m(x)$. But, as $q(x)\in F[x]$, then $q(x)m(x)\in\cyc{m(x)}$, i.e. $a(x)\in\cyc{m(x)}$. {\it Ad $4$.} Let $p(x)\in F[x]$ be irreducible. We have that $\cyc{p(x)}\trianglelefteq F[x]$. Assume that there exists $J\trianglelefteq F[x]$ such that $\cyc{p(x)}\subset J\subseteq F[x]$. Such $J$ exists because $p(x)\neq 0$ and $F[x]$ is an integral domain, so it cannot be a trivial ring. Also, $F[x]-\cyc{p(x)}\neq\emptyset$ (i.e. $F[x]\neq\cyc{p(x)}$), because, i.e. $p(x)q(x)+1\in F[x]$, but $p(x)\notin\cyc{p(x)}$. If the latter were the case we would have $p(x)q(x)+1=p(x)r(x)$, for some $r(x)\in F[x]$, which would imply $1=p(x)[r(x)-q(x)]$, i.e. that $p(x)$ is a constant polynomial, a contradiction to the assumption that it is irreducible. So, $\cyc{p(x)}\subset J\subseteq F[x]$ is possible and let $a(x)\in J-\cyc{p(x)}$ such that $a(x)\neq 0$. As $a(x)\notin\cyc{p(x)}$, then there does not exist $b(x)$ such that $a(x)=p(x)b(x)$, i.e. $p(x)\nmid a(x)$. That implies that, as $p(x)$ is irreducible, $\gcd{(a(x),p(x))}=1$. By Bezout's lemma for polynomials, there exist $q(x),r(x)\in J$ such that $a(x)q(x)+p(x)r(x)=1$. As $a(x),q(x)\in J$ then $a(x)q(x)\in J$. Also, $p(x)r(x)\in\cyc{p(x)}\subset J$, so $a(x)q(x)+p(x)r(x)=1\in J$. But, if we take $a(x)\in F[x]$, where $a(x)\neq 0$, then $a(x)1\in J$, i.e. $a(x)\in J$ (because $J\trianglelefteq F[x]$). That implies $F[x]\subseteq J$, and with $J\subseteq F[x]$, we have $J=F[x]$, meaning $\cyc{p(x)}$ is maximal ideal of $F[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $S$ be the set of all polynomials $a_0+a_1 x+\cdots+a_m x^m\in F[x]$, such that $a_0+a_1+\cdots+a_m=0$. Then, $x-1\in S$ and $S=\cyc{x-1}$. Also, $F[x]\slash\cyc{x-1}\cong F$.

\noindent\newline{\bf Proof.} It is obvious that $1x-1=x-1\in S$ because $1+(-1)=0$. Let $S'=\cyc{x-1}$. Let $(x-1)q(x)\in\cyc{x-1}$. Then, if we take $q(x)=q_m x^m+\cdots+q_1 x+q_0$, we have $(x-1)(q_m x^m+\cdots+q_1 x+q_0)=q_m x^{m+1}+(q_{m-1}-q_m)x^m+\cdots+(q_1-q_2) x^2+(q_0-q_1) x-q_0$. Then, obviously $q_m+(q_{m-1}-q_m)+\cdots+(q_1-q_2)+(q_0-q_1)-q_0=(q_m-q_m)+(q_{m-1}-q_{m-1})+\cdots+(q_2-q_2)+(q_1-q_1)+(q_0-q_0)=0$. Therefore $(x-1)q(x)\in S$, i.e. $S'\subseteq S$. Let $q(x)\in S$. Then, $q_m+\cdots+q_1+q_0=0$. We will show that $x-1|q(x)$, for all degrees $m\in\Z^{+}-\{1\}$ (note that $0\in S$ and $0\in\cyc{x-1}$, so we will exclude that case; also note that there are no constant polynomials in $\cyc{x-1}$). Then, let $q(x)=q_m x^m+\cdots+q_1 x+q_0\in S$. We have $q_m+\cdots+q_1+q_0=0$. Then, $q_0=-(q_m+\cdots+q_1)$ and we have $q_m(x^m-1)+q_{m-1}(x^{m-1}-1)+\cdots+q_1(x-1)$. From a previous proposition (for all rings with unity, at the beginning of the ring theory), we know that $x-1|x^n-1$, for all $n\in\Z^{+}$. Therefore, we can write $q(x)=q_m x^m+\cdots+q_1 x+q_0=(x-1)[q_m a_m(x)+\cdots+q_1 a_1(x)]$, where $a_n(x)\in F[x]$, for all $0<n\leq m$, such that $(x^n-1)=(x-1)a_n(x)$. From that we have $x-1|q(x)$ and it must be that $x-1\in\cyc{x-1}$, i.e. $S\subseteq\cyc{x-1}$. That implies $S=\cyc{x-1}$.

Now, define $f:F[x]\rightarrow F$ such that $f(q_m x^m+\cdots+q_1 x+q_0)=q_m+\cdots+q_1+q_0$. It is obvious that $f$ is a well defined function, as it is defined for all $q(x)\in F[x]$ and if $q(x)=p(x)$, it follows that $q_i=p_i$, for all $0\leq i\leq\deg{q(x)}=\deg{p(x)}$, so $q_{\deg{q(x)}}+\cdots+q_1+q_0=p_{\deg{q(x)}}+\cdots+p_1+p_0$. If we take $a\in F$, then, $a x-a\in F[x]$ and $a+(-a)=0$, so there exists $a x-a$ such that $f(a x-a)=a$. However, $f$ is not injective as also $a x^m-a\in F[x]$, for all $m\in\Z^{+}$, and $a+(-a)=0$. But, as it is surjective, as we have proved, by fundamental homomorphism theorem for rings, we have $F\cong F[x]\slash\ker{f}$. It is obvious that $\ker{f}=\{q_m x^m+\cdots+q_1 x+q_0\in F[x]:\ q_m+\cdots+q_1+q_0=0\}=\cyc{x-1}$, so $F\cong F[x]\slash\cyc{x-1}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field. Then, $F[x,y]$ is not a principal ideal domain.

\noindent\newline{\bf Proof.} Let $J\trianglelefteq F[x,y]$ such that $J$ contains all polynomials in $F[x,y]$ whose constant coefficient is zero (it is obvious that $J$ is an ideal, as the sum and product of two polynomials with zero constant coefficients will give a polynomial with a zero constant coefficient; also the product of a polynomial with a zero coefficient and a polynomial with a non-zero coefficient will again yield a polynomial with a zero constant coefficient). Assume that $\cyc{q(x,y)}=J$. But, then $x+y\in J$ and it must be $x+y=q(x,y)p(x,y)$, where $p(x,y)\in F[x,y]$. But, it is obvious that $x+y$ cannot split in $F[x,y]$. It's degree is $1$ and it would have to be $q(x,y)=1$ and $p(x,y)=0$ or $p(x,y)=1$ and $q(x,y)=0$; the latter is not possible at all, but the former would only imply that $q(x,y)=x+y$, meaning that $x+y$ is irreducible. It is obvious that, for example, $x+y\nmid x$, although $x\in J$, so $J$ cannot be a principal ideal domain.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} In $\Q$, using the Euclidean algorithm find the greatest common divisor of: (a) $x^3+1$ and $x^4+x^3+2x^2+x-1$; (b) $x^{24}-1$ and $x^{15}-1$. Express the greatest common divisors as the linear combinations of those two pairs of polynomials.

\noindent\newline{\bf Solution.} (a) Using the algorithm for polynomial division, we have:

\begin{eqnarray*}
&&(x^4+x^3+2x^2+x-1):(x^3+1)=x+1\\
&-&\underline{(x^4+x)}\\
&&x^3+2x^2-1\\
&-&\underline{(x^3+1)}\\
&&2x^2-2.
\end{eqnarray*}

\noindent\newline Thus, $x^4+x^3+2x^2+x-1=(x^3+1)(x+1)+(2x^2-2)$ and we then divide $x^3+1$ by $2x^2-2$ to get:

\begin{eqnarray*}
&&(x^3+1):(2x^2-2)=\frac{1}{2}x\\
&-&\underline{(x^3-x)}\\
&&x+1.
\end{eqnarray*}

\noindent\newline From that we have $x^3+1=\frac{x}{2}\left(2x^2-2\right)+(x+1)$. Then, we divide $2x^2-2$ by $x+1$. That is easy to do because $2x^2-2=2(x^2-1)=2(x+1)(x-1)$, so $2x^2-2=(2x-2)(x+1)$. As here remainder is zero, the last residue is $x+1$, and that means that:

\begin{equation*}
\gcd{(x^4+x^3+2x^2+x-1,x^3+1)}=x+1.
\end{equation*}

\noindent\newline In order to express the greatest common divisor as the linear combination of these two polynomials, we need to roll back the following process:

\begin{eqnarray*}
x^4+x^3+2x^2+x-1&=&(x^3+1)(x+1)+(2x^2-2),\\
x^3+1&=&\frac{x}{2}\left(2x^2-2\right)+(x+1),\\
2x^2-2&=&(2x-2)(x+1).
\end{eqnarray*}

\noindent\newline  From the first row, we have $2x^2-2=(x^4+x^3+2x^2+x-1)-(x^3+1)(x+1)$. Substituting that for $2x^2-2$ obtained from the second row as $x+1=(x^3+1)-\frac{x}{2}(2x^2-2)$, we finally get $x+1=(x^3+1)-\frac{x}{2}\left[(x^4+x^3+2x^2+x-1)-(x+1)(x^3+1)\right]$. That is equivalent to:

\begin{equation*}
x+1=\left(\frac{x^2}{2}+\frac{x}{2}+1\right)\left(x^3+1\right)+\left(-\frac{x}{2}\right)\left(x^4+x^3+2x^2+x-1\right).
\end{equation*}

\noindent\newline Notice that this would not be possible to find in $\Z[x]$ as we have rational coefficients in some terms. (b) We will divide $x^{24}-1$ by $x^{15}-1$ first:

\begin{eqnarray*}
&&\left(x^{24}-1\right):\left(x^{15}-1\right)=x^9\\
&-&\underline{\left(x^{24}-x^9\right)}\\
&&x^9-1.
\end{eqnarray*}

\noindent\newline Therefore, $x^{24}-1=x^9\left(x^{15}-1\right)+\left(x^9-1\right)$. Now, we divide $x^{15}-1$ by $x^9-1$. We have:

\begin{eqnarray*}
&&\left(x^{15}-1\right):\left(x^9-1\right)=x^6\\
&-&\underline{\left(x^{15}-x^6\right)}\\
&&x^6-1.
\end{eqnarray*}

\noindent\newline So, as $x^{15}-1=x^6\left(x^9-1\right)+\left(x^6-1\right)$, we divide $x^9-1$ by $x^6-1$ and:

\begin{eqnarray*}
&&\left(x^9-1\right):\left(x^6-1\right)=x^3\\
&-&\underline{\left(x^9-x^3\right)}\\
&&x^3-1.
\end{eqnarray*}

\noindent\newline Then, $x^9-1=x^3\left(x^6-1\right)+\left(x^3-1\right)$ and we must divide $x^6-1$ by $x^3-1$. That is easy to do because $x^6-1=\left(x^3+1\right)\left(x^3-1\right)$. We now know, as the last residue is $x^3-1$, that

\begin{equation*}
\gcd{\left(x^{24}-1,x^{15}-1\right)}=x^3-1.
\end{equation*}

\noindent\newline In order to express the greatest common divisor as the linear combination of $x^{24}-1$ and $x^{15}-1$, we need to roll back the whole process:

\begin{eqnarray*}
x^{24}-1&=&x^9\left(x^{15}-1\right)+\left(x^9-1\right),\\
x^{15}-1&=&x^6\left(x^9-1\right)+\left(x^6-1\right),\\
x^9-1&=&x^3\left(x^6-1\right)+\left(x^3-1\right),\\
x^6-1&=&\left(x^3+1\right)\left(x^3-1\right).
\end{eqnarray*}

\noindent\newline First, $x^3-1=(x^9-1)-x^3(x^6-1)$ (from the third row). Then, from the second row, $x^6-1=(x^{15}-1)-x^6(x^9-1)$ and substituting that in the third row we have $x^3-1=(x^9-1)-x^3\left[(x^{15}-1)-x^6(x^9-1)\right]$. That can be written more neatly as $x^3-1=(x^9-1)(x^9+1)-x^3(x^{15}-1)$. From the first row we have $x^9-1=(x^{24}-1)-x^9(x^{15}-1)$, and substituting that in the former expression, we get $x^3-1=\left(x^9+1\right)\left[(x^{24}-1)-x^9(x^{15}-1)\right]-x^3(x^{15}-1)$. Finally, that is equivalent to:

\begin{equation*}
x^3-1=\left(x^9+1\right)\left(x^{24}-1\right)+\left(-x^{18}-x^9-x^3\right)\left(x^{15}-1\right).
\end{equation*}

\noindent\newline{\bf Problem.} In $\zmod{3}$, using the Euclidean algorithm find the greatest common divisor of $x^3+\overline{1}$ and $x^4+x^3+\overline{2}x^2+x-\overline{1}$. Express the greatest common divisor as the linear combination of those two polynomials.

\noindent\newline{\bf Solution.} Using the algorithm for polynomial division to divide $x^4+x^3+\overline{2}x^2+x-\overline{1}$ by $x^3+\overline{1}$ we have:

\begin{eqnarray*}
&&\left(x^4+x^3+\overline{2}x^2+x-\overline{1}\right):\left(x^3+\overline{1}\right)=x+\overline{1}\\
&-&\underline{(x^4+x)}\\
&&x^3+\overline{2}x^2-\overline{1}\\
&-&\underline{(x^3+\overline{1})}\\
&&\overline{2}x^2-\overline{2}.
\end{eqnarray*}

\noindent\newline That means that $x^4+x^3+\overline{2}x^2+x-\overline{1}=\left(x+\overline{1}\right)\left(x^3+\overline{1}\right)+\left(\overline{2}x^2-\overline{2}\right)$. Then we divide $x^3+\overline{1}$ by $\overline{2}x^2-\overline{2}$ and we have:

\begin{eqnarray*}
&&\left(x^3+\overline{1}\right):\left(\overline{2}x^2-\overline{2}\right)=\overline{2}x\\
&-&\underline{(x^3-x)}\\
&&x+\overline{1}.
\end{eqnarray*}

\noindent\newline From that we have $x^3+\overline{1}=\overline{2}x\left(\overline{2}x^2-\overline{2}\right)+\left(x+\overline{1}\right)$. We divide $\overline{2}x^2-\overline{2}$ by $x+\overline{1}$ easily as $\overline{2}x^2-\overline{2}=\overline{2}\left(x^2-\overline{1}\right)=\overline{2}\left(x+\overline{1}\right)\left(x-\overline{1}\right)$. We have $\overline{2}x^2-\overline{2}=\left(\overline{2}x-\overline{2}\right)\left(x+\overline{1}\right)$. That implies:

\begin{equation*}
\gcd{\left(x^4+x^3+\overline{2}x^2+x-\overline{1},x^3+\overline{1}\right)}=x+\overline{1}.
\end{equation*}

\noindent\newline Unrolling the following process will give us the greatest common divisor as the linear combination of those two polynomials:

\begin{eqnarray*}
x^4+x^3+\overline{2}x^2+x-\overline{1}&=&\left(x+\overline{1}\right)\left(x^3+\overline{1}\right)+\left(\overline{2}x^2-\overline{2}\right),\\
x^3+\overline{1}&=&\overline{2}x\left(\overline{2}x^2-\overline{2}\right)+\left(x+\overline{1}\right),\\
\overline{2}x^2-\overline{2}&=&\left(\overline{2}x-\overline{2}\right)\left(x+\overline{1}\right).
\end{eqnarray*}

\noindent\newline From the second row we have $x+\overline{1}=\left(x^3+\overline{1}\right)-\overline{2}x\left(\overline{2}x^2-\overline{2}\right)$ and from the first row $\overline{2}x^2-\overline{2}=\left(x^4+x^3+\overline{2}x^2+x-\overline{1}\right)-\left(x+\overline{1}\right)\left(x^3+\overline{1}\right)$. Substituting latter expression into the former expression we finally obtain the equality for the greatest common divisor, $x+\overline{1}=\left(x^3+\overline{1}\right)-\overline{2}x\left[\left(x^4+x^3+\overline{2}x^2+x-\overline{1}\right)-\left(x+\overline{1}\right)\left(x^3+\overline{1}\right)\right]$. That is equivalent to:

\begin{equation*}
x+\overline{1}=x\left(x^4+x^3+\overline{2}x^2+x+\overline{2}\right)+\left(\overline{2}x^2+\overline{2}x+\overline{1}\right)\left(x^3+\overline{1}\right).
\end{equation*}

\noindent\newline{\bf Proposition.} Let $F$ be a field, $G\subseteq F[x]$ such that $a(x)\in G$ if and only if constant term of $a(x)$ is non-zero. Let $h:G\rightarrow G$ be a mapping defined by:

\begin{equation*}
h(a_0+a_1 x+\cdots+a_m x^m)=a_m+a_{m-1}x+\cdots+a_0 x^m.
\end{equation*}

\noindent\newline Then:

\begin{enumerate}
\item $h$ is a function and, for all $a(x),b(x)\in G$, $h(a(x)b(x))=h(a(x))h(b(x))$;
\item $h$ is injective, surjective and $h\circ h=\epsilon$;
\item $a(x)$ is irreducible if and only if $h(a(x))$ is irreducible;
\item If $c\in F$, $a(c)=0$ if and only if $[h\left(a(x)\right)]\left(c^{-1}\right)=0$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} It is obvious that $h$ is defined for all $a(x)$. Assume $a(x)=b(x)$. Then, $\deg{a(x)}=\deg{b(x)}$ and they have equal corresponding coefficients. It's easy to see that then $h(a(x))=h(b(x))$, so $h$ is a function. Let $a(x),b(x)\in G$. As the constant term of $a(x),b(x)$ is non-zero, then $\deg{a(x)}=\deg{h(a(x))}$ and $\deg{b(x)}=\deg{h(b(x))}$. So, $\deg{h(a(x)b(x))}=\deg{a(x)b(x)}=\deg{a(x)}+\deg{b(x)}=\deg{h(a(x))}+\deg{h(b(x))}$. Then, the $k$-th term of $h(a(x)b(x))$ is equal to $c_{\deg{c}-k}$ and $k$-th term of $h(a(x))h(b(x))$ is equal to:

\begin{equation*}
c_k=\sum_{i+j=k}{a_{\deg{a(x)}-i}b_{\deg{b(x)}-j}}=\sum_{\deg{a(x)}-i+\deg{b(x)}-j=k}{a_i b_j}=\sum_{i+j=\deg{a(x)b(x)}-k}{a_i b_j}.
\end{equation*}

\noindent\newline That is equal to the $k$-th coefficient of $h(a(x)b(x))$. Therefore, $h(a(x))h(b(x))=h(a(x)b(x))$.

{\it Ad $2$.} Let $h(a(x))=h(b(x))$. Then, $\deg{h(a(x))}=\deg{h(b(x))}=m$ and their coefficients correspond. But then, also their $(m-i)$-th coefficients correspond, so $a(x)=b(x)$. If $a(x)\in G$, then it is obvious that there exists $b(x)\in G$ such that $h(b(x))=a(x)$. Finally, $h$ carries $i$-th coefficient to $(m-i)$-th and applying it again, it will carry $(m-i)$-th coefficient to $m-(m-i)=i$, so $h(h(a(x)))=a(x)$.

{\it Ad $3$.} {\it Necessity.} Assume that $a(x)$ is irreducible and that $h(a(x))$ is not irreducible. Then, $h(a(x))=p(x)q(x)$, for some $p(x),q(x)\in F[x]$. Applying $h$ again gives us $h(h(a(x)))=h(p(x)q(x))$, i.e. $a(x)=h(p(x))h(q(x))$, meaning that $a(x)$ is not irreducible, which is a contradiction. {\it Sufficiency.} Follows the same line of reasoning as necessity.

{\it Ad $4$.} Let $c\in F$. {\it Necessity.} Assume $a(c)=0$. Then, $a(x)=(x-c)q(x)$, for some $q(x)\in G$ and we have $h(a(x))=h((x-c)q(x))$, which implies $h(a(x))=h(x-c)h(q(x))$. But, $h(x-c)=-c x+1$, so $h(a(x))=(1-c x)h(q(x))$. Now, from $-c x+1=0$ we have $-c x=-1$, and after multiplying by $-c^{-1}\in F$, $x=c^{-1}$. Thus, if $b(x)=h(a(x))$, we have $b(c^{-1})=(1-c c^{-1})h(q(x))=0h(q(x))=0$. So, $b(c^{-1})=[h(a(x))](c^{-1})=0$. {\it Sufficiency.} The line of reasoning is the same as for necessity.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Let $a_0+a_1 x+\cdots+a_m x^m=(b_0+\cdots+b_p x^p)(c_0+\cdots+c_q x^q)$. Factor $h(a(x))$ (where $h$ is the function from the theorem above).

\noindent\newline{\bf Solution.} Let $b(x)=h(a(x))$. Then, $b(x)=h((b_0+\cdots+b_p x^p)(c_0+\cdots+c_q x^q))=h(b_0+\cdots+b_p x^p)h(c_0+\cdots+c_q x^q)=(b_0 x^p+\cdots+b_{p-1} x+b_p)(c_0 x^p+\cdots+c_{q-1} x+c_q)$.

\newpage

\begin{center}
{\bf Substitution in polynomials}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $A$ be a ring, $p(x)\in A[x]$ and $c\in A$. If $p(x)=p_m x^m+\cdots+p_1 x+p_0$, we define $p(c)=p_m c^m+\cdots+p_1 c+p_0$. Furthermore, if $p(c)=0$, then we say that $c$ is a {\bf root} of $p(x)$ in $A$.

\noindent\newline{\bf Remark.} Notice that if $c\in A$, then also $p(c)\in A$ (it is closed with respect to powers with non-negative exponents, multiplication and addition). Also, in this light we can say that $p$ is a function from $A$ to $A$, i.e. $p:A\rightarrow A$. But, then notion of equality of polynomials and equality of polynomial functions do not necessarily coincide; polynomial functions $p:A\rightarrow A$ and $q:A\rightarrow A$ are equal if $p(x)=q(x)$ for all $x\in A$.

\noindent\newline{\bf Lemma.} Let $A$ be an integral domain. Then, $x-c|x^k-c^k$ in $A[x]$, for all $k\in\Z^{+}$ and $c\in A$.

\noindent\newline{\bf Proof.} Let $c\in A$. Let $k=1$. Trivial, $x^1-c^1=x-c=1(x-c)$ (as $1\in A$), so $x-c|x-c$. Assume that the statement is true for some $k\in\Z^{+}$, i.e. $x^k-c^k=(x-c)q(x)$, for some $q(x)\in A[x]$. We will show it is true for $k+1$. We have $x^{k+1}-c^{k+1}=x^k x+c^k x-c^k c-c^k x=x(x^k-c^k)+c^k x-c^k c=x(x^k-c^k)+c^k(x-c)$. By assumption of induction, that is equivalent to $x^{k+1}-c^{k+1}=x (x-c) q(x)+c^k(x-c)=(x-c)(x q(x)+c^k)$, so $x-c|x^{k+1}-c^{k+1}$. Thus, $x-c|x^k-c^k$, for all $k\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $A$ be an integral domain, $p(x)\in A[x]$ and $c\in A$. Then, $c$ is a root of $p(x)$ in $A$ if and only if $x-c|p(x)$.

\noindent\newline{\bf Proof.} {\it Necessity\footnote{Proven for fields easier as follows. Assume that $c\in F$ is a root of $p(x)$ and that $x-c\nmid p(x)$. That would mean that there exist $q(x),r(x)\in F[x]$ such that $p(x)=(x-c)q(x)+r(x)$, where $0\leq\deg{r(x)}<\deg{(x-c)}=1$. That implies that $\deg{r(x)}=0$, i.e. $r(x)=r_0$, where $r_0\in F$. Now, from $p(c)=0$ we have $p(c)=(c-c)q(c)+r_0$, i.e. $0=0q(c)+r_0$. That would imply $r_0=0$, which is a contradiction to the fact that the degree of $r(x)$ is defined. Therefore, it must be that $x-c|p(x)$.}.} Let $c\in A$ be a root of $p(x)$, i.e. $p(c)=0$. Then, $p(x)=p(x)-0=p(x)-p(c)$. From that, if $p(x)=p_m x^m+\cdots+p_1 x+p_0$, we have $p(x)-p(c)=(p_m x^k+\cdots+p_1 x+p_0)-(p_m c^k+\cdots+p_1 c+p_0)=p_m\cdot(x^k-c^k)+\cdots+p-1(x-c)$. By previous lemma $x-c|x^k-c^k$, for all $k\in\Z^{+}$, so there exist $q_m,\cdots,q_1\in A[x]$ such that $p(x)-p(c)=p_m\cdot(x-c)q_m(x)+\cdots+p_1(x-c)q_1(x)=(x-c)(p_m q_m(x)+\cdots+p_1 q_1(x))$. Therefore, $x-c|p(x)-p(c)$, and as $p(x)-p(c)=p(x)-0=p(x)$, we have $x-c|p(x)$.

{\it Sufficiency.} Let $c\in A$ and $x-c|p(x)$. That means that there exists $q(x)\in A[x]$ such that $p(x)=(x-c)q(x)$. Substituting $c$ in place of $x$ gives us $p(c)=(c-c)q(c)$, i.e. $p(c)=0q(c)$. That implies $p(c)=0$, i.e. $c$ is a root of $p(x)$ in $A$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $A$ be an integral domain and $p(x)\in A[x]$. If $c_1,\ldots,c_m\in A$, where $m\in\Z^{+}$, are distinct roots of $p(x)$ in $A$, then $(x-c_1)\cdots(x-c_m)|p(x)$.

\noindent\newline{\bf Proof.} Let $m=1$. Then, $c_1\in A$ is a root of $p(x)$ so, by previous theorem, $x-c_1|p(x)$. Assume the statement is true for some $m$. Then, assume $c_1,\ldots,c_m,c_{m+1}$ are roots of $p(x)$. Then, as $c_1,\ldots,c_m$ are distinct roots of $p(x)$, we have that $(x-c_1)\cdots(x-c_m)|p(x)$, i.e. there exists $q(x)\in A[x]$ such that $p(x)=q(x)(x-c_1)\cdots(x-c_m)$. As $c_{m+1}$ is a root of $p(x)$, then $0=p(c_{m+1})=q(c_{m+1})(c_{m+1}-c_1)\cdots(c_{m+1}-c_m)$. As $A$ is an integral domain, then so $A[x]$ is an integral domaina and either $q(c_{m+1})=0$ or $c_{m+1}-c_i=0$, for some $i\in\{1,\ldots,m\}$. If $q(c_{m+1})=0$, then $c_{m+1}$ is a root of $q(x)$ and we have that $x-c_{m+1}|q(x)$, by previous theorem. In other words, there exists $q_1(x)\in A[x]$ such that $q(x)=q_1(x)(x-c_{m+1})$. That implies $p(x)=q_1(x)(x-c_{m+1})(x-c_1)\cdots(x-c_m)$, i.e. $(x-c_1)\cdots(x-c_{m+1})|p(x)$. If $q(c_{m+1})\neq 0$, then $c_{m+1}-c_i=0$ implies $c_{m+1}=c_i$, which is a contradiction to the fact that $c_1,\ldots,c_{m+1}$ are distinct.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $A$ be an integral domain and $p(x)\in A[x]$. If $\deg{p(x)}=m$, for some $m\in\Z^{+}$, then $p(x)$ has at most $m$ distinct roots.

\noindent\newline{\bf Proof.} Assume that $p(x)$ has more than $m$ roots, i.e. $c_1,\ldots,c_{m+k}$, where $k\in\Z^{+}$. Then, by previous corollary, $(x-c_1)\cdots(x-c_{m+k})|p(x)$. So, there exists $q(x)\in A[x]$ such that $p(x)=q(x)(x-c_1)\cdots(x-c_{m+k})$. But that means, because $\deg{(x-c_i)}=1$, for all $i\in\{1,\ldots,m+k\}$, that $m=\deg{p(x)}=\deg{q(x)}+(m+k)$, i.e. $\deg{q(x)}=-k$, which is impossible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $q(x)\in\Q[x]$ and let $r_1,\ldots,r_m\in\Q$ be roots of $q(x)$. Then, there exists $p(x)\in\Z[x]$ with same roots as $q(x)$.

\noindent\newline{\bf Proof.} By previous corollary, $q(x)=q_1(x)(x-r_1)\cdots(x-r_m)$. Let $f:\Q\rightarrow\Z^{+}$ be a function defined with $f\left(\frac{a}{b}\right)=b$. Then, it is easy to see that $f\left(r_1\cdots r_m\right)(x-r_1)\cdots(x-r_m)$ is a polynomial with integer coefficients and that its roots are $r_1,\ldots,r_m$. Then, let $g:\Q[x]\rightarrow\Z^{+}$ be a function defined with $g\left(\frac{a_m}{b_m}x^m+\cdots+\frac{a_1}{b_1}x+\frac{a_0}{b_0}\right)=b_m\cdots b_1 b_0$. Then, $g(q_1(x))q_1(x)$ is obviously a polynomial with integer coefficients. So,

\begin{eqnarray*}
p(x)&=&g\left(q_1(x)\right)f\left(r_1\cdots r_m\right)q(x)\\
&=&g\left(q_1(x)\right)q_1(x)f\left(r_1\cdots r_m\right)(x-r_1)\cdots(x-r_m)
\end{eqnarray*}

\noindent\newline is a polynomial with integer coefficients and with roots $r_1,\ldots,r_m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $q(x)\in\Z[x]$ such that $q(x)=q_m x^m+\cdots+q_1 x+q_0$ and $m\in\Z^{+}$. If there exists $\frac{a}{b}\in\Q$ such that $q\left(\frac{a}{b}\right)=0$ and $\gcd{(a,b)}=1$, then $a|q_0$ and $b|q_m$.

\noindent\newline{\bf Proof.} For $m=1$, it is easy to see that, if $q_1\frac{a}{b}+q_0=0$, then $q_1 a=(-q_0)b$, so as $\gcd{(a,b)}=1$, by Euclid's lemma, we have $a|q_0$ and $b|q_1$. Let $m=2$. Assume there exists $\frac{a}{b}\in\Q$ such that $q_m\left(\frac{a}{b}\right)^m+\cdots+q_1\left(\frac{a}{b}\right)+q_0=0$. If we multiply that equality with $b^m$, we get $q_m a^m+q_{m-1} a^{m-1} b+\cdots+q_1 a b^{m-1}+q_0 b^m=0$. That is equivalent to $(-q_m)a^m+(-q_{m-1})a^{m-1} b+\cdots+(-q_1)a b^{m-1}=q_0 b^m$. If we factor out $a$ on the left-hand side, we get $a c=q_0 b^m$, where $c=(-q_m)a^{m-1}b+(-q_{m-1})a^{m-2}b^{m-1}+\cdots+(-q_2)a b^{m-2}+(-q_1)b^{m-1}$. As $q_i,a^i,b^i\in\Z$, for all $i\in\Z^{+}_0$, then also $c\in\Z$. So, from $a c=q_0 b^m$, because $\gcd{(a,b)}=1$, by Euclid's lemma, we get that $a|q_0$. Similarly, from $q_m a^m+q_{m-1} a^{m-1} b+\cdots+q_1 a b^{m-1}+q_0 b^m=0$ we get $q_m a^m=(-q_{m-1})a^{m-1} b+\cdots+(-q_1)a b^{m-1}+(-q_0)b^m=0$. When we factor out $b$ on the right-hand side we get $q_m a^m=b d$, where $d=(-q_{m-1})a^{m-2}+\cdots+(-q_1)a b^{m-2}+(-q_0)b^{m-1}$. Again, it is obvious that $d\in\Z$, because $q_i,a^i,b^i\in\Z$ for all $i\in\Z^{+}_0$. So, by Euclid's lemma, from $q_m a^m=b d$ and $\gcd{(a,b)}=1$, we have that $b|q_m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $q(x)\in\Z[x]$ such that $q(x)=x^m+q_{m-1}x^{m-1}+\cdots+q_1 x+q_0$ (it is monic) and $m\in\Z^{+}$. If $q\left(a\right)\neq 0$, for all $a\in\Z$ such that $a|q_0$, then $q(x)$ has no roots in $\Q$.

\noindent\newline{\bf Proof.} Assume that there exists $\frac{a}{b}\in\Q$ such that $q\left(\frac{a}{b}\right)=0$. Then, by previous proposition, it must be that $a|q_0$ and $b|q_m$. But, as $q(x)$ is monic, we have $q_m=1$, so $b=\pm 1$. That implies that $\frac{a}{b}=\frac{\pm a}{1}\in\Z$ and we can denote $c=\frac{\pm a}{1}=\pm a$. As $a|q_0$, then also $\pm a|q_0$, i.e. $c|q_0$. But, assumption of proposition implies that, as $c\in\Z$ and as $c|q_0$ that $q(c)\neq 0$, which is a contradiction to our assumption.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma (Gauss).} Let $a(x),b(x),c(x)\in\Z[x]$ such that $a(x)=b(x)c(x)$. Let $p\in P$. If $p$ divides every coefficient of $a(x)$, then $p$ divides every coefficient of $b(x)$ or $c(x)$.

\noindent\newline{\bf Proof.} As $p$ divides all coefficients of $a(x)$, then we can write $a(x)=p a_1(x)$. But then $p a_1(x)=b(x)c(x)$. That implies that $p|b(x)c(x)$. If $p|b(x)$ then $p$ divides every coefficient of $b(x)$ and we are done. If $p\nmid b(x)$, then, as the only divisors of $p$ are $\pm 1$ and $\pm p$, it must be that $\gcd{(p,b(x))}=1$. That implies that there exist $q(x),r(x)\in\Z[x]$ such that $p q(x)+b(x)r(x)=1$. From that we have $b(x)r(x)=1-p q(x)$. From $p a_1(x)=b(x)c(x)$ we have $p a_1(x)r(x)=b(x)c(x)r(x)$ and then $p a_1(x)r(x)=c(x)[1-p q(x)]$. That is equivalent to $p a_1(x)r(x)+c(x)p q(x)=c(x)$, i.e. $p[a_1(x)r(x)+c(x)q(x)]=c(x)$. From that follows that $p|c(x)$, i.e. $p$ divides every coefficient of $c(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma (Gauss).} Let $a(x)\in\Z[x]$ and $b(x),c(x)\in\Q[x]$ such that $a(x)=b(x)c(x)$. Then, there exist $b_1(x),c_1(x)\in\Z[x]$ such that $a(x)=b_1(x)c_1(x)$, where $b_1(x)=k b(x)$ and $c_1(x)=l c(x)$ for some $k,l\in\Z$.

\noindent\newline{\bf Proof.} Let $f:\Q\rightarrow\Z^{+}$ be a function defined with $f\left(\frac{m}{n}\right)=n$. Also, let $g:\Q[x]\rightarrow\Z^{+}$ be defined with $g\left(a_m x^m+\cdots+a_1 x+a_0\right)=f(a_0)f(a_1)\cdots f(a_m)$. Then, as in a previous corollary, we have that $g(b(x))b(x)\in\Z[x]$ and $g(c(x))c(x)\in\Z[x]$. We define $k=g(b(x))$ and $l=g(c(x))$. Also, we define $b_2(x)=k b(x)$ and $c_2(x)=l c(x)$. Then, $b_2(x)c_2(x)=k l b(x) c(x)=k l a(x)$. By using fundamental theorem of arithmetic, $k=p_1\cdots p_s$ and $l=q_1\cdots q_t$, where $p_i,q_j\in P$, for all $i\in\{1,\ldots,s\}$, where $s\in\Z^{+}$, and for all $j\in\{1,\ldots,t\}$, where $t\in\Z^{+}$. Then, by previous lemma, from $b_2(x)c_2(x)=k l a(x)$, which is equivalent to $b_2(x)c_2(x)=(p_1\cdots p_s)(q_1\cdots q_t)a(x)$, we have that $p_i$ and $p_j$ divide all coefficients of $b_2(x)$ or $c_2(x)$. Thus, after dividing coefficients of $b_2(x)$ and $c_2(x)$ with $p_i$ and $q_j$, we obtain $b_1(x),c_1(x)\in\Z[x]$ such that $a(x)=b_1(x)c_1(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $a(x)=\left(3x-2\right)\left(4x+\frac{3}{5}\right)$. Then, from the proof of Gauss' lemma, we have $k=g\left(3x-2\right)=f\left(3\right)f\left(-2\right)=1$ and $l=g\left(4x+\frac{3}{5}\right)=f(4)f\left(\frac{3}{5}\right)=5$. Then, $b_2=3x-2$ and $c_2=5\left(4x+\frac{3}{5}\right)=20x+3$. We have $k l a(x)=k l b_2(x) c_2(x)$, i.e. $5 a(x)=(3x-2)(20x+3)$. We see that Gauss' lemma doesn't apply here. Why is that? Because we have $a(x)$ has to be in $\Z[x]$, and here, implicitly, $(3x-2)(4x+\frac{3}{5})\notin\Z[x]$.

\noindent\newline{\bf Theorem (Eisenstein's Criterion)} Let $a(x)\in\Z[x]$ and $m\in\Z^{+}$ such that $a(x)=a_m x^m+\cdots+a_1 x+a_0$ and $a_m\neq 0$. If there exists $p\in P$ such that $p\nmid a_m$, $p^2\nmid a_0$ and $p\mid a_i$, for all $i\in\{0,\ldots,m-1\}$, then $a(x)$ is irreducible over $\Q$.

\noindent\newline {\bf Proof.} Assume that $a(x)$ is reducible over $\Q$. Then, there exist $q_1(x),r_1(x)\in\Q[x]$ such that $a(x)=q_1(x)r_1(x)$. But, as $a(x)\in\Z[x]$ and $q_1(x),r_1(x)\in\Q[x]$, we can apply the previous lemma to obtain $q(x),r(x)\in\Z[x]$ such that $a(x)=q(x)r(x)$. Let $m,n(q(x)),n(r(x))\in\Z^{+}_0$ (here we consider a function $n:\Z[x]\rightarrow\Z^{+}_0$) so that $a(x)=a_m x^m+\cdots+a_1 x+a_0$, $q(x)=q_{n(q(x))} x^{n(q(x))}+\cdots+q_1 x+q_0$ and $r(x)=r_{n(r(x))} x^{n(r(x))}+\cdots+r_1 x+r_0$. Let $f:\Z[x]\rightarrow\left(\zmod{p}\right)[x]$ be a function defined with $f(a_m x^m+\cdots+a_1 x+a_0)=\overline{a_m}x^m+\cdots+\overline{a_1}x+\overline{a_0}$. Thus, $f$ is a homomorphism (we have already proved it before). If we take $a(x)=q(x)r(x)$, as $f$ is a function on $\Z[x]$, we have $f(a(x))=f(q(x)r(x))$. As $f$ is a homomorphism, that is equivalent to $f(a(x))=f(q(x))f(r(x))$. Now, as $p$ divides $a_0,a_1,\ldots,a_{m-1}$, we have $f(a(x))=\overline{a_m}x^m$. From that we have $\overline{a_m}x^m=\left(\overline{q_{n(q(x))}} x^{n(q(x))}+\cdots+\overline{q_1} x+\overline{q_0}\right)\left(\overline{r_{n(r(x))}} x^{n(r(x))}+\cdots+\overline{r_1} x+\overline{r_0}\right)$. It is obvious that $\overline{q_{n(q(x))}}\cdot\overline{r_{n(r(x))}}=\overline{a_m}$. As $\zmod{p}$ is a field, then $\zmod{p}[x]$ is an integral domain and it has to be that $\overline{q_{n(q(x))}},\overline{r_{n(r(x))}}\neq 0$. But, also $\overline{0}=\overline{q_0}\cdot\overline{r_0}$ implies that $\overline{q_0}=0$ or $\overline{r_0}=0$. Assume that $\overline{q_0}=0$. Let $n=\min{\{i\in\{1,\ldots,n(q(x))\}:\ p\nmid q_i\}}$. If $n>n(r(x))$, then we can assume $\overline{r_j}=\overline{0}$ for all $j\in\{n(r(x)),\ldots,n\}$. We have $\overline{a_n}=\sum_{i+j=n}{\overline{q_i}\overline{r_j}}=\overline{q_0}\overline{r_n}+\overline{q_1}\overline{r_{n-1}}+\cdots+\overline{q_{n-1}}\overline{r_1}+\overline{q_n}\overline{r_0}$. But, by assumption all $q_i$ such that $0<i<n$ are divisible by $p$ so they are equal to $\overline{0}$ and we only have $\overline{a_n}=\overline{q_n}\overline{r_0}$. Also, $\overline{a_n}=0$, as it is divisible by $p$ (we of course assumed that $q(x)$ and $p(x)$ are non-constant so it cannot be that $n=m$) and we have $\overline{0}=\overline{q_n}\overline{r_0}$. But, as $\left(\zmod{p}\right)[x]$ is an integral domain we have either $\overline{q_n}=\overline{0}$ or $\overline{r_0}=\overline{0}$. If $\overline{q_n}=\overline{0}$, then it is a contradiction to well-ordering property because we assumed that $n$ is a minimal index such that $p\nmid q_n$ (and $\overline{q_n}=\overline{0}$ implies $p|q_n$). The only thing we are left with is $\overline{r_0}=\overline{0}$, so $p|r_0$. But, we also assumed that $p|q_0$ (from $\overline{q_0}=\overline{0}$) and that would mean that $r_0=p r'_0$ and $q_0=p q'_0$. Then, $a_0=r_0 q_0=p r'_0 p q'_0=p^2 r'_0 q'_0$, which is a contradiction to $p^2\nmid a_0$. Therefore, it must be that $a(x)$ is irreducible over $\Q$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field, $p(x)\in F[x]$ and $c\in F$. Then, $\deg{p(x)}=\deg{p(x-c)}$.

\noindent\newline{\bf Proof.} Let $m\in\Z^{+}$, $\deg{p(x)}=m$ and $p(x)=p_m x^m+\cdots+p_1 x+p_0$. Then, $p(x-c)=p_m (x-c)^m+\cdots+p_1(x-c)+p_0$. By binomial formula, the largest power of $(x-c)^m$ is $x^m$, so the largest power in $p(x-c)$ is again $p_m x^m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field, $p(x)\in F[x]$ and $c\in F$. If $p(x-c)$ is irreducible over $F$, then $p(x)$ is irreducible over $F$.

\noindent\newline{\bf Proof.} Assume that $p(x-c)$ is irreducible over $F$, but that $p(x)$ is not irreducible over $F$, i.e. there exist $q(x),r(x)\in F[x]$ such that $p(x)=q(x)r(x)$ with $\deg{q(x)},\deg{r(x)}\neq 0$. Then, $p(x-c)=q(x-c)r(x-c)$, but by previous proposition $\deg{q(x-c)}=\deg{q(x)}\neq 0$ and $\deg{r(x-c)}=\deg{r(x)}\neq 0$ meaning that $p(x-c)$ is not irreducible, a contradiction. Therefore, $p(x)$ is also irreducible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} A polynomial of the form $\frac{x^m-1}{x-1}$, where $m\in\Z^{+}$, is called {\bf cyclotomic polynomial}.

\noindent\newline{\bf Proposition.} Let $p(x)=x^m-1$, where $m\in\Z^{+}$. Then, $x-1|x^m-1$, for all $m\in\Z^{+}$ and $\frac{x^m-1}{x-1}=x^{m-1}+\cdots+x+1$.

\noindent\newline{\bf Proof.} Let $m=1$. Then, $x-1|x-1$. Assume the statement is true for $m$. Then, $x^{m+1}-1=x^m x-1+x-x=x(x^m-1)+(x-1)$. As $(x-1)|(x^m-1)$, there exist $q(x)\in\Q[x]$ such that $x^m-1=q(x)(x-1)$. So we have $x^{m+1}-1=x q(x)(x-1)+(x-1)=(x-1)(x q(x)+1)$, i.e. $(x-1)|x^{m+1}$, therefore the statement is true for all $m\in\Z^{+}$. Let $m=1$. Then, $\frac{x-1}{x-1}=1$. Assume the statement is true for some $m\in\Z^{+}$. Then, $\frac{x^{m+1}-1}{x-1}=\frac{x^m x-1+x-x}{x-1}=\frac{x(x^m-1)+x-1}{x-1}=\frac{x(x^m-1)}{x-1}+\frac{x-1}{x-1}=x(x^{m-1}+\cdots+x+1)+1=x^m+\cdots+x+1$, so the statement is true for all $m\in\Z^{+}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p\in P$. Then, $x^{p-1}+\cdots+x+1$ is irreducible over $\Q$.

\noindent\newline{\bf Proof.} From previous proposition, $x^{p-1}+\cdots+x+1=\frac{x^p-1}{x-1}$. Let $q(x)=\frac{x^p-1}{x-1}$. Then, $q(x+1)=\frac{(x+1)^p-1}{(x+1)-1}$. That is equivalent to:

\begin{eqnarray*}
q(x+1)&=&\frac{\left(\binom{p}{p}x^p+\binom{p}{p-1}x^{p-1}+\cdots+\binom{p}{1}x+\binom{p}{0}\right)-1}{x}\\\\
&=&\frac{x^p+\binom{p}{p-1}x^{p-1}+\cdots+\binom{p}{1}x+1-1}{x}=\frac{x^p+\binom{p}{p-1}x^{p-1}+\cdots+\binom{p}{1}x}{x}\\\\
&=&x^{p-1}+\binom{p}{p-1}x^{p-2}+\cdots+\binom{p}{1}=x^{p-1}+p x^{p-2}+\cdots+\binom{p}{2}x+p.
\end{eqnarray*}

\noindent\newline From a previous proposition, we know that $p|\binom{p}{k}$, for all $0<k<p$. Therefore, as $p\nmid 1$ (the leading coefficient), as $p|\binom{p}{k}$, where $0<k<p$, which are coefficients of $x^k$, and as $p|p$, but $p^2\nmid p$ (the constant term of $q(x+1)$), we can apply Eisenstein's criterion and conclude that $q(x+1)$ is irreducible over $\Q$. From the previous proposition, we conclude that, as $q(x+1)$ is irreducible over $\Q$, then $q(x)=\frac{x^p-1}{x-1}$ is irreducible over $\Q$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If a polynomial $p(x)\in\Z[x]$ is irreducible over $\Z$, then it is irreducible over $\Q$.

\noindent\newline{\bf Proof.} Assume that there exist $q(x),r(x)\in\Q[x]$ such that $p(x)=q(x)r(x)$. Then, by previous corollary, as $p(x)\in\Z[x]$ and $q(x),r(x)\in\Q[x]$, there exist $q_1(x),r_1(x)\in\Z[x]$ such that $p(x)=q_1(x)r_1(x)$. But, that would mean that $p(x)$ is not irreducible, which is a contradiction.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} For the following problems we will deal with $\zmod{7}$ field, so we will write out the addition and multiplication table (the squares are bold):

\begin{center}
\begin{parbox}{0.4\linewidth}{\begin{tabular}{c|ccccccc}
$+$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ \\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$\\
$\overline{1}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ & $\overline{0}$\\
$\overline{2}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ & $\overline{0}$ & $\overline{1}$\\
$\overline{3}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$\\
$\overline{4}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$\\
$\overline{5}$ & $\overline{5}$ & $\overline{6}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$\\
$\overline{6}$ & $\overline{6}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$\\
\end{tabular}}
\end{parbox}
\hskip 1.5cm
\begin{parbox}{0.4\linewidth}{\begin{tabular}{c|ccccccc}
$\cdot$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$ \\
\hline
$\overline{0}$ & ${\bf \overline{0}}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$\\
$\overline{1}$ & $\overline{0}$ & ${\bf \overline{1}}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{5}$ & $\overline{6}$\\
$\overline{2}$ & $\overline{0}$ & $\overline{2}$ & ${\bf \overline{4}}$ & $\overline{6}$ & $\overline{1}$ & $\overline{3}$ & $\overline{5}$\\
$\overline{3}$ & $\overline{0}$ & $\overline{3}$ & $\overline{6}$ & ${\bf \overline{2}}$ & $\overline{5}$ & $\overline{1}$ & $\overline{4}$\\
$\overline{4}$ & $\overline{0}$ & $\overline{4}$ & $\overline{1}$ & $\overline{5}$ & ${\bf \overline{2}}$ & $\overline{6}$ & $\overline{3}$\\
$\overline{5}$ & $\overline{0}$ & $\overline{5}$ & $\overline{3}$ & $\overline{1}$ & $\overline{6}$ & ${\bf \overline{4}}$ & $\overline{2}$\\
$\overline{6}$ & $\overline{0}$ & $\overline{6}$ & $\overline{5}$ & $\overline{4}$ & $\overline{3}$ & $\overline{2}$ & ${\bf \overline{1}}$\\
\end{tabular}}
\end{parbox}
\end{center}

\noindent\newline{\bf Problem.} Find all the roots of the following polynomials in $\zmod{7}[x]$, and factor the polynomials: (a) $x^3+x^2+x+\overline{1}$; (b) $\overline{3}x^4+x^2+\overline{1}$; (c) $x^5+\overline{1}$; (d) $x^4+\overline{1}$; (e) $x^4+\overline{4}$.

\noindent\newline{\bf Solution.} First, we will note that all squares in $\zmod{7}$ are $\overline{0}$, $\overline{1}$, $\overline{2}$ and $\overline{4}\}$. That is easily checked by $\overline{1^2}=\overline{6^2}=\overline{1}$, $\overline{2^2}=\overline{5^2}=\overline{4}$ and $\overline{3^2}=\overline{4^2}=\overline{2}$. Also, $\overline{0^2}=\overline{0}$.

(a) We can group the factors to have $x^3+x^2+x+\overline{1}=x(x^2+\overline{1})+(x^2+\overline{1})=(x^2+\overline{1})(x+\overline{1})$. Therefore, one of the roots is $\overline{-1}=\overline{6}$. Now, we can easily see that $x^2+\overline{1}$ cannot be factored further because, by checking all the squares in $\zmod{7}[x]$, $\overline{1}+\overline{1}=\overline{2}$, $\overline{2}+\overline{1}=\overline{3}$ and $\overline{4}+\overline{1}=\overline{5}$. Obviously, also, $\overline{0}+\overline{1}=\overline{1}$. As $x^2+\overline{1}$ can only be factored as two polynomials of degree one, if it doesn't have a root, it is irreducible. Therefore, $x^3+x^2+x+\overline{1}=(x^2+\overline{1})(x-\overline{6})$.

(b) We have $p(x)=\overline{3}x^4+x^2+\overline{1}$. To see if the polynomial is divisible by $x-\overline{a}$, where $\overline{a}\in\zmod{7}$, and as we only have $x^2$ in $p(x)$, we can substitute the squares only. So, we can take $t=x^2$ and then have $p(t)=\overline{3}t^2+t+\overline{1}$. Now, we can only have $t\rightarrow\overline{0},\overline{1},\overline{2},\overline{4}$. So, $\overline{0}$ is obviously not a root, then $p(\overline{1})=\overline{3}+\overline{2}=\overline{5}$, $p(\overline{2})=\overline{3}\overline{4}+\overline{3}=\overline{15}=\overline{1}$ and $p(\overline{4})=\overline{3}\overline{2}+\overline{5}=\overline{11}=\overline{4}$. So, $\overline{3}t^2+t+\overline{1}$ cannot be expressed as a product of two polynomials of degree one, and so cannot $\overline{3}x^4+x^2+\overline{1}$, as the only difference is in replacing $t$ in the factorisation with $x^2$. From this is also obvious that $p(x)$ is then not divisible by any $x-\overline{a}$.

(c) It is obvious that $\overline{-1}^5+\overline{1}=\overline{-1}+\overline{1}=\overline{0}$, so $x+\overline{1}|x^5+\overline{1}$. Also, note that $x+\overline{1}=x-\overline{6}$. We have:

\begin{eqnarray*}
&&(x^5+\overline{1}):(x-\overline{6})=x^4+\overline{6}x^3+x^2+\overline{6}x+\overline{1}\\
&-&\underline{(x^5-\overline{6}x^4)}\\
&&\overline{6}x^4+\overline{1}\\
&-&\underline{(\overline{6}x^4-x^3)}\\
&&x^3+\overline{1}\\
&-&\underline{(x^3-\overline{6}x^2)}\\
&&\overline{6}x^2+\overline{1}\\
&-&\underline{(\overline{6}x^2-x)}\\
&&x+\overline{1}\\
&-&\underline{(x-\overline{6})}\\
&&\overline{0}
\end{eqnarray*}

\noindent\newline Thus, $x^5+\overline{1}=(x-\overline{6})(x^4+\overline{6}x^3+x^2+\overline{6}x+\overline{1})$. Now, $x^4+\overline{6}x^3+x^2+\overline{6}x+\overline{1}=x^4-x^3+x^2-x+\overline{1}=x^2(x^2+1)-x(x^2+\overline{1})+\overline{1}=(x^2+\overline{1})(x^2-x)+\overline{1}=x(x^2+\overline{1})(x-\overline{1})+\overline{1}$. Obviously, $\overline{0}$ and $\overline{1}$ are not roots. Now, $\overline{2}\cdot\overline{5}\cdot\overline{1}+\overline{1}=\overline{11}$, so $\overline{2}$ is not a root. Furthermore, $\overline{3}\cdot\overline{10}\cdot\overline{2}+\overline{1}=\overline{5}$, so $\overline{3}$ is not a root. Also, $\overline{4}\cdot\overline{17}\cdot\overline{3}+\overline{1}=\overline{5}\cdot\overline{3}+\overline{1}=\overline{15}+\overline{1}=\overline{2}$, so $\overline{4}$ is not a root. Then, for $\overline{5}=\overline{-2}$, we have $-\overline{2}\cdot\overline{5}\cdot\overline{-3}+\overline{1}=\overline{31}=\overline{3}$, so $\overline{5}$ is not a root. Finally, $\overline{6}=\overline{-1}$ gives us $-\overline{1}\cdot\overline{2}\cdot\overline{-2}+\overline{1}=\overline{5}$, so $\overline{6}$ is not a root. Thus, $x^4-x^3+x^2-x+\overline{1}$ is not divisible by any $x-\overline{a}$, where $\overline{a}\in\zmod{7}$. Thus, for factorization, we need only check if we can write $x^4-x^3+x^2-x+\overline{1}=(x^2+\overline{a_1}x+\overline{a_0})(x^2+\overline{b_1}x+\overline{b_0})$, but that is a lot of work which we will leave for later.

(d) Let us observe $x^4+\overline{1}$. It is obvious that $x^4+\overline{1}=\left(x^2\right)^2+\overline{1}$ has no roots as $\overline{0}^2+\overline{1}=\overline{1}$, $\overline{1}^2+\overline{1}=\overline{2}$, $\overline{2}^2+\overline{1}=\overline{5}$ and $\overline{4}^2+\overline{1}=\overline{17}=\overline{3}$.

(e) Similarly, $x^6+\overline{1}=\left(x^2\right)^3+\overline{1}$ has no roots. We have $\overline{0}^3+\overline{1}=\overline{1}$, $\overline{1}^3+\overline{1}=\overline{2}$, $\overline{2}^3+\overline{1}=\overline{9}=\overline{2}$ and $\overline{4}^3+\overline{1}=\overline{65}=\overline{2}$.

\noindent\newline{\bf Problem.} Use Fermat's theorem to find all the roots of the following polynomials in $\zmod{7}[x]$: (a) $x^{100}-\overline{1}$; (b) $\overline{3}x^{98}+x^{19}+\overline{3}$; (c) $\overline{2}x^{74}-x^{55}+\overline{2}x+\overline{6}$.

\noindent\newline{\bf Solution.} (a) We know that $\overline{a}^{6}=\overline{1}$, for all $\overline{a}\in\zmod{7}-\{\overline{0}\}$. We have $100=6\cdot 16+4$, so $\left(\overline{a}^6\right)^{16}=\overline{1}$. So we are looking for $\overline{a}$ which satisfies $\overline{a}^{96}\cdot\overline{a}^4=\overline{1}$, i.e. $\overline{a}^4=\overline{1}$. From that we have $\left(\overline{a}^2\right)^2=\overline{1}$. So, obviously we have $\overline{1}^2=\overline{1}$. Then, $\overline{2}^2=\overline{4}$, $\overline{4}^2=\overline{2}$. Thus, it is only possible that $\overline{a}\in\{\overline{1},\overline{6}\}$, i.e. $x_1=\overline{1}$ and $x_2=\overline{6}$.

(b) We have $\overline{3}x^{98}+x^{19}+\overline{3}=\overline{3}\left(x^{98}+\overline{5}x^{19}+\overline{1}\right)$. If there exists a root $\overline{a}\in\zmod{7}$, then $\overline{3}\left(\overline{a}^{98}+\overline{5}\cdot\overline{a}^{19}+\overline{1}\right)=\overline{0}$, and after multiplying that by $\overline{3}^{-1}$ we have $\overline{a}^{98}+\overline{5}\cdot\overline{a}^{19}+\overline{1}=\overline{0}$. We also know that $98=6\cdot 16+2$ and $19=6\cdot 3+1$. So, $\overline{a}^{98}+\overline{5}\cdot\overline{a}^{19}+\overline{1}=\left(\overline{a}^{6}\right)^{16}\overline{a}^2+\overline{5}\cdot\left(\overline{a}^{6}\right)^3\overline{a}+\overline{1}$. By Fermat's theorem, $\overline{a}^{6}=\overline{1}$ and we have $\left(\overline{a}^{6}\right)^{16}\overline{a}^2+\overline{5}\cdot\left(\overline{a}^{6}\right)^3\overline{a}+\overline{1}=\overline{a}^2+\overline{5}\cdot\overline{a}+\overline{1}$. We have to find $\overline{a}\in\zmod{7}$ such that $\overline{a}^2+\overline{5}\cdot\overline{a}+\overline{1}=\overline{0}$. Here we have $\overline{t}^2=(\overline{5}^2-\overline{4})\overline{4}^{-1}=\overline{21}\cdot\overline{2}=\overline{42}=\overline{0}$. Then, $\overline{t}=\overline{0}$ and we have $\overline{a}=\overline{0}-\overline{5}\cdot\overline{2}^{-1}=\overline{2}\cdot\overline{2}^{-1}=\overline{1}$. We see that $\overline{1}^2+\overline{5}\cdot\overline{1}+\overline{1}=\overline{7}=\overline{0}$. Furthermore, as $\overline{t}=\overline{0}$, this solution must be the only solution in $\zmod{7}$, which we can check by $\left(\overline{a}-\overline{1}\right)^2=\overline{a}^2-\overline{2}\cdot\overline{a}+\overline{1}=\overline{a}^2+\overline{5}\cdot\overline{a}+\overline{1}$.

(c) Let us observe $\overline{2}x^{74}-x^{55}+\overline{2}x+\overline{6}=\overline{2}x^{74}+\overline{6}x^{55}+\overline{2}x+\overline{6}=\overline{2}\left(x^{74}+\overline{3}x^{55}+x+\overline{3}\right)$. It is sufficient to observe $x^{74}+\overline{3}x^{55}+x+\overline{3}$. If there exists a root $\overline{a}\in\zmod{7}$, then it must be that $\overline{a}^{74}+\overline{3}\cdot\overline{a}^{55}+\overline{a}+\overline{3}=\overline{0}$. We find out that $74=12\cdot 6+2$ and $55=9\cdot 6+1$. So, by Fermat's theorem, we have $\overline{a}^{6}=\overline{1}$ and it follows that $\overline{a}^{74}+\overline{3}\cdot\overline{a}^{55}+\overline{a}+\overline{3}=\left(\overline{a}^{6}\right)^{12}\overline{a}^2+\overline{3}\left(\overline{a}^{6}\right)^{9}\overline{a}+\overline{a}+\overline{3}=\overline{a}^2+\overline{3}\cdot\overline{a}+\overline{a}+\overline{3}=\overline{a}^2+\overline{4}\cdot\overline{a}+\overline{3}$. We need to solve $\overline{a}^2+\overline{4}\cdot\overline{a}+\overline{3}=\overline{0}$. We have $\overline{t}^2=(\overline{16}-\overline{12})\overline{4}^{-1}=\overline{1}$. That implies that there are two possibilities, $\overline{t_1}=\overline{1}$ and $\overline{t_2}=\overline{6}$. We will calculate $\overline{s}=-\overline{4}\cdot\overline{2}^{-1}=-\overline{4}\overline{4}=-\overline{16}=\overline{5}$, so that $\overline{a_{1,2}}=\overline{t_{1,2}}+\overline{s}$. For $\overline{t_1}=\overline{1}$ we have $\overline{a_1}=\overline{1}+\overline{5}=\overline{6}$. For $\overline{t_2}=\overline{6}$, we have $\overline{a_2}=\overline{6}+\overline{5}=\overline{11}=\overline{4}$. That can be checked by $(\overline{a}-\overline{6})(\overline{a}-\overline{4})=\overline{a}^2-\overline{10}\cdot\overline{a}+\overline{24}=\overline{a}^2+\overline{4}\cdot\overline{a}+\overline{3}$. So, the only two solutions in $\zmod{7}$ are $\overline{a_1}=\overline{6}$ and $\overline{a_2}=\overline{4}$.

\noindent\newline{\bf Problem.} Using Fermat's theorem, find polynomials of degree $\leq 6$ which determine the same functions as the following polynomials in $\zmod{7}[x]$: (a) $\overline{3}x^{75}-\overline{5}x^{54}+\overline{2}x^{13}-x^2$; (b) $\overline{4}x^{108}+\overline{6}x^{101}-\overline{2}x^{81}$; (c) $\overline{3}x^{103}-x^{73}+\overline{3}x^{55}-x^{25}$.

\noindent\newline{\bf Solution.} (a) Let $p(x)=\overline{3}x^{75}-\overline{5}x^{54}+\overline{2}x^{13}-x^2$. It is obvious that, for all $\overline{a}\in\zmod{7}-\{\overline{0}\}$, we have $p(\overline{a})=\overline{3}\left(\overline{a}^{6}\right)^{12}\overline{a}^{3}-\overline{5}\left(\overline{a}^6\right)^9+\overline{2}\left(\overline{a}^6\right)^2\overline{a}-\overline{a}^2$. Now, using Fermat's theorem, we have $\overline{a}^6=\overline{1}$, for all $\overline{a}\in\zmod{7}-\{\overline{0}\}$. So, $p(\overline{a})=\overline{3}\cdot\overline{a}^3-\overline{5}+\overline{2}\cdot\overline{a}-\overline{a}^2$, and that can be written more neatly as a new polynomial $q_1(x)=\overline{3}x^3+\overline{6}x^2+\overline{2}x+\overline{2}$. It is obvious that now, $p(\overline{a})=q_1(\overline{a})$, for all $\overline{a}\in\zmod{7}-\{0\}$. But notice that $p(\overline{0})=\overline{0}$ and $q_1(\overline{0})=\overline{2}$. That can be fixed by defining a new polynomial $q(x)=\overline{2}x^6+\overline{3}x^3+\overline{6}x^2+\overline{2}x$. Now, $q(\overline{0})=\overline{0}$, and due to Fermat's theorem, function values for other elements remain the same.

(b) Let $p(x)=\overline{4}x^{108}+\overline{6}x^{101}-\overline{2}x^{81}$. Then, for all $\overline{a}\in\zmod{7}[x]$ we have $p(\overline{a})=\overline{4}\left(\overline{a}^6\right)^{18}+\overline{6}\left(\overline{a}^6\right)^{96}\overline{a}^5-\overline{2}\left(\overline{a}^6\right)^{13}\overline{a}^3$. Using Fermat's theorem, and due to the need for $p(\overline{0})=q(\overline{0})$, as argued in the previos case, we will define new polynomial $q(x)=\overline{4}x^6+\overline{6}x^5+\overline{5}x^3$. It is easy to see that now $p(\overline{a})=q(\overline{a})$, for all $\overline{a}\in\zmod{7}$.

(c) Let $p(x)=\overline{3}x^{103}-x^{73}+\overline{3}x^{55}-x^{25}$. Arguing as in the previos cases, we define $q(x)=\overline{3}x-x+\overline{3}x-x=\overline{6}x-\overline{2}x=\overline{4}x$. Now, due to Fermat's theorem, $p(\overline{a})=q(\overline{a})$, for all $\overline{a}\in\zmod{7}$.

\noindent\newline{\bf Remark.} Notice that, due to Fermat's theorem, every polynomial in $\zmod{p}[x]$ has the same roots as a polynomial of degree $<p$, which can be obtained by dividing exponents of different terms with $p-1$ and disregarding $x^{p-1}$, thus leaving only the residue, whose degree is, due to the division with remainder theorem, less than $p-1$, and by that less than $p$. Also, the only problem is the zero element for which we need to set the constant term to include $x^{p-1}$.

\noindent\newline{\bf Problem.} Find all the rational roots of the following polynomials, and factor them into irreducible polynomials in $\Q[x]$, $\R[x]$ and $\C[x]$: (a) $9x^3+18x^2-4x-8$; (b) $4x^3-3x^2-8x+6$; (c) $2x^4+3x^3-8x-12$; (d) $6x^4-7x^3+8x^2-7x+2$.

\noindent\newline{\bf Solution.} (a) If there is a root $\frac{a}{b}\in\Q$, then $a\in\pm\{1,2,4,8\}$ and $b\in\pm\{1,3,9\}$. So we don't search through all, we will just show (discovered by a lucky guess) that $9\left(\frac{2}{3}\right)^3+18\left(\frac{2}{3}\right)^2-4\cdot\frac{2}{3}-8=9\cdot\frac{8}{27}+18\cdot\frac{4}{9}-\frac{8}{3}-8=\frac{8}{3}+8-\frac{8}{3}+8=0$. So, we can divide $9x^3+18x^2-4x-8$ by $x-\frac{2}{3}$:

\begin{eqnarray*}
&&(9x^3+18x^2-4x-8):\left(x-\frac{2}{3}\right)=9x^2+24x+12\\
&-&\underline{\left(9x^3-6x^2\right)}\\
&&24x^2-4x-8\\
&-&\underline{(24x^2-16x)}\\
&&12x-8\\
&-&\underline{(12x-8)}\\
&&0
\end{eqnarray*}

\noindent\newline So, $9x^3+18x^2-4x-8=\left(x-\frac{2}{3}\right)\left(9x^2+24x+12\right)=\left(3x-2\right)\left(3x^2+8x+4\right)$. From that we have $x_1=\frac{2}{3}$ and:

\begin{equation*}
x_{2,3}=\frac{-8\pm\sqrt{64-48}}{6}=\frac{-8\pm 4}{6}.
\end{equation*}

\noindent\newline Therefore, $x_2=-2$ and $x_3=-\frac{2}{3}$. So, we can write $9x^3+18x^2-4x-8=\left(3x-2\right)\cdot 3\left(x+2\right)\left(x+\frac{2}{3}\right)$, i.e. $9x^3+18x^2-4x-8=(3x-2)(3x+2)(x+2)$.

(b) We have $4x^3-3x^2-8x+6$. We check for rational roots in $\frac{a}{b}\in\Q$ by searching through $a\in\pm\{1,2,3,6\}$ and $b\in\pm\{1,2,4\}$. After some calculations we can see that $4\left(\frac{3}{4}\right)^3-3\left(\frac{3}{4}\right)^2-8\cdot\frac{3}{4}+6=\frac{3^3}{4^2}-\frac{3^3}{4^2}-6+6=0$. Now we use the division algorithm:

\begin{eqnarray*}
&&(4x^3-3x^2-8x+6):\left(x-\frac{3}{4}\right)=4x^2-8\\
&-&\underline{(4x^3-3x^2)}\\
&&-8x+6\\
&-&\underline{(-8x+6)}\\
&&0
\end{eqnarray*}

\noindent\newline Thus, $4x^3-3x^2-8x+6=\left(x-\frac{3}{4}\right)(4x^2-8)=4\left(x-\frac{3}{4}\right)(x^2-2)=(4x-2)(x^2-2)$. Polynomial $x^2-2$ does not split over $\Q$, as there does not exist a rational number such that $x^2=2$. But we know that $x^2-2=(x-\sqrt{2})(x+\sqrt{2})$, so $4x^3-3x^2-8x+6=(4x-2)(x-\sqrt{2})(x+\sqrt{2})$ over $\R$. The three solutions, therefore, are $x_1=\frac{3}{4}$, $x_2=\sqrt{2}$ and $x_3=-\sqrt{2}$.

(c) For rational roots $\frac{a}{b}\in\Q$ of $2x^4+3x^3-8x-12$ we search in $a\in\pm\{1,2,3,4,6,12\}$ and $b\in\pm\{1,2\}$. It is easy to see that $1$ and $-1$ are not roots, so we search roots that are not integers and see that $2\left(-\frac{3}{2}\right)^4+3\left(-\frac{3}{2}\right)^3+8\cdot\frac{3}{2}-12=\frac{3^4}{2^3}-\frac{3^4}{2^3}+12-12=0$. By usind division algorithm,

\begin{eqnarray*}
&&(2x^4+3x^3-8x-12):\left(x+\frac{3}{2}\right)=2x^3-8\\
&-&\underline{(2x^4+3x^3)}\\
&&-8x-12\\
&-&\underline{(-8x-12)}\\
&&0
\end{eqnarray*}

\noindent\newline So, $x_1=\frac{3}{2}$ and $2x^4+3x^3-8x-12=(2x^3-8)\left(x+\frac{3}{2}\right)=(x^3-4)(2x+3)$. The polynomial cannot split any further in $\Q[x]$. Now we solve $x^3-4=0$, i.e. $x^3=4$. That means that at least one of the roots is $x_2=\sqrt[3]{4}$. Using the division algorithm again we have:

\begin{eqnarray*}
&&\left(x^3-4\right):\left(x-\sqrt[3]{4}\right)=x^2+x\sqrt[3]{4}+2\sqrt[3]{2}\\
&-&\underline{\left(x^3-x^2\sqrt[3]{4}\right)}\\
&&x^2\sqrt[3]{4}-4\\
&-&\underline{\left(x^2\sqrt[3]{4}-2x\sqrt[3]{2}\right)}\\
&&2x\sqrt[3]{2}-4\\
&-&\underline{\left(2x\sqrt[3]{2}-4\right)}\\
&&0
\end{eqnarray*}

\noindent\newline Thus, over $\R$, we have $2x^4+3x^3-8x-12=\left(2x+3\right)\left(x-\sqrt[3]{4}\right)(x^2+x\sqrt[3]{4}+2\sqrt[3]{2})$. Using the quadratic equation formula:

\begin{equation*}
x_{3,4}=\frac{-\sqrt[3]{4}\pm\sqrt{2\sqrt[3]{2}-8\sqrt[3]{2}}}{2}=-\frac{\sqrt[3]{4}}{2}\pm i\frac{\sqrt{6\sqrt[3]{2}}}{2}.
\end{equation*}

\noindent\newline That can be elaborated further:

\begin{equation*}
x_{3,4}=-\sqrt[3]{\frac{4}{8}}\pm i\frac{\sqrt{3\sqrt[3]{16}}}{2}=-\sqrt[3]{\frac{1}{2}}\pm i\sqrt{3}\sqrt[6]{\frac{2^4}{2^6}}.
\end{equation*}

\noindent\newline Finally,

\begin{equation*}
x_{3,4}=-\sqrt[3]{\frac{1}{2}}\pm i\sqrt[6]{\frac{27}{4}}
\end{equation*}

\noindent\newline Therefore, the polynomial splits over $\C$ as:

\begin{equation*}
2x^4+3x^3-8x-12=\left(2x+3\right)\left(x-\sqrt[3]{4}\right)\left(x+\sqrt[3]{\frac{1}{2}}+i\sqrt[6]{\frac{27}{4}}\right)\left(x+\sqrt[3]{\frac{1}{2}}-i\sqrt[6]{\frac{27}{4}}\right).
\end{equation*}

(d) We need to find roots of $6x^4-7x^3+8x^2-7x+2$. We look for rational roots $\frac{a}{b}\in\Q$ by looking at $a\in\pm\{1,2\}$ and $b\in\pm\{1,2,3,6\}$. We can easily eliminate $1$ and $-1$ in the beginning by looking at the sum $6-7+8-7+2=2$ for $1$ and $6+7+8+7+2=30$ for $-1$. By going through all possibilities, i.e. $\pm 2,\pm\frac{1}{2},\pm\frac{1}{3},\pm\frac{2}{3},\pm\frac{1}{6}$ (we already eliminated $1$ and $-1$), or by a lucky guess (as in the author's case), we see that $6\left(\frac{2}{3}\right)^4-7\left(\frac{2}{3}\right)^3+8\left(\frac{2}{3}\right)^2-7\cdot\frac{2}{3}+2=2\cdot 3\cdot\frac{2^4}{3^4}-7\cdot\frac{2^3}{3^3}+2^3\cdot\frac{2^2}{3^2}-\frac{2\cdot 7}{3}+\frac{2\cdot 3}{3}=\frac{2^5}{3^3}-\frac{7\cdot 2^3}{3^3}+\frac{3\cdot 2^5}{3^3}-\frac{2\cdot 7\cdot 3^2}{3^3}+\frac{2\cdot 3^3}{3^3}=\frac{2^5-7\cdot 2^3+3\cdot 2^5-2\cdot 7\cdot 3^2+2\cdot 3^3}{3^3}=\frac{2^3(4-7+12)+3^2(6-14)}{3^3}=\frac{2^3\cdot 9+3^2\cdot(-8)}{3^3}=\frac{8\cdot 9-9\cdot 8}{3^3}=\frac{0}{3^3}=0$. We use division algorithm like so:

\begin{eqnarray*}
&&(6x^4-7x^3+8x^2-7x+2):\left(x-\frac{2}{3}\right)=6x^3-3x^2+6x-3\\
&-&\underline{(6x^4-4x^3)}\\
&&-3x^3+8x^2-7x+2\\
&-&\underline{(-3x^3+2x^2)}\\
&&6x^2-7x+2\\
&-&\underline{(6x^2-4x)}\\
&&-3x+2\\
&-&\underline{-3x+2}\\
&&0
\end{eqnarray*}

\noindent\newline Thus we have $6x^4-7x^3+8x^2-7x+2=\left(x-\frac{2}{3}\right)(6x^3-3x^2+6x-3)=(3x-2)(2x^3-x^2+2x-1)$. To factor $2x^3-x^2+2x-1$ we look for rational roots $\frac{a}{b}\in\Q$ such that $a\in\{1,-1\}$ and $b\in\pm\{1,2\}$. At a glance we see that we can eliminate $1$ and $-1$. So, we are left with $\pm\frac{1}{2}$ and see that $2\left(\frac{1}{2}\right)^3-\left(\frac{1}{2}\right)^2+2\cdot\frac{1}{2}-1=2\cdot\frac{1}{2^3}-\frac{1}{2^2}+1-1=\frac{1}{2^2}-\frac{1}{2^2}=0$. So, we use the division algorithm once more:

\begin{eqnarray*}
&&(2x^3-x^2+2x-1):\left(x-\frac{1}{2}\right)=2x^2+2\\
&-&\underline{(2x^3-x^2)}\\
&&(2x-1)\\
&-&\underline{(2x-1)}\\
&&0
\end{eqnarray*}

\noindent\newline So, $6x^4-7x^3+8x^2-7x+2=(3x-2)(2x^3-x^2+2x-1)=(3x-2)(2x^2+2)\left(x-\frac{1}{2}\right)$, i.e. $6x^4-7x^3+8x^2-7x+2=(3x-2)(2x-1)(x^2+1)$ over $\Q$. Now, $x^2+1$ cannot be factored over $\R$, but it can be factored over $\C$ as $x^2+1=(x-i)(x+i)=x^2-i^2=x^2+1$. Therefore, the roots are $x_1=\frac{2}{3}$, $x_2=\frac{1}{2}$, $x_{3,4}=\pm i$ and we have $6x^4-7x^3+8x^2-7x+2=(3x-2)(2x-1)(x-i)(x+i)$.

\noindent\newline{\bf Problem.} Find associates with integer coefficients and rational roots for each of the following polynomials: (a) $x^3+\frac{3}{2}x^2-\frac{4}{9}x-\frac{2}{3}$; (b) $\frac{1}{2}x^3-\frac{1}{4}x^2-\frac{1}{2}x+\frac{1}{4}$; (c) $x^3\sqrt{3}+\frac{1}{\sqrt{3}}x^2-x\sqrt{3}-\frac{1}{\sqrt{3}}$. Then, factor them over $\R$.

\noindent\newline{\bf Solution.} (a) We have $18\left(x^3+\frac{3}{2}x^2-\frac{4}{9}x-\frac{2}{3}\right)=18x^3+27x^2-8x-12$. For rational roots of the form $\frac{a}{b}\in\Q$ we look in $a\in\pm\{1,2,3,4,6,12\}$ and $b\in\pm\{1,2,3,4,6,9,18\}$. Well, how did the author get to the first root? Let us denote $p(x)=18x^3+27x^2-8x-12$. We calculated $p(1)$ and got $p(1)=25$. Then we calculated $p\left(\frac{1}{6}\right)=-\frac{25}{2}$. As the change of sign happened, our root has to be somewhere between $1$ and $\frac{1}{6}$. That includes $\frac{2}{6}=\frac{1}{3}$, $\frac{3}{6}=\frac{1}{2}$, $\frac{4}{6}=\frac{2}{3}$ and $\frac{5}{6}$. For $\frac{1}{3}$ we have $p\left(\frac{1}{3}\right)=-11$, so actually, we got closer to our root, or so we may assume (if the values of the polynomial don't fluctuate too much), so looking further we get $p\left(\frac{1}{2}\right)=-7$. For the next one, $p\left(\frac{2}{3}\right)=0$, and that's our root! Also, our assumption that we were getting closer to the root was correct (by what ways, we may only wonder, but later we will see that actually we were just sliding down the polynomial; it will turn out it has two more rational roots, but below zero, a place we never even touched). We use the division algorithm now (notice that due to Gauss' lemma we could divide the polynomial by $3x-2$ instead of $x-\frac{2}{3}$):

\begin{eqnarray*}
&&(18x^3+27x^2-8x-12):\left(3x-2\right)=6x^2+13x+6\\
&-&\underline{(18x^3-12x^2)}\\
&&39x^2-8x-12\\
&-&\underline{(39x^2-26x)}\\
&&18x-12\\
&-&\underline{(18x-12)}\\
&&0
\end{eqnarray*}

\noindent\newline So, $18x^3+27x^2-8x-12=\left(3x-2\right)(6x^2+13x+6)$. By using the formula for the quadratic equation we obtain:

\begin{equation*}
x_{2,3}=\frac{-13\pm\sqrt{169-144}}{12}=\frac{-13\pm 5}{12}.
\end{equation*}

\noindent\newline From that we have $x_2=-\frac{2}{3}$ and $x_3=-\frac{3}{2}$ (and our poor old root $x_1=\frac{2}{3}$). Thus, $18x^3+27x^2-8x-12=(3x-2)\cdot 6\left(x+\frac{3}{2}\right)\left(x+\frac{2}{3}\right)$. By using Gauss' lema we can simplify that from $18x^3+27x^2-8x-12=(3x-2)\cdot6\cdot\frac{1}{2}\left(2x+3\right)\cdot\frac{1}{3}\left(3x+2\right)$ to:

\begin{equation*}
18x^3+27x^2-8x-12=6(3x-2)(2x+3)(3x+2).
\end{equation*}

\noindent\newline Of course, the factorization of the original polynomial is:

\begin{equation*}
x^3+\frac{3}{2}x^2-\frac{4}{9}x-\frac{2}{3}=\frac{1}{3}(3x-2)(2x+3)(3x+2).
\end{equation*}

(b) We can take the associate $4\left(\frac{1}{2}x^3-\frac{1}{4}x^2-\frac{1}{2}x+\frac{1}{4}\right)=2x^3-x^2-2x+1$. Now, we can easily factor that as $2x^3-x^2-2x+1=(2x^3-2x)-(x^2-1)=2x(x^2-1)-(x^2-1)=(x^2-1)(2x-1)=(x-1)(x+1)(2x-1)$, so our roots are $x_1=\frac{1}{2}$ and $x_{2,3}=\pm 1$. Also, the factorization of the original polynomial is:

\begin{equation*}
\frac{1}{2}x^3-\frac{1}{4}x^2-\frac{1}{2}x+\frac{1}{4}=\frac{1}{4}(x-1)(x+1)(2x-1).
\end{equation*}

(c) We have $\sqrt{3}\left(x^3\sqrt{3}+\frac{1}{\sqrt{3}}x^2-x\sqrt{3}-\frac{1}{\sqrt{3}}\right)=3x^3+x^2-3x-1$. Again, this polynomial can be easily factored as $3x^3+x^2-3x-1=(3x^3-3x)+(x^2-1)=3x(x^2-1)+(x^2-1)=(x^2-1)(3x+1)=(x+1)(x-1)(3x+1)$. So the roots are $x_1=-\frac{1}{3}$ and $x_{2,3}=\pm 1$. The factorization of the original is:

\begin{equation*}
x^3\sqrt{3}+\frac{1}{\sqrt{3}}x^2-x\sqrt{3}-\frac{1}{\sqrt{3}}=\frac{1}{\sqrt{3}}(x+1)(x-1)(3x+1).
\end{equation*}

\noindent\newline{\bf Problem.} Does $2x^4+3x^2-2$ have any rational roots? Can it be factored into two polynomials of lower degree in $\Q[x]$?

\noindent\newline{\bf Solution.} We look for rational roots $\frac{a}{b}\in\Q$ by looking at $a\in\pm\{1,2\}$ and $b\in\pm\{1,2\}$. Thus, we will check all the roots, noting that their signs don't change the value, as all the exponents of the terms in the polynomial are even: $p(-2)=p(2)=42$, $p(-1)=p(1)=3$, $p\left(-\frac{1}{2}\right)=p\left(\frac{1}{2}\right)=-\frac{9}{8}$. Therefore, there are no rational roots. But, taking $t=x^2$, we can try to solve $2t^2+3t-2=0$. By quadratic formula we have:

\begin{equation*}
t_{1,2}=\frac{-3\pm\sqrt{9+16}}{4}=\frac{-3\pm 5}{4}.
\end{equation*}

\noindent\newline So, we have $t_1=-2$ and $t_2=\frac{1}{2}$ and that implies $2t^2+3t-2=2(t+2)\left(t-\frac{1}{2}\right)=(t+2)(2t-1)$. Now, taking the substitution back, we have:

\begin{equation*}
2x^4+3x^2-2=(x^2+2)(2x^2-1).
\end{equation*}

\noindent\newline Thus, the polynomial can be factored into two polynomials of lower degree over $\Q$, but it has no rational roots (a nice counterexample for the theorem).

\noindent\newline{\bf Proposition.} Let $F$ be a field, $c\in F$ and $p(x)\in F[x]$. Then,

\begin{enumerate}
\item The remainder of $p(x)$, when divides by $x-c$, is $p(c)$;
\item $x-c|p(x)-p(c)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $p(x)\in F[x]$. By division with remainder theorem, there exist $q(x),r(x)\in F[x]$ such that $p(x)=q(x)(x-c)+r(x)$, where $0\leq\deg{r(x)}<\deg{(x-c)}$. But, $\deg{(x-c)}=1$, so $r(x)=a$, where $a\in F$. That implies $p(x)=q(x)(x-c)+a$. Now, $p(c)=q(c)(c-c)+a$, i.e. $p(c)=0+a$. Indeed, $p(c)=a$, which is the remainder.

{\it Ad $2$.} Let $p(x)\in F[x]$ and $c\in F$. Then, there exist $q(x),r(x)\in F[x]$ such that $p(x)-p(c)=q(x)(x-c)+r(x)$, where $0\leq\deg{r(x)}<\deg{(x-c)}=1$, which implies $r(x)=a$, for some $a\in F$. By taking $x=c$ we have $p(c)-p(c)=q(c)(c-c)+a$ and that means $0=0+a$, i.e. $a=0$. From that we have $r(x)=0$ meaning that $x-c|p(x)-p(c)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Every polynomial over some field $F$ has the same roots as any of its associates.

\noindent\newline{\bf Proof.} Let $F$ be a field, $a\in F$ and $p(x),q(x)\in F[x]$ such that $p(x)=a q(x)$. If $c\in F$ is a root of $p(x)$, then $p(c)=0$. That implies $p(c)=a q(c)$, i.e. $0=a q(c)$. By multiplying the equality with $a^{-1}\in F$, we have $0=q(c)$, meaning that $c$ is also a root of $q(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} If $a(x),b(x)\in F[x]$ have the same roots in a field $F$, are they necessarily associates?

\noindent\newline{\bf Solution.} For example, $(x-3)(x^2+1)$ and $(x-3)(2x^2+1)$ have the same roots in $\R$ (that is $x_1=3$), but they are not associates.

\noindent\newline{\bf Proposition.} Let $F$ be a field. If $a(x)\in F[x]$ is a monic polynomial of degree $m\in\Z^{+}$, and $a(x)$ has $m$ distinct roots $c_1,\ldots,c_m\in F$, then $a(x)=(x-c_1)\cdots(x-c_m)$.

\noindent\newline{\bf Proof.} Let $m=1$. Then, $a(x)$ is of degree $1$ and has $1$ root $c_1$. As $a(x)$ is of degree $1$, it is of the form $a(x)=x+a_0$ (by assumption monic). As $c_1\in F$ is a root, then $x-c_1$ divides $x+a_0$, i.e. there exists $q(x)\in F[x]$ such that $x+a_0=(x-c_1)q(x)$. As $1=\deg{(x+a_0)}=\deg{((x-c_1)q(x))}=\deg{(x-c_1)}+\deg{q(x)}=1+\deg{q(x)}$, it must be that $\deg{q(x)}=0$, i.e. $q(x)=q_0$, for some $q_0\in F$. Thus, $x+a_0=q_0 x-q_0 c_1$. But, that implies $q_0=1$, i.e. $a(x)=x-c_1$. Assume that the statement holds for all monic polynomials of degree $m\in\Z^{+}$ with $m$ roots. Let $a(x)$ be a monic polynomial of $(m+1)$-st degree with roots $c_1,\ldots,c_{m+1}$. Then, $x-c_{m+1}|a(x)$ and we have $a(x)=q(x)(x-c_{m+1})$. That implies that $q(x)$ is monic (as $a(x)$ is monic and $x-c_{m+1}$ is monic). As $x-c_{m+1}\neq x-c_i$, for all $i\in\{1,\ldots,m\}$ and as $x-c_i|a(x)$, it must be that $x-c_i|q(x)$. As $q(x)$ is of $m$-th degree and is divisible by $m$ polynomials of the form $x-c_i$, it has $m$ roots $c_1,\ldots,c_m$, so by assumption $q(x)=(x-c_1)\cdots(x-c_m)$. That implies $a(x)=(x-c_1)\cdots(x-c_{m+1})$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field. Assume $a(x),b(x)\in F[x]$ have degree $<m$, for some $m\in\Z^{+}-\{1\}$. If $a(c_i)=b(c_i)$ for all $c_1,\ldots,c_m\in F$ (all distinct), then $a(x)=b(x)$.

\noindent\newline{\bf Proof.} Let $p(x)=a(x)-b(x)$. Then, if $a(x)\neq b(x)$, we have $0\leq\deg{p(x)}\leq\max{\{\deg{a(x)},\deg{b(x)}\}}=n<m$. But, $p(c_i)=a(c_i)-b(c_i)=0$, so $x-c_i|p(x)$, for all $i\in\{1,\ldots,m\}$. Thus, we have $(x-c_1)\ldots(x-c_m)q(x)=p(x)$. But, on the left hand side we have degree of at least $m$ and on the right hand side $n$, i.e. $m+k=n$, i.e. $m\leq n$. That is in contradiction with $n<m$ so it must be $a(x)=b(x)$. Thus, $\deg{p(x)}$ is undefined and $p(x)=0$. Also, $x-c_i|0$, for all $c_i\in F$, where $i\in\{1,\ldots,m\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} There are infinitely many irreducible polynomials over any field $F$.

\noindent\newline{\bf Proof.} Let $I\subseteq F[x]$ such that $p(x)\in I$ if and only if $p(x)$ is irreducible. That means that $a_1 x+a_0\in I$, for all $a_1,a_0\in F-\{0\}$, by definition. So, if $F$ is infinite, then $I$ is certainly infinite. But if $F$ is finite, then the number of $a_1 x+a_0\in I$ is finite. So, let us assume that $I$ itself is finite, i.e. $I=\{p_1(x),\ldots,p_m(x)\}$, where $m\in\Z^{+}$. Take $q(x)=p_1(x)\cdots p_m(x)+1$. As $F[x]$ is a unique factorization domain, $q(x)$ can be written down as a product of irreducible polynomials, i.e. it is divisible by at least one irreducible polynomial. Then, as $I$ contains all irreducible polynomials, $q(x)$ is either one of them, or divisible by one of them (both cases come down to the latter). So, assume, without loss of generalty, $p_1(x)|q(x)$. We have $q(x)=p_1(x)r(x)$, where $r(x)\in F[x]$. So, $p_1(x)r(x)=p_1(x)\cdots p_m+1$. Then, $p_1(x)(r(x)-p_2(x)\cdots p_m(x))=1$, but that is impossible as $p_1(x)$ would divide $1$. So, there has to be some irreducible polynomial outside of $I$ that divides $q(x)$ (or is $q(x)$ itself). The process can be repeated infinitely, meaning that $I$ is infinite.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} How many roots does $x^2-x$ have in $\zmod{10}$? In $\zmod{11}$?

\noindent\newline{\bf Solution.} In $\zmod{10}$, $x^2-x$ has four roots: $\overline{0}^2-\overline{0}=\overline{0}$, $\overline{1}^2-\overline{1}=\overline{0}$, $\overline{5}^2-\overline{5}=\overline{20}=\overline{0}$ and $\overline{6}^2-\overline{6}=\overline{30}=\overline{0}$. In $\zmod{11}$, it has two roots, $\overline{0}$ and $\overline{1}$. That happens because $\zmod{10}$ is not a field (and the theorem that states that polynomial of degree $m$ has at most $m$ roots depends on the division alorithm for polynomials, which in turn can be proved only over a field).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Show that each of the following polynomials is irreducible over $\Q$: (a) $3x^4-8x^3+6x^2-4x+6$; (b) $\frac{2}{3}x^5+\frac{1}{2}x^4-2x^2+\frac{1}{2}$; (c) $\frac{1}{5}x^4-\frac{1}{3}x^3-\frac{2}{3}x+1$; (d) $\frac{1}{2}x^4+\frac{4}{3}x^3-\frac{2}{3}x^2+1$.

\noindent\newline{\bf Solution.} (a) We have $2\nmid 3$, $2|-8$, $2|6$, $2|-4$, $2|6$ and $2^2=4\nmid 6$, so by Eisenstein, the polynomial $3x^4-8x^3+6x^2-4x+6$ is irreducible over $\Q$. (b) We can take $6\left(\frac{2}{3}x^5+\frac{1}{2}x^4-2x^2+\frac{1}{2}\right)=4x^5+3x^4-12x^2+3$, thus $3\nmid 4$, $3|3$, $3|-12$, $3|3$ and $3^2=9\nmid 3$. By Eisenstein's criterion, $4x^5+3x^4-12x^2+3$ is irreducible and, by a previous proposition, it's associate $\frac{2}{3}x^5+\frac{1}{2}x^4-2x^2+\frac{1}{2}$ is also irreducible. (c) Take $15\left(\frac{1}{5}x^4-\frac{1}{3}x^3-\frac{2}{3}x+1\right)=3x^4-5x^3-10x+15$. Then, $5\nmid 3$, $5|-5$, $5|-10$, $5|15$ and $5^2=25\nmid 15$. Therefore, by Eisenstein, the polynomial, along with its associate, is irreducible. (d) Let $6\left(\frac{1}{2}x^4+\frac{4}{3}x^3-\frac{2}{3}x^2+1\right)=3x^4+8x^3-4x^2+6$. So, $2\nmid 3$, $2|8$, $2|-4$, $2|6$ and $2^2=4\nmid 6$. By Eisenstein's criterion, the polynomial is irreducible, and by a previous proposition, so are all of its associates.

\noindent\newline{\bf Proposition.} Let $F$ be a field, $a\in F$ and $p(x)\in F[x]$. Then, $p(x)$ is irreducible if and only if $p(x+a)$ is irreducible.

\noindent\newline{\bf Proof.} Notice that $\deg{p(x)}=\deg{p(x+a)}$. {\it Necessity.} Assume $p(x)$ is irreducible and that $p(x+a)$ is reducible. Then there exist $s(x),t(x)\in F[x]$ such that $p(x+a)=s(x)t(x)$. But, then, $p(x+a-a)=s(x-a)t(x-a)$, i.e. $p(x)=s(x-a)t(x-a)$. Therefore, $p(x)$ is reducible, which is a contradiction. {\it Sufficiency.} Follows the same reasoning as necessity.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Show that $x^4+4x+1$ is irreducible over $\Q$ by using the change of variable $x\rightarrow x+1$ and Eisenstein's criterion.

\noindent\newline{\bf Solution.} Let $p(x)=x^4+4x+1$. Then, $p(x+1)=(x+1)^4+4(x+1)+1=x^4+4x^3+6x^2+4x+1+4x+4+1=x^4+4x^3+6x^2+8x+6$. As $2\nmid 1$, $2|4$, $2|6$, $2|8$, $2|6$ and $2^2=4\nmid 6$, then by Eisenstein, $p(x+1)$ is irreducible, and so is $p(x)$, by previous proposition.

\noindent\newline{\bf Problem.} Find an appropriate change of variable to prove that the following are irreducible in $\Q[x]$: (a) $x^4+2x^2-1$; (b) $x^3-3x+1$; (c) $x^4+1$; (d) $x^4-10x^2+1$.

\noindent\newline{\bf Solution.} (a) Let $p(x)=x^4+2x^2-1$. Then, $p(x+k)=(x+k)^4+2(x+k)^2-1=x^4+4x^3k+6x^2k^2+4x k^3+k^4+2x^2+4x k+2k^2-1=x^4+4x^3 k+2(3k^2+2)x^2+4k(k^2+1)x+(k^4+2k^2-1)$. For $k=1$ we have the constant term to be $1^4+2\cdot 1^2-1=2$, and the others are obviously divisible by $2$ (except for the leading coefficient). Also, constant term is then not divisible by $2^2=4$. So, by Eisenstein, $p(x+1)$ is irreducible and so is $p(x)$. (b) Let $p(x)=x^3-3x+1$. Then, $p(x+k)=(x+k)^3-3(x+k)+1=x^3+3x^2 k+3x k^2+k^3-3 x+3 k+1=x^3+3k x^2+3(k^2-1)x+(k^3+3 k+1)$. For $k=-1$ we have $(-1)^3+3\cdot -1+1=-1-3+1=-3$, which is divisible by $3$ but not by $3^2=9$. Other coefficients, except the leading coefficient, are visibly divisible by $3$, so by Eisenstein's criterion, $p(x-1)$ is irreducible and, by a previous proposition, so is $p(x)$. (c) Let $p(x)=x^4+1$. Then, $p(x+k)=x^4+4x^3k+6x^2k^2+4x k^3+k^4+1$ and taking $k=1$ gives us $k^4+1=2$, again divisible by $2$ and not divisible by $2^2=4$, while other coefficients, except the leading one, are divisible by $2$. So, by Eisenstein, $p(x+1)$ is irreducible, and so is $p(x)$.

(d) Let $q(x)=x^4-10x^2+1$. Let $k\in\Z$. Then, $q(x+k)=(x+k)^4-10(x+k)^2+1=x^4+4x^3 k+6x^2 k^2+4x k^3+k^4-10x^2-20x k-10k^2+1=x^4+(4 k)x^3+(6k^2-10)x^2+(4k^3-20k)x+(k^4-10k^2+1)$. Assume that there exists $p\in P$ that satisfies Eisenstein's criterion. Then, $p|4 k$. If $p|4$, then it must be that $p=2$. If $k$ is even then it cannot be proven by Eisenstein's criterion, as $k^4-10k^2$ is then even and $k^4-10k^2+1$ is odd, so $2\nmid k^4-10k^2+1$. Yet, if $k$ is odd, then $k=2k'+1$ and we would have the constant term to be $(2l+1)^4-10(2l+1)^2+1=16l^4+32l^3-16l^2-32l-8=4(4l^4+8l^3-4l^2-8l-2)$, which is divisible by $2^2$. Thus, it is also not provable by Eisenstein's criterion. Next, assume that from $p|4k$ we have $p|k$, i.e. $k=p l$ for some $l\in\Z^{+}$. Then, $q(x+p l)=x^4+(4 p l)x^3+(6p^2 l^2-10)x^2+(4p^3 l^3-20p l)x+(p^4 l^4-10p^2 l^2+1)$. Now, it also must be that $p|6p^2 l^2-10$. Assume that is the case. Then, there exists $t\in\Z$ such that $6p^2 l^2-10=p t$. From that we have $10=p(6pl^2-t)$, and it is either $p=2$ or $p=5$. The case $p=2$ is out of consideration because we already tested for even $k$, which proved to be impossible. So, we have $q(x+5 l)$ and the constant term would be $5^4l^4-5^2\cdot 10 l^2+1$. If that were divisible by $5$, there would exist $s\in\Z$ such that $5^4l^4-5^2\cdot 10 l^2+1=5 s$. That would imply $1=-5(5^3l^4-5^2\cdot 2l^2-s)$. As $-5\nmid 1$, it must be that $125l^4-50l^2-s=\frac{1}{5}$. That is impossible since $l,s\in\Z$ and so $125l^4-50l^2-s\in\Z$ while $\frac{1}{5}\notin\Z$. Therefore, the irreducibility of $x^4-10x^2+1$ cannot be proved by Eisenstein's criterion along with an affine transformation.

\noindent\newline{\bf Remark.} Dual version of Eisenstein's criterion can be stated by using the fact that $h(a_0+a_1 x+\cdots+a_m x^m)=a_m+a_{m-1}x+\cdots+a_0 x^m$ matches irreducible polynomials with irreducible polynomials.

\noindent\newline{\bf Problem.} Show that each of the following polynomials is irreducible in $\Q[x]$: (a) $6x^4+4x^3-6x^2-8x+5$; (b) $x^4-\frac{1}{2}x^2+\frac{3}{2}x-\frac{4}{3}$; (c) $x^3+\frac{1}{2}x^2-\frac{3}{2}x+\frac{6}{5}$.

\noindent\newline{\bf Solution.} (a) As $2|6$, $2^2\nmid 6$, $2|4$, $2|-6$, $2|8$ and $2\nmid 5$, by dual of Eisenstein's criterion, $6x^4+4x^3-6x^2-8x+5$ is irreducible (over $\Q$). (b) We have $6\left(x^4-\frac{1}{2}x^2+\frac{3}{2}x-\frac{4}{3}\right)=6x^4-3x^2+9x-8$. As $3|6$, $9\nmid 6$, $3|0$, $3|-3$, $3|9$ and $3\nmid 8$, then by dual of Eisenstein's criterion, polynomial $6x^4-3x^2+9x-8$ is irreducible over $\Q$, and by a previous proposition, so is its associate $x^4-\frac{1}{2}x^2+\frac{3}{2}x-\frac{4}{3}$. (c) Take $10\left(x^3+\frac{1}{2}x^2-\frac{3}{2}x+\frac{6}{5}\right)=10x^3+5x^2-15x+12$. Then, $5|10$, $25\nmid 10$, $5|5$, $5|-15$ and $5\nmid 12$, so by dual of Eisenstein's criterion, polynomial $10x^3+5x^2-15x+12$ is irreducible over $\Q$, and by a previous proposition, so is its associate $x^3+\frac{1}{2}x^2-\frac{3}{2}x+\frac{6}{5}$.

\noindent\newline{\bf Proposition.} Let $F$ be a field, $m\in\{2,3\}$ and $a(x)\in F[x]$ such that $\deg{a(x)}=m$. Then, $a(x)$ is irreducible if and only if $a(x)$ has no roots in $F$.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $a(x)$ be irreducible. Assume that it has a root $c\in F$. Then, $x-c|a(x)$, i.e. there exists $q(x)\in F[x]$ such that $a(x)=(x-c)q(x)$. Note that, as $\deg{a(x)}\in\{2,3\}$, then $1\leq\deg{q(x)}\leq 2$, depending on the degree of $a(x)$. That is in contradiction that $a(x)$ is irreducible. {\it Sufficiency.} Assume that $a(x)$ has no roots in $F$ and that $a(x)$ is reducible. Let $\deg{a(x)}\in\{2,3\}$. Then, there exist $q(x),p(x)\in F[x]$ such that $a(x)=q(x)p(x)$. But then, if $\deg{p(x)}=1$, we have $\deg{q(x)}\in\{1,2\}$, so $p(x)=p_1 x+p_0$. That implies $a(x)=(p_1 x+p_0)q(x)$. But, taking $p_1 x+p_0=0$ gives us $x=-p_0 p_1^{-1}$ and it is obvious that $a(-p_0 p_1^{-1})=(-p_1 p_0 p_1^{-1}+p_0)q(-p_0 p_1^{-1})=(-p_0+p_o)q(-p_0 p_1^{-1})=0 q(-p_0 p_1^{-1})=0$, which is in contradiction to our assumption that $a(x)$ has no roots in $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove that the following polynomials are irreducible in $\Q[x]$: (a) $\frac{1}{2}x^3+2x-\frac{3}{2}$; (b) $3x^2-2x-4$; (c) $x^3+x^2+\frac{3}{2}x+\frac{1}{2}$; (d) $x^3+\frac{1}{2}$; (e) $x^2-\frac{5}{2}x+\frac{3}{2}$.

\noindent\newline{\bf Solution.} (a) We can take $2\left(\frac{1}{2}x^3+2x-\frac{3}{2}\right)=x^3+4x-3$. Remember that, if $x^3+4x-3$ has a rational root $\frac{a}{b}\in\Q$, then $a\in\pm\{1,3\}$ and $b\in\{1,-1\}$. So, for $1$ we have $1+4-3=2$ and for $-1$ we have $-1-4-3=-8$. For $3$ we have $27+12-3\neq 0$ and for $-3$ we have $-27-12-3\neq 0$. Therefore, $x^3+4x-3$ has no rational roots, and is irreducible (by a previous proposition for $m=3$) as is its associate $\frac{1}{2}x^3+2x-\frac{3}{2}$. (b) For $3x^2-2x-4$ we have possible roots in $R=\left\{\pm1,\pm2,\pm4,\pm\frac{1}{3},\pm\frac{2}{3},\pm\frac{4}{3}\right\}$. Let $p(x)=3x^2-2x-4$. Then, $p(R)=\left\{-3,1,\pm4,12,36,52,-\frac{13}{3},-\frac{4}{3}\right\}$. As $0\notin p(R)$, then $p(x)$ has no rational roots and is irreducible due to the previous proposition. (c) We can take $p(x)=2\left(x^3+x^2+\frac{3}{2}x+\frac{1}{2}\right)=2x^3+2x^2+3x+1$. The possible rational roots are $R=\left\{\pm1,\pm\frac{1}{2}\right\}$. Then, $p(R)=\left\{8,-12,\frac{13}{4},-\frac{1}{4}\right\}$. Thus, $0\notin p(R)$, so $p(x)$ has no rational roots. Thus it is irreducible and so is its associate $x^3+x^2+\frac{3}{2}x+\frac{1}{2}$. (d) Let $p(x)=2\left(x^3+\frac{1}{2}\right)=2x^3+1$. Then we look for roots in $R=\left\{\pm1,\pm\frac{1}{2}\right\}$. We have $p(R)=\left\{3,-1,\frac{5}{4},\frac{3}{4}\right\}$ and $0\notin p(R)$, so $p(x)$ has no rational roots and is irreducible over $\Q$. (e) Take $p(x)=2\left(x^2-\frac{5}{2}x+\frac{3}{2}\right)=2x^2-5x+3$. Then, roots of $p(x)$ are in $R=\left\{\pm1,\pm3,\pm\frac{1}{2},\pm\frac{3}{2}\right\}$. But, $p(1)=0$, so $p(x)$ is not irreducible over $\Q$.

\noindent\newline{\bf Proposition.} Suppose a monic polynomial $p(x)$ of degree $4$ in $F[x]$ has no roots in $F$. Then, $p(x)$ is reducible if and only if it is a product of two irreducible quadratics $x^2+a x+b$ and $x^2+c x+d$.

\noindent\newline{\bf Proof.} Let $p(x)\in F[x]$ such that $p(x)=x^4+a_3 x^3+a_2 x^2+a_1 x+a_0$. {\it Necessity.} Assume that $p(x)$ is reducible, has no roots in $F$ and is not a product of two quadratics. Then, the only other option is that $p(x)=(x+a)(x^3+b x^2+c x+d)$. But that would mean that $-a\in F$ is a root of $p(x)$, which is a contradiction to assumption that $p(x)$ has no roots in $F$. Therefore, $p(x)$ is a product of two quadratics. If one of the quadratics $q(x)$ were reducible, that would mean that $x+a|q(x)$ and that would imply $x+a|p(x)$ (as $q(x)|p(x)$ and that relation is transitive) which would again mean that $-a\in F$ is a root and that would be in contradiction to assumption that $p(x)$ has no roots in $F$. {\it Sufficiency.} Trivial. If $p(x)=(x^2+a x+b)(x^2+c x+d)$ then it is by definition reducible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Note that if $p(x)=(x^2+a x+b)(x^2+c x+d)$, then $p(x)=x^4+(a+c)x^3+(b+a c+d)x^2+(a d+b c)x+b d$.

\noindent\newline{\bf Problem.} Prove that the following polynomials are irreducible or reducible in $\Q[x]$: (a) $x^4-5x^2+1$; (b) $3x^4-x^2-2$; (c) $x^4+x^3+3x+1$.

\noindent\newline{\bf Solution.} (a) The only possible roots of $p(x)=x^4-5x^2+1$ are $1$ and $-1$. We have $p(1)=p(-1)=1-5+1=-3$, so $p(x)$ has no roots in $\Q$. Then, the only other possibility is that $p(x)$ is a product of two quadratics. That would mean that (considering the previous remark) $a+c=0$, $b+a c+d=-5$, $a d+b c=0$ and $b d=1$. First we have $c=-a$. That implies $b+a c+d=b-a^2+d=-5$ and $a d-a b=0$, i.e. $a(d-b)=0$. If $a=0$, then we would have $a=c=0$, $b+d=-5$ and $b d=1$. Combining latter and former would give $b^2+b d=-5b$, i.e. $b^2+1=-5b$. From that we have $b^2+5b+1=0$. But, that polynomial is irreducible (as $1+5+1=7$ and $1-5+1=-3$), so $b$ cannot be rational, i.e. if $a=0$, the polynomial $p(x)$ cannot be written as a product of two polynomials in $\Q[x]$. Assume $a\neq 0$. Then, as $\Q$ is an integral domain, $a(d-b)=0$ implies $d=b$. But, that would give from $b d=1$ that $b^2=1$. So, $b=\pm 1$ and then $\pm 1\cdot d=1$, i.e. $\pm d=1$. That is of course $d=\pm 1$. From $b-a^2+d=-5$ we would have $\pm1-a^2+\pm1=-5$, i.e. $-a^2+\pm 2=-5$. First case, $-a^2+2=-5$ gives us $-a^2+7=0$, i.e. $a^2-7=0$. That polynomial has no rational roots, so $a$ is not rational. Also, for the second case $-a^2-2=-5$ we would have $-a^2+3=0$, i.e. $a^2-3=0$. That polynomial also has no rational roots (the only possible choices are $3$ and $-3$), $a$ is again not rational. Therefore, in any case $p(x)$ cannot be written down as a product of two quadratics in $\Q[x]$.

(b) Let $p(x)=3x^4-x^2-2$. Then, its roots are in $R=\left\{\pm1,\pm2,\pm\frac{1}{3},\pm\frac{2}{3}\right\}$ and we have $p(1)=p(-1)=3-1-2=0$. Thus, $3x^4-x^2-2=(x-1)(x+1)(x^2+a x+b)$, for some $a,b\in\Q$. From that we have $3x^4-x^2-2=(x^2-1)(a x^2+b x+c)=a x^4+b x^3+(c-a)x^2-b x-c$. That implies $a=3$, $b=-b=0$, $c-a=-1$ and $-c=-2$, i.e. $c=2$. So, $2-a=-1$ and we have $-a=-3$, i.e. $a=3$, which is in accordance with the polynomial. Thus, $3x^4-x^2-2=(x^2-1)(3 x^2+2)$. It is obvious that $3 x^2+2$ is irreducible. Its possible roots are in $R=\left\{\pm1,\pm2,\pm\frac{1}{3},\pm\frac{2}{3}\right\}$ leading us to $3 R^2+2=\left\{5,14,\frac{7}{3},\frac{10}{3}\right\}$. So, as $0\notin 3 R^2+2$, $3x^2+2$ is irreducible.

(c) Take $p(x)=x^4+x^3+3x+1$. The only possibilities for roots are $1$ and $-1$ so $p(1)=1+1+3+1=6$ and $p(-1)=1-1-3+1=2-4=-2$. Therefore, $p(x)$ must be a product of two quadratics. So, taking in consideration previous remark, $x^4+x^3+3x+1=x^4+(a+c)x^3+(b+a c+d)x^2+(a d+b c)x+b d$. That implies $a+c=1$, $b+ac+d=0$, $a d+b c=3$ and $b d=1$. We have $c=1-a$ and $b+a(1-a)+d=0$. That gives us $b+a-a^2+d=0$. Multiplying that by $b$ gives us $b^2+a b-a^2 b+b d=0$, i.e. $b^2+b(a-a^2)+1=0$. Now, the only two options are that $b\in\{1,-1\}$. If $b=1$, then $d=1$ and $a-a^2+2=0$. We can see that $-1-1+2=0$ so $a=-1$ is one possibility. Then we would have $c=2$. Also, $a d+b c=-1\cdot 1+1\cdot 2=-1+2=1\neq 3$, so that doesn't work. The other possibility for solutions of $a-a^2+2=0$ is $2-2^2+2=0$. So, assume $a=2$. Then, $c=-1$ and we would have $1-2\cdot 1+1=2-2=0$, but also $2\cdot 1-1\cdot 1=1\neq 3$, so that is impossible. Assume $b=-1$. Then, $1-a+a^2+1=0$, i.e. $a^2-a+2=0$. We see that the possible roots for $a$ are $R=\{\pm1,\pm2\}$ and that $R^2-R+2=\{2,4,8\}$. So, as $0\notin R^2-R+2$, $a^2-a+2$ has no rational roots and the case for $b=-1$ is impossible. Thus, $p(x)$ cannot be shown as a product of two quadratics and it has no rational roots. In conclusion, $x^4+x^3+3x+1$ is irreducible over $\Q$.

\noindent\newline{\bf Remark.} For the following problems we will deal with $\zmod{5}$ field, so we will write out the addition and multiplication table (the squares are bold):

\begin{center}
\begin{parbox}{0.3\linewidth}{\begin{tabular}{c|ccccc}
$+$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ \\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$\\
$\overline{1}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$\\
$\overline{2}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$\\
$\overline{3}$ & $\overline{3}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$\\
$\overline{4}$ & $\overline{4}$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$\\
\end{tabular}}
\end{parbox}
\hskip 1cm
\begin{parbox}{0.3\linewidth}{\begin{tabular}{c|ccccc}
$\cdot$ & $\overline{0}$ & $\overline{1}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$ \\
\hline
$\overline{0}$ & ${\bf \overline{0}}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$\\
$\overline{1}$ & $\overline{0}$ & ${\bf \overline{1}}$ & $\overline{2}$ & $\overline{3}$ & $\overline{4}$\\
$\overline{2}$ & $\overline{0}$ & $\overline{2}$ & ${\bf \overline{4}}$ & $\overline{1}$ & $\overline{3}$\\
$\overline{3}$ & $\overline{0}$ & $\overline{3}$ & $\overline{1}$ & ${\bf \overline{4}}$ & $\overline{2}$\\
$\overline{4}$ & $\overline{0}$ & $\overline{4}$ & $\overline{3}$ & $\overline{2}$ & ${\bf \overline{1}}$\\
\end{tabular}}
\end{parbox}
\end{center}

\noindent\newline Also, notice that, as $\overline{x}^2\in\{\overline{1},\overline{4}\}$, for all $\overline{x}\in\zmod{5}-\{\overline{0}\}$, then $\overline{x}^4=\overline{1}$, for all $\overline{x}\in\zmod{5}-\{0\}$. That is actually a consequence of Fermat's theorem. :-)

\noindent\newline{\bf Problem.} Prove that the following polynomials are irreducible in $\zmod{5}[x]$: (a) $\overline{2}x^3+x^2+\overline{4}x+\overline{1}$; (b) $x^4+\overline{2}$; (c) $x^4+\overline{4}x^2+\overline{2}$; (d) $x^4+\overline{1}$.

\noindent\newline{\bf Solution.} (a) Let $p(x)=\overline{2}x^3+x^2+\overline{4}x+\overline{1}$. We only need to check values for $\overline{0}, \overline{1}$, $\overline{2}$, $\overline{3}$ and $\overline{4}$. Thus, $p(\overline{0})=\overline{1}$, $p(\overline{1})=\overline{8}=\overline{3}$, $p(\overline{2})=\overline{2}\cdot\overline{8}+\overline{4}+\overline{9}=\overline{29}=\overline{4}$, $p(\overline{3})=\overline{2}\cdot\overline{2}+\overline{4}+\overline{3}=\overline{11}=\overline{1}$, $p(\overline{4})=\overline{8}+\overline{1}+\overline{2}=\overline{1}$. Therefore, $p(x)$ has no roots in $\zmod{5}$, so by a previous proposition, it is irreducible in $\zmod{5}[x]$.

(b) Let $p(x)=x^4+\overline{2}$. First we will check for the roots. We have $p(\overline{0})=\overline{2}$, $p(\overline{1})=p(\overline{2})=p(\overline{3})=p(\overline{4})=\overline{1}+\overline{2}=\overline{3}$, so there are no roots. The only possibility is that $p(x)$ is a product of two quadratics, that is, $x^4+\overline{2}=(x^2+\overline{a}x+\overline{b})(x^2+\overline{c}x+\overline{d})$. From that we have $\overline{a}+\overline{c}=\overline{0}$, $\overline{b}+\overline{a c}+\overline{d}=\overline{0}$, $\overline{a d}+\overline{b c}=\overline{0}$ and $\overline{b d}=\overline{2}$. We have $\overline{c}=-\overline{a}$, so $\overline{b}-\overline{a}^2+\overline{d}=\overline{0}$ and $\overline{a}(\overline{d}-\overline{b})=\overline{0}$. Assume $\overline{a}=\overline{0}$. That would imply $\overline{b}+\overline{d}=\overline{0}$. Multiplying that by $\overline{d}$ would give us $\overline{2}+\overline{d}^2=\overline{0}$, i.e. $\overline{d}^2=\overline{3}$. There does not exist such $\overline{d}\in\zmod{5}$. Assume $\overline{a}\neq 0$. Then, from $\overline{a}(\overline{d}-\overline{b})=\overline{0}$ we have $\overline{d}=\overline{b}$. That gives us $\overline{b d}=\overline{b}^2=\overline{2}$. There does not exist such $\overline{b}\in\zmod{5}$, so $p(x)$ must be irreducible in $\zmod{5}[x]$.

(c) Take $p(x)=x^4+\overline{4}x^2+\overline{2}$. As $\overline{x}^4=\overline{1}$, for all $\overline{x}\in\zmod{5}$ it's easy to see that $p(\overline{1})=p(\overline{4})=\overline{1}+\overline{4}+\overline{2}=\overline{2}$ and $p(\overline{2})=p(\overline{3})=\overline{1}+\overline{1}+\overline{2}=\overline{4}$. Also, $p(\overline{0})=\overline{2}$, so we will be observing $p(x)$ as a product of two quadratics, $x^2+\overline{a}x+\overline{b}$ and $x^2+\overline{c}x+\overline{d}$. Using the previous remark we can see that $\overline{a}+\overline{c}=\overline{0}$, i.e. $\overline{c}=-\overline{a}$. Then, using that fact, we have $\overline{b}-\overline{a}^2+\overline{d}=\overline{4}$, $\overline{a}(\overline{d}-\overline{b})=\overline{0}$ and $\overline{b d}=\overline{2}$. Assume that $\overline{a}=\overline{0}$. Then we have $\overline{c}=0$ and $\overline{b}+\overline{d}=\overline{4}$. Multiplying that by $\overline{d}$ gives us $\overline{2}+\overline{d}^2+\overline{d}=\overline{0}$, i.e. $\overline{d}^2+\overline{d}+\overline{2}=\overline{0}$. Let $q(x)=x^2+x+\overline{2}$. Then, $q(\overline{1})=\overline{4}$, $q(\overline{2})=\overline{3}$, $q(\overline{3})=\overline{4}$ and $q(\overline{4})=\overline{2}$. Therefore, there does not exist $\overline{d}\in\zmod{5}$ such that $\overline{d}^2+\overline{d}+\overline{2}=\overline{0}$ and $\overline{a}=\overline{0}$ is an impossibility. Thus, it must be that $\overline{a}\neq 0$. Then from $\overline{a}(\overline{d}-\overline{b})=\overline{0}$ we get $\overline{b}=\overline{d}$ and then $\overline{b}^2=\overline{2}$. It is easy to see from the multiplication table above that there does not exist such $\overline{b}\in\zmod{5}$. That implies that $p(x)$ is irreducible over field $\zmod{5}$.

(d) Let $p(x)=x^4+\overline{1}$. It is easy to see that $p(\overline{0})=\overline{1}$ and $p(\overline{1})=p(\overline{2})=p(\overline{3})=p(\overline{4})=\overline{1}$, due to Fermat's theorem. Therefore, as $p(x)$ has no roots in $\zmod{5}$, and is of degree $4$, it can only be that $p(x)=(x^2+\overline{a}x+\overline{b})(x^2+\overline{c}x+\overline{d})$, if it is reducible. That implies, by observing previous remark, $\overline{a}+\overline{c}=\overline{b}+\overline{a c}+\overline{d}=\overline{a d}+\overline{b c}=\overline{0}$ and $\overline{b d}=\overline{1}$. We have $\overline{a}=\overline{-c}$ and then $\overline{b}-\overline{a}^2+\overline{d}=\overline{0}$ and $\overline{a}(\overline{d}-\overline{b})=\overline{0}$. If $\overline{a}=\overline{0}$ we have $\overline{b}+\overline{d}=\overline{0}$, i.e. $\overline{d}=-\overline{b}$. Thus, $-\overline{b}^2=\overline{1}$, i.e. $\overline{b}^2=\overline{4}$. If $\overline{b}=\overline{2}$, then $\overline{d}=\overline{3}$. Also, remember that $\overline{a}=\overline{c}=\overline{0}$. Then, checking $p(x)=(x^2+\overline{2})(x^2+\overline{3})=x^4+\overline{2}x+\overline{3}x+\overline{6}=x^4+\overline{5}x+\overline{1}=x^4+\overline{1}$ and $p(x)$ is reducible over field $\zmod{5}$.

\noindent\newline{\bf Proposition.} Let $m\in\Z^{+}$ and let $h:\Z[x]\rightarrow\zmod{m}[x]$ be a mapping defined as:

\begin{equation*}
h(a_m x^m+\cdots+a_1 x+a_0)=\overline{a_m}x^m+\cdots+\overline{a_1}x+\overline{a_0}.
\end{equation*}

\noindent\newline Then, $h$ is a homomorphism and if $h(a(x))$ is irreducible in $\zmod{m}[x]$ and $a(x)$ is monic, then $a(x)$ is irreducible in $\Z[x]$.

\noindent\newline{\bf Proof.} We have already proved that $h$ is a homomorphism in a previous proposition. Let $a(x)\in\Z[x]$ be a monic polynomial. Let $h(a(x))$ be irreducible in $\zmod{m}[x]$ and assume $a(x)$ is reducible in $\Z[x]$. As $a(x)$ is reducible in $\Z[x]$, there exist $q(x),r(x)\in\Z[x]$ such that $a(x)=q(x)r(x)$ (and degrees of $q(x)$ and $r(x)$ are greater or equal to one). Then, $h(a(x))=h(q(x)r(x))$, as $h$ is a function. As $h$ is also a homomorphism, we have $h(a(x))=h(q(x))h(r(x))$. Now, as the leading coefficient of $a(x)$ is $1$, then the leading coefficient of $h(a(x))$ is $\overline{1}$. Thus, the degree of $h(a(x))$ remains the same as the degree of $a(x)$. That also implies that degrees of $h(q(x))$ and $h(r(x))$ remain the same as $q(x)$ and $r(x)$. But, that would imply that $h(a(x))$ is reducible, which is a contradiction to assumption that $h(a(x))$ is irreducible.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove that $x^4+10x^3+7$ is irreducible in $\Q[x]$ by using the natural homomorphism from $\Z$ to $\zmod{5}$.

\noindent\newline{\bf Solution.} We have $h(x^4+10x^3+7)=x^4+\overline{2}$ in $\zmod{5}[x]$. We have proved in the previous problem that $x^4+\overline{2}$ is irreducible, and so is $x^4+10x^3+7$ irreducible in $\Z[x]$. But then, due to Gauss' lemma (applicable as $x^4+10x^3+7\in\Z[x]$, it is also irreducible in $\Q[x]$).

\noindent\newline{\bf Problem.} Prove that the following polynomials are irreducible in $\Q[x]$: (a) $x^4-10x^2+1$; (b) $x^4+7x^3+14x^2+3$; (c) $x^5+1$.

\noindent\newline{\bf Solution.} (a) In $\zmod{5}$, we have $x^4+\overline{1}$, which is, again, irreducible as argued in the previous problem. Then, as $x^4-10x^2+1\in\Z[x]$, by Gauss' lemma, it is also irreducible in $\Q[x]$. (b) In $\zmod{7}$ we have $x^4+\overline{3}$. For $x=\overline{4}$, we have $\overline{16}^2+\overline{3}=\overline{2}^2+\overline{3}=\overline{4}+\overline{3}=\overline{7}$. So, we can't prove the irredicibility of $x^4+7x^3+14x^2+3$ by using a homomorphism from $\Z$ to $\zmod{7}$. However, in $\zmod{2}$, we have $x^4+x^3+\overline{1}$. It is obvious that $\overline{0}$ and $\overline{1}$ are not the roots of $x^4+x^3+\overline{1}$. So, assume that $x^4+x^3+\overline{1}=(x^2+\overline{a}x+\overline{b})(x^2+\overline{c}x+\overline{d})$. It is obvious that $\overline{b}=\overline{d}=\overline{1}$. But then $\overline{c+a}=\overline{1}$ (for $x^3$), $\overline{d+b+a c}=\overline{0}$, i.e. $\overline{a c}=\overline{0}$ (for $x^2$) and $\overline{a d+b c}=\overline{0}$. Assume $\overline{a}=\overline{1}$. If $\overline{c}=\overline{1}$, then $\overline{c+a}=\overline{0}$. If $\overline{c}=\overline{0}$, then $\overline{a+c}=\overline{0}$ and $\overline{a c}=\overline{0}$, which is in accordance with the formulae. But, also we need $\overline{c b}+\overline{a d}=\overline{0}$, i.e. $\overline{0}+\overline{1}=\overline{1}$, which is a contradiction. Therefore, $x^4+7x^3+14x^2+3$ is irreducible in $\zmod{2}[x]$ and so it is in $\Z[x]$ and, by Gauss' lemma (as $x^4+7x^3+14x^2+3\in\Z[x]$), in $\Q[x]$. (c) In $\zmod{7}[x]$ we have already shown that $x^5+\overline{1}$ is irreducible. So, $x^5+1$ is irreducible in $\Z[x]$ and, because $x^5+1\in\Z[x]$, by Gauss' lemma, it is also irreducible in $\Q[x]$.

\noindent\newline{\bf Theorem (Lagrange interpolation formula).} Let $F$ be a field and let $a_1,\ldots,a_m\in F$ and $b_1,\ldots,b_m\in F$, where $a_i\neq a_j$ (but some $b_i$ might be equal), for all $i\neq j$, $i,j\in\{1,\ldots,m\}$. Then, there exists a unique polynomial $p(x)\in F[x]$ such that $\deg{p(x)}=m-1$ and $p(a_i)=b_i$, for all $i\in\{1,\ldots,m\}$, given with formula:

\begin{equation*}
p(x)=\sum_{i=1}^{m}{b_i[q_i(a_i)]^{-1}q_i(x)},
\end{equation*}

\noindent\newline where

\begin{equation*}
q_i(x)=\prod_{\substack{j=1\\j\neq i}}^{m}{(x-a_j)}.
\end{equation*}

\noindent\newline{\bf Proof.} {\it Existence.} Existence is of course implied by formula, as we see that it confines to properties of the field $F$. All we need to do is check that $p(a_i)=b_i$, for all $i\in\{1,\ldots,m\}$. First, notice that $q_i(a_i)\neq 0$. For, if it were that $q_i(a_i)=0$, then it would mean that $x-a_i|q_i(x)$, but that is impossible because by definition of $q_i(x)$, $x-a_i\nmid q_i(x)$. The only other possibility would be $x-a_i=x-a_j$, for some $j\in\{1,\ldots,m\}$ and $i\neq j$, but that would imply $a_i=a_j$ for some $i\neq j$, which is contrary to assumption that $a_1,\ldots,a_m$ are all distinct. Furthermore, we have:

\begin{equation*}
p(a_j)=\sum_{i=1}^{m}{b_i[q_i(a_i)]^{-1}q_i(a_j)}.
\end{equation*}

\noindent\newline Then, we can view the sum as:

\begin{equation*}
p(a_j)=\sum_{i=1}^{j-1}{b_i[q_i(a_i)]^{-1}q_i(a_j)}+b_j[q_j(a_j)]^{-1}q_j(a_j)+\sum_{i=j+1}^{m}{b_i[q_i(a_i)]^{-1}q_i(a_j)}.
\end{equation*}

\noindent\newline When, $i\neq j$, notice that $q_i(a_j)=0$ as, by definition, $x-a_j|q_i(x)$, for all $j\in\{1,\ldots,m\}-\{i\}$. Then, all members of the sum vanish except for $j$-th member (the member in the "middle" of the equality above). Thus we have:

\begin{equation*}
p(a_j)=\sum_{i=1}^{j-1}{b_i[q_i(a_i)]^{-1}0}+b_j[q_j(a_j)]^{-1}q_j(a_j)+\sum_{i=j+1}^{m}{b_i[q_i(a_i)]^{-1}0}.
\end{equation*}

\noindent\newline That is,

\begin{equation*}
p(a_j)=b_j[q_j(a_j)]^{-1}q_j(a_j).
\end{equation*}

\noindent\newline As $q_j(a_j)\neq 0$, then $[q_j(a_j)]^{-1}q_j(a_j)=1$ and we have $p(a_j)=b_j$, for all $j\in\{1,\ldots,m\}$. Now, the degree of the polynomial is of course $m-1$ as the degree of $q_i(x)$ is $m-1$. The final polynomial $p(x)$ includes only the sums of $q_i(x)$ along with some zero degree coefficients.

{\it Uniqueness.} Assume that there exist two polynomials $p(x),q(x)\in F[x]$, such that $p(x)\neq q(x)$ and $p(a_i)=q(a_i)=b_i$, for all $i\in\{1,\ldots,m\}$ and that $\deg{p(x)}=\deg{q(x)}=m-1$. Then, if $r(x)=p(x)-q(x)$ it must be $r(x)\neq 0$ and $r(x)\leq\max{\{\deg{p(x)},\deg{q(x)}\}}=m-1$. Also, $r(a_i)=p(a_i)-q(a_i)=b_i-b_i=0$, for all $i\in\{1,\ldots,m\}$, so $a_i$ is the root of $r(x)$ in $F$ and we have $x-a_i|r(x)$ for all $i\in\{1,\ldots,m\}$. But, that implies $r(x)=t(x)(x-a_1)\cdots(x-a_m)$ and we have $\deg{r(x)}=\deg{t(x)}+m$, i.e. $m-1\leq\deg{r(x)}>m-1$. That would imply $m-1>m-1$ which is impossible. So, it must be $p(x)=q(x)$, which is possible as then $r(x)=0$ and $x-a_i|0$, for all $i\in\{1,\ldots,m\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $F$ be a finite field, $S,T\subseteq F$ non-empty sets and let $f:S\rightarrow T$ be a function. There exists a polynomial function $p:S\rightarrow T$ such that $p(x)=f(x)$, for all $x\in S$. Also, $\deg{p(x)}=|S|-1$.

\noindent\newline{\bf Proof.} As $F$ is finite, then $S$ and $T$ are finite. Assume $|S|=m$, i.e. $S=\{s_1,\ldots,s_m\}$ (of course $s_i\neq s_j$, for all $i\in\{1,\ldots,m\}$). Then, let $f(S)=\{f(s_1),\ldots,f(s_m)\}$ (some $f(s_i)$ might not be distinct, as $f$ is not necessarily an injection). Thus, by Lagrange interpolation formula, there exists unique $p(x)\in F[x]$ such that $p(s_i)=f(s_i)$, for all $i\in\{1,\ldots,m\}$. It is easy to see now that $p(x)=f(x)$, for all $x\in S$. Also, from Lagrange interpolation formula, we have $\deg{p(x)}=|S|-1$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $F$ be a field, $q(x)\in F[x]$ and $a_1,\ldots,a_m\in F$. The unique polynomial $p(x)\in F[x]$ such that $\deg{p(x)}<m$ and $p(a_1)=t(a_1),\ldots,p(a_m)=t(a_m)$ is called the {\bf Lagrange interpolator} for $q(x)$ and $a_1,\ldots,a_m$.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $q(x)\in F[x]$. Let $p(x)\in F[x]$ be the Lagrange interpolator for $q(x)$ and $a_1,\ldots,a_m\in F$. Then, the remainder of $q(x)$ divided by $(x-a_1)\cdots(x-a_m)$ is $p(x)$.

\noindent\newline{\bf Proof.} Let $p(x)$ be the Lagrange interpolator for $q(x)$ and $a_1,\ldots,a_m\in F$. Then, $p(a_i)=q(a_i)$ for all $i\in\{1,\ldots,m\}$. By theorem of polynomial division we have $q(x)=(x-a_1)\cdots(x-a_m)+r(x)$, where $0\leq\deg{r(x)}<m$. Then, $p(a_i)=q(a_i)=(a_i-a_1)\cdots(a_{i-1}-a_i)(a_i-a_i)(a_i-a_{i+1})\cdots(a_i-a_m)+r(a_i)=0+r(a_i)=r(a_i)$, for all $i\in\{1,\ldots,m\}$. As $p(a_i)=q(a_i)=r(a_i)$ and as $\deg{p(x)},\deg{r(x)}<m$, by a previous proposition, $p(x)=r(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Find three polynomials in $\zmod{5}[x]$ which determine the same function as $x^2-x+\overline{1}$.

\noindent\newline{\bf Solution.} Let $f(x)=x^2-x+\overline{1}$. Then, $f(\overline{0})=f(\overline{1})=\overline{1}$, $f(\overline{2})=f(\overline{4})=\overline{3}$, $f(\overline{3})=\overline{2}$. By using the previous proposition, we can see that for $p_1(x)=x(x-\overline{1})(x-\overline{2})(x-\overline{3})(x-\overline{4})+f(x)$ we have $p_1(x)=f(x)$, for all $x\in\zmod{5}$. So, after some calculation $p_1(x)=x^5+x^2+\overline{3}x+\overline{1}$. Now, other polynomials can be obtained by changing powers of $(x-\overline{a})$, for some $\overline{a}\in\zmod{5}$. Thus, $p_2(x)=x(x-\overline{1})(x-\overline{2})^2(x-\overline{3})(x-\overline{4})+f(x)=x^6+\overline{3}x^5+x+\overline{1}$ and $p_2(x)=x(x-\overline{1})(x-\overline{2})^2(x-\overline{3})^2(x-\overline{4})+f(x)=x^7+x^5+\overline{4}x^3+x^2+\overline{3}x+\overline{1}$.

\noindent\newline{\bf Proposition.} Let $p\in P$. Then, in $\zmod{p}[x]$:

\begin{equation*}
x^p-x=x(x-\overline{1})(x-\overline{2})\cdots(x-(\overline{p-1})).
\end{equation*}

\noindent\newline{\bf Proof.} By Fermat's theorem $x^{p-1}=\overline{1}$ for all $x\in\zmod{p}-\{\overline{0}\}$. That is, $x^p=x$. From that we have $x^p-x=\overline{0}$, for all $x\in\zmod{p}-\{\overline{0}\}$. Also $\overline{0}^p-\overline{0}=\overline{0}$. Therefore, $x^p-x=\overline{0}$, for all $x\in\zmod{p}$. If $q(x)=x^p-x$, then $q(\overline{a})=\overline{0}$ for all $\overline{a}\in\zmod{p}$. Therefore, $x-\overline{a}|x^p-x$ for all $\overline{a}\in\zmod{p}$. As $\deg{(x^p-x)}=p$ and there are $p$ roots, we have $x^p-x=x(x-\overline{1})\cdots(x-(\overline{p-1}))$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $p\in P$ and $a(x),b(x)\in\zmod{p}[x]$ such that $a(x)=b(x)$ for all $x\in\zmod{p}$. Then, $x^p-x|a(x)-b(x)$.

\noindent\newline{\bf Proof.} If $a(x)=b(x)$ for all $x\in\zmod{p}$, then also $a(x)-b(x)=0$ for all $x\in\zmod{p}$. Therefore, $x-\overline{q}|a(x)-b(x)$ for all $\overline{q}\in F$. Thus, $x(x-\overline{1})\cdots(x-(\overline{p-1}))|a(x)-b(x)$, i.e. $x^p-x|a(x)-b(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a finite field and $a(x),b(x)\in F[x]$. If $a(x)=b(x)$, for all $x\in F$, and if $|F|>\deg{a(x)},\deg{b(x)}$, then\footnote{Notice that $a(x)=b(x)$ for all $x\in F$ is an equality for polynomial functions while $a(x)=b(x)$ means that the polynomial $a(x)$ is equal to the polynomial $b(x)$, i.e. they have the same degrees and coefficients.} $a(x)=b(x)$.

\noindent\newline{\bf Proof.} As $F$ is finite, then $F=\{c_1,\ldots,c_m\}$, for some $m\in\Z^{+}-\{1\}$. Thus, by a previous proposition, as $a(c_i)=b(c_i)$, for all $i\in\{1,\ldots,m\}$, and as $\deg{a(x)},\deg{b(x)}<m$, then $a(x)=b(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a finite field and $J=\{a(x)\in F[x]:\ (\forall x\in F)(a(x)=0)\}$. Then, $J\trianglelefteq F[x]$.

\noindent\newline{\bf Proof.} First, by definition $J\subseteq F[x]$. Take $a(x),b(x)\in J$. Then, as $a(x)=0$ and $b(x)=0$ for all $x\in F$, then $a(x)-b(x)=0+0=0$, for all $x\in F$. Also, $a(x)b(x)=0\cdot 0=0$, for all $x\in F$. That implies $a(x)-b(x),a(x)b(x)\in J$. If $c(x)\in F[x]$, then $a(x)c(x)=0 c(x)=0$ and $c(x)a(x)=c(x) 0=0$, for all $x\in F$, and that implies $a(x)c(x),c(x)a(x)\in J$. In other words, $J\trianglelefteq F[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that, as $F[x]$ is a principal ideal domain, $J$ is a principal ideal. So, if $J=\cyc{g(x)}$ and $a(x)\in J$, then $g(x)=a(x)=0$, for all $x\in F$. Generator is $g(x)=x^p-x$. If it were one of a lesser degree, it could only be $0$. For a field with $a_1,\ldots,a_m$ as its elements, of course $J=\cyc{(x-a_1)\cdots(x-a_m)}$.

\noindent\newline{\bf Proposition.} Let $F=\{a_1,\ldots,a_m\}$ be a finite field. Let $\mathcal{F}(F)$ be the ring of all functions from $F$ to $F$, defined in the same way as $\mathcal{F}(\R)$. Let $h:F[x]\rightarrow\mathcal{F}(F)$ send every polynomial $a(x)$ to the polynomial function which it determines. Then, $h$ is a homomorphism from $F[x]$ onto\footnote{It is surjective.} $\mathcal{F}(F)$ and $F[x]\slash\cyc{(x-a_1)\cdots(x-a_m)}\cong\mathcal{F}(F)$.

\noindent\newline{\bf Proof.} It is obvious that $h$ is a well-defined function. For every $p(x)\in F[x]$ there exists $p\in\mathcal{F}(F)$ such that $h(p(x))=p$. Then, if $p(x)=q(x)$, i.e. $p(x)$ and $q(x)$ have the same coefficients (and same degrees) we also have $p(x)=q(x)$, for all $x\in F$, i.e. $p=q$. Take $p\in\mathcal{F}(F)$. Then, by a previous proposition, there exists $q(x)\in F[x]$ such that $q(x)=p(x)$, for all $x\in F$. Therefore, $h(q(x))=p$ and $h$ is surjective. Now we will show that $h$ is a homomorphism. Let $h_1=h(p(x)+q(x))$, $h_p=h(p(x))$ and $h_q=h(q(x))$. Then, $[h_p+h_q](x)=p(x)+q(x)$, for all $x\in F$. But, $h_1(x)=p(x)+q(x)$, for all $x\in F$, so $h_1(x)=[h_p+h_q](x)$, for all $x\in F$, i.e. $h_1=h_p+h_q$. The same is proved for multiplication up to the change of symbol of operation. Thus, $h$ is a surjective homomorphism. We have $\ker{h}=\{p(x)\in F[x]:\ h(p(x))=0\}$, i.e. $\ker{h}=\{p(x)\in F[x]:\ (\forall x\in F)(p(x)=0)\}$, and from previous proposition and remark $\ker{h}=\cyc{(x-a_1)\cdots(x-a_m)}$. By FHT that implies $\mathcal{F}(F)$ and $F[x]\slash\cyc{(x-a_1)\cdots(x-a_m)}\cong\mathcal{F}(F)$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Extensions of fields}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} Let $F$ be a field and $K\leq F$. We say that $K$ is a {\bf subfield} of $F$ and that $F$ is an {\bf extension field} of $K$.

\noindent\newline{\bf Remark.} Notice that from the beginning of the ring theory we have proved that $K\leq F$, when $F$ is a field, implies that $K$ is also a field.

\noindent\newline{\bf Definition.} Let $E$ and $F$ be fields such that $F\leq E$. Let $c\in E$ and $a(x)\in F[x]$. The mapping $\sigma_{c}:F[x]\rightarrow E$ defined with $\sigma_c(a(x))=a(c)$ is called a {\bf substitution function}.

\noindent\newline{\bf Theorem.} Let $E$ be a field, $F\leq E$ and $c\in E$. Then, the substitution function $\sigma_{c}$ is a homomorphism.

\noindent\newline{\bf Proof.} It is obvious that $\sigma_{c}$ is well-defined. Also, if $a(x)=b(x)$, then $a(c)=b(c)$, i.e. $\sigma_{c}(a(x))=\sigma_{c}(b(x))$. Then, $\sigma_c(a(x)+b(x))=a(c)+b(c)=\sigma_c(a(x))+\sigma_c(b(x))$. Also, $\sigma_c(a(x)b(x))=a(c)b(c)=\sigma_c(a(x))\sigma_c(b(x))$. Thus, $\sigma_c$ is a homomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $E$ be a field, $F\leq E$ and $c\in E$. Then, there exists $p(x)\in F[x]$ such that $\ker{\sigma_{c}}=\cyc{p(x)}$.

\noindent\newline{\bf Proof.} We have $\ker{\sigma_c}=\{a(x)\in F[x]:\ \sigma_c(a(x))=0\}=\{a(x)\in F[x]:\ a(c)=0\}$. Thus, $a(x)\in\ker{\sigma_c}$ if and only if $c\in E$ is a root of $a(x)$. Also, $\ker{\sigma_c}\trianglelefteq F[x]$, and as $F[x]$ is a principal ideal domain, $\ker{\sigma_c}$ is principal, i.e. $\ker{\sigma_c}=\cyc{p(x)}$, for some $p(x)\in F[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $E$ be a field, $F\leq E$ and $c\in E$. We say that a polynomial $a(x)\in F[x]$ such that $\deg{a(x)}>0$, is a {\bf minimal polynomial of $c$ over $F$} if:

\begin{enumerate}
\item $a(x)$ is monic and $a(c)=0$;
\item For all $b(x)\in F[x]$, if $b(c)=0$ then $\deg{a(x)}\leq\deg{b(x)}$.
\end{enumerate}

\noindent{\bf Theorem.} Let $E$ be a field, $F\leq E$ and $c\in E$. Then a minimal polynomial of $c$ over $F$ exists\footnote{In fact, monic generator of $\ker{\sigma_c}$ is a minimal polynomial of $c$ over $F$.} and is unique.

\noindent\newline{\bf Proof.} Let us observe the substitution function $\sigma_c:F[x]\rightarrow E$. We have already proved that it is a homomorphism and that:

\begin{equation*}
\ker{\sigma_c}=\{a(x)\in F[x]:\ a(c)=0\}.
\end{equation*}

\noindent\newline {\it Existence.} We have already shown that, as $F[x]$ is a PID, then there exists $p(x)\in\ker{\sigma_c}$ such that $\ker{\sigma_c}=\cyc{p(x)}$. If $p(x)$ is not monic then it is obvious that for some $p_{\deg{p(x)}}^{-1}\in F$ (the inverse of the leading coefficient of $p(x)$), $p_{\deg{p(x)}}^{-1}p(x)$ is monic and is in $\cyc{p(x)}$. So, we can assume that $p(x)$ already is monic. Now we will show that $p(x)$ is a minimal polynomial of $c$ over $F$. It is obvious, as $p(x)\in\ker{\sigma_c}$, that $p(c)=0$. Now, assume that there exists $b(x)\in F[x]$ such that $b(c)=0$. That implies that $b(x)\in\ker{\sigma_c}=\cyc{p(x)}$, i.e. there exists $q(x)\in F[x]$ such that $b(x)=p(x)q(x)$. Then $\deg{b(x)}=\deg{p(x)}+\deg{q(x)}$, that is equivalent to $\deg{p(x)}=\deg{b(x)}-\deg{q(x)}$, which implies $\deg{p(x)}\leq\deg{b(x)}$. Thus $p(x)$ satisfies all conditions to be a minimal polynomial of $c$ over $F$.

{\it Uniqueness.} Assume that $q(x)\in F[x]$ is also a minimal polynomial of $c$ over $F$. Then, $q(c)=0$, so $q(x)\in\cyc{p(x)}$. Thus, there exists $r(x)\in F[x]$ such that $q(x)=p(x)r(x)$. But, as $q(x)$ is minimal, it must be $\deg{q(x)}\leq\deg{p(x)}$. But, also as $p(x)$ is minimal, $\deg{p(x)}\leq\deg{q(x)}$. That implies $\deg{p(x)}=\deg{q(x)}$. So from $q(x)=p(x)r(x)$ we have $\deg{q(x)}=\deg{p(x)}+\deg{r(x)}$, i.e. $\deg{q(x)}-\deg{p(x)}=\deg{r(x)}$. But, $\deg{q(x)}=\deg{p(x)}$ so $\deg{r(x)}=0$, i.e. $r(x)=v$, for some $v\in F$. Thus, $q(x)=v p(x)$. But, as both $p(x)$ and $q(x)$ are monic, their leading coefficients are $1$. That means that $1=v\cdot 1$ and from that follows that $v=1$. Thus we have $q(x)=p(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $E$ be a field, $F\leq E$, $c\in E$ and $p(x)\in F[x]$ such that $p(c)=0$. Then, $p(x)$ is a minimal polynomial of $c$ over $F$ if and only if $p(x)$ is monic and irreducible.

\noindent\newline{\bf Proof.} {\it Necessity.} Let $p(x)$ be a minimal polynomial of $c$ over $F$. If $p(x)$ was not monic, that would be an immediate contradiction to definition of a minimal polynomial. Therefore, assume that $p(x)$ is monic, but reducible. Then, there exist $q(x),r(x)\in F[x]$, with $\deg{q(x)},\deg{r(x)}>0$, such that $p(x)=q(x)r(x)$. As $p(c)=0$ we have $q(c)r(c)=0$. As $F[x]$ is an integral domain, it must be that $q(c)=0$ or $r(c)=0$. Assume that $q(c)=0$, without loss of generality. Then, as $\deg{p(x)}=\deg{q(x)}+\deg{r(x)}$, we have $\deg{q(x)}=\deg{p(x)}-\deg{r(x)}$ and from that $\deg{q(x)}<\deg{p(x)}$ (as surely $r(x)\neq 0$). But, that is in contradiction that $p(x)$ is minimal, i.e. that $\deg{p(x)}\leq\deg{t(x)}$, for all $t(x)\in F[x]$ such that $t(c)=0$ (and that also includes $q(x)$). Thus, $p(x)$ cannot be reducible and must be irreducible (and also monic).

{\it Sufficiency.} Let $p(x)\in F[x]$ be monic and irreducible with $p(c)=0$. Assume that some $q(x)\in F[x]$ is a minimal polynomial of $c$ over $F$. Then, $\ker{\sigma_c}=\cyc{q(x)}$ by a previous theorem. So, it also must be that $p(x)\in\cyc{q(x)}$ and there must exist $r(x)\in F[x]$ such that $p(x)=q(x)r(x)$. But, as $p(x)$ is irreducible, it must be that $r(x)=u$, for some $u\in F$ (and obviously $\deg{p(x)}=\deg{q(x)}$). Therefore, $p(x)=u q(x)$. As $q(x)$ is monic, and so is $p(x)$, the coefficients give rise to equation $1=u\cdot 1$, so it must be $u=1$. That gives us $p(x)=q(x)$, i.e. $p(x)$ is a minimal polynomial of $c$ over $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $E$ be a field, $F\leq E$ and $c\in E$. If there exists $a(x)\in F[x]-\{0\}$ such that $a(c)=0$, then we say that $c\in E$ is {\bf algebraic over $F$}. If there does not exist $a(x)\in F[x]-\{0\}$ such that $a(c)=0$, then we say that $c\in E$ is {\bf transcendental over $F$.}

\noindent\newline{\bf Definition.} Let $E$ be a field, $F\leq E$ and let $c\in E$ be algebraic over $F$. We define the set $F(c)=\{a(c):\ a(x)\in F[x]\}$ and say that $F(c)$ is a {\bf field generated by $F$ and $c$}.

\noindent\newline{\bf Remark.} Notice that the existence of the minimal polynomial $c$ over $F$ automatically implies that $c$ is also algebraic over $F$ (as it is the root of the polynomial with coefficients in $F$).

\noindent\newline{\bf Theorem.} Let $E$ be a field, $F\leq E$, $c\in E$ and $p(x)$ a minimal polynomial of $c$ over $F$. Then,

\begin{equation*}
F(c)\cong F[x]\slash\cyc{p(x)}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $\sigma_c:F[x]\rightarrow F$ be a substitution function. We have already proved that $\sigma_c$ is a homomorphism. We will prove that $\cyc{p(x)}=\ker{\sigma_c}$, where $p(x)$ is a minimal polynomial. As $p(c)=0$, that implies $p(x)\in\ker{\sigma_c}$. We know that $\ker{\sigma_c}$ is generated by some $q(x)\in F[x]$, i.e. $\ker{\sigma_c}=\cyc{q(x)}$ so $p(x)\in\cyc{q(x)}$ implies $p(x)=q(x)r(x)$, for some $r(x)$. But, by a previous theorem, as $p(x)$ is a minimal polynomial of $c$ over $F$, it is irreducible and it must be that $r(x)=u$ for some $u\in F$. So, we have $p(x)=q(x)u$ (with $\deg{p(x)}=\deg{q(x)}$). Take $a(x)\in\cyc{p(x)}$. Then, $a(x)=p(x)t(x)$, for some $t(x)\in F[x]$. We have $a(x)=q(x)(u t(x))$, and as $u t(x)\in F[x]$, it's $a(x)\in\cyc{q(x)}$, i.e. $\cyc{p(x)}\subseteq\cyc{q(x)}$ (although this is understandable as $p(x)\in\cyc{q(x)}$, so the principal ideal generated by $p(x)$ is obviously a subring of $\cyc{q(x)}$). Now, take $a(x)\in\cyc{q(x)}$. We have $a(x)=q(x)t(x)$, for some $t(x)\in F[x]$. From $p(x)=u q(x)$, as $u\in F$, we have $q(x)=u^{-1} p(x)$. Then, $a(x)=p(x)(u^{-1}t(x))$, so $u^{-1}t(x)\in F[x]$ and $a(x)\in\cyc{p(x)}$, meaning $\cyc{q(x)}\subseteq\cyc{p(x)}$. Finally, that means $\cyc{q(x)}=\cyc{p(x)}$ and $\ker{\sigma_c}=\cyc{p(x)}$. Also, $\ran{\sigma_c}=\{\sigma_c(a(x)):\ a(x)\in F[x]\}=\{a(c):\ a(x)\in F[x]\}=F(c)$. So, by FHT, $\ran{\sigma_c}\cong F[x]\slash\ker{\sigma_c}$, i.e. $F(c)\cong F[x]\slash\cyc{p(x)}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $E$ be a field, $F\leq E$ and $c\in E$. Then $J\subseteq F(c)$, where $J$ is a field such that $c\in J$ and $F\subseteq J$, implies\footnote{In other words, $F(c)$ is the {\bf smallest field} containing $F$ and $c\in F$.} $F(c)=J$.

\noindent\newline{\bf Proof.} Assume that there exists $a\in F(c)-J$. As $a\in F(c)$, then $a=p(c)$, for some $p(x)\in F[x]$. But, as the coefficients of $p(x)$ are in $F$, then clearly $p(c)\in F\subseteq J$, which is a contradiction. Thus, $F(c)=J$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem (Basic theorem of field extensions).} Let $F$ be a field and $a(x)\in F[x]-\{0\}$ such that $\deg{a(x)}>0$ and $a(x)=a_m x^m+\cdots+a_1 x+a_0$. There exists (an extension) field $E$ along with injective homomorphism $\phi:F\rightarrow E$ and $c\in E$ such that $\phi(F)\leq E$ and $\overline{a}(c)=0$, where $\overline{a}(x)=\phi(a_m)x^m+\cdots+\phi(a_1)x+\phi(a_0)$.

\noindent\newline{\bf Proof.} First, consider the case when $a(x)$ is irreducible and monic (if it is not monic, we can just observe its monic associate). Let $J=\cyc{a(x)}$. Then, $F[x]\slash J$ is a field and we can take $E=F[x]\slash J$. Every element in $E'$ is of the form $J+p(x)$, for some $p(x)\in F[x]$. Now, we want to have $F$ as a subfield of the field $E$ or some field isomorphic to it. Thus, we can consider adjusting $F$ with a map $\phi:F\rightarrow E$ defined by $\phi(t)=J+t$, for all $t\in F$. First, we will show that $\phi$ is function. Take $t\in F$. Then, also $t\in F[x]$ and there exists $J+t\in E$ such that $\phi(t)=J+t$. Assume that $t_1=t_2$. Then, $J+t_1,J+t_2\in E$. If we take $j+t_1\in J+t_1$, then $j+t_1=j+t_2\in J+t_2$, so $J+t_1\subseteq J+t_2$. Similarly, if $j+t_2\in J+t_2$, then, $j+t_2=j+t_1\in J+t_1$, which means $J+t_2\subseteq J+t_1$. So, $J+t_1=J+t_2$, i.e. $\phi(t_1)=\phi(t_2)$ and $\phi$ is a function. Now, for all $t_1,t_2\in F$, $\phi(t_1+t_2)=J+(t_1+t_2)=(J+t_1)+(J+t_2)=\phi(t_1)+\phi(t_2)$ and $\phi(t_1 t_2)=J+(t_1 t_2)=(J+t_1)(J+t_2)=\phi(t_1)\phi(t_2)$ implies that $\phi$ is a homomorphism. Also, note that $\ran{\phi}=\{J+t:\ t\in F\}$.

We will now show that it is also an injection. If $J+t_1=J+t_2$, for some $t_1,t_2\in\ran{\phi}$ (so it must be $t_1,t_2\in F$), we need to show $t_1=t_2$. Well, $J+t_1=J+t_2$ implies $t_1-t_2\in J$. What is the kernel of $\phi$? We have $\ker{\phi}=\{t\in F:\ J+t=J\}=\{t\in F:\ t\in J\}$. So, $t_1-t_2\in\ker{\phi}$. We know that $\ker{\phi}\trianglelefteq F$. As $F$ is a field, it has only trivial ideals, i.e. either $\ker{\phi}=\{0\}$ or $\ker{\phi}=F$. What if $\ker{\phi}=F$? If we took $t\in F$, we would have $t\in\ker{\phi}$ and that would give us $t\in J$. That would imply $F\subseteq J=\cyc{a(x)}$. I.e. there would exist $t\in\cyc{a(x)}$ such that $t=a(x)q(x)$, for some $q(x)$, meaning that both $a(x)$ and $q(x)$ are zero degree polynomials, against the assumption. So, we can only have $\ker{\phi}=\{0\}$, then $t_1-t_2=0$, and $t_1=t_2$, making $\phi$ injective.

Now, $\ran{\phi}\leq E$. Assume $a(x)=x^m+a_{m-1}x^{m-1}+\cdots+a_1 x+a_0$. As $a_i\in F$, then $J+a_i\in\ran{\phi}$. So, we can define:

\begin{equation*}
\overline{a}(x):=x^m+(J+a_{m-1})x^{m-1}+\cdots+(J+a_1)x+(J+a_0).
\end{equation*}

\noindent\newline Because $x\in F[x]$, we can take $c=J+x\in E=F[x]\slash J$ and observe $\overline{a}(c)=\overline{a}(J+x)$ in the following manner:

\begin{equation*}
\overline{a}(J+x)=(J+x)^m+(J+a_{m-1})(J+x)^{m-1}+\cdots+(J+a_1)(J+x)+(J+a_0).
\end{equation*}

\noindent\newline After using the fact that $(J+x)^i=J+x^i$, for all $i\in\Z^{+}$, we get:

\begin{equation*}
\overline{a}(J+x)=(J+x^m)+(J+a_{m-1})(J+x^{m-1})+\cdots+(J+a_1)(J+x)+(J+a_0).
\end{equation*}

\noindent\newline We use the fact that $(J+a_i)(J+x^i)=J+(a_i x^i)$ and we have:

\begin{equation*}
\overline{a}(J+x)=(J+x^m)+(J+a_{m-1}x^{m-1})+\cdots+(J+a_1 x)+(J+a_0).
\end{equation*}

\noindent\newline Now, after taking the sum of all members,

\begin{equation*}
\overline{a}(J+x)=J+(x^m+a_{m-1}x^{m-1}+\cdots+a_1 x+a_0).
\end{equation*}

\noindent\newline But, that implies that $\overline{a}(J+x)=J+a(x)$. As $J=\cyc{a(x)}$, we will show that $\cyc{a(x)}+a(x)=\cyc{a(x)}$. Take $p(x)\in\cyc{a(x)}+a(x)$. That means that there exists $q(x)\in F[x]$ such that $p(x)=a(x)q(x)+a(x)=a(x)(q(x)+1)\in\cyc{a(x)}$, so $\cyc{a(x)}+a(x)\subseteq\cyc{a(x)}$. If we take $p(x)\in\cyc{a(x)}$, then there exists $q(x)\in F[x]$ such that $p(x)=q(x)a(x)=q(x)a(x)-a(x)+a(x)=a(x)(q(x)-1)+a(x)\in\cyc{a(x)}+a(x)$, so $\cyc{a(x)}\subseteq\cyc{a(x)}+a(x)$, which finally implies $\cyc{a(x)}=\cyc{a(x)}+a(x)$, i.e. $J+a(x)=J$. So, $\overline{a}(J+x)=J+a(x)=J$ and $J$ is a zero in $E=F[x]\slash J$. The difference in polynomials $\overline{a}(x)$ and $a(x)$ is merely in notation by which we immersed $a(x)$ in $E$, as we could have easily defined $a_i:=J+a_i$ and then calculate $a_i(J+x)=J+(a_i x)$, in the same fashion. Finally, if $a(x)$ was not irreducible, it could have been factor into irreducible factors, and we could, for one of them repeat the process and that will also be the root of $a(x)=p(x)q(x)$, where we may assume $p(x)$ is irreducible, as $a(c)=p(c)q(c)=0q(c)=0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Corollary.} Let $F$ be a field and $a(x)\in F[x]-\{0\}$ such that $\deg{a(x)}=m$, where $m\in\Z^{+}$. There exists an extension field $E$ of $F$ which contains all $m$ roots of $a(x)$.

\noindent\newline{\bf Proof.} This can be obtained by repeating previous theorem $m$ times. First we obtain a root for $a(x)$, and then find an extension of the polynimial divided by $x-c$, where $c$ is a root in that extension, and so on.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} One of the most notable examples would be the field of complex numbers $\C$, obtained by observing solutions to the equation $x^2+1=0$ in $\R$. This polynomial is obviously irreducible in $\R$ as it is of degree $2$ and can only be written as a product of two polynomials of degree $1$. But then it would be reducible if and only if it had a root in $\R$. Yet, if there was a root $I$ in $\R$, it would have to satisfy $I^2=-1$, which is impossible. Therefore, we observe extension field $\R[x]\slash\cyc{x^2+1}$, which we can, because $x^2+1$ is irreducible in $\R$. Obviously $\left(\cyc{x^2+1}+x\right)^2+\left(\cyc{x^2+1}+1\right)=\cyc{x^2+1}+(x^2+1)=\cyc{x^2+1}$, and as $\cyc{x^2+1}$ is a zero in $\R[x]\slash\cyc{x^2+1}$, it is a root of the polynomial $x^2+1$.  We can take $I=\cyc{x^2+1}+x$. Notice that then $I^2+\left(\cyc{x^2+1}+1\right)=\cyc{x^2+1}$, i.e. $I$ is a root of $x^2+1=0$ (with coefficients from isomorphic copy of $\R$). Also, the elements of the isomorphic copy of $\R$ look like $\cyc{x^2+1}+c$, where $c\in\R$, while all other elements look like $\cyc{x^2+1}+(a x+b)$, where $a,b\in\R$. Actually, all elements in $\R[x]\slash\cyc{x^2+1}$ can be written as $\cyc{x^2+1}+(a x+b)=\left(\cyc{x^2+1}+a\right)\left(\cyc{x^2+1}+x\right)+\left(\cyc{x^2+1}+b\right)=\left(\cyc{x^2+1}+a\right)I+\left(\cyc{x^2+1}+b\right)$. It is then clear that $\C=\R(I)\cong\R[x]\slash\cyc{x^2+1}$.

\noindent\newline{\bf Problem.} Prove that each of the following numbers is algebraic over $\Q$: (a) $\sqrt{1+\sqrt{2}}$; (b) $i$; (c) $\sqrt{2}$; (d) $2+3i$; (e) $\sqrt{1+\sqrt[3]{2}}$; (f) $\sqrt{i-\sqrt{2}}$; (g) $\sqrt{2}+\sqrt{3}$; (h) $\sqrt[3]{2}+\sqrt[3]{4}$.

\noindent\newline{\bf Solution.} (a) Let $x=\sqrt{1+\sqrt{2}}$. Then, $x^2=1+\sqrt{2}$ and $x^2-1=\sqrt{2}$. From that we obtain $x^4-2x^2+1=2$, i.e. $x^4-2x^2-1=0$. So, $p(x)=x^4-2x^2-1$ is the polynomial with rational coefficients such that $p\left(\sqrt{1+\sqrt{2}}\right)=0$ and from that $\sqrt{1+\sqrt{2}}$ is algebraic over $\Q$. (b) We already know that, if $p(x)=x^2+1$, where obviously $p(x)\in Q[x]$, then $p(i)=i^2+1=-1+1=1$, so $i$ is algebraic over $\Q$. (c) We have $p(x)=x^2-2\in\Q[x]$, and $p\left(\sqrt{2}\right)=2-2=0$, so $\sqrt{2}$ is algebraic over $\Q$. (d) We know that $z+\overline{z},z\cdot\overline{z}\in\R$ so we can use that fact and form $p(x)=(x-(2+3i))(x-(2-3i))=x^2-2x+3ix-2x-3ix+(2+3i)(2-3i)=x^2-4x+4-9i^2=x^2-4x+13\in\Q[x]$. Obviously $p(2+3i)=0$, so $2+3i$ is algebraic over $\Q$. (e) Let $x=\sqrt{1+\sqrt[3]{2}}$. We have $x^2=1+\sqrt[3]{2}$ and from that $x^2-1=\sqrt[3]{2}$. Then, $x^6-3x^2+3x-1=2$ and $p(x)=x^6-3x^2+3x-3$ is the polynomial in $\Q[x]$ such that $p\left(\sqrt[3]{2}\right)=0$. (f) Let $x=\sqrt{i-\sqrt{2}}$. We have $x^2=i-\sqrt{2}$. Then, $x^4=-1-2i\sqrt{2}+2$. So, $x^4-1=-2i\sqrt{2}$ and $x^8-2x^4+1=-8$. From that we have $x^8-2x^4+9=0$. So, $p(x)=x^8-2x^4+9\in\Q[x]$ and $p\left(\sqrt{i-\sqrt{2}}\right)=0$ and $\sqrt{i-\sqrt{2}}$ is algebraic over $\Q$.

(g) Let $x=\sqrt{2}+\sqrt{3}$. Then, $x^2=2+2\sqrt{6}+3$, i.e. $x^2-5=2\sqrt{6}$. That gives us $x^4-10x^2+1=0$. Thus, $x^4-20x^2+19=p(x)\in\Q[x]$ and $p\left(\sqrt{2}+\sqrt{3}\right)=0$, so $\sqrt{2}+\sqrt{3}$ is algebraic over $\Q$. We know that $p(x)=x^2-2$ gives $p(\sqrt{2})=2-2=0$ and that $q(x)=x^2-3$ gives $q(\sqrt{3})=3-3=0$ and that $\sqrt{2}$ and $\sqrt{3}$ are algebraic over $\Q$. Now, there is another way of proving this. Let $p=\sqrt{2}$ and $q=\sqrt{3}$. Then, $p^2=2$, $p^3=2p$, $p^4=4$ and $q^2=3$, $q^3=3q$, $q^4=9$. We have $(p+q)^4=p^4+4p^3q+6p^2q^2+4p q^3+q^4=4+8p q+36+12p q+9=49+20p q$. Also $(p+q)^2=p^2+2p q+q^2=2+2p q+3=5+2p q$ and obviously $-10(p+q)^2=-50-20p q$. Therefore, $(p+q)^4-10(p+q)^2=49+20p q-50-20p q=-1$. Finally, $(p+q)^4-10(p+q)^2+1=-1+1=0$. Thus, again we have $x^4+10x^2+1=0$ and $p+q$ is algebraic over $\Q$.

(h) We have $x=\sqrt[3]{2}+\sqrt[3]{4}$. We can take $x=\sqrt[3]{2}\left(1+\sqrt[3]{2}\right)$. So, $x^3=2(1+\sqrt[3]{2})^3$, i.e. $x^3=4+6\sqrt[3]{4}+6\sqrt[3]{2}+2$. That is, $x^3-6=6\left(\sqrt[3]{4}+\sqrt[3]{2}\right)$. But, that is obviously $x^3-6=6x$, i.e. $x^3-6x-6=0$. Therefore, $p(x)=x^4-6x-6\in\Q[x]$ with $p\left(\sqrt[3]{2}+\sqrt[3]{4}\right)=0$ proves that $\sqrt[3]{2}+\sqrt[3]{4}$ is algebraic over $\Q$.

\noindent\newline{\bf Problem.} Prove that: (a) $\sqrt{\pi}$ is algebraic over $\Q(\pi)$; (b) $\sqrt{\pi}$ is algebraic over $\Q\left(\pi^2\right)$; (c) $\pi^2-1$ is algebraic over $\Q\left(\pi^3\right)$.

\noindent\newline{\bf Solution.} (a) Notice that

\begin{equation*}
\Q(\pi)=\left\{a_m\pi^m+\cdots+a_1\pi+a_0:\ \left(\forall i\in\{1,\ldots,m\}\right)\left(a_i\in\Q\right)\right\}.
\end{equation*}

\noindent\newline That also means that $\Q\subset\Q\left(\pi\right)$ (obviously a proper subset as $\pi\notin\Q$). Therefore, $\sqrt{\pi}=x$ gives us $x^2=\pi$. Taking $p(x)=x^2-\pi$ gives us $p\left(\sqrt{\pi}\right)=\pi-\pi=0$ and also $p(x)\in\Q(\pi)$. Therefore, $\sqrt{\pi}$ is algebraic over $\Q(\pi)$. (b) Notice that $\pi\notin\Q\left(\pi^2\right)$. So, from $\sqrt{\pi}=x$ and $\pi=x^2$, we must continue to $\pi^2=x^4$. Therefore $p\left(\sqrt{\pi}\right)=0$ with $p(x)=x^4-\pi^2\in\Q\left(\pi^2\right)[x]$ and $\sqrt{\pi}$ is algebraic over $\Q\left(\pi^2\right)$. (c) Take $x=\pi^2-1$. Then, $x+1=\pi^2$ and taking $x^3+3x^2+3x+1=\pi^6$. That is equivalent to $x^3+3x^2+3x+1-\pi^6=0$. As $\pi^3\in\Q\left(\pi^3\right)$, then also $\pi^6=1\cdot\left(\pi^3\right)^2+0\cdot\pi^3+0\in\Q\left(\pi^3\right)$. So, $p\left(\pi^2-1\right)=0$, where $p(x)=x^3+3x^2+3x-1-\pi^6\in\Q\left(\pi^3\right)[x]$ and $\pi^2-1$ is algebraic over $\Q\left(\pi^3\right)$.

\noindent\newline{\bf Problem.} Find the minimum polynomial of each of the following numbers over $\Q$: (a) $1+2i$; (b) $1+\sqrt{2}$; (c) $1+\sqrt{2i}$; (d) $\sqrt{2+\sqrt[3]{3}}$; (e) $\sqrt{3}+\sqrt{5}$; (f) $\sqrt{1+\sqrt{2}}$.

\noindent\newline{\bf Solution.} (a) Let $p(x)=(x-1-2i)(x-1+2i)=x^2-x+2xi-x-2xi+5=x^2-2x+5$. It is obvious that $p(1+2i)=0$, from the very way we defined $p(x)$. To show that $p(x)$ is irreducible over $\Q$ we will check for rational roots by observing divisors of $5$. As $5$ is prime, then we only need to check for $x\in\{\pm1,\pm5\}$. We have $p(1)=1-2+5=4$, $p(-1)=1+2+5=8$, $p(5)=25-10+5=15+5=20$ and $p(-5)=25+10+5=40$. Thus, $p(x)$ has no rational roots, and as it is of degree $2$, it is irreducible over $\Q$. That also implies that $p(x)=x^2-2x+5$ is a minimal polynomial of $1+2i$ over $\Q$.

(b) Let $x=1+\sqrt{2}$. Then, $x-1=\sqrt{2}$. From that we have $x^2-2x+1=2$, i.e. $x^2-2x-1=0$. That means that we can take $p(x)=x^2-2x-1$ and $p\left(1+\sqrt{2}\right)=0$. To show that $p(x)$ is irreducible over $\Q$ we observe possible roots $x\in\{\pm 1\}$. Then, $p(1)=1-2-1=-2$ and $p(-1)=1+2-1=2$. Thus $p(x)$ is a minimal polynomial if $1+\sqrt{2}$ over $\Q$.

(c) Let $x=1+\sqrt{2i}$. Then, $x-1=\sqrt{2i}$ and $(x-1)^2=2i$. From that we have $(x-1)^4=2$, i.e. $p(x)=(x-1)^4-2=0$. Take $x\rightarrow x+1$. Then, $t(x)=p(x+1)=x^4-2$. Remember that, if $p(x+1)$ is irreducible, then so is $p(x)$. For, if it were that $p(x+1)$ was irreducible and $p(x)$ reducible, we could write $p(x)=q(x)r(x)$, but also $p(x+1)=q(x+1)r(x+1)$ (the detailed discussion is in a proposition above, the only thing left to argue is that linear transformation preservs degree of a polynomial) which would be a contradiction. So, let's observe $t(x)=x^4-2$. We have $2|-2$, $2^2=4\nmid-2$, $2|0$ (coefficients for $x$, $x^2$ and $x^3$) and $2\nmid 1$ (leading coefficient of $t(x)$), so by Eisenstein's criterion, polynomial $t(x)$ is irreducible over $\Q$, but then so is $p(x+1)$ and $p(x)$. Thus, the minimal polynomial for $1+\sqrt{2i}$ over $\Q$ is $(x-1)^4-2$.

(d) Let $x=\sqrt{2+\sqrt[3]{3}}$. Then, $x^2=2+\sqrt[3]{3}$ and $\left(x^2-2\right)^3=3$. So, let $p(x)=(x^2-2)^3-3=x^6-6x^4+12x^2-11$. In $\zmod{3}[x]$ we have $\overline{p}(x)=x^6-\overline{11}=x^6+\overline{1}$. For now, we will not prove irreducibility of this polynomial - although it is irreducible: by Fermat's little theorem, it has no roots, because for all $\overline{a}\in\{\overline{1},\overline{2}\}$, we have $\overline{a}^2=\overline{1}$; so, $\overline{p}(\overline{a})=\left(\overline{a}^2\right)^3+\overline{1}=\overline{1}^2+\overline{1}=\overline{2}$, for all $\overline{a}\in\zmod{3}-\{\overline{0}\}$, and it is obvious that $\overline{p}(\overline{0})=\overline{1}$). So, I will, for now exclude the check for reduction of $\overline{p}(x)$ as a product of two polynomials, either when they're of degree $2$ and $3$, or when they're both of degree $3$.

(e) Let $x=\sqrt{3}+\sqrt{5}$. We have $x^2=3+2\sqrt{15}+5$ and $x^2-8=2\sqrt{15}$. From this we have $x^4-16x^2+64=60$, i.e. $p(x)=x^4-16x^2+4$. The possible roots are $\pm1$, $\pm2$ and $\pm4$. We can see that $p(\pm1)=1-16+4=-11\neq0$, $p(\pm2)=16-16\cdot4+4=20-64=-44\neq0$ and $p(\pm4)=4^4-16\cdot16+4=4^2\cdot4^2-4^2\cdot4^2+4=4\neq0$. Therefore, we must only check if $x^4-16x^2+4=(x^2+a x+b)(x^2+c x+d)$. From this we have $b d=4$, $a d+b c=0$, $a c+b+d=-16$ and $a+c=0$. Thus, $c=-a$ and we have $a d-a b=0$, i.e. $a(d-b)=0$. Thus, either $a=0$, or $b=d$. If $a=0$, we have $b+d=-16$ and $b d=4$. This gives us, after multiplying with $d$, $b d+d^2=-16d$, i.e. $d^2+16d+4=0$. We can take $q(x)=x^2+16x+4$. Then, all we need to do is check for roots in $\{\pm1,\pm2,\pm4\}$. We see that $q(1)=5+16=21$, $q(-1)=5-16=-11$, $q(2)=8+32=40$, $q(-2)=8-32=-24$, $q(4)=20+64=84$, $q(-4)=20-64=-44$. Thus, $d^2+16d+4=0$ has no rational solutions, and there does not exist $d\in\Q$ that would satisfy $b d=4$ and $b+d=-16$. So, we may assume $d-b=0$, i.e. $b=d$. But, then $b d=4$ implies $b^2=4$ and $2 b=-16$. We would have $b=d=\pm 2$, but then $-a^2+2\cdot(\pm 2)=-16$, i.e. $a^2=16+\pm4$. First case would yield $a^2=12$ and second $a^2=20$. Such rational numbers do not exist. Thus, $p(x)$ is irreducible.

(f) Let $x=\sqrt{1+\sqrt{2}}$. Then, $x^2-1=\sqrt{2}$ and $x^4-2x^2+1=2$. From that we have $p(x)=x^4-2x^2-1$. The possibilities for roots lie in $\pm1$. It is obvious that $p(\pm1)=1-2-1=-2$. Thus, we are left with $x^4-2x^2-1=(x^2+a x+b)(x^2+c x+d)$. So, $b d=-1$, $a d+b c=0$, $a c+b+d=-2$ and $a+c=0$. We have $c=-a$ and that implies $a d-a b=0$, i.e. $a(d-b)=0$. If $a=0$, we have $b+d=-2$ and $b d=-1$. After multiplying former equality by $d$ we have $d^2+2d-1=0$. Let $q(x)=x^2+2x-1$. Then, solutions lie in $\pm1$. We have $q(1)=1+2-1=2$ and $q(-1)=1-2-1=-2$. Thus, there does not exist such $d\in\Q$ and it must be that $b-d=0$, i.e. $b=d$. From that we have, as $b d=-1$, that $d^2=-1$, which is again impossible. Thus, $p(x)$ is irreducible.

\noindent\newline{\bf Problem.} Show that the minimal polynomial of $\sqrt{2}+i$ is: (a) $x^2-2x\sqrt{2}+3$ over $\R$; (b) $x^4-2x^2+9$ over $\Q$; (c) $x^2-2i x-3$ over $\Q(i)$.

\noindent\newline{\bf Solution.} (a) If $p(x)=x^2-2x\sqrt{2}+3$ were reducible over $\R$, we would have $a^2-2a\sqrt{2}+3=0$, for some $a\in\R$ and we would have $p(x)=(x-a)q(x)$, for some $q(x)\in\R[x]$, $\deg{q(x)}=1$. But, as $p\left(\sqrt{2}+i\right)=2+2i\sqrt{2}-1-4-2i\sqrt{2}+3=0$ and $p\left(\sqrt{2}-i\right)=2-2i\sqrt{2}-1-4+2i\sqrt{2}+3=0$, so either $x-(\sqrt{2}+i)|x-a$ or $x-(\sqrt{2}+i)|q(x)$. As $x-(\sqrt{2}+i)|x-a$ would imply $a=\sqrt{2}+i\notin\R$, it must be that $q(x)=x-(\sqrt{2}+i)$ (due to $\deg{q(x)}=1$). Then, we have $p(x)=(x-a)(x-(\sqrt{2}+i))$. But, as $x-(\sqrt{2}-i)$ is also a root, and $x-(\sqrt{2}-i)\nmid x-(\sqrt{2}+i)$ (due to obvious reasons), it must be that $x-(\sqrt{2}-i)|x-a$ which would impliy $a=\sqrt{2}-i\notin\R$. Thus, $p(x)$ is irreducible over $\R$ and $p(\sqrt{2}+i)=0$, so $p(x)$ is the minimal polynomial of $\sqrt{2}+i$ over $\R$.

(b) Let $p(x)=x^4-2x^2+9$. Then, $p\left(\sqrt{2}+i\right)=4+8i\sqrt{2}-12-4i\sqrt{2}+1-4-4i\sqrt{2}+2+9=16+8i\sqrt{2}-16-8i\sqrt{2}=0$. Now, the only possible roots are $\pm1$, $\pm3$ and $\pm9$. We have $p\left(\pm1\right)=1-2+9=8$, $p\left(\pm3\right)=81-18+9=54$, $p\left(\pm9\right)=6561-162+9=6408$. Thus, the only possibility is that $x^4-2x+9=(x^2+a x+b)(x^2+c x+d)$. From that we have $b d=9$, $a d+b c=0$, $a c+b+d=-2$ and $a+c=0$. From the latter expression we get $c=-a$, so from $a d+b c=0$ we have $a d-a b=0$, i.e. $a(d-b)=0$. Thus, as $\Q$ is an integral domain, we have $a=0$ or $d=b$. If $a=0$, then $b+d=-2$ and $b d=9$. Multiplying former equality by $d$ we have $d^2+2d+9=0$. Let $q(x)=x^2+2x+9$. The possible roots are $\pm1$, $\pm3$, $\pm9$. We have $q(1)=1+9+2=12$, $q(-1)=1+9-2=8$, $q(3)=9+9+6=24$, $q(-3)=9+9-6=12$, $q(9)=81+9+18=108$ and $q(-9)=81+9-18=72$. As $q(x)$ has no roots in $\Q$, then $d^2+2d+9=0$ has no rational solutions and this case is impossible. Now, if $b=d$, we have $d^2=9$, i.e. $b=d=\pm 3$. Also, $-a^2+2\cdot(\pm 3)=-2$, that is, $a^2=2+\pm6$. In the first case we have $a^2=8$ and in the second case $a^2=-4$. Both cases are impossible (in $\Q$ of course), so $p(x)$ is irreducible and is a minimal polynomial of $\sqrt{2}+i$ over $\Q$.

(c) Let $p(x)=x^2-2i x-3$. Then, $p(\sqrt{2}+i)=2+2i\sqrt{2}-1-2i\sqrt{2}+2-3=0$. To show that $p(x)$ is irreducible over $\Q(i)$, we will assume that $x^2-2i x-3=(x+a+bi)(x+c+di)$, where $a,b,c,d\in\Q$. Then, $(a+c)+i(b+d)=-2 i$ and $a c-b d+i(a d+b c)=-3$ So, $a+c=0$, $b+d=-2$, $a c-b d=-3$ and $a d+b c=0$. First equality gives us $c=-a$, so we have $a d-a b=0$, i.e. $a(d-b)=0$. As $\Q$ is an integral domain, $a=0$ or $b=d$. If $a=0$, then $b d=-3$ and $d+b=-2$. If we multiply latter equality by $d$, we get $d^2+2d-3=0$. We can see that $d\in\{1,-3\}$. If $d=1$, then $b=-3$. But, then $a c-b d=-3$ would imply $0-(-3)\cdot1=3\neq -3$, so this case is impossible. If $d=-3$, then $b=1$, and again we would have $0-1\cdot(-3)=3\neq-3$. Assume $b=d$. Then, $d^2=-3$, which is impossible. Thus, $p(x)$ is irreducible over $\Q(i)$, which implies $p(x)$ is the minimal polynomial of $\sqrt{2}+i$ over $\Q(i)$.

\noindent\newline{\bf Problem.} Find the minimum polynomial of the following numbers over the indicated fields:

\begin{enumerate}
\item[(a)] $\sqrt{3}+i$ over $\R$; over $\Q$; over $\Q(i)$; over $\Q\left(\sqrt{3}\right)$;
\item[(b)] $\sqrt{i+\sqrt{2}}$ over $\R$; over $\Q$; over $\Q(i)$; over $\Q\left(\sqrt{2}\right)$.
\end{enumerate}

\noindent{\bf Solution.} (a) Let $x=\sqrt{3}+i$. Then, $x-\sqrt{3}=i$ and $x^2-2x\sqrt{3}+3=-1$. Thus, we can take $p_1(x)=x^2-2x\sqrt{3}+4$. To show that it is irreducible over $\R$, we will note that the root of $p_1(x)$ is also $\sqrt{3}-i$, and that the factorization is then $p_i(x)=(x-\sqrt{3}-i)(x-\sqrt{3}+i)$. So, if there was a real root $c$ of $p_1(x)$, we would have $x-c|x-\sqrt{3}-i$ or $x-c|x-\sqrt{3}+i$, which would be a contradiction in each case (as $c$ would be complex).

Now, over $\Q$, we need to get rid of both $\sqrt{3}$ and $i$. First we have $x-\sqrt{3}=i$. Then, $x^2-2x\sqrt{3}+3=-1$ and $2x\sqrt{3}=4+x^2$, which gives us $12x^2=16+8x^2+x^4$, and we can take $x^4-4x^2+16=0$. Let $p_2(x)=x^4-4x^2+16$. Then, the roots are in $\{\pm1,\pm2,\pm4,\pm8,\pm16\}$. We have $p_2(\pm1)=1-4+16=13$, $p_2(\pm2)=16$, $p_2(\pm4)=208$, $p_2(\pm8)=3856$ and $p_2(\pm16)=64528$ so $p_2(x)$ has no roots in $\Q$. But, maybe it can be reduced as $x^4-4x^2+16=(x^2+a x+b)(x^2+c x+d)$. Then we have $a+c=0$, $a c+b+d=-4$, $a d+b c=0$ and $b d=16$. We have $c=-a$ and then $a d-a b=0$, i.e. $a(d-b)=0$. As $\Q$ is an integral domain, either $a=0$ or $d=b$. Assume $a=0$. Then, $b+d=-4$ and $b d=16$. We multiply former equality by $d$ and get $d^2+4d+16=0$. This polynomial is irreducible over $\Q$ as its discriminant is $D=16-64<0$. Thus, this is an impossible case, as there does not exist such $d$. Assume $b=d$. Then we have $b+b=-4$, i.e. $2b=-4$, which gives us $b=-2$. But, from $b d=16$, we have $b^2=4\neq 16$, which is again impossible.

In this case, we don't have to worry about $i$. Let $x=\sqrt{3}+i$. Then, $x-i=\sqrt{3}$ and we have $x^2-2i x-1=3$, i.e. $p_3(x)=x^2-2i x-4$. Assume that $x^2-2i x-4=(x+(a+b i))(x+(c+d i))$. Then, $a+c+i(b+d)=-2 i$ and $a c-b d+i(b c+a d)=-4$. From the former equality we have $b+d=-2$, $a+c=0$ and from the latter $b c+a d=0$ and $a c-b d=-4$. Therefore, because $c=-a$, we have $-a^2-b d=-4$ and $-a b+a d=0$, i.e. $a(d-b)=0$. As $\Q$ is an integral domain, either $a=0$ or $d=b$. If $a=0$, we have $-b d=-4$, i.e. $b d=4$. Also we have $b+d=-2$, so multiplying that by $d$ gives us $d^2+2d+4=0$. The discriminant of this equation is $D=4-16<0$, so there are no real, and consequently, no rational solutions. Thus, it must be $b=d$. But then $b^2=4$, i.e. $b=\pm 2$, but $b+b=-2$ gives us $2b=-2$, i.e. $b=-1\neq\pm2$, which is again impossible.

Finally, in $\Q(\sqrt{3})$, we disregard $\sqrt{3}$. Let $x=\sqrt{3}+i$. Then, $x-\sqrt{3}=i$ and $x^2-2x\sqrt{3}+3=-1$, i.e. $p_4(x)=x^2-2x\sqrt{3}+4$. All $\sqrt{3}^m$ yield either a rational number or, if $m=2k+1$, $\sqrt{3^{2 k}\cdot 3}=3^k\sqrt{3}$, so in $\Q(\sqrt{3})$, every number is of the form $a+b\sqrt{3}$, where $a,b\in\Q$. Assume $x^2-2x\sqrt{3}+4=(x+(a+b\sqrt{3}))(x+(c+d\sqrt{3}))$. We have $a+c+(b+d)\sqrt{3}=-2\sqrt{3}$ and $a c+3b d+(a d+b c)\sqrt{3}=4$. So, $a+c=0$, $b+d=-2$, $a c+3b d=4$ and $a d+b c=0$. From $c=-a$ we have $a d-a b=0$, i.e. $a(d-b)=0$. As $\Q$ is an integral domain, $a=0$ or $b=d$. Assume $a=0$. Then, $3 b d=4$, i.e. $b d=\frac{4}{3}$. Also, as $b+d=-2$, multiplying that by $d$ gives us $d^2+2d+\frac{4}{3}=0$. The discriminant is $4-\frac{16}{3}=4-5\frac{1}{3}<0$, and there are no real, and by that no rational, solutions. Therefore, it must be $d=b$. But then $b^2=\frac{4}{3}$ and $2b=-2$, i.e. $b=-1$. It is obvious that this is a contradiction because $1\neq\frac{4}{3}$.

(b) Let $x=\sqrt{i+\sqrt{2}}$. We need to get rid of $i$ only. Thus, $x^2=i+\sqrt{2}$ and we have $x^2-\sqrt{2}=i$. After squaring that we get $x^4-2x^2\sqrt{2}+2=-1$ and we can take $p_1(x)=x^4-2x^2\sqrt{2}+3$. It is obvious that $\pm\sqrt{i+\sqrt{2}}$ will be roots of $p_1(x)$. But, due to squaring in $x^2-\sqrt{2}=i$, we will also have the roots $\pm\sqrt{-i+\sqrt{2}}$. As all roots are complex, the existence of a real root would imply that a real number equals a complex number, which would be a contradiction. The only other possibility is that $p_1(x)$ can be written as a product of two degree $2$ irreducible polynomials. But, that would imply that these irreducible polynomials over $\R$ would have to divide some product of two $x-c$, where $c$ is a root. But, these products will always contain $i$, as multiplication will only remove the square root over $i$, but not $i$. Let $x=\sqrt{i+\sqrt{2}}$. We need to get rid of $\sqrt{2}$ and $i$ to be able to find a minimal polynomial over $\Q$. We have $x^2=i+\sqrt{2}$ and $x^2-\sqrt{2}=i$. Then, $x^4-2x^2\sqrt{2}+2=-1$. Then, $x^4+3=2x^2\sqrt{2}$. Squaring that gives us $x^8+6x^4+9=8x^4$, i.e. $x^8-2x^4+9=0$. Thus, $p_2(x)=x^8-2x^4+9$. For now we will not check irreducibility. Let $x=\sqrt{i+\sqrt{2}}$. We need to get rid of $\sqrt{2}$ only to find a minimal polynomial over $\Q(i)$. Then, $x^2=i+\sqrt{2}$ and $x^2-i=\sqrt{2}$. Thus, $x^4-2ix^2-1=2$ and we can take $p_3(x)=x^4-2ix^2-3$. Let $x=\sqrt{i+\sqrt{2}}$. Over $\Q\left(\sqrt{2}\right)$ we need only remove $i$. So, we have $x^2=i+\sqrt{2}$ and $x^2-\sqrt{2}=i$. Finally, $x^4-2x^2\sqrt{2}+2=-1$ and we can take $p_4(x)=x^4-2x^2\sqrt{2}+3$. We will not check irreducibility for $p_2(x)$, $p_3(x)$ and $p_4(x)$. It is too cumbersome for me now.

\noindent\newline{\bf Problem.} For each of the following polynomials $p(x)$, find a number $a$ such that $p(x)$ is the minimum polynomial of $a$ over $\Q$: (a) $x^2+2x-7$; (b) $x^4+2x^2-1$; (c) $x^4-10x^2+1$.

\noindent\newline{\bf Solution.} (a) We can use the formula for the quadratic equation and get $x_{1,2}=\frac{-2\pm\sqrt{28}}{2}$. Thus, we can take $a=\sqrt{7}-1$. Then $a^2+2a-7=0$ and $x^2+2x-7$ is irreducible over $\Q$ (the divisors $\pm1$ and $\pm7$ are obviously not roots). (b) Let $t=x^2$. Then we have $x^4+2x^2-1=t^2+2t-1=0$ and $t_{1,2}=\frac{-2\pm\sqrt{8}}{2}$. We can take $t=\sqrt{2}-1$ and then $a=\sqrt{\sqrt{2}-1}$. (c) Let $t=x^2$. Then, $t^2-10t+1=0$ and $t_{1,2}=\frac{10\pm\sqrt{96}}{2}$ and we can take $t=5+2\sqrt{6}$ and $a=\sqrt{5+2\sqrt{6}}$.

\noindent\newline{\bf Problem.} Find a monic irreducible polynomial $p(x)$ such that $\Q[x]\slash\cyc{p(x)}$ is isomorphic to: (a) $\Q\left(\sqrt{2}\right)$; (b) $\Q\left(1+\sqrt{2}\right)$; (c) $\Q\left(\sqrt{1+\sqrt{2}}\right)$.

\noindent\newline{\bf Solution.} (a) Let $p(x)=x^2-2$. Then, $\sqrt{2}^2-2=2-2=0$ and $p(x)$ is irreducible over $\Q$ (for $\pm1$ we have $p\left(\pm1\right)=1-2=-1$ and $p\left(\pm2\right)=4-2=2$, so there are no roots over $\Q$). Thus, by the previous theorem, $\Q[x]\slash\cyc{x^2-2}\cong\Q[x]\left(\sqrt{2}\right)$. (b) Let $x=1+\sqrt{2}$. Then, $x-1=\sqrt{2}$ and we have $x^2-2x+1=2$ and we can take $p(x)=x^2-2x-1$. Polynomial $p(x)$ is obviously irreducible as $p(1)=1-2-1=-2$ and $p(-1)=1+2-1=2$. By the previous theorem, $\Q\slash\cyc{x^2-2x-1}\cong \Q[x]\left(1+\sqrt{2}\right)$. (c) Let $x=\sqrt{1+\sqrt{2}}$. Then, $x^2=1+\sqrt{2}$. We have $x^2-1=\sqrt{2}$ which gives us $x^4-2x^2+1=2$ and we can take $p(x)=x^4-2x^2-1$. We will not check irreducibility of this polynomial for now, as extensive work of testing irreducibility has already been done in previous chapter. By previous theorem we have $\Q[x]\slash\cyc{x^4-2x^2-1}\cong\Q\left(\sqrt{1+\sqrt{2}}\right)$.

\noindent\newline{\bf Proposition.} Let $F$ be a field, $p(x)\in F[x]$ irreducible polynomial with $\deg{p(x)}=m$, $m\in\Z^{+}-\{1\}$ and let $c\in E$, where $E$ is an extension field of $F$, such that $p(c)=0$. Let $a\in F(c)$. Then, there exists a unique $r(x)\in F[x]$ with $\deg{r(x)}<m$, such that $a$ can be written as $r(c)$.

\noindent\newline{\bf Proof.} As $c$ is a root of irreducible polynomial $p(x)$, by previous theorem we have $F(c)\cong F[x]\slash\cyc{p(x)}$. Then, there exists an isomorphism $\phi:F(c)\rightarrow F[x]\slash\cyc{p(x)}$ and, as $a\in F(c)$, we have $\phi(a)=\cyc{p(x)}+t(x)$, for some $t(x)\in F[x]$.  Thus, $\phi(a)=\cyc{p(x)}+t(x)$.

Recall that this result is obtained by a surjective homomorphism, or the substitution function, $\sigma_c:F[x]\rightarrow F(c)$ whose kernel is $p(x)$. The function is defined with $\sigma_c(q(x))=q(c)$, for all $q(x)\in F[x]$. As $a\in F(c)$, and $\sigma_c$ is surjective, there exists $t(x)\in F[x]$ such that $\sigma_c(t(x))=a$, i.e. $t(c)=a$. Now, if the degree of $t(x)$ is less then $m$, we simply take $r(x)=t(x)$. If it is not, by division algorithm, there exist $q(x),r(x)\in F[x]$ such that $t(x)=q(x)p(x)+r(x)$, with $0\leq\deg{r(x)}<\deg{p(x)}=m$. As $t(x)\in F[x]$, there exists $\cyc{p(x)}+t(x)\in F[x]\slash\cyc{p(x)}$. But, then $\cyc{p(x)}+t(x)=\cyc{p(x)}+[q(x)p(x)+r(x)]=\cyc{p(x)}+r(x)$, implying $t(x)-r(x)\in\cyc{p(x)}$. As $\cyc{p(x)}$ is the kernel of $\sigma_c$, that means that $\sigma_c(t(x)-r(x))=0$, i.e. $t(c)-r(c)=0$. That finally implies $t(c)=r(c)$, that is $a=r(c)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Explain why there are exactly four elements in $\zmod{2}[x]\slash\cyc{x^2+x+\overline{1}}$. List these four elements, and give their addition and multiplication tables.

\noindent\newline{\bf Solution.} Let $p(x)=x^2+x+\overline{1}$. If $a\in\zmod{2}[x]\slash\cyc{p(x)}$, then there exists $t(x)\in\zmod{2}[x]$ such that $a=\cyc{p(x)}+t(x)$. By division algorithm there exist $q(x),r(x)$ such that $t(x)=q(x)p(x)+r(x)$, where $0\leq\deg{r(x)}<2$. Thus, $r(x)=A x+B$, for some $A,B\in\zmod{2}$, and $t(x)=q(x)p(x)+(A x+B)$. That implies $a=\cyc{p(x)}+t(x)=\cyc{p(x)}+[q(x)p(x)+(A x+B)]=\cyc{p(x)}+[A x+B]$. As $A,B\in\zmod{2}$, there are only four options: $\cyc{p(x)}$, $\cyc{p(x)}+\overline{1}$, $\cyc{p(x)}+x$ and $\cyc{p(x)}+[x+\overline{1}]$. Let us use the following notation: $A x+B:=\cyc{p(x)}+[A x+B]$. Also, note that $x^2=(x^2+x+\overline{1})-(x+\overline{1})=(x^2+x+\overline{1})+(x+\overline{1})$. Thus, $x^2+x=(x^2+x+\overline{1})+(\overline{1})$, $(x+\overline{1})^2=x^2+\overline{1}=(x^2+x+\overline{1})+(x)$. The addition and multiplication tables are:

\begin{center}
\begin{parbox}{0.45\linewidth}{\begin{tabular}{c|cccc}
$+$ & $\overline{0}$ & $\overline{1}$ & $x$ & $x+\overline{1}$\\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $x$ & $x+\overline{1}$\\
$\overline{1}$ & $\overline{1}$ & $\overline{0}$ & $x+\overline{1}$ & $x$\\
$x$ & $x$ & $x+\overline{1}$ & $\overline{0}$ & $\overline{1}$\\
$x+\overline{1}$ & $x+\overline{1}$ & $x$ & $\overline{1}$ & $\overline{0}$\\
\end{tabular}}
\end{parbox}
\hskip 0.5cm
\begin{parbox}{0.45\linewidth}{\begin{tabular}{c|cccc}
$\cdot$ & $\overline{0}$ & $\overline{1}$ & $x$ & $x+\overline{1}$\\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$\\
$\overline{1}$ & $\overline{0}$ & $\overline{1}$ & $x$ & $x+\overline{1}$\\
$x$ & $\overline{0}$ & $x$ & $x+\overline{1}$ & $\overline{1}$\\
$x+\overline{1}$ & $\overline{0}$ & $x+\overline{1}$ & $\overline{1}$ & $x$\\
\end{tabular}}
\end{parbox}
\end{center}

\noindent\newline{\bf Problem.} Describe $\zmod{2}[x]\slash\cyc{x^3+x+1}$.

\noindent\newline{\bf Solution.} Let $F=\zmod{2}[x]\slash\cyc{x^3+x+\overline{1}}$ are of the form $\cyc{p(x)}+[A x^2+B x+C]$, so there are eight possible configurations of coefficients. Due to shortness of space, we can denote $\overline{A B C}:=\cyc{p(x)}+[A x^2+B x+C]$. The addition is then the same as in binary arithmetic, but the digits do not carry. The table for addition is:

\begin{center}
\begin{tabular}{c|cccccccc}
$+$ & $\overline{0}$ & $\overline{1}$ & $\overline{10}$ & $\overline{11}$ & $\overline{100}$ & $\overline{101}$ & $\overline{110}$ & $\overline{111}$\\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $\overline{10}$ & $\overline{11}$ & $\overline{100}$ & $\overline{101}$ & $\overline{110}$ & $\overline{111}$\\
$\overline{1}$ & $\overline{1}$ & $\overline{0}$ & $\overline{11}$ & $\overline{10}$ & $\overline{101}$ & $\overline{100}$ & $\overline{111}$ & $\overline{110}$\\
$\overline{10}$ & $\overline{10}$ & $\overline{11}$ & $\overline{0}$ & $\overline{1}$ & $\overline{110}$ & $\overline{111}$ & $\overline{100}$ & $\overline{101}$\\
$\overline{11}$ & $\overline{11}$ & $\overline{10}$ & $\overline{1}$ & $\overline{0}$ & $\overline{111}$ & $\overline{110}$ & $\overline{101}$ & $\overline{100}$\\
$\overline{100}$ & $\overline{100}$ & $\overline{101}$ & $\overline{110}$ & $\overline{111}$ & $\overline{0}$ & $\overline{1}$ & $\overline{10}$ & $\overline{11}$\\
$\overline{101}$ & $\overline{101}$ & $\overline{100}$ & $\overline{111}$ & $\overline{110}$ & $\overline{1}$ & $\overline{0}$ & $\overline{11}$ & $\overline{10}$\\
$\overline{110}$ & $\overline{110}$ & $\overline{111}$ & $\overline{100}$ & $\overline{101}$ & $\overline{10}$ & $\overline{11}$ & $\overline{0}$ & $\overline{1}$\\
$\overline{111}$ & $\overline{111}$ & $\overline{110}$ & $\overline{101}$ & $\overline{100}$ & $\overline{11}$ & $\overline{10}$ & $\overline{1}$ & $\overline{0}$\\
\end{tabular}
\end{center}

\noindent\newline The multiplication is somewhat cumbersome, although we essentialy multiply the elements and divide them with $x^3+x+\overline{1}$ and get the remainder. Thus we get (I did it with some help from the computer):

\begin{center}
\begin{tabular}{c|cccccccc}
$+$ & $\overline{0}$ & $\overline{1}$ & $\overline{10}$ & $\overline{11}$ & $\overline{100}$ & $\overline{101}$ & $\overline{110}$ & $\overline{111}$\\
\hline
$\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$\\
$\overline{1}$ & $\overline{0}$ & $\overline{1}$ & $\overline{10}$ & $\overline{11}$ & $\overline{100}$ & $\overline{101}$ & $\overline{110}$ & $\overline{111}$\\
$\overline{10}$ & $\overline{0}$ & $\overline{10}$ & $\overline{100}$ & $\overline{110}$ & $\overline{11}$ & $\overline{1}$ & $\overline{111}$ & $\overline{101}$\\
$\overline{11}$ & $\overline{0}$ & $\overline{11}$ & $\overline{110}$ & $\overline{101}$ & $\overline{111}$ & $\overline{100}$ & $\overline{1}$ & $\overline{10}$\\
$\overline{100}$ & $\overline{0}$ & $\overline{100}$ & $\overline{11}$ & $\overline{111}$ & $\overline{110}$ & $\overline{10}$ & $\overline{101}$ & $\overline{1}$\\
$\overline{101}$ & $\overline{0}$ & $\overline{101}$ & $\overline{1}$ & $\overline{100}$ & $\overline{10}$ & $\overline{111}$ & $\overline{11}$ & $\overline{110}$\\
$\overline{110}$ & $\overline{0}$ & $\overline{110}$ & $\overline{111}$ & $\overline{1}$ & $\overline{101}$ & $\overline{11}$ & $\overline{10}$ & $\overline{100}$\\
$\overline{111}$ & $\overline{0}$ & $\overline{111}$ & $\overline{101}$ & $\overline{10}$ & $\overline{1}$ & $\overline{110}$ & $\overline{100}$ & $\overline{11}$\\
\end{tabular}
\end{center}

\noindent\newline{\bf Problem.} Describe $\zmod{3}[x]\slash\cyc{x^3+x^2+\overline{2}}$.

\noindent\newline{\bf Solution.} In this exercise we have a field with $3^3$, i.e. twenty seven, elements. I will not draw out the addition and multiplication table.

\noindent\newline{\bf Proposition.} Let $F$ be any field and $c,d\in E-\{0\}$, where $E$ is an extension field of $F$. Then:

\begin{enumerate}
\item If $c$ is algebraic over $F$, so are $c+k$ and $k c$, for all $k\in F$;
\item If $c$ is algebraic over $F$, so is $c^{-1}$;
\item If $c d$ is algebraic over $F$, then $c$ is algebraic over $F(d)$;
\item If $c+d$ is algebraic over $F$, then $c$ is algebraic over $F(d)$;
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Assume that $c$ is algebraic over $F$. Then there exists $p(x)\in F[x]$ such that $p(c)=0$. As $k\in F$, then $p(x-k)\in F[x]$. Also, $p(c+k-k)=p(c)=0$, thus $c+k$ is algebraic over $F$, as it is the root of the polynomial $p(x-k)$. Also, as $k\in F$, then $k^{-1}\in F$ and we have $p(k^{-1}x)\in F[x]$. Also, $p(k^{-1}c k)=p(k^{-1} k c)=p(1c)=p(c)=0$, so $k c$ is algebraic over $F$. {\it Ad $2$.} As $c$ is algebraic over $F$, then there exists $p(x)\in F[x]$ such that $p(c)=0$. Assume that $p(x)=p_m x^m+\cdots+p_1 x+p_0$. Then, $p_m c^m+\cdots+p_1 c+p_0=0$. Multiplying that by $c^{-m}$ gives us $p_m+\cdots+p_1 c^{1-m}+p_0 c^{-m}=0$, and terms are of the form $p_i c^{i-m}=p_i\left(c^{-1}\right)^{m-i}$, for all $i\in\{1,\ldots,m\}$. Thus, we get that $p_m+p_{m-1}c^{-1}+\cdots+p_2\left(c^{-1}\right)^{m-2}+p_1\left(c^{-1}\right)^{m-1}+p_0\left(c^{-1}\right)^m=0$. Taking $\overline{p}(x)=p_0 x^m+\cdots+p_{m-1} x+p_m$ it is easy to see that $\overline{p}(c^{-1})=0$, and, as $\overline{p}(x)\in F[x]$, $c^{-1}$ is algebraic over $F$. {\it Ad $3$.} Assume that $c d$ is algebraic over $F$. Then there exists $p(x)\in F[x]$ such that $p(c d)=0$. Assume that $p(x)=p_m x^m+\cdots+p_1 x+p_0$. Then, $p(c d)=p_m (c d)^m+\cdots+p_1(c d)+p_0=(p_m d^m) c^m+\cdots+(p_1 d) c+p_0=0$. As $p_i d^i\in F(d)$, then $p(x d)\in F(d)[x]$ and $p(c d)=0$, which implies $c$ is algebraic over $F(d)$. {\it Ad $4$.} Assume that $c+d$ is algebraic over $F$. Then, there exists $p(x)\in F[x]$ such that $p(c+d)=0$. It is easy to see that $p(x+d)\in F(d)$ and that, because $p(c+d)=0$, we have that $c$ is algebraic over $F(d)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field and $K$ an extension of $F$. Let $a\in K$. Then, the minimal polynomial of $a$ over $F$ is of degree $1$ if and only if $a\in F$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume that the minimal polynomial of $a$ over $F$ is of degree $1$. Let that polynomial be $p(x)=x+p_0$ (it has to be monic). Then, $p(a)=0$, i.e. $a+p_0=0$. Then, $a=-p_0$. As $p_0\in F$, by definition, then also $a=-p_0\in F$. {\it Sufficiency.} Let $a\in F$. Then, we will show that $p(x)=x-a$ is a minimal polynomial of $a$ over $F$. We have $p(a)=a-a=0$, and $p(x)\in F$ because $a\in F$. Also, $p(x)$ is monic. As $\deg{p(x)}=1$, there can be no polynomial of a lesser degree whose root is $a$. Therefore $x-a\in F[x]$ is the minimal polynomial of $a$ over $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Name a field (not $\R$ or $\C$) which contains a root of $x^5+2x^3+4x^2+6$.

\noindent\newline{\bf Solution.} Let $p(x)=x^5+2x^3+4x^2+6$. We have $2\nmid 1$, $2|0$, $2|4$, $2|6$ and $2^2=4\nmid 6$, so by Eisenstein's criterion $p(x)$ is irreducible over $\Q$. Thus, $\Q\slash\cyc{x^5+2x^3+4x^2+6}$ is a field and $\cyc{p(x)}+x\in F[x]$ is the root of $\overline{p}(x)$. We have $p(\cyc{p}+x)=\cyc{p(x)}$ as in the basic theorem for field extensions.

\noindent\newline{\bf Problem.} Prove that $\Q(1+i)\cong\Q(1-i)$ and $\Q\left(\sqrt{2}\right)\ncong\Q\left(\sqrt{3}\right)$.

\noindent\newline{\bf Solution.} The minimal polynomial of $1+i$ over $\Q$ is obtained by setting $x=1+i$. Then, $x-1=i$ and $x^2-2x+1=-1$, which gives us $p(x)=x^2-2x+2$. Similarly, $x=1-i$ gives us $x-1=-i$ and $x^2-2x+1=-1$. That means that $q(x)=x^2-2x+2$ is the minimal polynomial of $1-i$ over $\Q$. Thus, $\Q(1+i)\cong\Q\slash\cyc{x^2-2x+2}\cong\Q(1-i)$, which gives us $\Q(1+i)=\Q(1-i)$.

Assume that there exists an isomorphism $\phi:\Q\left(\sqrt{2}\right)\rightarrow\Q\left(\sqrt{3}\right)$. For all $x\in\dom{\phi}$, we have $\phi\left(x^2\right)=\phi(x x)=\phi(x)\phi(x)=\left(\phi(x)\right)^2$. Thus, if some $x\in\dom{\phi}$ is a square then it must be also that $\phi(x)$ is a square in $\cod{\phi}$. Take $x=\sqrt{2}$. We have that $x^2=2$ in $\Q\left(\sqrt{2}\right)$. So, it also must be that $\phi(x^2)=\phi(2)$, i.e. $\phi(x)^2=\phi(2)$. Note that $\phi(2)=\phi(1+1)=\phi(1)+\phi(1)$, and as $1$ is a unit, then it must be $\phi(1)=1$, and so $\phi(2)=1+1=2$. Therefore there must exist some $y\in\Q\left(\sqrt{3}\right)$ such that $y^2=2$. But, as $y\in\Q\left(\sqrt{3}\right)$, it is of the form $y=a+b\sqrt{3}$ and we have $y^2=a^2+2a b\sqrt{3}+3 b^2$. So, as $y^2=2$, we have $a^2+2 a b\sqrt{3}+3 b^2=2$, i.e. $(a^2+3 b^2-2)+(2 a b)\sqrt{3}=0$. That is equivalent to $(2 a b)\sqrt{3}=2-a^2-3b^2$. Assume $a=0$. Then, $0=2-3b^2$, i.e. $\frac{2}{3}=b^2$. This is impossible because there does not exist such rational $b$ (we can check through the divisors of the free term for rational solutions; we will find none). Assume $b=0$. Then, $0=2-a^2$, i.e. $2=a^2$. This is again impossible as there does not exist a rational number which, squared, yields $2$ (elementary proof). Assume $a\neq 0$ and $b\neq 0$. Then we can divide $(2 a b)\sqrt{3}=2-a^2-3b^2$ with $2 a b$ and get $\sqrt{3}=\frac{2-a^2-3b^2}{2 a b}$, which would imply, as the expression on the right-hand side is rational, that $\sqrt{3}\in\Q$, which is a contradiction. Therefore, our assumption that there exists an isomorphism $\phi$ from $\Q\left(\sqrt{2}\right)$ to $\Q\left(\sqrt{3}\right)$ has brought about a contradiction and it must be that $\Q\left(\sqrt{2}\right)\ncong\Q\left(\sqrt{3}\right)$.

\noindent\newline{\bf Proposition.} Let $F$ be a field. If $p(x)\in F[x]$ is irreducible and $\deg{p(x)}=2$, then $F[x]\slash\cyc{p(x)}$ contains both roots of $p(x)$.

\noindent\newline{\bf Proof.} Assume that $F$ is a field and that $p(x)=x^2+a x+b\in F[x]$ is irreducible. Then, $F[x]\slash\cyc{p(x)}\cong F(c)$, where $c^2+a c+b=0$. Multiplying that equality with $c^{-2}$ (which exists in $F(c)$ as $F(c)$ is a field) gives us $1+a c^{-1}+b c^{-2}=0$. Multiplying with $b$ gives us $b+a (b c^{-1})+\left(b^2\left(c^{-1}\right)^2\right)=0$, i.e. $b+a (b c^{-1})+\left(b c^{-1}\right)^2=0$. It is obvious that $b c^{-1}$ is another root of $p(x)$. Another way of proving that is taking $p(-a-c)=(a^2+2 a c+c^2)+(-a^2-a c)+b=c^2+a c+b=0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Let $F$ and $E$ be fields. Recall the definition of $F(a)$. It is a field such that $F\subseteq F(a)$, $a\in F(a)$ and, $F\subseteq E$ and $a\in E$ implies $F(a)\subseteq E$. Similarly, $F(a,b)$ is a field such that $F\subseteq F(a,b)$, $a,b\in F$ and, $F\subseteq E$ and $a,b\in E$ implies $F(a,b)\subseteq E$.

\noindent\newline{\bf Proposition.} Let $F$ be a field, $k\in F-\{0\}$ and $c,d\in E-\{0\}$, where $E$ is some extension field of $F$. Then:

\begin{enumerate}
\item $F(c)=F(c+k)$;
\item $F(c)=F(k c)$;
\item $F(c^2)\subseteq F(c)$ and the converse is not necessarily true;
\item $F(c+d)\subseteq F(c,d)$ and the converse is not necessarily true.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} As $k\in F$, then $c+k\in F(c)$. So, we have $c+k\in F(c)$ and $F\subseteq F(c)$, which implies $F(c+k)\subseteq F(c)$. Similarly, $c\in F(c+k)$ (we can take $c=(c+k)-k$, due to $k\in F$) and $F\subseteq F(c+k)$, which implies $F(c+k)\subseteq F(c)$. From that follows $F(c)=F(c+k)$. {\it Ad $2$.} Again, as $k\in F$, we have $k c\in F(c)$. Also, $F\subseteq F(c)$ by definition of $F(c)$, but that implies that $F(k c)\subseteq F(c)$. Also, $c\in F(k c)$ (because $c=k^{-1}(k c)$; this works because $k\in F$) and $F\subseteq F(k c)$, so $F(c)\subseteq F(k c)$ and $F(c)=F(k c)$. {\it Ad $3$.} We have $c^2\in F(c)$ (because obviously $c^2=c^2+0c+0$) and $F\subseteq F(c)$, finally implies $F(c^2)\subseteq F(c)$. The converse is not necessarily true, a counterexample would be $\Q=\Q(2)\subseteq\Q(\sqrt{2})$, but $\sqrt{2}\notin\Q$. {\it Ad $4$.} Notice that $F(c,d)$ is a field containing $c$ and $d$ so it also must contain their sum, i.e. $c+d\in F(c,d)$. Also, $F\subseteq F(c,d)$, by definition, which with former expression gives us $F(c+d)\subseteq F(c,d)$. The converse is not necessarily true because it is not necessary that $c,d\in F(c+d)$; a good counterexample is $\Q(\sqrt{2}+\sqrt{3})\subseteq\Q(\sqrt{2},\sqrt{3})$, but obviously $\sqrt{2}\notin\Q(\sqrt{2}+\sqrt{3})$. We would have $a_1(\sqrt{2}+\sqrt{3})+a_0=\sqrt{2}$, and from that $\sqrt{2}(a_1-1)+a_1\sqrt{3}=-a_0$, but $-a_0\in\Q$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field, $k\in F$, $c\in E$, where $E$ is some extension of $F$, let $p(x)\in F[x]$. Then,

\begin{enumerate}
\item $c+k$ is a root of $p(x)$ if and only if $c$ is a root of $p(x+k)$;
\item $k c$ is a root of $p(x)$ if and only if $c$ is a root of $p(k x)$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} {\it Necessity.} Assume $c+k$ is a root of $p(x)$. Then, $p(c+k)=0$. From this it is evident that $c$ is a root of $p(x+k)$, because $p(c+k)=0$. {\it Sufficiency.} Assume $c$ is a root of $p(x+k)$. Then, $p(c+k)=0$, which is the same as saying $c+k$ is a root of $p(x)$. {\it Ad $2$.} {\it Necessity.} Assume $k c$ is a root of $p(x)$. Then, $p(k c)=0$, which means that $c$ is a root of $p(k x)$. {\it Sufficiency.} Let $c$ be a root of $p(k x)$. Then, $p(k c)=0$, meaning that $k c$ is a root of $p(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field, $k\in F$ and let $p(x)\in F[x]$ be an irreducible polynomial. Then:

\begin{enumerate}
\item $F[x]\slash\cyc{p(x+k)}\cong F[x]\slash\cyc{p(x)}$;
\item $F[x]\slash\cyc{p(k x)}\cong F[x]\slash\cyc{p(x)}$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $c\in E$ be a root of $p(x+k)$, where $E$ is an extension of $F$. Then, $c+k$ is a root of $p(x)$. From the previuos proposition we have that $F[x]\slash\cyc{p(x+k)}\cong F(c)$ and $F[x]\slash\cyc{p(x)}\cong F(c+k)$. From the previous proposition, as $k\in F$, we have $F(c+k)=F(c)$, so $F[x]\slash\cyc{p(x+k)}\cong F(c)=F(c+k)\cong F[x]\slash\cyc{p(x)}$. {\it Ad $2$.} Let $E$ be an extension of $F$ and $c\in E$ a root of $p(k x)$. Then, by previous proposition, $k c$ is a root of $p(x)$. Thus, by previous proposition, $F[x]\slash\cyc{p(k x)}\cong F(c)$ and $F[x]\slash\cyc{p(x)}\cong F(k c)$. By previous proposition, $F(c)=F(k c)$, so $F[x]\slash\cyc{p(x)}\cong F(k c)=F(c)\cong F[x]\slash\cyc{p(k x)}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove that $\zmod{11}[x]\slash\cyc{x^2+\overline{1}}\cong\zmod{11}[x]\cyc{x^2+x+\overline{4}}$.

\noindent\newline{\bf Solution.} Let us observe $p(x+\overline{k})=(x+\overline{k})^2+\overline{1}$. From that we have $p(x+\overline{k})=x^2+\overline{2k}x+\overline{k^2+1}$. If we compare that with $x^2+x+\overline{4}$, we get $\overline{2 k}=\overline{1}$ and $\overline{k^2+1}=\overline{4}$. From former equality we get, as $\overline{2}\cdot\overline{6}=\overline{12}=\overline{1}$, that $\overline{k}=\overline{6}$; substituting that in latter equality we have $\overline{36+1}=\overline{4}$, i.e. $\overline{37}=\overline{4}$, which is correct, due to the fact that $37\equiv 4\pmod 11$. So, if $p(x)=x^2+\overline{1}$, then $p(x+\overline{6})=x^2+x+\overline{4}$, and we have $\zmod{11}[x]\slash\cyc{p(x)}\cong\zmod{11}[x]\slash\cyc{p(x+\overline{6})}$, from the previous proposition.

\noindent\newline{\bf Problem.} If $a$ is a root of $x^2-2$ and $b$ is a root of $x^2-4x+2$, then $\Q(a)\cong\Q(b)$.

\noindent\newline{\bf Solution.} Let us take $(x+k)^2-2=x^2+(2k)x+(k^2-2)$. Then, $2 k=-4$ gives us $k=-2$. Substituting that into $k^2-2$ gives us $4-2=2$. So, $(x-2)^2-2=x^2-4x+2$. From the previous proposition, as $-2\in\Q$ and as $a$ is a root of $x^2-2$, we have $\Q\slash\cyc{x^2-2}\cong\Q(a)$. Also, as $b$ is a root of $(x-2)^2-2$, we have $\Q\slash\cyc{x^2-4x+2}\cong\Q(b)$. Then, by the previous proposition, $\Q(a)=\Q\slash\cyc{x^2-2}\cong\Q\slash\cyc{x^2-4x+2}=\Q(b)$.

\noindent\newline{\bf Problem.} If $a$ is a root of $x^2-2$ and $b$ is a root of $x^2-\frac{1}{2}$, then $\Q(a)\cong\Q(b)$.

\noindent\newline{\bf Solution.} Let $a$ be a root of $p(x)=x^2-2$. Then, as $p\left(2x\right)=4x^2-2=4\left(x^2-\frac{1}{2}\right)$, $b$ is a root of $p(2x)$. By previous proposition, as $2\in\Q$, we have $\Q(a)=\Q\slash\cyc{p(x)}\cong\Q\slash\cyc{p(2x)}=\Q(b)$.

\noindent\newline{\bf Definition.} If the minimal polynomial of $c$ over $F$ has degree $2$, we say that $F(c)$ is a {\bf quadratic extension} of $F$. Also, if $b^2=a$ for some $a,b\in F$, we say that $b$ is the square root of $a$ and write $b=\sqrt{a}$. If, for some $a\in F$, there does not exist $b\in F$ such that $b^2=a$, then we say that $a$ is a non-square in $F$.

\noindent\newline{\bf Proposition.} If $F$ is a field with $\rchar{F}\neq 2$, any quadratic extension of $F$ is of the form $F\left(\sqrt{a}\right)$, for some $a\in F$.

\noindent\newline{\bf Proof.} Let $q(x)$ be the minimal polynomial of $c$ over $F$. Then, $F(c)\cong F[x]\slash\cyc{q(x)}$ and we have $q(c)=0$. But, we have that $\deg{q(x)}=2$, so $q(x)=x^2+a_1 x+a_0$, where $a_1,a_0\in F$. Then, $0=q(c)=c^2+a_1 c+a_0$. Let $\rchar{F}=p$, $p\neq 2$. From that we have $q(x+k)=(x+k)^2+a_1(x+k)+a_0$ and $q(x+k)=x^2+2kx+k^2+a_1 x+a_1 k+a_0=x^2+x(2k+a_1)+(k^2+a_1 k+a_0)$. Let $k=\left(\frac{p-1}{2}\right)\cdot(-a_1)$. That will work because $p\neq 2$, so $\frac{p-1}{2}\in\Z$. Then, $q\left(x+\left(\frac{p-1}{2}\right)a_1\right)=x^2+x(2(\frac{p-1}{2})a_1+a_1)+\left(\left(\frac{p-1}{2}\right) a_1\right)^2+a_1\left(\left(\frac{p-1}{2}\right) a_1\right)+a_0$. Let $-a=\left(\left(\frac{p-1}{2}\right) a_1\right)^2+a_1\left(\left(\frac{p-1}{2}\right) a_1\right)+a_0$. Then, $q\left(x+\left(\frac{p-1}{2}\right)a_1\right)=x^2+x((p-1)a_1+a_1)-a$. Notice that $(p-1)a_1+a_1=p a_1=0$, because $\rchar{F}=p$. Then, $q\left(x+\left(\frac{p-1}{2}\right)a_1\right)=x^2-a$. Let $r(x)=x^2-a$. As $\left(\frac{p-1}{2}\right)\in F$, then, by a previous proposition, $F[x]\slash\cyc{r(x)}\cong F[x]\slash\cyc{q(x)}$ and we have, because $\sqrt{a}^2-a=a-a=0$, we have $F[x]\slash\cyc{x^2+a}\cong F\left(\sqrt{a}\right)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a finite field with $\rchar{F}\neq 2$ and let $F^{\ast}$ be a multiplicative group of $F$. Let $a,b\in F$. If $a$ and $b$ are non-squares in $F$, then $a b^{-1}$ is a square in $F$.

\noindent\newline{\bf Proof.} Let $S=\{x^2:\ x\in F^{\ast}\}$. It is obvious that $S\subseteq F^{\ast}$, as $F^{\ast}$ is a group and therefore closed with respect to multiplication (and also squaring). Then, if $x^2,y^2\in S$, we have, $x^2 y^2=(x y)^2$. That is true because $F$ is a field, which implies that $F^{\ast}$ is Abelian. Thus, $S\leq F^{\ast}$. As $F$ is a field, if $x\in F$, then $-x\in F$. That is also true for $F^{\ast}$ (which actually contains all elements of $F$, but without zero, as $F$ is a field). Then, either $x\neq -x$ or $x=-x$. Can it be the latter case? Assume $x=-x$, i.e. $x+x=0$. That would imply $2x=0$, that is, $\rchar{F}=2$, which is a contradiction. So we construct $S(x_i)$ inductively. Take $x_1\in F^{\ast}$. Then, $S(x_1)=\{x_1,-x_1\}$ (from that, as $x_1\neq -x_1$, $|S(x_1)|=2$). Then, we construct $S(x_2)$ by taking $x_2\in F^{\ast}-S(x_1)$ and letting $S(x_2)=\{x_2,-x_2\}$. Thus we continue by taking $x_{i+1}\in F^{\ast}-\left(S(x_1)\cup\cdots\cup S(x_i)\right)$ with $S(x_{i+1})=\{x_{i+1},-x_{i+1}\}$. As $F^{\ast}$ is finite, the construction ends with partition $S(x_i)\cap S(x_j)=\emptyset$, if $i\neq j$ and $F^{\ast}=S(x_1)\cup\cdots\cup S(x_m)$ for some $m\in\Z^{+}$. That implies $F^{\ast}=2\cdots m$. Now, observe that $(S(x_i))^2\subseteq S$ and we have $S(x_i)=\{x_i^2\}$, as $x_i^2=(-x_i)^2$. Can it be that $(S(x_i))^2=(S(x_j))^2$ for some $(S(x_i))\neq(S(x_j))$? That would imply $x_i^2=x_j^2$, i.e. $x_i^2-x_j^2=0$. Then, $(x_i-x_j)(x_i+x_j)=0$. As $F^{\ast}$ is also an integral domain, either $x_i-x_j=0$ or $x_i+x_j=0$. From the former condition we get $x_i=x_j$, and from the latter $x_i=-x_j$, both a contradiction to $(S(x_i))\neq(S(x_j))$. Thus, $|S|=(S(x_1))^2+\cdots+(S(x_m))^2=m$. Therefore, $[F^{\ast}:S]=\frac{|F^{\ast}|}{|S|}=\frac{2m}{m}=2$. Take $a,b\in F^{\ast}$ that are not squares, i.e. $a,b\in F^{\ast}-S$. Then assume $S a=S$. That would imply $a\in S$, a contradiction. Same goes for $b$. Thus, we have $S a\neq S$ and $S b\neq S$. As the index of $S$ in $F^{\ast}$ is $2$, $S a\neq S b$ would imply that it is at least $3$. So, it must be $S a=S b$, and from that we have $a b^{-1}\in S$. In other words, $a b^{-1}$ is a square.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Any two quadratic extensions of a finite field are isomorphic.

\noindent\newline{\bf Proof.} Let $F\left(\sqrt{a}\right)$ and $F\left(\sqrt{b}\right)$ be quadratic extensions of field $F$. Then, $F\left(\sqrt{a}\right)\cong F\slash\cyc{x^2-a}$ and $F\left(\sqrt{b}\right)\cong F\slash\cyc{x^2-b}$. Let $p(x)=x^2-a$ and $q(x)=x^2-b$. We want $p(k\sqrt{b})=0$. Thus, $p(k\sqrt{b})=(k\sqrt{b})^2-a=k^2 b-a$. Then, $k^2 b-a=0$ implies $k^2 b=a$, i.e. $k^2=a b^{-1}$. Then, as $a b^{-1}$ is a square (followed by $a,b$ non-squares), we can denote $k=\sqrt{a b^{-1}}$. Therefore, $\sqrt{a b^{-1}}\in F^{\ast}$ and $p(\sqrt{a b^{-1}}x)$ is defined so that $p(\sqrt{a b^{-1}}\sqrt{b})=0$. Also, by a previous proposition, we have $F\left(\sqrt{a}\right)\cong F\slash\cyc{p(x)}\cong F\slash\cyc{p(\sqrt{a b^{-1}}x)}\cong F\left(\sqrt{b}\right)$. Therefore, $F\left(\sqrt{a}\right)\cong F\left(\sqrt{b}\right)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} Notice that the only non-squares in $\R$ are in $\R^{-}=\{-x\in\R:\ x>0\}$. Thus, if $x,y\in\R^{-}$, then $\frac{x}{y}>0$ and it is a square. Thus, any two simple extensions of $\R$ are isomorphic (and isomorphic to $\C$). That can be seen from the fact that the proof from above, although for finite fields, actually depends on the fact that $a b^{-1}$ is a square if $a$ and $b$ are non squares. The different notation here is that $a b^{-1}$ is $\frac{a}{b}$.

\noindent\newline{\bf Proposition.} Let $F\trianglelefteq E$, $\rchar{F}\neq 2$, and let $a,b\in E$. Let $p(x)$ be minimal polynomial of $a$, and of $b$, over $F$ such that $\deg{p(x)}=2$. Then, $F(a)=F(b)$.

\noindent\newline{\bf Proof.} If $a=b$, we are done. Assume $a\neq b$ and $p(x)=x^2+k x+l$ for some $k,l\in F$. Then, $p(a)=0=p(b)$, i.e. $a^2+k a+l=b^2+k b+l$. From this we get $a^2-b^2+k a-k b=0$. This is equivalent to $(a-b)(a+b)+k(a-b)=0$, or $(a-b)(a+b+k)=0$. As $a\neq b$, then $a-b\neq 0$, so it must be $a+b+k=0$, i.e. $a=-(b+k)$. Let us examine $F(b)$. As $k\in F\subseteq F(b)$, then $a=-(b+k)\in F(b)$. As $F(b)$ contains $a$ and $F$, and as $F(a)$ is the smallest field containing $a$ and $F$, it has to be $F(a)\subseteq F(b)$. In the same way, from $a=-(b+k)$ we can get $-(a+k)=b$, we get $b\in F(a)$, meaning $F(b)\subseteq F(a)$. That gives us $F(a)=F(b)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field, and let $c$ be transcendental over $F$. Then:

\begin{enumerate}
\item $\{a(c):\ a(x)\in F[x]\}$ is an integral domain isomorphic to $F[x]$;
\item $F(c)$ is the field of quotients\footnote{Not quotient field! Field of quotients $Q$ for this problem would look like:
\begin{equation*}
Q=\left\{\frac{a(c)}{b(c)}:\ \left(a(x)\in F[x]\right)\wedge\left(b(x)\neq 0\right)\right\}.
\end{equation*}} of $\{a(c):\ a(x)\in F[x]\}$, and is isomorphic to $F(x)$, the field of quotients of $F[x]$;
\item $c+k$, $c^m$ and $k c$, where $k\in F$, $m\in\Z^{+}$, are transcendental over $F$;
\item Every element in $F(c)-F$ is transcendental over $F$.
\end{enumerate}

\noindent{\bf Proof.} Let $A=\{a(c):\ a(x)\in F[x]\}$. {\it Ad $1$.} Let $f:A\rightarrow F[x]$ be a mapping defined with $f(a^m c^m+\cdots+a_1 c+a_0)=a^m x^m+\cdots+a_1 x+a_0$. First, we will show that $f$ is a well-defined function. It is obvious that for any $a(c)\in A$ there exists $a(x)\in F[x]$. Now, assume that $a^m c^m+\cdots+a_1 c+a_0=b^n c^n+\cdots+b_1 c+b_0$. Assume that $m=n$, allowing some coefficients to be equal to zero. Then, $c^m(a_m-b_m)+\cdots+c(a_1-b_1)+(a_0-b_0)=0$. If it were that all $a_i$ and $b_i$ are not equal, $c$ would be root of the polynomial $x^m(a_m-b_m)+\cdots+x(a_1-b_1)+(a_0-b_0)$, which would contradict the fact that $c$ is transcendental over $F$. Thus, it must be that $a_i=b_i$ and $m=n$ (so this would be a zero polynomial, excluded from definition of what it means for an element to be algebraic). Now, the surjectivity and injectivity are rather straightforward. Same thing for $f(a(c)b(c))=a(x)b(x)$ and $f(a(c)+b(c))=a(x)+b(x)$. {\it Ad $2$.} Let $Q$ be the field of quotients. Then it is obvious that $c\in Q$ and $F\subseteq Q$. But, $F(c)$ is the smallest field containing $F$ and $c$, so it must be $F(c)\subseteq Q$. As every element in $Q$ is of the form $\frac{a(c)}{b(c)}$. But, as $F$ and $c$ are in $F(c)$, then $a(c)$ and $b(c)$ are in $F(c)$. Also\footnote{I will elaborate on this more in the future; for now I consider it to be rather intuitive.}, $[b(c)]^{-1}$ is in $F(c)$ and we have $a(c)[b(c)]^{-1}\in F(c)$, implying $Q\subseteq F(c)$. Therefore, we have $F(c)=Q$. From the previous problem we have that $A=\{a(c):\ a(x)\in F[x]\}$ is isomorphic to $F[x]$, i.e. $A\cong F[x]$. As these integral domains are isomorphic, then so are their fields of quotients. {\it Ad $3$.} Let $c$ be transcendental over $F$ and let $k\in F$. Assume that $c+k$ is algebraic over $F$. Then there exists $p(x)\in F[x]$ such that $p(c+k)=0$. By a previos proposition, $c+k$ is root of $p(x)$ if and only if $c$ is root of $p(x+k)$. As $p(x+k)\in F[x]$, due to $k\in F$, this is a contradiction to the assumption that $c$ is transcendental over $F$. Similarly, if $p(k c)=0$, then $c$ is root of $p(k x)\in F[x]$, and that cannot be as $c$ is transcendental over $F$. Finally, assume $c^m$ is algebraic over $F$. Then, for some $p(x)\in F[x]$, we have $p(c^m)=0$, i.e. $p_n \left(c^m\right)^n+\cdots+p_1 c^m+p_0=0$. But, that is equivalent to $p_n c^{m n}+\cdots+p_1 c^m+p_0=0$, which implies that $c$ is root of $p(x^m)\in F[x]$, again a contradiction to assumption that $c$ is transcendental over $F$. {\it Ad $4$.} Assume that $t\in F(c)-F$ is algebraic over $F$. Then, there exists $p(x)\in F[x]$ such that $p(t)=0$. As $t=\frac{a(c)}{b(c)}$, then $p(t)$ is of the form $p(t)=p_m t^m+\cdots+p_1 t+p_0=0$. Multiplying everything by $b(c)^m$ gives us $0=p_m [a(c)]^m+p_{m-1} [a(c)]^{m-1} b(c)+\cdots+p_1 a(c)[b(c)]^{m-1}+p_0[b(c)]^m$. That would imply that all $[a(c)]^k[b(c)]^{m-k}\in F$. But that would mean that $a(c)$ and $b(c)$ cannot contain $c$ (due to previous proposition), and that they have to be in $F$. Then also $t\in F$, which is contrary to our assumption.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field and $a(x),b(x)\in F[x]$. Then, if $a(x)$ and $b(x)$ have a common root $c$ in some extension of $F$, they have a common factor of positive degree in $F[x]$.

\noindent\newline{\bf Proof.} Let $E$ be an extension of $F$ where $c\in E$. Then, we observe $\sigma_c:F[x]\rightarrow E$. As $a(c)=b(c)=0$, then $a(x),b(x)\in\ker{\sigma_c}=\cyc{p(x)}$, for some $p(x)\in F[x]$. Thus, $a(x)=p(x)r(x)$ and $b(x)=p(x)s(x)$. Their common factor is $p(x)$, which is obviously of positive degree. Assume that $\deg{p(x)}=0$. Then, $p(x)=a$, for some $a\in F$. Therefore, $\cyc{a}=\{a q(x):\ q(x)\in F[x]\}$ and we have $\cyc{a}=F[x]$. Thus, $\sigma_c(a(x))=0$, for all $a(x)$. So, also $\sigma_c(x)=0$, i.e. $c=0$, which is impossible as that would mean $c\in F$, not in an extension of $F$. Therefore, $\deg{p(x)}>0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field and $a(x),b(x)\in F[x]$. Let $K$ be any extension of $F$. Then, $\gcd{(a(x),b(x))}=1$ in $F[x]$ if and only if $\gcd{(a(x),b(x))}=1$ in $K[x]$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $\gcd{(a(x),b(x))}=1$ in $F[x]$. Then by Bezout's lemma, $a(x)p(x)+b(x)q(x)=1$, for some $p(x),q(x)\in F[x]$. But, as $p(x),q(x)\in F[x]$, i.e. their coefficients are in $F\leq K$, then they are also in $K$, so $p(x),q(x)\in K[x]$. Thus we have $\gcd{(a(x),b(x))}=1$, by Bezout's lemma. {\it Sufficiency.} Assume that $\gcd{(a(x),b(x))}=1$ in $K[x]$. Then there exist $p(x),q(x)\in K[x]$ such that $a(x)p(x)+b(x)q(x)=1$. Assume that $\gcd{(a(x),b(x))}=g(x)$ in $F[x]$. Then, there exist $r(x),s(x)\in F[x]$ such that $a(x)=g(x)r(x)$ and $b(x)=g(x)s(x)$. But, then also $g(x),r(x),s(x)\in K[x]$, as $F[x]\subseteq K[x]$. So, from Bezout's equality we get $g(x)r(x)p(x)+g(x)s(x)q(x)=1$. That is equivalent to $g(x)[r(x)p(x)+s(x)q(x)]=1$ and we have $g(x)|1$. Thus, $a(x)$ and $b(x)$ are also relatively prime in $F[x]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $a(x)=a_m x^m+\cdots+a_1 x+a_0\in F[x]$, where $F$ is a field. The {\bf derivative} of $a(x)$ is defined as $a'(x)=m a_m x^{m-1}+\cdots+2a_2 x+a_1$.

\noindent\newline{\bf Proposition.} Let $F$ be a field, $\lambda,\mu\in F$ and $a(x),b(x)\in F[x]$. Then:

\begin{enumerate}
\item $[\lambda a(x)+\mu b(x)]'=\lambda a'(x)+\mu b'(x)$;
\item $[a(x)b(x)]'=a'(x)b(x)+a(x)b'(x)$.
\end{enumerate}

\noindent{\bf Proof.} Let $a(x)=a_m x^m+\cdots+a_1 x+a_0$ and $b(x)=b_m x^m+\cdots+b_1 x+b_0$ (allowing some $b_i$ to equal zero so to not force degree equality). {\it Ad $1$.} Then, $[a(x)+b(x)]'=[(a_m+b_m)x^m+\cdots+(a_1+b_1)x+(a_0+b_0)]'=m(a_m+b_m)x^{m-1}+\cdots+(a_1+b_1)=(m a_m x^{m-1}+\cdots+a_1)+(m b_m x^{m-1}+\cdots+b_1)=a'(x)+b'(x)$. Also, $[\lambda a(x)]'=[\lambda a_m x^m+\cdots+\lambda a_1 x+\lambda a_0]'=\lambda m a_m x^{m-1}+\cdots+\lambda a_1=\lambda(m a_m x^{m-1}+\cdots+a_1)=\lambda a'(x)$. Therefore, $[\lambda a(x)+\mu b(x)]'=[\lambda a(x)]'+[\mu b(x)]'=\lambda a'(x)+\mu b'(x)$. {\it Ad $2$.} Assume $a(x)=a_{m+1} x^{m+1}+A(x)$, when necessary. First, we will prove $[\lambda x^k a(x)]'=k \lambda x^{k-1} a(x)+\lambda x^k a'(x)$. Assume the statement is true for $\deg{a(x)}=0$. Then, $a(x)=a_0$ and the result directly follows from previos problem. Then, assume the statement is true when $\deg{a(x)}=m$. Then, $[\lambda x^k a_{m+1} x^{m+1}+\lambda x^k A(x)]'=[(\lambda a_{m+1})x^{k+m+1}]'+[\lambda x^k A(x)]'=(k+m+1)(\lambda a_{m+1})x^{k+m}+k \lambda x^{k} A'(x)+k \lambda x^{k-1} A(x)=k(\lambda a_{m+1})x^{k+m}+(m+1)(\lambda a_{m+1})x^{k+m}+\lambda x^k A'(x)+k \lambda x^{k-1} A(x)=\lambda k x^{k-1}[a_{m+1} x^{m+1}+A(x)]+x^k\lambda[(m+1)a_{m+1}x^m+A'(x)]=\lambda k x^{k-1} a(x)+x^k\lambda a'(x)$. This proves the first part needed. Then, note that $[a_0 b_0]'=0=0+0=a_0 0+b_0 0=a_0[b_0]'+[a_0]'b_0$. Assume $\deg{a(x)}\geq\deg{b(x)}$ and assume the statement is true for all polynomials with degree less than or equal to $m$. Then, $[a_{m+1} x^{m+1} b(x)+A(x)b(x)]'=[a_{m+1} x^{m+1} b(x)]+[A(x)b(x)]'=(m+1)a_{m+1}x^m b(x)+a_{m+1} x^{m+1} b'(x)+A(x)b'(x)+A'(x)b(x)=b(x)[(m+1)a_{m+1}x^m+A'(x)]+b'(x)[a_{m+1}x^{m+1}+A(x)]=b(x) a'(x)+b'(x) a(x)$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a field such that $\rchar{F}=0$. Let $a(x)\in F[x]$ such that $a'(x)=0$. Then, $a(x)$ is a constant polynomial. If $\rchar{F}\neq 0$, then this is not necessarily true.

\noindent\newline{\bf Proof.} Let $a(x)\in F[x]$ and $a'(x)=0$. Assume that $a(x)$ is a non constant polynomial, i.e. $a(x)=a_m x^m+\cdots a_1 x+a_0$. Then, $a'(x)=m a_m x^{m-1}+\cdots+a_1=0$. That would imply that $i a_i=0$, for all $i\in\{1,\ldots,m\}$. But, that is impossible because $\rchar{F}=0$ and $a(x)$ must be a constant polynomial. If $\rchar{F}\neq 0$, for example in $\zmod{5}$, we have $[x^5+\overline{1}]'=(5\cdot\overline{1})x^4=\overline{5}x^4=\overline{0}x^4=\overline{0}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Find the derivatives of the following polynomials in $\zmod{5}[x]$: (a) $x^6+\overline{2}x^3+x+\overline{1}$; (b) $x^5+\overline{3}x^2+\overline{1}$; (c) $x^{15}+\overline{3}x^{10}+\overline{4}x^5+\overline{1}$.

\noindent\newline{\bf Solution.} (a) We have $[x^6+\overline{2}x^3+x+\overline{1}]'=(6\cdot\overline{1})x^5+(3\cdot\overline{2})x^2+(1\cdot\overline{1})=\overline{6}x^5+\overline{6}x^2+\overline{1}=x^5+x^2+\overline{1}$. (b) Following in the similar fashion, we may conclude that $[x^5+\overline{3}x^2+\overline{1}]'=\overline{5}x^5+\overline{6}x=\overline{0}+x=x$. (c) Same as before, $[x^{15}+\overline{3}x^{10}+\overline{4}x^5+\overline{1}]'=\overline{15}x^{14}+\overline{30}x^9+\overline{20}x^4=\overline{0}+\overline{0}+\overline{0}=\overline{0}$.

\noindent\newline{\bf Proposition.} If $F$ is a field with $\rchar{F}=p$, for some $p\in P$ and if $a(x)\in F[x]$ with $a'(x)=0$, then the only nonzero terms of $a(x)$ are of the form $a_{k p}x^{k p}$, for some $k\in\Z^{+}$.

\noindent\newline{\bf Proof.} Assume that some $a_i x^i\neq 0$ (meaning $a_i\neq 0$). Then, the derivative of that term is $(i\cdot a_i)x^{i-1}$. As $a'(x)=0$, then $i\cdot a_i=0$. As $a_i\neq 0$, then $\rchar{F}|i$, so it must be $i=\rchar{F} k$, for some $k\in\Z^{+}$. That implies $i=p k$, and $a_i x^i=a_{p k}x^{p k}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $a(x)\in F[x]$ and let $K$ be an extension of field $F$. An element $c\in K$ is called a {\bf multiple root} of $a(x)$ if $(x-c)^m|a(x)$, for some $m\in\Z^{+}-\{1\}$.

\noindent\newline{\bf Theorem.} A polynomial $a(x)\in F[x]$ has a multiple root in some extension of $F$ if and only if there exists $b(x)\in F[x]$, $\deg{b(x)}>0$, such that $b(x)|a(x)$ and $b(x)|a'(x)$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume that a polynomial $a(x)\in F[x]$ has a multiple root $c\in K$. Then, $(x-c)^m|a(x)$, for some $m\in\Z^{+}-\{1\}$. That implies that $a(x)=(x-c)^m q(x)$, for some $q(x)\in K[x]$. Then, $a'(x)=[(x-c)^m q(x)]'=m (x-c)^{m-1} q(x)+(x-c)^m q'(x)$. As $m>1$, then $m-2\geq 0$ and we can extract $(x-c)$ from $a'(x)$. We have $a'(x)=(x-c)[m(x-c)^{m-2}+(x-c)^{m-1} q'(x)]$ and that implies $(x-c)|a'(x)$, i.e. $c$ is a root of $a'(x)$. Then, by a previous proposition, there exists $b(x)\in F[x]$, $\deg{b(x)}>0$ such that $b(x)|a(x)$ and $b(x)|a'(x)$. {\it Sufficiency.} Assume $b(x)|a(x)$ and $b(x)|a'(x)$ and that $a(x)$ does not have a multiple root. As $b(x)\in F[x]$, and $\deg{b(x)}>0$, it has a root $c$ in some extension $K$ of $F$. Then, $b(x)=(x-c)p(x)$. So, $a(x)=b(x)q(x)$ and $a'(x)=b'(x)q(x)+b(x)q'(x)$. We have $b'(x)=p(x)+(x-c)p'(x)$ and $a'(x)=p(x)q(x)+(x-c)p'(x)q(x)+(x-c)p'(x)q'(x)$. Also, $a'(x)=(x-c)p(x)r(x)$ and we have $(x-c)p(x)r(x)=p(x)q(x)+(x-c)p'(x)q(x)+(x-c)p'(x)q'(x)$. This is equivalent to $p(x)q(x)=(x-c)[p(x)r(x)-p'(x)q(x)-p'(x)q'(x)]$. That implies that $x-c|p(x)$ or $x-c|q(x)$, by Euclid's lemma. If $x-c|p(x)$ then $b(x)=(x-c)(x-c)s(x)$ and $a(x)=(x-c)^2 s(x)q(x)$, i.e. it has a multiple root in $K$. If $x-c|q(x)$, then $a(x)=(x-c)p(x)q(x)=(x-c)p(x)(x-c)t(x)=(x-c)^2 p(x)t(x)$, which means $a(x)$ has a multiple root $c$ in $K$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Show that each of the following polynomials has no multiple roots in any extension of its field of coefficients: (a) $x^3-7x^2+8\in\Q[x]$; (b) $x^2+x+\overline{1}\in\zmod{5}[x]$; (c) $x^{100}-\overline{1}\in\zmod{7}[x]$.

\noindent\newline{\bf Solution.} (a) If $a(x)=x^3-7x^2+8$, then $a'(x)=3x^2-14x$. We will divide these two polynomials and find the greatest common divisor. We have:

\begin{eqnarray*}
&&(x^3-7x^2+8):(3x^2-14x)=\frac{1}{3}x-\frac{7}{9}\\
&-&\underline{\left(x^3-\frac{14}{3}x^2\right)}\\
&&-\frac{7}{3}x^2+8\\
&-&\underline{\left(-\frac{7}{3}x^2+\frac{98}{9}x\right)}\\
&&-\frac{98}{9}x+8.
\end{eqnarray*}

\noindent\newline So, we have $x^3-7x^2+8=(3x^2-14x)\left(\frac{1}{3}x-\frac{7}{9}\right)+\left(-\frac{98}{9}x+8\right)$. Then we divide $(3x^2-14x)$ by $\left(-\frac{98}{9}x+8\right)$ and get:

\begin{eqnarray*}
&&(3x^2-14x):\left(-\frac{98}{9}x+8\right)=-\frac{27}{98}x+\frac{2601}{2401}\\
&-&\underline{\left(3x^2-\frac{108}{49}x\right)}\\
&&-\frac{578}{49}x\\
&-&\underline{\left(-\frac{578}{49}x+\frac{20808}{2401}\right)}\\
&&-\frac{20808}{2401}.
\end{eqnarray*}

\noindent\newline Therefore, $3x^2-14x=\left(-\frac{27}{98}x+\frac{2601}{2401}\right)\left(-\frac{98}{9}x+8\right)-\frac{20808}{2401}$ and, if we divide $-\frac{98}{9}x+8$ with $-\frac{20808}{2401}$ we get $-\frac{98}{9}x+8=-\frac{-20808}{2401}\left(-\frac{117649}{93636}x-\frac{2401}{2601}\right)+0$. The greatest common divisor is the monic polynomial in the equivalence class of associates of $-\frac{20808}{2401}$ and that is $1$. Therefore, polynomial $x^3-7x^2+8$ and its derivative $3x^2-14x$ have no common factors in $\Q[x]$ and, in conclusion, $x^3-7x^2+8$ has no multiple roots.

(b) We observe $q(x)=x^2+x+\overline{1}$ over the field $\zmod{5}$. Then, $q'(x)=\overline{2}x+\overline{1}$. We will find their greatest common divisor. We have:

\begin{eqnarray*}
&&\left(x^2+x+\overline{1}\right):\left(\overline{2}x+\overline{1}\right)=\overline{3}x+\overline{4}\\
&-&\underline{\left(x^2+\overline{3}x\right)}\\
&&\overline{3}x+\overline{1}\\
&-&\underline{\left(\overline{3}x+\overline{4}\right)}\\
&&\overline{2}.
\end{eqnarray*}

\noindent\newline From that we have $x^2+x+\overline{1}=\left(\overline{3}x+\overline{4}\right)\left(\overline{2}x+\overline{1}\right)+\overline{2}$, and finally $\overline{2}x+\overline{1}=\overline{2}\left(x+\overline{3}\right)+\overline{0}$. Therefore, the greatest common divisor is the monic associate of $\overline{2}$ and that is $\overline{1}$. That implies that $q(x)$ and $q'(x)$ have no common divisors and that implies that $q(x)$ has no multiple roots.

(c) Let $r(x)=x^{100}-\overline{1}$ in $\zmod{7}[x]$. Then, $r'(x)=(100\cdot\overline{1})x^{99}=\overline{100}x^{99}=\overline{2}x^{99}$. We divide $r(x)$ with $r'(x)$. Thus we get:

\begin{eqnarray*}
&&\left(x^{100}-\overline{1}\right):\left(\overline{2}x^{99}\right)=\overline{4}x\\
&-&\underline{\left(x^{100}\right)}\\
&&\overline{6}.
\end{eqnarray*}

\noindent\newline Therefore, we have $x^{100}-\overline{1}=\overline{4}x\cdot\overline{2}x^{99}+\overline{6}$. Finally, $\overline{2}x^{99}=\overline{6}\cdot\overline{5}x^{99}+\overline{9}$, so $\gcd{r(x),r'(x)}=\overline{1}$ and $r(x)$ has no multiple roots (the reasoning is the same as above).

\newpage

\begin{center}
{\bf Vector spaces}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} A {\bf vector space} over a field $F$ is an ordered triple $(V,\oplus,\odot)$, where $\oplus$ denotes {\bf vector addition}\footnote{$\oplus:V\times V\rightarrow V$} and $\odot$ {\bf scalar multiplication}\footnote{$\odot:F\times V\rightarrow V$}, and $V$ is a set such that:

\begin{itemize}
\item $(V,\oplus)$ is an Abelian group;
\item For any $k,l\in F$ and ${\bf a},{\bf b}\in V$:
\begin{enumerate}
\item $k\odot{\bf a}\in V$;
\item $k\odot({\bf a}\oplus{\bf b})=(k\odot{\bf a})\oplus(k\odot{\bf b})$;
\item $(k+l)\odot{\bf a}=(k\odot{\bf a})\oplus(l\odot{\bf a})$ (the $+$ sign denotes addition in $F$);
\item $k\odot(l\odot{\bf a})=(k\cdot l)\odot{\bf a}$ (the $\cdot$ sign denotes multiplication in $F$);
\item $1\odot{\bf a}={\bf a}$ (the $1$ denotes unity in $F$).
\end{enumerate}
\end{itemize}

\noindent The elements of $V$ are called {\bf vectors} and the elements of $F$ are called {\bf scalars}.

\noindent\newline{\bf Remark.} From now on, due to previous definition, we will make no difference between $\oplus$ and $+$, and between $\odot$ and $\cdot$ (unless stated otherwise).

\noindent\newline{\bf Theorem.} Let $V$ be a vector space over $F$. Then, for every $\vek{a}\in V$ and $k\in F$:

\begin{enumerate}
\item $0\vek{a}=\vek{0}$ (where $\vek{0}$ is the neutral element in $V$);
\item $k\vek{0}=\vek{0}$;
\item If $k\vek{a}=\vek{0}$ then $k=0$ or $\vek{a}=\vek{0}$;
\item $(-1)\vek{a}=-\vek{a}$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} We have $0\vek{a}=(0+0)\vek{a}$. By distributivity, $(0+0)\vek{a}=0\vek{a}+0\vek{a}$. From these two equalities we have $0\vek{a}=0\vek{a}+0\vek{a}$ and, as $V$ with vector addition (and $0\vek{a}\in V$) is an Abelian group, we can add $-0\vek{a}$ on both sides of equality and get $0\vek{a}-0\vek{a}=0\vek{a}+0\vek{a}-0\vek{a}$. As $-0\vek{a}$ is inverse of $0\vek{a}$, and neutral element is $\vek{0}$, we have $\vek{0}=0\vek{a}$. {\it Ad $2$.} We have  $k\vek{0}=k(\vek{0}+\vek{0})$, as $\vek{0}$ is the neutral element. By distributivity, $k(\vek{0}+\vek{0})=k\vek{0}+k\vek{0}$. Again, $k\vek{0}\in V$ and its inverse is $-k\vek{0}$, which with $k\vek{0}$ simply yields $\vek{0}$, the neutral element in $V$. So, $k\vek{0}=k\vek{0}+k\vek{0}$ is equivalent to $k\vek{0}-k\vek{0}=k\vek{0}+k\vek{0}-k\vek{0}$, i.e. $\vek{0}=k\vek{0}$. {\it Ad $3$.} Assume that $k\vek{a}=\vek{0}$ and that $k\neq 0$ and $\vek{a}\neq\vek{0}$. That implies that $k$ has an inverse in $F$ and that $k^{-1}(k\vek{a})=k^{-1}\vek{0}$. From that we have $(k^{-1} k)\vek{a}=\vek{0}$, which is equivalent to $\vek{a}=\vek{0}$, a contradiction to our assumption that $\vek{a}\neq\vek{0}$. Therefore, $\vek{a}=0$ or $k=0$. {\it Ad $4$.} We have $(-1)\vek{a}+1\vek{a}=(-1+1)\vek{a}=\vek{0}$. So, $(-1)\vek{a}+\vek{a}=\vek{0}$. The inverse of $\vek{a}$ is $-\vek{a}$, so if we add that to the equality, we get $(-1)\vek{a}+\vek{a}-\vek{a}=\vek{0}-\vek{a}$, which is equivalent to $(-1)\vek{a}+\vek{0}=-\vek{a}$. That implies, as $\vek{0}$ is the neutral element in $V$, that $(-1)\vek{a}=-\vek{a}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $V$ be a vector space over field $F$ and let $U\subseteq V$. We say that $U$ is {\bf closed with respect to scalar multiplication} if $k\vek{a}\in U$, $\forall k\in F$ and $\vek{a}\in U$. We say that $U$ is a {\bf subspace} of $V$ if $U$ is closed with respect to (vector) addition and scalar multiplication.

\noindent\newline{\bf Remark.} Proof that a subspace $U$ of $V$ is a vector space, is straightforward. All the operations are preserved by definition, so any property of vector space, in its definition, holds. For example, take $(k+l)\vek{a}$. As $k+l\in F$, which is same for $U$ and $V$, and closed with respect to scalar multiplication, then $(k+l)\vek{a}\in U\subseteq V$; in $V$, it holds that $(k+l)\vek{a}=k\vek{a}+l\vek{a}$. The rest can be proved in the exact same fashion.

\noindent\newline{\bf Definition.} Let $U$ be a subspace of $V$ consisting of all the linear combinations of $\vek{a_1},\ldots,\vek{a_m}$ (i.e. each element of $U$ can be shown as a linear combination of these vectors). We call $U$ the subspace of $V$ {\bf spanned by} $\vek{a_1},\ldots,\vek{a_m}$.

\noindent\newline{\bf Definition.} Let $V$ be a vector space over field $F$ and let $\vek{a_1},\ldots,\vek{a_m}\in V$ and $k_1,\ldots,k_m\in F$. Then, the vector $k_1\vek{a_1}+\cdots+k_m\vek{a_m}$ is called a {\bf linear combination} of $\vek{a_1},\ldots,\vek{a_m}$.

\noindent\newline{\bf Definition.} Let $V$ be a vector space over $F$. $S=\{\vek{a_1},\ldots,\vek{a_m}\}$ be a set of distinct vectors $\vek{a_1},\ldots,\vek{a_m}\in V$. If there exist $k_1,\ldots,k_m\in F$ such that $\left|\{k\in F:\ k\neq 0\}\right|>0$ and $k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$, then we say that $S$ is {\bf linearly dependent}. If $S$ is not linearly dependent, we say that it is {\bf linearly independent.}

\noindent\newline{\bf Remark.} The previous definition actually states that $S$ is linearly dependent if at least one of the vectors in $S$ is a linear combination of the other vectors in $S$. Also, $S$ is linearly independent if $k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$ implies $k_1=\cdots=k_m=0$ (no vector in $S$ is equal to a linear combination of other vectors in $S$).

\noindent\newline{\bf Proposition.} Let $V$ be a vector space over a field $F$ and $S\subseteq V$. Then,

\begin{enumerate}
\item If $\vek{0}\in S$, $S$ is linearly dependent;
\item If $S=\{\vek{a}\}$, for some $\vek{a}\in V-\{\vek{0}\}$, then $S$ is linearly independent.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Assume that $\vek{0}\in S$. Then, $S=\{\vek{0},\vek{a_1},\ldots,\vek{a_m}\}$. Assume that $S$ is linearly independent, i.e. $k_0\vek{0}+k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$ implies $k_0=k_1=\cdots=k_m=0$. But, as $k_0\vek{0}=\vek{0}$, we have that $k_1\vek{a_1}+k_2\vek{a_2}+\cdots+k_m\vek{a_m}=\vek{0}$. Let $k\in F-\{0\}$. Then, $k\vek{0}=\vek{0}$, so we have $k\vek{0}+k_1\vek{a_1}+k_2\vek{a_2}+\cdots+k_m\vek{a_m}=k\vek{0}+\vek{0}$, i.e. $k\vek{0}+k_1\vek{a_1}+k_2\vek{a_2}+\cdots+k_m\vek{a_m}=\vek{0}$. Now, as $S$ is linearly independent by assumption, it must be that $k=k_1=\cdots=k_m=0$, but we assumed that $k\neq 0$, which is a contradiction and $S$ must be linearly dependent. {\it Ad $2$.} Let $S=\{\vek{a}\}\neq\{\vek{0}\}$ and assume that $S$ is linearly dependent. Then there exists $k\in F-\{0\}$ such that $k\vek{a}=\vek{0}$. But, that implies that $k=0$ or $\vek{a}=\vek{0}$. As the latter possibility cannot be, due to assumption that $\vek{a}\neq\vek{0}$, it must be $k=0$, which is a contradiction to assumption that $k\neq 0$. Thus, $S$ must be linearly independent.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $V$ be a vector space over $F$. If $S=\{\vek{a_1},\ldots,\vek{a_m}\}\subseteq V$ is linearly dependent, then there exists $\vek{a_i}\in S$, $i\in\{2,\ldots,m\}$, such that $\vek{a_i}$ is a linear combination of $\vek{a_1},\ldots,\vek{a_{i-1}}$.

\noindent\newline{\bf Proof.} As $S$ is linearly dependent, there exist $k_1,\ldots,k_m\in F$ such that $k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$ (where some $k_i$ are not zero). Let $j=\max\{i\in\{1,\ldots,m\}:\ k_i\neq 0\}$. Such $j$ exists as there is at least one element that is not zero, due to $S$ being linearly dependent. Then, $k_{j+1}\vek{a_{j+1}}+\cdots+k_m\vek{a_m}=\vek{0}$. Assume that $k_1\vek{a_1}+\cdots+k_j\vek{a_j}=\vek{a}$. Then we would have $\vek{a}+k_{j+1}\vek{a_{j+1}}+\cdots+k_m\vek{a_m}=\vek{a}$, i.e. $k_1\vek{a_1}+\cdots+k_j\vek{a_j}+k_{j+1}\vek{a_{j+1}}+\cdots+k_m\vek{a_m}=\vek{a}$, which implies that $\vek{a}=0$. Thus, $k_1\vek{a_1}+\cdots+k_j\vek{a_j}=\vek{0}$ and we can take $k_j\vek{a_j}=(-k_1)\vek{a_1}+\cdots+(-k_{j-1})\vek{a_{j-1}}$, i.e. $\vek{a_j}=(-k_1 k_j^{-1})\vek{a_1}+\cdots+(-k_{j-1} k_j)\vek{a_{j-1}}$, which means that $\vek{a_j}$ is a linear combination of $\vek{a_1},\ldots,\vek{a_{j-1}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $V$ be a vector space over $F$ and $U$ a subspace of $V$. If there exist $\vek{a_1},\ldots,\vek{a_m}\in U$ such that for any $\vek{a}\in U$ there exist $k_1,\ldots,k_m\in F$ such that $\vek{a}=k_1\vek{a_1}+\cdots+k_m\vek{a_m}$, then we say that $U$ is a {\bf subspace spanned by} $\vek{a_1},\ldots,\vek{a_m}$.

\noindent\newline{\bf Definition.} Let $V$ be a vector space over $F$. A set of vectors $\{\vek{a_1},\ldots,\vek{a_m}\}\subseteq V$ is called a {\bf basis} of $V$ if it is linearly independent and spans $V$.

\noindent\newline{\bf Theorem.} Let $V$ be a vector space over $F$. If $S,T\subseteq V$ are bases of $V$, then $|S|=|T|$.

\noindent\newline{\bf Poof.} Assume that $S$ and $T$ are bases of $V$ and that $S=\{\vek{s_1},\ldots,\vek{s_m}\}$ and $T=\{\vek{t_1},\ldots,\vek{t_n}\}$, where $m,n\in\Z^{+}$. As $\vek{t_i}\in V$, and $S$ spans $V$, there exist $k_1^{(i)},\ldots,k_m^{(i)}\in F$ such that $\vek{t_i}=k_1^{(i)}\vek{s_1}+\cdots+k_i^{(i)}\vek{s_i}+\cdots+k_m^{(i)}\vek{s_m}$. Notice that not all $k_j^{(i)}$ are zero as that would imply $\vek{t_i}=\vek{0}$, which would in turn imply, by previous proposition, that $T$ is not linearly independent. That expression can be rewritten as $(-1)\vek{t_i}+k_1^{(i)}\vek{s_1}+\cdots+k_i^{(i)}\vek{s_i}+\cdots+k_m^{(i)}\vek{s_m}=\vek{0}$. Let $\vek{s_j}$ be the last vector whose coefficient is not zero. If that was $\vek{t_i}$, that would again imply that all $k_1^{(i)},\ldots,k_m^{(i)}$ are zero and that $\vek{t_i}=\vek{0}$. So, such $\vek{s_j}$ must exist and as in the proof of the previous lemma, we can write $\vek{s_j}$ as a linear combination of $\vek{t_i},\vek{s_1},\ldots,\vek{s_{j-1}}$. In other words, $\vek{s_j}=p_0\vek{t_i}+p_1\vek{s_1}+\cdots+p_{j-1}\vek{s_{j-1}}$. But, that also means that $\vek{t_i}=(-p_1 p_0^{-1})\vek{s_1}+\cdots+(-p_1 p_0^{-1})\vek{s_{j-1}}+\vek{s_j}$.

We can write $B=\{\vek{t_i},\vek{s_1},\ldots,\vek{s_{j-1}},\vek{s_{j+1}},\ldots,\vek{s_m}\}$. Assume that $B$ is linearly dependent. Then, $\vek{0}=q_1\vek{t_i}+q_2\vek{s_1}+\cdots+q_{j}\vek{s_{j-1}}+q_{j+1}\vek{s_{j+1}}+\cdots+q_m\vek{s_m}$, where $q_1,\ldots,q_m\in F$ are not all zero. But, as $\vek{t_i}$ is the linear combination of $\vek{s_1},\cdots,\vek{s_j}$, we can write $\vek{0}=(q_2-q_1 p_1 p_0^{-1})\vek{s_1}+\cdots+(q_{j}-q_1 p_1 p_0^{-1})\vek{s_{j-1}}+q_1\vek{s_j}+q_{j+1}\vek{s_{j+1}}+\cdots+q_m\vek{s_m}$. But, as $\vek{s_1},\ldots,\vek{s_m}$ is linearly independent, $q_1=q_{j+1}=\cdots=q_m=0$. Therefore, $\vek{0}=q_2\vek{s_1}+\cdots+q_{j-1}\vek{s{j-1}}$, where $q_2,\ldots,q_{j-1}$ are not all zero. But, that would mean that $\vek{s_1},\ldots,\vek{s_{j-1}}$ is linearly dependent, which is a contradiction to the fact that $S$ is linearly independent. Also $B$ spans $V$ as, if $\vek{s}\in V$, we can write $\vek{a}$ as a linear combination of all $\vek{s_1},\ldots,\vek{s_j},\ldots,\vek{s_m}$. But, as $\vek{s_j}$ is the linear combination of $\vek{s_1},\ldots,\vek{s_{j-1}},\vek{t_i}$, then so is $\vek{a}$. Therefore, $B$ is the basis of $V$ and $|B|=|S|$. This process can be repeated for all $\vek{t_i}\in T$, leaving us to $B=T$ and $|B|=|S|$, i.e. $|S|=|T|$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $V$ be a vector space over $F$. If there exists $m\in\Z^{+}$ such that $|\base{F}|=m$, we say that $V$ is {\bf finite-dimensional} vector space and that $V$ is of {\bf dimension} $m$. We symbolize that by writing $\dim{V}=m$. If $V=\{\vek{0}\}$, we say $\dim{V}=0$.

\noindent\newline{\bf Lemma.} Let $V$ be a vector space over $F$. If the set $S=\{\vek{a_1},\ldots,\vek{a_m}\}\subseteq V$ spans $V$, then $B\subseteq S$, where $B$ is some basis of $V$.

\noindent\newline{\bf Proof.} Let $V$ be a vector space over $F$ and let $S=\{\vek{a_1},\ldots,\vek{a_m}\}$ be a set that spans $V$. That implies that every $\vek{a}$ can be written as a linear combination of elements of $S$. If the set $S$ is linearly independent, then by definition $S$ is a basis of $V$ and $S\subseteq S$. If $S$ is not linearly independent, then there exists some $\vek{a_i}$ that can be written as a linear combination of all other vectors. Let that vector be $\vek{a_1}$, for the sake of simplicity. Then, $\vek{a_1}=k_2\vek{a_2}+\cdots+k_m\vek{a_m}$, for some $k_2,\ldots,k_m\in F$ and not all $\vek{a_2},\ldots,\vek{a_m}$ are null-vectors. If $\vek{a}\in V$, then as $S$ spans $V$, we have $\vek{a}=l_1\vek{a_1}+\cdots+l_m\vek{a_m}$, for some $l_1,\ldots,l_m\in F$. Then, we can substitute $\vek{a_1}$ into:

\begin{eqnarray*}
\vek{a}&=&(l_1 k_2\vek{a_2}+\cdots+l_1 k_m\vek{a_m})+l_2\vek{a_2}+\cdots+l_m\vek{a_m}\\
&=&(l_1 k_2+l_2)\vek{a_2}+\cdots+(l_1 k_m+l_m)\vek{a_m}.
\end{eqnarray*}

\noindent\newline So, as $\vek{a}$ can be written as a linear combination of $a_2,\ldots,a_m$, then the set $S-\{\vek{a_1}\}$ still spans $V$. If the set $S-\{\vek{a_1}\}$ is linearly indenpendent, it is the basis of $V$ and $S-\{\vek{a_1}\}\subset S$. If it is not, we repeat the process as long as we do not obtain a linearly independent set of vectors. It can be done as $S$ is finite. The process terminates if we are left with only one vector, i.e. $\vek{a_m}$; by previous proposition, if $S=\{\vek{a_m}\}$, then $S$ is linearly independent and a basis of $V$, along with $\{\vek{a_m}\}\subseteq S$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $V$ be a finite vector space of $F$. If the set $S=\{\vek{a_1},\ldots,\vek{a_m}\}\subseteq V$ is linearly independent, then there exist $\vek{a_{m+1}},\ldots,\vek{a_n}\in V$ such that $S\cup\{\vek{a_{m+1}},\ldots,\vek{a_n}\}$ is a basis of $V$ (where $m,n\in\Z^{+}$ and $m\leq n$).

\noindent\newline{\bf Proof.} If $S$ spans $V$, we are done. Assume that $S$ doesn't span $V$ and $B=\{\vek{b_1},\ldots,\vek{b_r}\}$ is a basis of $V$. Then, as $B$ spans $V$, also $B\cup S$ spans $V$. Note that the set $B\cup S$ is not linearly independent, as it would mean $B\cup S$ is a basis and $|B\cup S|=|B|$, which cannot be, as $S\neq\emptyset$ by assumption. Therefore, $B\cup S$ is linearly dependent and spans $V$. As it is linearly dependent, then $k_1\vek{a_1}+\cdots+k_m\vek{a_m}+k_{m+1}\vek{b_1}+\cdots+k_{m+r}\vek{b_r}=\vek{0}$ and not all $k_1,\ldots,k_{r+m}$ are equal to zero. Assume that all $k_{m+1},\ldots,k_{m+r}$ are equal to zero. That would mean that $k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$, where some $k_1,\ldots,k_m$ are equal to zero, i.e. that $S$ is linearly dependent, which is contradiction to assumption that it is linearly independent. Therefore, there must exist some $k_{m+1},\ldots,k_{m+r}$ that are non-zero. Assume it is $k_{m+r}$. Let us put $\vek{b_r}$ in set $T$. As $B\cup S$ spans $V$, then any vector $\vek{v}$ in $V$ can be shown as a linear combination of vectors in $S\cup B$. So, as $\vek{b_{r}}$ can be shown as a linear combination of other vectors in $B\cup S$, the expression can be substituted by $\vek{b_{r}}$ in $\vek{v}$; so $(B\cup S)-\{\vek{b_r}\}$ still spans $V$. If it is linearly independent, we are done. If it is not, we repeat the process. Assume we exhaust all $\vek{b_1},\ldots,\vek{b_r}$. That cannot be, as it would mean, again, that $\vek{a_1},\ldots,\vek{a_m}$ are linearly dependent. Therefore, some $\vek{b_i}$ will remain, and it will be that $B-T\neq\emptyset$. So, we can make $S$ basis of $V$ by adding vectors in $B-T$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $V$ be a vector space over $F$ and $m\in\Z^{+}$ such that $\dim{V}=m$. Also, let $S=\{\vek{a_1},\ldots,\vek{a_m}\}\subseteq V$. Then:

\begin{enumerate}
\item If $S$ is linearly independent, then $S$ is a basis of $V$;
\item If $S$ spans $V$, then $S$ is a basis of $V$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Assume $S$ is linearly independent and that it does not span $V$. Then, by previous lemma there exist some vectors $\vek{b_1},\ldots,\vek{b_n}$, for some $n\in\Z^{+}$, such that $S\cup\{\vek{b_1},\ldots,\vek{b_n}\}$ is a basis of $V$ (not that set has $m+n$ elements). But then $\dim{V}=m+n>m$, which is a contradiction. Therefore $S$ spans $V$ and it is therefore the basis of $V$. {\it Ad $2$.} Assume $S$ spans $V$ but it is not basis of $V$ (i.e. not linearly independent). Then, by previous lemma we can remove some $\vek{a_i}\in S$ to make $S$ linearly independent and a basis of $V$. But then $|S|=\dim{V}<m$ which is a contradiction. Therefore, $S$ must be linearly independent and a basis of $V$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If $U$ and $V$ are vector spaces over a field $F$, a function $h:U\rightarrow V$ is called a homomorphism or a {\bf linear transformation} if it satisfies the following two conditions:

\begin{eqnarray*}
h(\vek{a}+\vek{b})&=&h(\vek{a})+h(\vek{b}),\\
h(k\vek{a})&=&k h(\vek{a}).
\end{eqnarray*}

\noindent\newline The kernel of $h$ is the set $\ker{h}=\{\vek{a}\in U:\ h(\vek{a})=\vek{0}\}$, called the {\bf null space} of $h$. The range of $h$ is the set $\ran{h}=\{\vek{b}\in V:\ (\exists\vek{a}\in U)(h(\vek{a})=\vek{b})\}$, called the {\bf range space} of $h$.

\noindent\newline{\bf Problem.} Prove that $\R^n$ along with operations:

\begin{eqnarray*}
(a_1,\ldots,a_m)+(b_1,\ldots,b_m)&=&(a_1+b_1,\ldots,a_m+b_m),\\
k(a_1,\ldots,a_m)&=&(k a_1,\ldots,k a_m),
\end{eqnarray*}

\noindent\newline for all $(a_1,\ldots,a_m),(b_1,\ldots,b_m)\in\R^n$ and $k\in\R$ is a vector space over the field $\R$.

\noindent\newline{\bf Solution.} Set is obviously non-empty. We know that $\R^n$ defined with such addition is Abelian group from direct product of groups (the very definition of addition in this problem). Let $\vek{a}=(a_1,\ldots,a_m)$ and $\vek{b}=(b_1,\ldots,b_m)$. It is obvious that $k\vek{a}\in\R$, as the very definition of scalar multiplication, along with $k a_i\in\R$, gives us $k\vek{a}=(k a_1,\ldots,k a_m)\in\R^n$. Next, we need to prove that $k(\vek{a}+\vek{b})=k\vek{a}+ k\vek{b}$. It is obvious that $\vek{a}+\vek{b}=(a_1+b_1,\ldots,a_m+b_m)$ from definition, so multiplying with $k$ (and using definition of scalar multiplication in this problem) gives us $k(\vek{a}+\vek{b})=(k a_1+k b_1,\ldots,k a_m+k b_m)$, that can be broken to $k(\vek{a}+\vek{b})=(k a_1,\ldots,k a_m)+(k b_1,\ldots,k b_m)$, and again using definition of scalar multiplication, we have $k(\vek{a}+\vek{b})=k(a_1,\ldots,a_m)+k(b_1,\ldots,b_m)$, i.e. $k(\vek{a}+\vek{b})=k\vek{a}+k\vek{b}$. Then, we need to prove that $(k+l)\vek{a}=k\vek{a}+l\vek{a}$, for $k,l\in\R$. From definition of scalar multiplication in this problem we have $(k+l)\vek{a}=((k+l)a_1,\ldots,(k+l)a_m)$, which is, due to distributivity over $\R$, equal to $(k a_1+l a_1,\ldots,k a_m+l a_m)$, and in the same fashion as in previous condition, we get that it equals $k\vek{a}+l\vek{a}$. Then, by using definition of scalar multiplication in this problem, we have $k(l\vek{a})=k(l a_1,\ldots,l a_m)=(k(l a_1),\ldots,k(l a_m))$. This gives us, due to $\R$ being associative, $k(l\vek{a})=((k l)a_1,\ldots,(k l)a_m)$. Using the definition of scalar multiplication from here again, we have $(k l)\vek{a}$. Finally $1\vek{a}=(1 a_1,\ldots,1 a_m)$ and as $1$ is neutral in $\R$, with respect to multiplication, we have $1\vek{a}=(a_1,\ldots,a_m)=\vek{a}$. That means that $\R^n$ with such defined operations, over the field $\R$ is a vector space.

\noindent\newline{\bf Problem.} Let $\mathcal{F}(\R)$ be a set of all functions from $\R$ to $\R$. Prove that $\mathcal{F}(\R)$, along with operations:

\begin{eqnarray*}
\left[f+g\right](x)&=&f(x)+g(x),\\
\left[k f\right](x)&=&k f(x),
\end{eqnarray*}

\noindent\newline for all $f,g\in\mathcal{F}(\R)$ and $k\in\R$ is a vector space over the field $\R$.

\noindent\newline{\bf Solution.} Set is obviously non-empty. We know from before that $\mathcal{F}(\R)$ with respect to addition is abelian. Also, if we take $f\in\mathcal{F}$ and $k\in\R$, by very definition in this problem, it is again $k f\in\mathcal{F}$. Next, using similar reasoning as in previous problem, $k f(x)+k g(x)=k(f(x)+g(x))$, as $k,f(x),g(x)\in\R$, which is distributive. That equals $k[f+g](x)=[k(f+g)](x)$. This means that $k f+k g=k(f+g)$. Next we have $[(k+l)f](x)=(k+l)f(x)=k f(x)+l f(x)=[k f](x)+[l f](x)$, i.e. $(k+l)f=k f+l f$. Then, $[(k l)f](x)=(k l)f(x)=k(l f(x))=k[l f](x)=[k(l f)](x)$. Finally, $[1 f](x)=1f(x)=f(x)$. This means that $\mathcal{F}(\R)$ is a vector space over $\R$. Note: it is more understandable if we use different signs for vector addition and scalar multiplication.

\noindent\newline{\bf Problem.} Let $\mathcal{P}(\R)$ be a set of all polynomials with coefficients in $\R$, along with polynomial addition and scalar multiplication defined with:

\begin{equation*}
k\cdot p(x)=k\cdot(a_m x^m+\cdots+a_1 x+a_0)=(k a_m)x^m+\cdots+(k a_1)x+(k a_0)=k p(x),
\end{equation*}

\noindent\newline for all $k\in\R$. Prove $\mathcal{P}(\R)$ is a vector space over field $\R$. Note: here we denote scalar multipliction with $\cdot$ and multiplication in $\R$ without any operation sign.

\noindent\newline{\bf Solution.} It is trivial to show that set is non-empty. First, we know that $\mathcal{P}(\R)$ is abelian and $k\cdot p(x)\in\mathcal{P}(\R)$, by very definition. Then, $k\cdot p(x)+k\cdot q(x)=k p(x)+k q(x)=k(p(x)+q(x))$. Further we have $(k+l)\cdot p(x)=(k+l)p(x)=k p(x)+l p(x)=k\cdot p(x)+l\cdot p(x)$. Also, $k\cdot(l\cdot p(x))=k\cdot (l p(x))=k(l p(x))=(k l) p(x)=(k l)\cdot p(x)$. Finally, $1\cdot p(x)=1 p(x)=p(x)$, which proves $\mathcal{P}(\R)$ defined as in this problem is a vector space over $\R$.

\noindent\newline{\bf Problem.} Let $\mathcal{M}(\R)$ be the set of all $2\times 2$ matrices of real numbers, with matrix addition and scalar multiplication defined with:

\begin{equation*}
k\cdot A=k\left[\begin{array}{cc}
$a$ & $b$\\
$c$ & $d$\\
\end{array}\right]=\left[\begin{array}{cc}
$k a$ & $k b$\\
$k c$ & $k d$\\
\end{array}\right]=k A
\end{equation*}

\noindent\newline for all $k\in\R$ is a vector space over $\R$.

\noindent\newline{\bf Solution.} The set is obviously non-empty. As we can view matrix $A$, structurally, as a polynomial, in the manner $p(x)=a x^3+b x^2+c x+d$, all reasoning is same as in previous problem.

\noindent\newline{\bf Problem.} Prove that $U=\{(a,b,c)\in\R^3:\ 2a-3b+c=0\}$ is a subspace of $\R^3$.

\noindent\newline{\bf Solution.} Set is obviously non-empty, as $(0,0,0)\in U$. We can see that $U\subseteq\R^3$ from definition. Let us prove $U$ is closed with respect to vector addition and scalar multiplication. Let $u_1=(a_1,b_1,c_1)$ and $u_2=(a_2,b_2,c_2)$ such that $2 a_i-3 b_i+c_i=0$, for $i\in\{1,2\}$. Thus, $u_1,u_2\in U$. We have $u_1+u_2=(a_1+a_2,b_1+b_2,c_1+c_2)$. So, $2(a_1+a_2)-3(b_1+b_2)+(c_1+c_2)=2 a_1+2 a_2-3 b_1-3 b_2+c_1+c_2=(2 a_1-3 b_1+c_1)+(2 a_2-3 b_2+c_2)=0+0=0$, so $u_1+u_2\in U$. Let $k\in F$. We have $2(k a_1)-3(k b_1)+(k c_1)=k(2a_1-3b_1+c_1)=k 0=0$, so $k u_1=(k a_1,k b_1,k c_1)\in U$. Therefore, $U$ is a subspace of $\R^3$.

\noindent\newline{\bf Problem.} Prove that the set of all $(x,y,z)\in\R^3$ which satisfy the pair of equations $a x+b y+c z=0$ and $d x+e y+f z=0$ is a subspace of $\R^3$.

\noindent\newline{\bf Solution.} Set is obviously non-empty as $(0,0,0)$ satisfies these equations. Let $S\subseteq\R^3$ be a set containing $(x,y,z)$ which satisfy $a x+b y+c z=0$ and $d x+e y+f z=0$. By definition we have that $S$ is a subset of $\R^3$. Let us take $(x_1,y_1,z_1),(x_2,y_2,z_2)\in S$. Then, $a x_1+b y_1+c z_1=0$ and $a x_2+b y_2+c z_2=0$. Adding these two equations gives us $a(x_1+x_2)+b(y_1+y_2)+c(z_1+z_2)=0$. We do this in the same manner for condition that $d x+e y+f z=0$, and arrive to the same conclusion; namely, that equation holds for $(x_1+x_2,y_1+y_2,z_1+z_2)=(x_1,y_1,z_1)+(x_2,y_2,z_2)$. Finally, if $k\in\R$ then multiplying $a x_1+b y_1+c z_1=0$ with $k$ gives us $a(k x_1)+b(k y_1)+c (k z_1)=0$ (same can be shown for second equation), meaning $(k x_1,k y_1,k z_1)\in S$.

\noindent\newline{\bf Problem.} Prove that $S=\{f\in\mathcal{F}(\R):\ f(1)=0\}$ is a subspace of $\mathcal{F}(\R)$.

\noindent\newline{\bf Solution.} Set is obviously non-empty, as $f(1)=0$, for $f(x)=x-1$. By definition, $S\subseteq\mathcal{F}(\R)$. Take $f,g\in S$. Then, $f(1)=g(1)=0$. We have $[f+g](x)=f(x)+g(x)$, so $[f+g](1)=f(1)+g(1)=0+0=0$. Therefore, $f+g\in S$. Let $k\in\R$. Then, $[k f](x)=k f(x)$, so $[k f](1)=k f(1)=k0=0$. This means $k f\in S$. We have shown that $S$ is a subspace of $\mathcal{F}(\R)$.

\noindent\newline{\bf Problem.} Prove that $S=\{f\in\mathcal{F}(\R):\ (\exists c\in\R)(\forall x\in[0,1])(f(x)=c)\}$ is a subspace of $\mathcal{F}(\R)$.

\noindent\newline{\bf Solution.} Set is non-empty, which is trivial to show, i.e. $f\in S$, where $f(x)=5$ (on whole $\R$, so also on $[0,1]$). It is obvious that $S\subseteq\mathcal{F}(\R)$, by definition of $S$. Let $f,g\in S$. Then, $f(x)=g(x)=c$, for all $x\in[0,1]$. Then we have $[f+g](x)=f(x)+g(x)$, and restricting to $x\in[0,1]$ gives us $[f+g](x)=f(x)+g(x)=c+c=2c$, thus $f+g$ is again a constant function on $[0,1]$, meaning $f+g\in S$. Let $k\in\R$. Then, for $x\in[0,1]$, we have $[k f](x)=k f(x)=k c$. But, $k c\in\R$, so $k f$ is again a constant function, and $f\in S$.

\noindent\newline{\bf Problem.} Prove that $S=\{f\in\mathcal{F}(\R):\ (\forall x\in\R)(f(-x)=f(x))\}$ is a subspace of $\mathcal{F}(\R)$. Is the same true for odd functions?

\noindent\newline{\bf Solution.} Set is obviously non-empty, as $\cos(x)\in S$. By definition, $S\subseteq\mathcal{F}(\R)$. Let $f,g\in S$. Then, $f$ and $g$ are even functions and $[f+g](-x)=f(-x)+g(-x)=f(x)+g(x)=[f+g](x)$. Also $[k f](-x)=k f(-x)=k f(x)=[k f](x)$, for any $k\in\R$. Therefore, $f+g$ and $k f$ are even, meaning $S$ is a subspace of $\mathcal{F}(\R)$. Same holds for odd functions.

\noindent\newline{\bf Problem.} Prove that $S=\{p(x)\in\mathcal{P}(\R):\ \deg{p(x)}\leq m\}$, for some $m\in\Z^{+}$, is a subspace of $\mathcal{P}(\R)$.

\noindent\newline{\bf Solution.} Set is obviously non-empty, as we can define a polynomial with real coefficients for each $m\in\Z^{+}$. It is also obvious that $S\subseteq\mathcal{P}(\R)$. Take $p(x),q(x)\in S$. Then, their sum, $p(x)+q(x)$ is also of the same degree, and must be in $S$. Same thing for $k p(x)$, where $k\in\R$; multiplying with a real number won't change its degree (unless multiplied by zero, but then the (zero) degree is surely less then any positive $m$). Therefore, $S$ is a subspace of $\mathcal{P}(\R)$.

\noindent\newline{\bf Problem.} Prove that $B=\{(0,0,0,1),(0,0,1,1),(0,1,1,1),(1,1,1,1)\}$ is a basis of $\R^4$.

\noindent\newline{\bf Solution.} Obviously $B\subseteq\R^4$. First we will test linear independence of vectors in $B$. Let $a(0,0,0,1)+b(0,0,1,1)+c(0,1,1,1)+d(1,1,1,1)=(0,0,0,0)$. (From now on we will denote $(0,0,\ldots,0)=\vek{0}$.) Then we have $(0,0,0,a)+(0,0,b,b)+(0,c,c,c)+(d,d,d,d)=\vek{0}$, which gives us set of linear equations:

\begin{eqnarray*}
d&=&0,\\
c+d&=&0,\\
b+c+d&=&0,\\
a+b+c+d&=&0.
\end{eqnarray*}

\noindent\newline We can see that, as $d=0$, then $c+0=0$ gives $c=0$. Then, $b+0+0=0$ gives $b=0$, and in the same way we get $a=0$. Therefore, set $B$ is linearly dependent. Does $B$ span $\R^4$? Let $(x_1,x_2,x_3,x_4)\in\R^4$. We can see that $(x_1,x_2,x_3,x_4)=(x_4-x_3-x_2-x_1)(0,0,0,1)+(x_3-x_2-x_1)(0,0,1,1)+(x_2-x_1)(0,1,1,1)+x_1(1,1,1,1)$. Therefore, being linearly independent and spanning $\R^4$, $B$ is basis of $\R^4$.

\noindent\newline{\bf Problem.} If $\vek{a}=(1,2,3,4)$ and $\vek{b}=(4,3,2,1)$, explain why $\{\vek{a},\vek{b}\}$ may be extended to a basis of $\R^4$. Find a basis which includes $\vek{a}$ and $\vek{b}$.

\noindent\newline{\bf Solution.} Let us first prove that $\vek{a}$ and $\vek{b}$ are linearly independent over $\R$. Let $k_1,k_2\in\R$. Then, let $k_1\vek{a}+k_2\vek{b}=\vek{0}$, i.e. $k_1(1,2,3,4)+k_2(4,3,2,1)=\vek{0}$ From that we get:

\begin{eqnarray*}
k_1+4k_2&=&0,\\
2k_1+3k_2&=&0,\\
3k_1+2k_2&=&0,\\
4k_1+k_2&=&0.
\end{eqnarray*}

\noindent\newline From the first line we get $k_1=-4k_2$, and putting that in the last line gives us $4\cdot(-4k_2)+k_2=0$, i.e. $-15k_2=0$. That implies $k_2=0$, and, as $k_1=-4k_2$, also that $k_1=0$, meaning $\vek{a}$ and $\vek{b}$ are linearly independent over $\R$. Let $\vek{c}=(0,1,0,0)$ and $\vek{d}=(0,0,1,0)$. Testing for linear independence through $k_1\vek{a}+k_2\vek{c}+k_3\vek{d}+k_4\vek{b}=\vek{0}$, we get:

\begin{eqnarray*}
k_1+4k_4&=&0,\\
2k_1+k_2+3k_4&=&0,\\
3k_1+k_3+2k_4&=&0,\\
4k_1+k_4&=&0.
\end{eqnarray*}

\noindent\newline In the same fashion as before, we get $k_1=k_4=0$. Putting that into the rest equations, obviously leads to $k_2=k_3=0$, thus the set of vectors $\vek{a}$, $\vek{b}$, $\vek{c}$ and $\vek{d}$ is linearly independent. Let us show that any $\vek{x}=(x_1,x_2,x_3,x_4)$ can be represented as a linear combination of $\vek{a}$, $\vek{b}$, $\vek{c}$ and $\vek{d}$. Let $(x_1,x_2,x_3,x_4)=k_1(1,2,3,4)+k_2(0,1,0,0)+k_3(0,0,1,0)+k_4(4,3,2,1)$. We have a set of equations:

\begin{eqnarray*}
x_1&=&k_1+4k_4,\\
x_2&=&2k_1+k_2+3k_4,\\
x_3&=&3k_1+k_3+2k_4,\\
x_4&=&4k_1+k_4.
\end{eqnarray*}

\noindent\newline From the first equation we have $k_1=x_1-4k_4$, and from the last one $k_4=x_4-4k_1$. Putting that into first equation gives us $k_1=x_1-4x_4+16k_1$, and finally, also putting $k_1$ into expression for $k_4$:

\begin{eqnarray*}
k_1&=&-\frac{1}{15}x_1+\frac{4}{15}x_4,\\
k_4&=&\frac{4}{15}x_1-\frac{1}{15}x_4.
\end{eqnarray*}

\noindent\newline From second equation we have $k_2=x_2-2k_1-3k_4$. Then, putting $k_1$ and $k_4$ into the former equation, we have:

\begin{equation*}
k_2=x_2+\frac{2}{15}x_1-\frac{8}{15}x_4-\frac{12}{15}x_1+\frac{3}{15}x_4=-\frac{2}{3}x_1+x_2-\frac{1}{3}x_4.
\end{equation*}

\noindent\newline From the equation for $x_3$, we have $k_3=x_3-3k_1-2k_4$. Putting expressions for $k_1$ and $k_4$ gives us:

\begin{equation*}
k_3=x_3+\frac{3}{15}x_1-\frac{12}{15}x_4-\frac{8}{15}x_1+\frac{2}{15}x_4=-\frac{1}{3}x_1+x3-\frac{2}{3}x_4.
\end{equation*}

\noindent\newline Thus, for any $(x_1,x_2,x_3,x_4)$ we have shown a way to represent it through coefficients $k_1$, $k_2$, $k_3$ and $k_4$ as a linear combination of $\vek{a}$, $\vek{b}$, $\vek{c}$ and $\vek{d}$.

\noindent\newline{\bf Problem.} Let $A$ be the set of eight vectors $(x,y,z)$ where $x,y,z\in\{1,2\}$. Prove that $A$ spans $\R^3$, and find a subset of $A$ which is a basis of $\R^3$.

\noindent\newline{\bf Solution.} Let $\vek{a}=(2,1,1)$, $\vek{b}=(1,2,1)$ and $\vek{c}=(1,1,2)$. We will immediately show their linear independence and we will prove that they span $\R^3$ as a subset of $A$. Let $k_1,k_2,k_3\in\R$ and $\vek{0}=k_1(2,1,1)+k_2(1,2,1)+k_3(1,1,2)$. Then we get a set of equations:

\begin{eqnarray*}
2k_1+k_2+k_3&=&0,\\
k_1+2k_2+k_3&=&0,\\
k_1+k_2+2k_3&=&0.
\end{eqnarray*}

\noindent\newline From first equation we have $k_2=-2k_1-k_3$ and from third $k_1=-k_2-2k_3$. Putting that into first equation we get $k_2=2k_2+4k_3-k_3$, i.e. $k_2=-3k_3$. Then, from second equation $k_1-6k_3+k_3=0$, which gives us $k_1=5k_3$. We put that, along with $k_2=-3k_3$ in the first equation and get $10k_3-3k_3+k_3=0$. It is obvious that $k_3=0$ which implies that also $k_1=k_2=0$. Thus, $\vek{a}$, $\vek{b}$ and $\vek{c}$ are linearly independent over $\R$. Now, let us take $(x,y,z)\in\R^3$ and $(x,y,z)=k_1(2,1,1)+k_2(1,2,1)+k_3(1,1,2)$. This gives rise to following equations:

\begin{eqnarray*}
x&=&2k_1+k_2+k_3,\\
y&=&k_1+2k_2+k_3,\\
z&=&k_1+k_2+2k_3.
\end{eqnarray*}

\noindent\newline Second equation gives us $k_3=y-k_1-2k_2$ and third $k_2=z-k_1-2k_3$. Putting second into third gives $k_2=z-k_1-2y+2k_1+4k_2$. That means $k_1=-3k_2-z+2y$. Putting that into expression for $k_3$ gives us $k_3=y+3k_2+z-2y-2k_2$, i.e. $k_3=k_2-y+z$.
Putting that into first equation yields $x=-6k_2-2z+4y+k_2+k_2-y+z=-4k_2+3y-z$, i.e.:

\begin{equation*}
k_2=-\frac{1}{4}x+\frac{3}{4}y-\frac{1}{4}z.
\end{equation*}

\noindent\newline Now, if we put that into expression for $k_3$, we have:

\begin{equation*}
k_3=-\frac{1}{4}x-\frac{1}{4}y+\frac{3}{4}z.
\end{equation*}

\noindent\newline Finally, if we put the same expression, for $k_2$, in equality for $k_1$ we have:

\begin{equation*}
k_1=\frac{3}{4}x-\frac{1}{4}y-\frac{1}{4}z.
\end{equation*}

\noindent\newline Therefore, we have shown that the set of vectors $\vek{a}$, $\vek{b}$ and $\vek{c}$ is a basis for $\R^3$ (i.e. is linearly independent and spans $\R^3$).

\noindent\newline{\bf Problem.} Let $\mathcal{P}_m(\R)=\{p(x)\in\mathcal{P}(\R):\ \deg{p(x)}\leq m\}$, for some $m\in\Z^{+}$. Prove that $S=\{1,x,x^2,\ldots,x^m\}$ is a basis of $\mathcal{P}_m(\R)$. Then find another basis of $\mathcal{P}_m(\R)$.

\noindent\newline{\bf Solution.} It is obvious that $S\subseteq\mathcal{P}_m(\R)$, as $\deg{x^i}\leq m$, for all $i\in\{1,\ldots,m\}$, by definition. To show that the vectors are independent, let $k_1,k_2,\ldots,k_{m+1}\in\R$ and $k_1+k_2 x+\cdots+k_{m+1} x^m=0$. Here we implicitly state that $0$ on the right-hand side is actually a zero polynomial, not specifically a real number. The polynomial on the left-hand side will be a zero polynomial, by definition, if all of its coefficients are equal to zero, i.e. if $k_1=k_2=\ldots=k_{m+1}=0$. Therefore $S$ is linearly independent. Choose any $p(x)\in\mathcal{P}_m(\R)$. Let its degree be $n\leq m$ and $p(x)=k_1+k_2 x+\cdots+k_{n+1} x^n$. It is obvious that $p(x)$ can be, and already is, represented by vectors in $S$. Thus, $S$ spans $\mathcal{P}_m(\R)$. Another example of a base would be:

\begin{equation*}
T=\{1,1+x,1+x+x^2,\ldots,1+x+x^2+\cdots+x^m\}.
\end{equation*}

\noindent\newline Let $k_1,k_2,\ldots,k_{m+1}\in\R$ and

\begin{equation*}
k_1+k_2(1+x)+k_3(1+x+x^2)+\cdots+k_{m+1}(1+x+x^2+\cdots+x^m)=0.
\end{equation*}

\noindent\newline From that we have:

\begin{equation*}
(k_1+k_2+\ldots+k_{m+1})+(k_2+\ldots+k_{m+1})x+\ldots+(k_m+k_{m+1})x^{m-1}+k_{m+1}x^m=0.
\end{equation*}

\noindent\newline As $0$ on the right-hand side is zero polynomial, then all the coefficients on the left-hand side must also equal zero, which bring us to following system of equations:

\begin{eqnarray*}
k_1+k_2+\cdots+k_{m+1}&=&0,\\
k_2+k_3+\cdots+k_{m+1}&=&0,\\
&\vdots&\\
k_m+k_{m+1}&=&0,\\
k_{m+1}&=&0.
\end{eqnarray*}

\noindent\newline Due to the last equality, namely, $k_{m+1}=0$, we see that we produce a sort of domino effect. The equation before the last thus gives us $k_m+0=0$, i.e. $k_m=0$. Therefore, we get $k_1=k_2=ldots=k_{m+1}=0$. Now, let $p(x)=l_1+l_2 x+\cdots+l_{n+1} x^n$, with $l_1,l_2,\ldots,l_{n+1}\in\R$ and $n\leq m$. We want to show that $p(x)$ can be shown as a linear combination of vectors in $T$, i.e. we must find coefficients $k_1,\ldots,k_{m+1}$ such that:

\begin{equation*}
k_1+k_2(1+x)+k_3(1+x+x^2)+\cdots+k_{m+1}(1+x+\ldots+x^m)=l_1+l_2 x+\cdots+l_{n+1} x^n.
\end{equation*}

\noindent\newline Similarly to the procedure above, we get the following system of equations (note that for $n>m$ coefficients in $p(x)$ are equal to zero):

\begin{eqnarray*}
l_1&=&k_1+\cdots+k_{m+1},\\
l_2&=&k_2+\cdots+k_{m+1},\\
&\vdots&\\
l_{n+1}&=&k_{n+1}+\cdots+k_{m+1},\\
0&=&k_{n+2}+\cdots+k_{m+1},\\
&\vdots&\\
0&=&k_m+k_{m+1},\\
0&=&k_{m+1}.
\end{eqnarray*}

\noindent\newline Notice that again we have $k_{m+1}=0$, and so we get up to $l_n$, having $k_{n+1}=\ldots=k_{m+1}$. Thus, from $l_{n+1}=k_{n+1}+k_{n+2}+\cdots+k_{m+1}$, we have $k_{n+1}=l_{n+1}$. For $l_n=k_n+k_{n+1}+\ldots+k_{m+1}$, we have $l_n=k_n+l_{n+1}$, i.e. $l_n=l_n-l_{n+1}$. Similarly, for $l_{n-1}=k_{n-1}+l_n-l_{n+1}+l_{n+1}$ we get $k_{n-1}=l_{n-1}-l_n$. By this logic we arrive at the following system of coefficients:

\begin{eqnarray*}
k_1&=&l_1-l_2,\\
k_2&=&l_2-l_3,\\
&\vdots&\\
k_n&=&l_n-l_{n+1},\\
k_{n+1}&=&l_{n+1}.
\end{eqnarray*}

\noindent\newline Therefore, $T$ spans $\mathcal{P}_m(\R)$ and is its base.

\noindent\newline{\bf Problem.} Find a basis for each of the following subspaces of $\R^3$: (a) $S_1=\{(x,y,z):\ 3x-2y+z=0\}$, (b) $S_2=\{(x,y,z):\ (x+y-z=0)\wedge(2x-y+z=0)\}$.

\noindent\newline{\bf Solution.} (a) Let us first observe which vectors would span $S_1$. Assume $(a_i,b_i,c_i)\in B_1$, where $B_1$ is basis of $S_1$, and $i\in\{1,2,3\}$. But, as also $B_1\subseteq S_1$, we have:

\begin{eqnarray*}
3a_1-2b_1+c_1&=&0,\\
3a_2-2b_2+c_2&=&0,\\
3a_3-2b_3+c_3&=&0.
\end{eqnarray*}

\noindent\newline If we let $a_1=b_2=c_3=0$, we have $c_1=2b_1$, $c_2=-3a_2$ and $3a_3=2b_3$. Therefore, we can choose vectors $(0,1,2)$, $(1,0,-3)$ and $(2,3,0)$ for basis $B_1$. We will show that they're linearly independent and span $S_1$. First, let $k_1,k_2,k_3\in\R$ and $k_1(0,1,2)+k_2(1,0,-3)+k_3(2,3,0)=\vek{0}$. From that we have:

\begin{eqnarray*}
k_2+2k_3&=&0,\\
k_1+3k_3&=&0,\\
2k_3-3k_2&=&0.
\end{eqnarray*}

\noindent\newline From the first equality we have $k_2=-2k_3$. Putting that into third equation gives us $2k_3+6k_3=0$, i.e. $k_3=0$. This gives us $k_1=0$ from second equation and $k_2=0$ from first, therefore vectors in $B_1$ are linearly independent. Let $(x,y,z)\in S_1$. We will show it can be represented as a linear combination of vectors in $B_1$. We have $(x,y,z)=k_1(0,1,2)+k_2(1,0,-3)+k_3(2,3,0)$, and from that:

\begin{eqnarray*}
x&=&k_2+2k_3,\\
y&=&k_1+3k_3,\\
z&=&2k_3-3k_2.
\end{eqnarray*}

\noindent\newline From first equation we have $k_2=x-2k_3$, and putting that in third gives us $z=2k_3-3x+6k_3$, that is:

\begin{equation*}
k_3=\frac{3}{8}x+\frac{1}{8}z.
\end{equation*}

\noindent\newline Putting that in expression for $k_2$ we get:

\begin{equation*}
k_2=\frac{1}{4}x-\frac{1}{4}z.
\end{equation*}

\noindent\newline Finally, from second equality, $k_1=y-3k_2$, this gives us:

\begin{equation*}
k_1=\frac{9}{8}x+y-\frac{3}{8}z.
\end{equation*}

\noindent\newline Therefore, $B_1$ spans $S_1$, is linearly independent, meaning it is base for $S_1$. (b) Let $S_2=\{(x,y,z):\ (x+y-z=0)\wedge(2x-y+z=0)\}$ and $B_2$ be basis of $S_2$. The two equations imply $x+y-z=2x-y+z$, i.e. $x-2y+2z=0$. Let $(a_i,b_i,c_i)\in B_2$, for $i\in\{1,2,3\}$. Then, it follows, due to also $B_2\subseteq S_2$ that $a_i-2b_i+2z_i=0$, for $i\in\{1,2,3\}$. If we choose $a_1=b_2=c_3=0$, we get:

\begin{eqnarray*}
-2b_1+2c_1&=&0,\\
a_2+2c_2&=&0,\\
a_3-2b_3&=&0.
\end{eqnarray*}

\noindent\newline From first equation we have $c_1=b_1$, from second $a_2=-2c_2$ and from third $a_3=2b_3$. Let us choose $(0,1,1)$ as first vector for $B_2$, and $(2,0,-1)$ as the second. It is obvious they are linearly independent, due to difference in zeros. Now, let us consider a third vector. For the third vector to be linearly dependent, it needs to be $(2b_3,b_3,c_3)=k_1(0,1,1)+k_2(2,0,-1)$, for some $k_1,k_2\in\R$. We have $2b_3=2k_2$ (i.e. $b_3=k_2$), $b_3=k_1$ and $c_3=k_1-k_2$. Thus, we have $k_1=k_2=b_3$, meaning $c_3=0$. So, if we choose $c_3=0$, it must not be that first two coordinates are of the form $2b_3$ and $b_3$. Let us choose $(1,1,0)$. Then, assume $k_1(0,1,1)+k_2(2,0,-1)+k_3(1,1,0)=\vek{0}$. We have:

\begin{eqnarray*}
2k_2+k_3&=&0,\\
k_1+k_3&=&0,\\
k_1-k_2&=&0.
\end{eqnarray*}

\noindent\newline From third equation we get $k_2=k_1$ and from second $k_3=-k_1$. If we put that into the first equation, we obtain $2k_1-k_1=0$, i.e. $k_1=0$. That implies that $k_2=k_3=0$, i.e. $B_2$ is linearly independent. Now, let us observe equation $(x,y,z)=k_1(0,1,1)+k_2(2,0,-1)+k_3(1,1,0)$, where $(x,y,z)\in S_2$. Then we get:

\begin{eqnarray*}
x&=&2k_2+k_3,\\
y&=&k_1+k_3,\\
z&=&k_1-k_2.
\end{eqnarray*}

\noindent\newline From second and third equations we get $k_3=y-k_1$ and $k_2=k_1-z$. Putting that into first equation yields $x=2k_1-2z+y-k_1$, i.e. $k_1=x-y+2z$. Then, second equation gives us $k_3=-x+2y-2z$, and third $k_2=x-y+z$. Therefore, $B_2$ is basis of $S_2$.

\noindent\newline{\bf Problem.} Find a basis for the subspace of $\R^3$ spanned by the set of vectors $(x,y,z)$ such that $x^2+y^2+z^2=1$.

\noindent\newline{\bf Solution.} Let $B$ denote basis of $S=\{(x,y,z)\in\R^3:\ x^2+y^2+z^2=1\}$. We can see that $(1,0,0),(0,1,0),(0,0,1)\in S$. Seeing that these vectors are linearly independent is trivial, and if they span $S$ it must be that $(x,y,z)=k_1(1,0,0)+k_2(0,1,0)+k_3(0,0,1)$. Is trivial to see that $B$ spans $S$, i.e. it is basis of $S$.

\noindent\newline{\bf Problem.} Let $U$ be the subspace of $\mathcal{F}(\R)$ spanned by $V=\{\cos{x}^2,\sin{x}^2,\cos{(2x)}\}$. Find the dimension of $U$, and then find a basis of $U$.

\noindent\newline{\bf Solution.} Let $O(x)=0$, for all $x\in[0,2\pi\rangle$. As $\cos{(2x)}=\cos^2{x}-\sin^2{x}$, it is obvious that $V$ is linearly dependent. Throwing out $\cos{(2x)}$, it is easy to show that $V$ will be linearly independent. Assume that it is not linearly independent. Then, $k_1\cos^2{x}+k_2\sin^2{x}=O(x)$, for all $x\in[0,2\pi\rangle$, and $k_1\neq0$ or $k_2\neq0$. Assume $k_1\neq0$. Now, $k_2=0$ or $k_2\neq0$. Assume $k_2=0$. Then, $k_1\cos^2{x}=O(x)$, for all $x\in[0,2\pi\rangle$. But, if we take $x=0$, then $k_1\cdot 1=0$, i.e. $k_1=0$, which is a contradiction to assumption that $k_1\neq0$. If $k_2\neq0$, then $k_1\cos^2{x}+k_2\sin^2{x}=O(x)$, for all $x\in[0,2\pi\rangle$. But if we choose $x=\frac{\pi}{2}$, we have $k_1\cdot 0+k_2\cdot 1=0$, i.e. $k_2=0$, which is a contradiction to assumption that $k_2\neq0$. Therefore, $V-\{\cos{(2x)}\}$ is linearly independent and is a basis for $V$ spanned by those three vectors (previous theorem).

\noindent\newline{\bf Problem.} Find a basis for the subspace of $\mathcal{P}(\R)$ spanned by:

\begin{equation*}
S=\{x^3+x^2+x+1,x^2+1,x^3-x^2+x-1,x^2-1\}.
\end{equation*}

\noindent\newline{\bf Solution.} $S$ is linearly dependent because:

\begin{equation*}
x^2+1=\frac{1}{2}\left(x^3+x^2+x+1\right)-\frac{1}{2}\left(x^3-x^2+x-1\right).
\end{equation*}

\noindent\newline But, set $S-\left(x^2+1\right)$ is linearly independent which can be easily checked. Let $k_1,k_2,k_3\in\R$. Then, let $O(x)$ be a null-polynomial and:

\begin{equation*}
k_1\left(x^3+x^2+x+1\right)+k_2\left(x^3-x^2+x-1\right)+k_3\left(x^2-1\right)=O(x).
\end{equation*}

\noindent\newline Then, from that we have following equations:

\begin{eqnarray*}
k_1+k_2&=&0,\\
k_1-k_2+k_3&=&0,\\
k_1+k_2&=&0,\\
k_1-k_2-k_3&=&0.
\end{eqnarray*}

\noindent\newline From first equation we have $-k_2=k_1$, and putting that in the second one gives us $-k_3=2k_1$. If we put expressions for $-k_2$ and $-k_3$ in the third equation we get $k_1+k_1+2k_1=0$. That gives $4k_1=0$, i.e. $k_1=0$. Then, also $k_2=k_3=0$, meaning $S-\{x^2+1\}$ is linearly independent. By previous theorem, $S-\{x^2+1\}$ is basis of subspace spanned by $S$.

\noindent\newline{\bf Proposition.} Let $V$ be a finite dimensional vector space. Then:

\begin{enumerate}
\item If $U$ is a subspace of $V$, then $\dim{U}\leq\dim{V}$.
\item If $U$ is a subspace of $V$, and $\dim{U}=\dim{V}$, then $U=V$.
\end{enumerate}

\noindent\newline{\bf Proof.} Ad $1$. Assume $B_V$ is basis of $V$. Then, $\dim{V}=|B_V|$, by definition. Choose any vector $\vek{a}\in U$. Then, as $U\subseteq V$, also $\vek{a}\in V$. That means that $\vek{a}$ can be represented by vectors in $B_V$. In other words, $B_V$ spans $U$. By previous lemma, that means that there exists $B_U\subseteq B_V$ such that $B_U$ is basis of $U$. That means $\dim{U}=|B_U|$ and $|B_U|\leq|B_V|$, i.e. $\dim{U}\leq\dim{V}$.

Ad $2$. We follow the same proof as in $1$, but, when we arrive at $\dim{U}=\dim{V}$, we have $|B_U|=|B_V|$. That, along with $B_U\subseteq B_V$ gives $B_U=B_V$. Assume $U\neq V$, i.e. $U\subset V$. Then there exists $\vek{v}\in V$ such that $\vek{v}\notin U$. As $\vek{v}\in V$, then $\vek{v}$ can be shown as a linear combination of vectors in $B_V$. But, as $B_V=B_U$, $\vek{v}$ can also be shown as a linear combination of vectors in $B_U$, meaning it is also in $U$, which is a contradiction to our assumption. Therefore, it must be that $U=V$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $V$ be a finite vector space. Any subset of an independent set in $V$ is independent.

\noindent\newline{\bf Proof.} Let $S\subseteq V$ be linearly independent. Let $T\subseteq S$ such that $S,T\neq\emptyset,\{\vek{0}\}$. Let us prove that $T$ is linearly independent. If $S=T$, we are done. Let $T=\{\vek{v_1},\ldots,\vek{v_m}\}$ and $S=\{\vek{v_1},\ldots,\vek{v_m},\vek{v_{m+1}},\ldots,\vek{n}\}$, where $m\leq n$. Let $\vek{v_i}\in T$. Assume $T$ is linearly dependent, i.e. there exist $k_2,\ldots,k_m\in\R$ such that $\vek{v_1}=k_2\vek{v_2}+\cdots+k_m\vek{v_m}$. But, as $\vek{v_1}\in S$, that would mean that $\vek{v_1}$ can be shown as a linear combination of vectors in $S$ in the following way:

\begin{equation*}
\vek{v_1}=k_2\vek{v_2}+\cdots+k_m\vek{v_m}+0\vek{v_{m+1}}+\cdots+0\vek{v_n}.
\end{equation*}

\noindent\newline We assumed that not all $k_i=0$, so our assumption that $\vek{v_1}$ is a linear combination of vectors in $S$ holds. That means that $S$ is not linearly independent, which is a contradiction to our assumption that it actually is. Therefore, it must be that $T$ is linearly independent also.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $V$ be a finite vector space. Any set of vectors containing a dependent set in $V$ is dependent.

\noindent\newline{\bf Proof.} Let $T\subseteq S\subseteq V$ be a set in $V$ such that $S,T\neq\emptyset,\vek{0}$. For $S,T=\vek{0}$ proof is trivial. Assume $T$ is linearly dependent. Let us prove that $S$ is then also linearly dependent. Assume contrary, assume that $S$ is linearly independent. By previous proposition, then it must also be that $T$ is linearly independent, which is a contradiction to our assumption. Therefore, it must be that $S$ is linearly dependent.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $V$ be a vector space such that $\dim{V}=m$. If $U\subseteq V$ such that $|U|>m$, then $U$ is linearly dependent.

\noindent\newline{\bf Proof.} Assume that $\base{V}=\{\vek{v_1},\ldots,\vek{v_m}\}$ and that $U=\{\vek{u_1},\ldots,\vek{u_n}\}$ is linearly independent, where $n>m$. Let us take $U_m=\{\vek{u_1},\ldots,\vek{u_m}\}$ (we throw out $\vek{u}_{m+1},\ldots,\vek{u_n}\}$). Then, $U_m$ is linearly independent, by previous proposition. Also, by previous proposition, as $|U_m|=m=\dim{V}$, we have that $U_m$ is a basis of $V$. That also implies that $U_m$ spans $V$. As $U_m\subseteq U$, then $U$ also spans $V$. That means that $U$ is also a basis of $V$ and then it must be $n=|U|=\dim{V}=m$, which is impossible because $n>m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Prove that if $U=\{\vek{a},\vek{b},\vek{c}\}$ is linearly independent, so is $V=\{\vek{a}+\vek{b},\vek{b}+\vek{c},\vek{c}+\vek{a}\}$.

\noindent\newline{\bf Solution.} Assume $V$ is linearly dependent and $F$ be a field over which vector space in this problem is defined. Then there exist $k_1,k_2,k_3\in F$, not all equal to zero, such that the equality $\vek{0}=k_1\left(\vek{a}+\vek{b}\right)+k_2\left(\vek{b}+\vek{c}\right)+k_3\left(\vek{c}+\vek{a}\right)$ holds. Then, $\vek{0}=k_1\vek{a}+k_1\vek{b}+k_2\vek{b}+k_2\vek{c}+k_3\vek{c}+k_3\vek{a}$. That can be regrouped so that $\vek{0}=\left(k_1+k_3\right)\vek{a}+\left(k_1+k_2\right)\vek{b}+\left(k_2+k_3\right)\vek{c}$. As $\vek{a}$, $\vek{b}$ and $\vek{c}$ are linearly independent by assumption, it must be that $k_1+k_3=k_1+k_2=k_2+k_3=0$. From that we have $k_1=-k_3$ and $k_2=-k_3$. Putting that into $k_1+k_2=0$ we have $-2k_3=0$, i.e. $k_3=0$. This also implies $k_1=k_2=0$. But, that is a contradiction to assumption that $k_1$, $k_2$ and $k_3$ are not all zero.

\noindent\newline{\bf Proposition.} If $B=\{\vek{a_1},\ldots,\vek{a_m}\}$ is a basis of vector space $V$ over $F$, so is $B'=\{k_1\vek{a_1},\ldots,k_m\vek{a_m}\}$ for any nonzero scalars $k_1,\ldots,k_m\in F$.

\noindent\newline{\bf Proof.} Let us show that $B'$ is linearly independent $V$. Let $l_1,\ldots,l_m\in F$ and $\vek{0}=l_1\left(k_1\vek{a_1}\right)+\cdots+l_m\left(k_m\vek{a_m}\right)$. This can be regrouped so that $\vek{0}=\left(k_1 l_1\right)\vek{a_1}+\cdots+\left(k_m l_m\right)\vek{a_m}$. But, as $\vek{a_1},\ldots,\vek{a_m}$ are linearly independent by assumption, it must be that $k_1 l_1=\ldots=k_m l_m=0$. Now, as $k_1,\ldots,k_m$ are non-zero, it must be that all $l_1,\ldots,l_m$ are zero, meaning $B'$ is linearly independent. Now, let us show that $B'$ spans $V$. Assume that for some $\vek{v}\in V$ we have $\vek{v}=l_1\vek{a_1}+\cdots+l_m\vek{a_m}$. Then, as $k_i$ are non-zero, they have inverses in $F$, and for $\vek{v}=q_1\left(k_1\vek{a_1}\right)+\cdots+q_m\left(k_m\vek{a_m}\right)$ we can take $q_i=l_i k_i^{-1}$. Thus, we have shown how to obtain linear combination as vectors in $B'$ for $\vek{v}$, meaning $B'$ spans $V$. In other words, as it is also linearly independent, it is a basis of $V$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} The space spanned by $A=\{\vek{a_1},\ldots,\vek{a_m}\}$ is the same as the space spanned by $B=\{\vek{b_1},\ldots,\vek{b_m}\}$ if and only if each $\vek{a_i}$ is a linear combination of $\vek{b_1},\ldots,\vek{b_m}$, and each $\vek{b_j}$ is a linear combination of $\vek{a_1},\ldots,\vek{a_m}$.

\noindent\newline{\bf Proof.} Let $V_A$ be a space spanned by $A$ and $V_B$ be a space spanned by $B$. {\it Necessity.} Assume $V_A=V_B$. If we take some $\vek{a_i}\in A\subseteq V_A$, then also, due to $V_A=V_B$ we have $\vek{v}\in V_B$ such that $\vek{v}=\vek{a_i}$. As $\vek{v}\in V_B$, that means it can be shown as a linear combination of $\vek{b_j}$. As $\vek{v}=\vek{a_i}$, that means $\vek{a_i}$ can be shown as a linear combination of $\vek{b_j}$. Proof is the same if we take $\vek{b_j}\in B\subseteq V_B$ first. {\it Sufficiency.} Assume each $\vek{a_i}$ is linear combination of vectors in $B$, and vice versa. Take $\vek{v}\in V_A$. Then, $\vek{v}$ can be shown as a linear combination of $\vek{a_i}$:

\begin{equation*}
\vek{v}=k_1\vek{a_1}+\cdots+k_m\vek{a_m}.
\end{equation*}

\noindent\newline But, as each $\vek{a_i}$ is a linear combination of vectors in $B$, we have:

\begin{equation*}
\vek{a_i}=k^{(i)}_1\vek{b_1}+k^{(1)}_2\vek{b_2}+\cdots+k^{(i)}_m\vek{b_m}.
\end{equation*}

\noindent\newline Thus, each $\vek{a_i}$ can be substituted to get:

\begin{equation*}
\vek{v}=k_1(k^{(1)}_1\vek{b_1}+k^{(1)}_2\vek{b_2}+\cdots+k^{(1)}_m\vek{b_m})+\cdots+k_m(k^{(m)}_1\vek{b_1}+k^{(m)}_2\vek{b_2}+\cdots+k^{(m)}_m\vek{b_m}).
\end{equation*}

\noindent\newline Then, it is easy to see that we can regroup vectors of linear combination of $\vek{v}$ and write:

\begin{equation*}
\vek{v}=k_1(k^{(1)}_1+k^{(2)}_1+\cdots+k^{(m)}_1)\vek{b_1}+\cdots+k_m(k^{(1)}_m+k^{(2)}_m+\cdots+k^{(m)}_m)\vek{b_m}.
\end{equation*}

\noindent\newline Therefore, $\vek{v}$ can be shown as a linear combination of $\vek{b_i}$ meaning $\vek{v}\in V_B$. Thus, as $\vek{v}\in V_A$ implies $\vek{v}\in V_B$ and same can be shown for vectors in $V_B$, we have $V_A=V_B$, meaning vector space spanned by $A$ is the same as space spanned by $B$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $U$ and $V$ be non-empty finite dimensional vector spaces over a field $F$, and let $h:U\rightarrow V$ be a linear transformation. Then:

\begin{enumerate}
\item The range of $h$ is a subspace of $V$. (It is called the range space of $h$.)
\item The kernel of $h$ is a subspace of $U$. (It is called the null-space of $h$.)
\item $h$ is injective if and only if the null space of $h$ is equal to $\{\vek{0}\}$.
\end{enumerate}

\noindent{\bf Proof.} {\it Ad $1$.} Let $\ran{h}=\{\vek{v}\in V:\ (\exists\vek{u}\in U)(h(\vek{u})=\vek{v})\}$. As $h$ is a function, and $U$ is non-empty, then it must be that at all $\vek{u}$ have at least one corresponding element $\vek{v}$ in $V$. So, we can safely say that $\ran{h}$ is non-empty, and that it is a subset of $V$, by its very definition. Take $\vek{v_1},\vek{v_2}\in\ran{h}$. Then, there exist $\vek{u_1},\vek{u_2}\in U$ such that $h(\vek{u_1})=\vek{v_1}$ and $h(\vek{u_2})=\vek{v_2}$. Adding those two equalities gives us $h(\vek{u_1})+h(\vek{u_2})=\vek{v_1}+\vek{v_2}$. As $h$ is a linear transformation, by definition, that is equivalent to $h(\vek{u_1}+\vek{u_2})=\vek{v_1}+\vek{v_2}$. But, that implies that $\vek{v_1}+\vek{v_2}\in\ran{h}$, i.e. $\ran{h}$ is closed with respect to vector addition. Take $\vek{v}\in\ran{h}$ and $k\in F$. Then, there exists $\vek{u}\in U$ such that $h(\vek{u})=\vek{v}$. If we multiply that by $k$, we get $k h(\vek{u})=k\vek{v}$. As $h$ is a linear transformation, that is equivalent to $h(k\vek{u})=k\vek{v}$, meaning $k\vek{v}\in\ran{h}$. As $\ran{h}$ is closed with respect to scalar multiplication also, it is a subspace of $V$.

{\it Ad $2$.} Let $\ker{h}=\{\vek{u}\in U:\ h(\vek{u})=\vek{0}\}$ be a kernel of $h$. It is easy to see that by definition $\ker{h}\subseteq U$. If we take some $h(\vek{u})=\vek{v}$, then $\vek{v}\in\ran{h}$, by definition of range space. But, as range of $h$ is a subspace of $V$, that means also $-\vek{v}\in\ran{h}$ ($\ran{h}$ with respect to addition is an Abelian group, by definition of vector space). That implies that there exists $\vek{u'}\in U$ such that $h(\vek{u'})=-\vek{v}$. This means $h(\vek{u})+h(\vek{u'})=\vek{v}-\vek{v}$, i.e. $h(\vek{u}+\vek{u'})=\vek{0}$. Thus we have proved that $\vek{u}+\vek{u'}\in\ker{h}$; in other words, $\ker{h}$ is non-empty. Take $\vek{u_1},\vek{u_2}\in\ker{h}$. Then, $h(\vek{u_1}+\vek{u_2})=h(\vek{u_1})+h(\vek{u_2})$ by definition of linear transformation. Also $h(\vek{u_1})+h(\vek{u_2})=\vek{0}+\vek{0}=\vek{0}$, as $\vek{u_1},\vek{u_2}\in\ker{h}$. That implies $h(\vek{u_1}+\vek{u_2})=\vek{0}$, i.e. $\vek{u_1}+\vek{u_2}\in\ker{h}$, which proves that $\ker{h}$ is closed with respect to vector addition. Let $\vek{u}\in\ker{h}$ and $k\in F$. Then, $h(k\vek{u})=k h(\vek{u})$, by definition of linear transformation. As $\vek{u}\in\ker{h}$, then $k h(\vek{u})=k\vek{0}=\vek{0}$, which implies $h(k\vek{u})=\vek{0}$, i.e. $h(k\vek{u})=\vek{0}$. That means $h(k\vek{u})\in\ker{h}$, i.e. $\ker{h}$ is closed with respect to scalar multiplication. Therefore, $\ker{h}$ is a subspace of $U$.

{\it Ad $3$.} {\it Necessity.} Assume $h$ is injective. That means that if $h(\vek{u_1})=h(\vek{u_2})$, then $\vek{u_1}=\vek{u_2}$. Assume that $\vek{u}\in\ker{h}$. We have $h(\vek{u})=\vek{0}$. As $\vek{u}\in\ker{h}$, then also it must be $\vek{u'}\in\ker{h}$ such that $\vek{u'}=-\vek{u}$. Then, $h(\vek{u})+h(\vek{u'})=h(\vek{u'})$. As $h$ is a homomorphism we have $h(\vek{u}+\vek{u'})=h(\vek{u'})$ which implies $\vek{u}+\vek{u'}=\vek{u'}$. That is equivalent to $\vek{u}=\vek{0}$. Our choice of $\vek{u}$ was arbitrary, so it must be that $\ker{h}=\{\vek{0}\}$. {\it Sufficiency.} As we assume $\ker{h}=\{\vek{0}\}$, then $\vek{0}\in\ker{h}$ implies $h(\vek{0})=\vek{0}$. That also implies that if we take $\vek{u},-\vek{u}\in U$, we will have $h(-\vek{u}+\vek{u})=h(\vek{0})$=\vek{0}. That means $h(-\vek{u})+h(\vek{u})=\vek{0}$, i.e. $h(-\vek{u})=-h(\vek{u})$. Take $h(\vek{u_1})=h(\vek{u_2})$. Then, $h(\vek{u_1})-h(\vek{u_2})=\vek{0}$. But, $-h(\vek{u_2})=h(-\vek{u_2})$ implies $h(\vek{u_1})+h(-\vek{u_2})=\vek{0}$, i.e. $h(\vek{u_1}-\vek{u_2})=\vek{0}$. From that follows $\vek{u_1}-\vek{u_2}\in\ker{h}=\{\vek{0}\}$, i.e. $\vek{u_1}-\vek{u_2}=\vek{0}$. Thus, $\vek{u_1}=\vek{u_2}$, which was deduced from $h(\vek{u_1})=h(\vek{u_2})$, means $h$ is injective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition\footnote{This proposition is practically the stronger version of previous lemma. The purpose of this is showing another way of proving it.}.} Let $U,V$ be finite dimensional vector spaces such that $U$ is a subspace of $V$. Let $B_U=\{\vek{u_1},\ldots,\vek{u_m}\}$, where $m\in\Z^{+}$ be a basis of $U$. Then, either there exists $n>0$ such that $B_V=\{\vek{u_1},\ldots,\vek{u_m},\vek{v_1},\ldots,\vek{v_n}\}$ is a basis of $V$, or we simply have $B_U=B_V$.

\noindent\newline{\bf Proof.} Let $U$ be a subspace of $V$ and let $B_U$ be defined as in proposition assumption. Take any $\vek{v_1}\in V-U$. If such $\vek{v_1}$ does not exist, we are done. If it were that $B_U\cup\{\vek{v_1}\}$ is linearly dependent, it would mean that $\vek{v_1}$ could be shown as a linear combination of $B_U$, i.e. $\vek{v_1}\in U$, which is contradiction to our assumption. If $B_U\cup\{\vek{v_1}\}$ spans $V$, we are done. If not, take $\vek{v_2}\in V-U$. If $B_U\cup\{\vek{v_1},\vek{v_2}\}$ is linearly dependent, then we take $\vek{v_3}$ instead, until we find $\vek{v_i}$ which makes $B_U\cup\{\vek{v_1},\vek{v_i}\}$ linearly independent. If such $\vek{v_i}$ does not exist, that would mean that all $\vek{v_2},\ldots,\vek{v_j}\in V-U$ (where $j$ is some natural number) can be shown as linear combination of $B_U\cup\{\vek{v_1}\}$, i.e. that $B_U\cup\{\vek{v_1}\}$ spans $V$ (as the rest of $V$ is in $U$, they are spanned by $B_U$), which is contradiction to assumption that it does not span $V$. Therefore such $\vek{v_i}$ must exist, and we have $B_U\cup\{\vek{v_1},\vek{v_i}\}$. We repeat the process until $B_U\cup\{\vek{v_1},\ldots,\vek{v_n}\}$ spans $V$ and is linearly independent (this process guarantees both).

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $U$ and $V$ be finite-dimensional vector spaces over $F$, and let $h:U\rightarrow V$ be a linear transformation. Then $\dim{U}=\dim{\ker{h}}+\dim{\ran{h}}$.

\noindent\newline{\bf Proof.} By previous theorem, if $\base{\ker{h}}=\{\vek{u_1},\ldots,\vek{u_m}\}$ is basis of $\ker{h}$, then it can be extended to basis of $U$ such that $\base{U}=\{\vek{u_1},\ldots,\vek{u_m},\vek{u_{m+1}},\ldots,\vek{u_{m+n}}\}$. Let $S=\base{U}-\base{\ker{h}}$. Let us take $\vek{v}\in\ran{h}$. As $\vek{v}$ is in range of $h$, then there exists $\vek{u}\in U$ such that $h(\vek{u})=\vek{v}$. As $\vek{u}\in U$, it can be shown as a linear combination of vectors in $\base{U}$, i.e.

\begin{equation*}
\vek{u}=k_1\vek{u_1}+\cdots+k_m\vek{u_m}+k_{m+1}\vek{u_{m+1}}+\cdots+k_{m+1}\vek{u_{m+1}}.
\end{equation*}

\noindent\newline As $h$ is a well-defined function, from equality above, we can get:

\begin{equation*}
h(\vek{u})=h(k_1\vek{u_1}+\cdots+k_m\vek{u_m}+k_{m+1}\vek{u_{m+1}}+\cdots+k_{m+1}\vek{u_{m+1}}).
\end{equation*}

\noindent\newline As $h$ is a homomorphism that is equivalent to:

\begin{equation*}
h(\vek{u})=h(k_1\vek{u_1})+\cdots+h(k_m\vek{u_m})+h(k_{m+1}\vek{u_{m+1}})+\cdots+h(k_{m+1}\vek{u_{m+1}}).
\end{equation*}

\noindent\newline As $\vek{u_1},\ldots,\vek{u_m}\in\ker{h}$ we have $h(\vek{u_1})=\ldots=h(\vek{u_m})=\vek{0}$. Also, as $h(\vek{u})=\vek{v}$ we have:

\begin{equation*}
\vek{v}=h(k_{m+1}\vek{u_{m+1}})+\cdots+h(k_{m+1}\vek{u_{m+1}}).
\end{equation*}

\noindent\newline In other words, we can show any $\vek{v}\in\ran{h}$ as a linear combination of vectors in $h(S)$, where $S=\base{U}-\base{\ker{h}}$. Now assume $h(S)=\{h(\vek{u_{m+1}}),\ldots,h(\vek{u_{m+n}})\}$ is linearly dependent. That means that some $\vek{u_{m+i}}$ can be shown as a linear combination of other vectors in $h(S)$. Assume $i=1$, for simplicity. Then:

\begin{equation*}
h(\vek{u_{m+1}})=k_2 h(\vek{u_{m+2}})+\cdots+k_n h(\vek{u_{m+n}}).
\end{equation*}

\noindent\newline We can rearrange that to get:

\begin{equation*}
k_2 h(\vek{u_{m+2}})+\cdots+k_n h(\vek{u_{m+n}})-h(\vek{u_{m+1}})=\vek{0}.
\end{equation*}

\noindent\newline As $h$ is a homomorphism, expression above is equivalent to:

\begin{equation*}
h(k_2\vek{u_{m+2}}+\cdots+k_n\vek{u_{m+n}}-\vek{u_{m+1}})=\vek{0}.
\end{equation*}

\noindent\newline But, that would mean that $\vek{u}=k_2\vek{u_{m+2}}+\cdots+k_n\vek{u_{m+n}}-\vek{u_{m+1}}\in\ker{h}$. As it is in $\ker{h}$, it can be shown as a linear combination of vectors in $\base{\ker{h}}=\{u_1,\ldots,u_m\}$, that is,

\begin{equation*}
k_2\vek{u_{m+2}}+\cdots+k_n\vek{u_{m+n}}-\vek{u_{m+1}}=l_1\vek{u_1}+\cdots+l_m\vek{u_m}.
\end{equation*}

\noindent\newline If we rearrange elements in above equality to get expression for $\vek{u_{m+1}}$, we have:

\begin{equation*}
\vek{u_{m+1}}=k_2\vek{u_{m+2}}+\cdots+k_n\vek{u_{m+n}}-l_1\vek{u_1}-\cdots-l_m\vek{u_m}.
\end{equation*}

\noindent\newline That would imply that $\vek{u_{m+1}}$ can be shown as a linear combination of vectors:

\begin{equation*}
\{\vek{u_1},\ldots,\vek{u_m},\vek{u_{m+2}},\ldots,\vek{u_{m+n}}\}=\base{U}-\{\vek{u_{m+1}}\}.
\end{equation*}

\noindent\newline That would mean that $\base{U}$ is not linearly independent, which is a contradiction to assumption that $\base{U}$ is a basis of $U$. Therefore, set $h(S)$ must be linearly independent. As $h(S)$ spans $\ran{h}$ and is linearly independent, it is a basis of $\ran{h}$, i.e. $\base{\ran{h}}=h(\base{U}-\base{\ker{h}})$. By definition of $\ran{h}$, every element has an original in $U$. Let $h(\vek{u_i})=h(\vek{u_j})$ and assume $i\neq j$, for some $\vek{u_i},\vek{u_j}\in S$. We will have $h(\vek{u_i})-h(\vek{u_j})=\vek{0}$. As $h$ is a homomorphism that would imply $h(\vek{u_i}-\vek{u_j})=\vek{0}$, i.e. $\vek{u_i}-\vek{u_j}\in\ker{h}$. That would mean $\vek{u_i}=\vek{u_j}+k_1\vek{u_1}+\cdots+k_m\vek{u_m}$. But, as $\vek{u_i}\in S=\base{U}-\base{\ker{h}}$, i.e. $\vek{u_i}$ is in basis of $U$, just as $\vek{u_j}$, that would mean $\vek{u_i}$ can be shown as a linear combination of vectors in $\base{U}$, i.e. $\base{U}$ is not linearly independent. Therefore, it must be $\vek{u_i}=\vek{u_j}$, i.e. $\dim{\ran{h}}=|h(S)|=|S|=\dim{U}-\dim{\ker{h}}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $U$ and $V$ be finite-dimensional vector spaces over $F$ such that $\dim{U}=\dim{V}$ and let $h:U\rightarrow V$ be a linear transformation. Then, $h$ is injective if and only if $h$ is surjective.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $h$ is injective. Then, by previous proposition, we have $\dim{\ker{h}}=0$. So, from previous theorem we have $\dim{U}=\dim{\ker{h}}+\dim{\ran{h}}=0+\dim{\ran{h}}=\dim{\ran{h}}$. From assumption we have:

\begin{equation*}
\dim{\ran{h}}=\dim{U}=\dim{V}.
\end{equation*}

\noindent\newline Then, $\dim{\ran{h}}=\dim{V}$ implies that $h$ is surjective. {\it Sufficiency.} Assume $h$ is surjective. Then, $\dim{V}=\dim{\ran{h}}$ and we have $\dim{U}=\dim{V}=\dim{\ran{h}}$. From previous theorem we have:

\begin{equation*}
\dim{\ran{h}}=\dim{\ker{h}}+\dim{\ran{h}}.
\end{equation*}

\noindent\newline In other words, $\dim{\ker{h}}=0$. Previous proposition then implies that $h$ is injective.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $U$ and $V$ be vector spaces over $F$. Let $h:U\rightarrow V$ be a homomorphism and let $h$ be injective. If $S$ is a linearly independent subset of $U$, then $h(S)$ is a linearly independent subset of $V$.

\noindent\newline{\bf Proof.} Assume that $S$ is linearly independent subset of $U$. Let us assume $h(S)$ is linearly dependent, i.e. $k_1 h(\vek{a_1})+\cdots+k_m h(\vek{a_m})=\vek{0}$ and not all $k_i$ are equal to zero. As $h$ is a homomorphism that is equal to $h(k_1\vek{a_1}+\cdots+k_m\vek{a_m})=\vek{0}$. That means that $k_1\vek{a_1}+\cdots+k_m\vek{a_m}\in\ker{h}$. But, as $h$ is injective, due to the previous proposition, $\ker{h}=\{\vek{0}\}$, and so it must be that $k_1\vek{a_1}+\cdots+k_m\vek{a_m}=\vek{0}$. But, as $\vek{a_1},\ldots,\vek{a_m}\in U$ and $k_1,\ldots,k_m\in F$, and not all equal zero, we have that vectors $\vek{a_1},\ldots,\vek{a_m}\in U$ are linearly dependent. If $U$ contains linearly dependent set, it cannot be linearly independent, which is a contradiction to our assumption. Therefore, $h(S)$ must be linearly independent also.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $U$ and $V$ be vector spaces over $F$. Let $h:U\rightarrow V$ be a homomorphism. Then, $h$ is injective if and only if $\dim{U}=\dim{h(U)}$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $h$ is injective. Then, by previous proposition we have $\dim{\ker{h}}=0$. From previous theorem, $\dim{U}=\dim{\ker{h}}+\dim{\ran{h}}$, and, as $\ran{h}=h(U)$ by definition, we have $\dim{U}=\dim{h(U)}$. {\it Sufficiency.} Assume $\dim{U}=\dim{h(U)}$. As $h(U)=\ran{h}$, from previous theorem we have, $\dim{h(U)}=\dim{\ker{h}}+\dim{h(U)}$, meaning $\dim{\ker{h}}=0$. In other words, $\ker{h}=\{\vek{0}\}$. So, it must be that $h$ is injective, by previous proposition.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $U$ and $V$ be vector spaces over the field $F$, and let $h:U\rightarrow V$ be a homomorphism. We say that $h$ is an {\bf isomorphism} if $h$ is bijective.

\noindent\newline{\bf Proposition.} Let $U$ and $V$ be vector spaces over $F$ such that $\dim{U}=\dim{V}$. Let $h:U\rightarrow V$ be a homomorphism. Then, following statements are equivalent:

\begin{itemize}
\item $h$ is injective;
\item $h$ is surjective;
\item $h$ is an isomorphism.
\end{itemize}

\noindent{\bf Proof.} By previous proposition we have that $h$ is injective if and only if $h$ is surjective. So, if $h$ is surjective, it is also injective, and then it follows that it is an isomorphism. If it is an isomorphism, it implies that $h$ is both injective and surjective. If $h$ is injective, it is also surjective, which implies it is an isomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $V$ be a finite dimensional vector space over $F$. Then, each vector in $V$ can be uniquely written as a linear combination of vectors in $\base{V}$.

\noindent{\bf Proof.} Assume $\vek{v}\in V$ and $\base{V}=\{\vek{v_1},\ldots,\vek{v_m}\}$. That means $\dim{V}=m$. As $\base{V}$ is basis of $V$, then $\vek{v}$ can be shown as a linear combination of vectors in $V$, i.e.

\begin{equation*}
\vek{v}=k_1\vek{v_1}+\cdots+k_m\vek{v_m},
\end{equation*}

\noindent\newline where $k_1,\ldots,k_m\in F$. Assume it can also be written as:

\begin{equation*}
\vek{v}=l_1\vek{v_1}+\cdots+l_m\vek{v_m},
\end{equation*}

\noindent\newline where $l_1,\ldots,l_m\in F$. Subtracting latter and former equalities we get

\begin{equation*}
\vek{0}=(k_1-l_1)\vek{v_1}+\cdots+(k_m-l_m)\vek{v_m}.
\end{equation*}

\noindent\newline But, as $\vek{v_1},\ldots,\vek{v_m}\in\base{V}$, they are linearly independent, so it must be $k_1-l_1=\ldots=k_m-l_m=0$, i.e. $k_1=l_1,\ldots,k_m=l_m$. In other words, each vector $\vek{V}\in V$ can be uniquely written as a linear combination of vectors in $\base{V}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Lemma.} Let $V$ be a vector space over $F$ and $\dim{V}=m$. Then, $F^m$ is also a vector space over $F$ with $\dim{F^m}=m$.

\noindent\newline{\bf Proof.} As $(F,+,\cdot)$ is a field, then $(F,+)$ is an abelian group. Then, by previous proposition, $(F,+)^m=(F^m,+_m)$ is also an Abelian group. Let us show $(F^m,+_m,\cdot_m)$, which we will designate as $F^m$ with $+$ and $\cdot$, over $F$ is a vector space. Note that $k\cdot_m(k_1,\ldots,k_m)=(k k_1,\ldots,k k_m)\in V$. Take $(k_1,\ldots,k_m),(l_1,\ldots,l_m)\in F^m$ and $k,l\in F$. Then,

\begin{equation*}
k((k_1,\ldots,k_m)+(l_1,\ldots,l_m))=(k k_1+k l_1,\ldots,k k_m+k l_m)=k(k_1,\ldots,k_m)+k(l_1,\ldots,l_m).
\end{equation*}

\noindent\newline Similarly, we have:

\begin{eqnarray*}
(k+l)(k_1,\ldots,k_m)&=&((k+l)k_1,\ldots,(k+l)k_m)\\
&=&(k k_1+l k_1,\ldots,k k_m+l k_m)=k(k_1,\ldots,k_m)+l(k_1,\ldots,k_m).
\end{eqnarray*}

\noindent\newline It is easy to see that:

\begin{equation*}
(k (l(k_1,\ldots,k_m)))=k(l k_1,\ldots,l k_m)=(k l k_1,\ldots,k l k_m)=(k l)(k_1,\ldots, k_m).
\end{equation*}

\noindent\newline Finally,

\begin{equation*}
1(k_1,\ldots,k_m)=(1k_1,\ldots,1k_m)=(k_1,\ldots,k_m).
\end{equation*}

\noindent\newline Therefore $F^m$ over $F$ is a vector space. Take any $k_1,\ldots,k_m\in F-\{0\}$ such that $k_i\neq k_j$ for $i\neq j$. Then let:

\begin{equation*}
B=\{(k_1,0,\ldots,0),(0,k_2,\ldots,0),\ldots,(0,0,\ldots,k_m)\}.
\end{equation*}

\noindent\newline Take $l_1,\ldots,l_m\in F$. If $\vek{0}=(l_1 k_1,l_2 k_2,\ldots,l_m k_m)$, we have $l_i k_i=0$, and as $F$ is a field and by that also an integral domain, there are no zero divisors and it must be $l_i=k_i=0$, due to surely $k_i\neq 0$. Therefore, $B$ is linearly independent. Now take $(l_1,\ldots,l_m)\in F^m$. We can take $(l_1,\ldots,l_m)=h_1(k_1,0,\ldots,0)+\cdots+h_m(0,0,\ldots,k_m)$, where $h_i=k_i^{-1} l_i$ (as $F$ is field there exists such $k_i^{-1}$). Thus, $\base{F^m}=B$ and $\dim{F^m}=m$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $V$ be a vector space such that $\dim{V}=m$. Then, there exists an isomorphism $h:V\rightarrow F^m$.

\noindent\newline{\bf Proof.} We have shown that $F^m$ is a vector space over $F$ and $\dim{F^m}=m$. Now, let $\base{V}=\{\vek{v_1},\ldots,\vek{v_m}\}$. Take any $\vek{v}\in V$, and it can be, by previous lemma, uniquely represented as $\vek{v}=k_1\vek{v_1}+\cdots+k_m\vek{v_m}$ Now, define $h(\vek{v})=(k_1,\ldots,k_m)$. Assume $h(\vek{v})=h(\vek{w})$, where $\vek{w}=l_1\vek{v_1}+\cdots+l_m\vek{v_m}$. That implies $(k_1,\ldots,k_m)=(l_1,\ldots,l_m)$, i.e. $k_i=l_i$, and by that $\vek{v}=\vek{w}$. Therefore, $h$ is injective and by previous proposition, it is an isomorphism.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} Let $U$ and $V$ be subspaces of vector space $T$. The {\bf sum of vector spaces} $U$ and $V$, denoted by $U+V$, is the set of all vectors $\vek{u}+\vek{v}$, where $\vek{u}\in U$ and $\vek{v}\in V$.

\noindent\newline{\bf Proposition.} Let $U$ and $V$ be subspaces of $T$ over $F$. Then, $U+V$ and $U\cap V$ are subspaces of $T$.

\noindent\newline{\bf Proof.} Take $\vek{u}+\vek{v}\in U+V$. As $\vek{u}\in U\subseteq T$, and $\vek{v}\in V\subseteq T$, then $\vek{u}+\vek{v}\in T$, meaning $U+V\subseteq T$. Let us take $\vek{u_1}+\vek{v_1},\vek{u_2}+\vek{v_2}\in U+V$. Adding those two vectors yields $\vek{w}=(\vek{u_1}+\vek{u_2})+(\vek{v_1}+\vek{v_2})$. As $\vek{u_1}+\vek{u_2}\in U$ (as $U$ is closed with respect to addition), and same holds for $\vek{v_1}+\vek{v_2}$ and $V$, then, $\vek{w}\in U+V$. Take $k\in F$. Then $k(\vek{u}+\vek{v})=(k\vek{u})+(k\vek{v})$. But, as $U$ and $V$ are subspaces of $T$ over field $F$ then $k\vek{u}\in U$ and $k\vek{v}\in V$. Therefore $k(\vek{u}+\vek{v})\in U+V$. All summed up, $U+V$ is a subspace of $T$. Let us take $\vek{w}\in U\cap V$. That means $\vek{w}\in U$ and $\vek{w}\in V$. Both are subsets of $T$, so $\vek{w}\in T$, meaning $U\cap V\subseteq T$. Take $\vek{w_1},\vek{w_2}\in U\cap V$. Then, as $\vek{w_1},\vek{w_2}\in U$, it is also that $\vek{w_1}+\vek{w_2}\in U$, as $U$ is closed with respect to vector addition. Same reasoning leads to $\vek{w_1}+\vek{w_2}\in V$. Therefore, $\vek{w_1}+\vek{w_2}\in U\cap V$. Also $k\vek{w}\in U$ as $\vek{w}\in U$, because $U$ is closed with respect to scalar multiplication. Also, $k\vek{w}\in V$, meaning $k\vek{w}\in U\cap V$, meaning $U\cap V$ is a subspace of $T$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} $T$ is said to be the {\bf direct sum of vector spaces} $U$ and $V$, if $T=U+V$ and $U\cap V=\{\vek{0}\}$. In that case, we write $T=U\oplus V$.

\noindent\newline{\bf Proposition.} Let $U$ and $V$ be subspaces of $T$. Then, $U\oplus V=T$ if and only if every $\vek{t}\in T$ can be uniquely written as $\vek{t}=\vek{u}+\vek{v}$, where $\vek{u}\in U$ and $\vek{v}\in V$.

\noindent\newline{\bf Proof.} {\it Necessity.} Assume $U\oplus V=T$. Then, choose $\vek{t}=\vek{u}+\vek{v}\in U\oplus V$. Assume $\vek{t}=\vek{u'}+\vek{v'}\in U\oplus V$. That means $\vek{u'}+\vek{v'}=\vek{u}+\vek{v}$, i.e. $\vek{u'}-\vek{u}=\vek{v}-\vek{v'}$, As $\vek{w}=\vek{u'}-\vek{u}\in U$, and $\vek{w}=\vek{v}-\vek{v'}\in V$, it must be that $\vek{w}\in U$ and $\vek{w}\in V$. But, by assumption $U\cap V=\{\vek{0}\}$, meaning $\vek{w}=\vek{0}$. That implies $\vek{0}=\vek{u'}-\vek{u}$ and $\vek{0}=\vek{v}-\vek{v'}$; in other words $\vek{u}=\vek{u'}$ and $\vek{v}=\vek{v'}$. {\it Sufficiency.} Assume that each $\vek{t}\in T$ can be uniquely written as $\vek{u}+\vek{v}\in U+V$. That means $T\subseteq U+V$. Take $\vek{u}+\vek{v}\in U+V$. It is obvious that, as $U\subseteq T$ and $V\subseteq T$, that $\vek{u}+\vek{v}\in T$. Therefore, $U+V\subseteq T$, implying $U+V=T$. Assume there exists $\vek{t}\in T$ such that $\vek{t}\in U\cap V$. Assume that $\vek{t}=\vek{u}+\vek{v}$. But, we can write $\vek{t}=\vek{0}+\vek{t}$, as $\vek{0}\in U$ and $\vek{t}\in V$. That means it must be that $\vek{u}=\vek{0}$. Similarly, we can write $\vek{t}=\vek{t}+\vek{0}$, because $\vek{t}\in U$ and $\vek{0}\in V$, but that implies $\vek{v}=\vek{0}$. Therefore, $\vek{t}=\vek{0}+\vek{0}=\vek{0}$ and $U\cap V=\{\vek{0}\}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $U$ be a subspace of finite dimensional vector space $T$. Then, there exists subspace $V$ of $T$ such that $\dim{V}=\dim{T}-\dim{U}$ and $T=U\oplus V$.

\noindent\newline{\bf Proof.} Assume $\base{T}=\{\vek{t_1},\ldots,\vek{t_m}\}$, where $\dim{T}=m$. As $U$ is subspace of $T$ then $\base{U}=\{\vek{t_1},\ldots,\vek{t_n}\}$, where $\dim{U}=n$. Let $V$ be a subspace generated by $B=\{\vek{t_{n+1}},\ldots,\vek{t_m}\}$. Then, $\dim{V}=m-n$. Now, let us take $\vek{t}\in T$. Then, we can write $\vek{t}$ uniquely as:

\begin{equation*}
\vek{t}=k_1\vek{t_1}+\cdots+k_n\vek{t_n}+k_{n+1}\vek{t_{n+1}}+\cdots+k_m\vek{t_m}.
\end{equation*}

\noindent\newline It is easy to see that:

\begin{eqnarray*}
k_1\vek{t_1}+\cdots+k_n\vek{t_n}&=&\vek{u}\in U\\
k_{n+1}\vek{t_{n+1}}+\cdots+k_m\vek{t_m}&=&\vek{v}\in V.
\end{eqnarray*}

\noindent\newline That means $\vek{t}=\vek{u}+\vek{v}$, where $\vek{u}\in U$ and $\vek{v}\in V$, and as writing down $\vek{t}$ as linear combination of vectors in $\base{T}$ is unique, then $\vek{t}$ is also uniquely written as sum of $\vek{u}$ and $\vek{v}$. By previous proposition, that implies $T=U\oplus V$, and at the same time we have $\dim{V}=\dim{T}-\dim{U}$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} If $U$ and $V$ are subspaces of finite dimensional vector space $T$, then:

\begin{equation*}
\dim{U+V}=\dim{U}+\dim{V}-\dim{U\cap V}.
\end{equation*}

\noindent\newline{\bf Proof.} Let $\vek{u}+\vek{v}\in U+V$. Then, if we consider $\dim{U}=m$ and $\dim{V}=n$, we can show $\vek{u}$ as a linear combination of $m$ vectors in $\base{U}$ and $\vek{v}$ as linear combination of $n$ vectors in $\base{V}$. But, as some of these base vectors can be both in $\base{U}$ and $\base{V}$, assume $k$ of them, adding $\vek{u}$ and $\vek{v}$ would mean that $\vek{u}+\vek{v}$ is written in $m+n-k$ number of vectors, in a unique manner. Those vectors, defining $\vek{u}+\vek{v}$, are also linearly independent (as vectors in $\base{U}$ and $\base{V}$ are), and they span $U+V$ (as taking any $\vek{u}+\vek{v}$, we can show it as linear combination of vectors from $\base{U}$ and $\base{V}$), so they form $\base{U+V}$. The dimension of this basis is $m+n-k=\dim{U}+\dim{V}-\dim{U\cap V}$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Degrees of field extensions}
\end{center}

\vskip 0.5cm

\noindent{\bf Proposition.} Let $E$ be a field, $F\leq E$ and $c\in E$. If $c$ is algebraic over $F$, then $F(c)$ is a vector space over $F$.

\noindent\newline{\bf Proof.} As $F(c)$ is a field, it is also an abelian group with respect to addition. If we take $k,l\in A$, $a(c),b(c)\in F(c)$, then it is obvious that $k a(c)=k(a_m c^m+\cdots+a_1 c+a_0)=(k a_m)c^m+\cdots+(k a_1)c+(k a_0)\in F(c)$, as $k a_i\in F$. Also, it is easy to see that $k(a(c)+b(c))=k a(c)+k b(c)$, $(k+l)a(c)=(k+l)(a_m c^m+\cdots+a_1 c+a_0)=(k+l)a_m c^m+\cdots+(k+l)a_1 c+(k+l)a_0=k a_m c^m+l a_m c^m+\cdots+k a_1 c+l a_1 c+k a_0+l a_0=k a(c)+ l a(c)$. Also, $k(l a(c))=k(l a^m c_m+\cdots+l a_1 c+l a_0)=(k l) a^m c_m+\cdots+(k l)a_1 c+(k l)a_0=(k l)a(c)$. Finally, $1a(c)=a(c)$ is trivial.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If extension $E$ of field $F$ is a vector space over $F$ with $\dim{E}=m$, we say that $E$ is an {\bf extension of degree $m$} over $F$, which we symbolize by writing $[E:F]=m$. If the dimension of vector space $E$ over $F$ is finite, we say that $E$ is a {\bf finite extension} of $F$. 

\noindent\newline{\bf Theorem.} Let $E$ be a finite field, $F\leq E$ and let $c\in E$ be algebraic over $F$. Then the degree of $F(c)$ is equal to the degree of the minimum polynomial of $c$ over $F$.

\noindent\newline{\bf Proof.} Let $p(x)\in F[x]$ be a minimum polynomial of $c$ over $F$. Minimal polynomial (of finite degree) exists (by definition) due to assumption that $c$ is algebraic over $F$. That means that $p(x)$ is monic, $p(c)=0$ and there exist no polynomial of lesser degree satisfying same conditions. Assume $p(x)=c^{m+1}+p_m c^m+\cdots+p_1 c+p_0$. Then, $\deg{p(x)}=m+1$. We will prove that $B=\{1,c,\ldots,c^m\}$ is basis of $F(c)$ and that $\dim{F(c)}=m+1$. Let us take $a(c)=a_n c^n+\cdots+a_1 c+a_0\cdot 1\in F(c)$. Then there exists $a(x)\in F[x]$ with same coefficients $a_i$. Assume $\deg{a(x)}=n>m$. Using division algorithm we get $a(x)=p(x)q(x)+r(x)$, where $0\leq\deg{r(x)}<\deg{p(x)}=m+1$. But, as $p(c)=0$, then $a(c)=0q(c)+r(c)$, i.e. $a(c)=r(c)$. That implies $n=\deg{a(x)}<\deg{r(x)}=m+1$, i.e. $n\leq m$, which is a contradiction. Therefore, it must be that $\deg{a(x)}<\deg{r(x)}$, and $a(c)$ is of form $a(c)=a_m c^m+\cdots+a_1 c+a_0$. In other words, $a(c)$ is a linear combination of elements in $B$. Assume $B$ is linearly dependent, i.e. $k_m c^m+\cdots+k_1 c+k_0=0$ and not all $k_i$ are equal to zero. Let $k(x)=k_m x^m+\cdots+k_1 x+k_0$. We do not know if $k_m$, $k_{m-1}$ and so on are equal to zero, so we can say at best $\deg{k(x)}\leq m$. As $k(c)=0$ and because $p(x)$ is minimal polynomial of $c$ over $F$, it must be that $m+1=\deg{p(x)}\leq\deg{k(x)}\leq m$, which is a contradiction. It can only be that degree of $k(x)$ is undefined, i.e. that $k(x)=0$. That of course implies $k_m=\ldots=k_1=k_0=0$, i.e. $B$ is linearly independent. Therefore, $B=\base{F(c)}$. The number of elements in $\base{F{c}}$ is $m+1$, the same as degree of minimum polynomial of $c$ over $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} Let $H\trianglerighteq G\trianglerighteq F$ be finite fields. Then, $[H:F]=[H:G][G:F]$.

\noindent\newline{\bf Proof.} Let $\{a_1,\ldots,a_m\}$ be a basis of $G$ over $F$ and $\{b_1,\ldots,b_n\}$ be a basis of $H$ over $G$. Then, $[G:F]=m$ and $[H:G]=n$. We will show that the set $B=\{a_i b_j:\ i\in\{1,\ldots,m\},\ j\in\{1,\ldots,n\}\}$ is a basis of $H$ over $F$. Assume $B$ is linearly dependent. Then, $\sum{k_{i,j}a_i b_j}=0$ and not all $k_{i,j}$ are equal to zero. We have:

\begin{equation*}
a_1(k_{1,1}b_1+\cdots+k_{1,n}b_n)+\cdots+a_m(k_{m,1}b_1+\cdots+k_{m,n}b_n)=0.
\end{equation*}

\noindent\newline But, as $a_1,\ldots,a_m$ are linearly independent, it must be that:

\begin{eqnarray*}
k_{1,1}b_1+k_{1,2}b_2+\cdots+k_{1,n}b_n&=&0,\\
k_{2,1}b_1+k_{2,2}b_2+\cdots+k_{2,n}b_n&=&0,\\
&\vdots&\\
k_{m,1}b_1+k_{m,2}b_2+\cdots+k_{m,n}b_n&=&0.
\end{eqnarray*}

\noindent\newline But, as $b_1,\ldots,b_n$ are also linearly independent, then all coefficients $k_{i,j}$ are equal to zero, meaning $a_i b_j$ are linearly independent. Let us take $\vek{h}\in H$. As $H$ is a vector space over $G$, then we can write $\vek{h}=b_1 k_1+\cdots+b_n k_n$, where $k_j\in G$. But, as $G$ is a vector space over $F$, we can write $k_j=l^{(j)}_1 a_1+\cdots+l^{(j)}_m a_m$. It is obvious that putting these expressions in $\vek{h}$ we will get:

\begin{equation*}
\vek{h}=b_1(l^{(1)}_1 a_1+\cdots+l^{(1)}_m a_m)+\cdots+b_n(l^{(n)}_1 a_1+\cdots+l^{(n)}_m a_m)=\sum{l^{(j)}_i a_i b_j}.
\end{equation*}

\noindent\newline That proves that $B$ spans $H$ over $F$, meaning $B$ is a basis of $H$ over $F$. It is easy to see that $[H:F]=m n=[G:F][H:G]$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Definition.} If $c$ is algebraic over $F$, we say that $F(c)$ is obtained by {\bf adjoining} $c$ to $F$. An extension $F(c)$ formed by adjoining a single element to $F$ is called a {\bf simple extension} of $F$. An extension $F(c_1,\ldots,c_m)$, formed by adjoining a finite number of elements  $c_1,\ldots,c_m$ is called an {\bf iterated extension}.

\noindent\newline{\bf Proposition.} Let $F$ be a field and let $c_1,\ldots,c_m$ be algebraic over $F$. Then, iterated extension $F(c_1,\ldots,c_m)$ is the smallest field containing $c_1,\ldots,c_m$ and $F$.

\noindent\newline{\bf Proof.} We have that, by previous proposition, $F(c_1)$ is a smallest field containing $c_1$ and $F$. Now, $F(c_1)$ itself being a field, $F(c_1,c_2)$ is smallest field containing $F(c_1)$ and $c_2$. Assume $G$ is smallest field containing $F$, $c_1$ and $c_2$. Then it must be that $F\subseteq G$ and $c_1,c_2\in G$. But, as $F$ and $c_1$ are in $G$, it must be that $F(c_1)\subseteq G$. Now, as $F(c_1)$ and $c_2$ are also in $G$, then, it must be that $F(c_1,c_2)\subseteq G$. But, by definition, $F(c_1,c_2)$ is the smallest field containing $F$, $c_1$ and $c_2$, so it must be $F(c_1,c_2)=G$ (due to are assumption that $G$ is smallest field containing $F$, $c_1$ and $c_2$). The rest of proof follows inductively.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} Let $F$ be a finite field. If $c_1,\ldots,c_m$ are algebraic over $F$, then iterated extension $F(c_1,\ldots,c_m)$ is a finite extension of $F$.

\noindent\newline{\bf Proof.} By previous theorem, as $c_1$ is algebraic over $F$, and as $F$ is finite, then dimension of $F(c_1)$ is finite. So, by definition, $F(c_1)$ is a finite extension of $F$. Again, as $c_2$ is algebraic over $F$, it is certainly algebraic over $F(c_1)$. Let us elaborate: as $c_2$ is algebraic over $F$, there exists polynomial $p(x)$ with coefficients in $F$ such that $p(c_2)=0$. But, coefficients of $F$ are also in $F(c_1)$, so $c_2$ is also algebraic over $F(c_1)$. Then, using previous theorem, dimension of $F(c_1,c_2)$ is finite. The rest of the proof goes inductively.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition\footnote{Converse of former proposition.}.} Every finite extension is an iterated extension.

\noindent\newline{\bf Proof.} Assume that $E$ is a finite extension of $F$. That means that, taking $E$ as a vector space over $F$, $\dim{E}=m$, where $m\in\Z^{+}$. Then, $\base{E}=\{\vek{v_1},\ldots,\vek{v_m}\}$ and each $\vek{v}\in E$ can be written as $\vek{v}=k_1\vek{v_1}+\cdots+k_m\vek{v_m}$. So, $E$ contains both $F$ and $\vek{v_1},\ldots,\vek{v_m}$. Let us observe $F(\vek{v_1},\ldots,\vek{v_m})$. It is obvious that $\vek{v}\in E$, written as above, implies $\vek{v}\in F(\vek{v_1},\ldots,\vek{v_m})$. In other words, $E\subseteq F(\vek{v_1},\ldots,\vek{v_m})$ and also $\vek{v_1},\ldots,\vek{v_m}\in F(\vek{v_1},\ldots,\vek{v_m})$. But, as $F(\vek{v_1},\ldots,\vek{v_m})$ is the smallest field containing $F$ and $\vek{v_1},\ldots,\vek{v_m}$, then $E\subseteq F(\vek{v_1},\ldots,\vek{v_m})$ implies $E=F(\vek{v_1},\ldots,\vek{v_m})$. I.e. $E$ is an iterated extension of $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Theorem.} If $E$ is a finite extension of $F$, every element in $E$ is algebraic over $F$.

\noindent\newline{\bf Proof.} Let $\vek{v}\in E$ and $\dim{E}=m$. Then, $V=\{\vek{1},\vek{v},\vek{v}^2,\ldots,\vek{v}^m\}$ is linearly dependent by previous proposition, because $|V|=m+1>m=\dim{E}$ and $V\subseteq E$ (easy to see, as $E$ is closed with respect to multiplication of vectors, so also $\vek{v}^i$ are also in $E$). That means that $k_m\vek{v}^m+\cdots+k_1\vek{v}+k_0\vek{1}=\vek{0}$, where not all $k_0,k_1,\ldots,k_m$ are equal to zero. Note that $k_0,k_1,\ldots,k_m\in F$ (as $E$ is a vector space over $F$). If we take $p(x)=k_m x^m+\cdots+k_1 x+k_0$, we see that $p(\vek{v})=\vek{0}$, i.e. $\vek{v}$ is the root of polynomial $p(x)\in F[x]$. That means that $\vek{v}$ is algebraic over $F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Remark.} It is easy to see that elements in $F(c_1,\ldots,c_m)$ are of the form:

\begin{equation*}
\sum_{i_1,\ldots,i_m}{k_{i_1,\ldots,i_m}c_1^{i_1}\cdots c_m^{i_m}}.
\end{equation*}

\noindent\newline{\bf Problem.} Find a basis for $\Q(i\sqrt{2})$ over $\Q$ and describe the elements of $\Q(i\sqrt{2})$.

\noindent\newline{\bf Solution.} Take $x=i\sqrt{2}$. By squaring this equality, we get $x^2=-2$, i.e. $x^2+2=0$. Is $x^2+2$ irreducible over $\Q$? There exists $2\in P$ such that $2\nmid 1$ (the leading coefficient) and $2^2=4\nmid 2$ (the free member), while $2|2$ (the free member) and $2|0$ (the coefficient in $x$), so by Eisenstein's criterion, $x^2+2$ is irreducible over $\Q$. From previous theorem, we showed that, if $p(x)=x^2+2$ is minimum polynomial of $i\sqrt{2}$ over $\Q$, then $B=\base{Q(i\sqrt{2})}=\{1,i\sqrt{2}\}$. If we take $\vek{v}\in\Q(i\sqrt{2})$, then, as $B$ spans $\Q(i\sqrt{2})$, we have that $\vek{v}$ is a linear combination of elements in $B$, i.e. $\vek{v}=a\cdot i\sqrt{2}+b\cdot1=a i\sqrt{2}+b$, where $a,b\in\Q$.

\noindent\newline{\bf Problem.} Show that every element of $\R(2+3i)$ can be written as $a+b i$, where $a,b\in\R$.

\noindent\newline{\bf Solution.} Let $x=2+3i$. Then, $x-2=3i$, and by squaring this equality, we get $x^2-4x+4=-9$, i.e. $x^2-4x+13=0$. Is $p(x)=x^2-4x+13$ irreducible over $\R$? One of the roots is $2+3i$, which means that $p(x)$ is irreducible over $\R$. Therefore $p(x)$ is minimum polynomial of $2+3i$ over $\R$. Now, from previous theorem, $B=\base{\R(2+3i)}=\{\vek{1},\vek{2+3i}\}$. Take $\vek{v}\in\R(2+3i)$ which can be shown as a linear combination of vectors in $B$, i.e. $\vek{v}=k+(2+3i)l$, where $k,l\in\R$. Thus, every $\vek{v}$ can be written as $(k+2l)+3li$. If we take $a,b\in\R$ such that $a=k+2l$ and $b=3l$, we have $\vek{v}=a+bi$. That means that $\R(2+3i)\subseteq\C$. Also, if we take $a+bi\in\C$, we can easily write it down using $k$ and $l$ from former equalities.

\noindent\newline{\bf Problem.} If $a=\sqrt{1+\sqrt[3]{2}}$ show that $\{1,2^{\frac{1}{3}},2^{\frac{2}{3}},a,2^{\frac{1}{3}}a,2^{\frac{2}{3}}a\}$ is a basis of $\Q(a)$ over $\Q$. Describe the elements of $\Q(a)$.

\noindent\newline{\bf Solution.} Let $x=\sqrt{1+\sqrt[3]{2}}$. Then, $x^2=1+\sqrt[3]{2}$, which is again equivalent to $x^2-1=\sqrt[3]{2}$. If we cube the equation, we get $x^6-3x^4+3x^2-1=2$, which is the same as $x^6-3x^4+3x^2-3=0$. If we consider $p(x)=x^6-3x^4+3x^2-3$, we can see that for $3\in P$, as $3$ divides each coefficient, except the leading one, and as $9\nmid-3$, by Eisenstein's criterion, $p(x)$ is irreducible over $\Q$, and by that minimal polynomial of $a=\sqrt{1+\sqrt[3]{2}}$. We have $\base{\Q(a)}=\{1,a,1+\sqrt[3]{2},a(1+\sqrt[3]{2}),1+2\sqrt[3]{2}+\sqrt[3]{4},a(1+2\sqrt[3]{2}+\sqrt[3]{4})\}$, and each $\vek{v}\in\Q(a)$ can be written as:

\begin{eqnarray*}
\vek{v}&=&k_1+k_2 a+k_3(1+\sqrt[3]{2})+k_4 a(1+\sqrt[3]{2})\\
&&+k_5(1+2\sqrt[3]{2}+\sqrt[3]{4})+k_6 a(1+2\sqrt[3]{2}+\sqrt[3]{4})\\
&=&(k_1+k_3+k_5)+a(k_2+k_4+k_6)+\sqrt[3]{2}(k_3+2k_5)+\sqrt[3]{2}a(k_4+2k_6)\\
&&+k_5\sqrt[3]{4}+k_6 a (\sqrt[3]{4}).
\end{eqnarray*}

\noindent\newline Thus we have shown that $B=\{1,2^{\frac{1}{3}},2^{\frac{2}{3}},a,2^{\frac{1}{3}}a,2^{\frac{2}{3}}a\}$ spans $\Q(a)$, for choosing any $\vek{v}$, it can be rewritten as linear combination of vectors in $B$. By previous proposition, as $|B|=\dim{\Q(a)}=6$, and as it spans $\Q(a)$, it is a basis of $\Q(a)$. Elements of $\Q(a)$ can be written as:

\begin{equation*}
\vek{v}=k_1+k_2\sqrt{1+\sqrt[3]{2}}+k_3\sqrt[3]{2}+k_4\sqrt[3]{2}\left(\sqrt{1+\sqrt[3]{2}}\right)+k_5\sqrt[3]{4}+k_6\sqrt[3]{4}\left(\sqrt{1+\sqrt[3]{2}}\right).
\end{equation*}

\noindent\newline{\bf Problem.} Find a basis of $\Q(\sqrt{2}+\sqrt[3]{4})$ over $\Q$, and describe the elements of $\Q(\sqrt{2}+\sqrt[3]{4})$.

\noindent\newline{\bf Solution.} Let $x=2^{\frac{1}{2}}+2^{\frac{2}{3}}$. Then, from $x-2^{\frac{1}{2}}=2^{\frac{2}{3}}$, by cubing, we get $x^3-3x^2 2^{\frac{1}{2}}+6x-2^{\frac{3}{2}}=4$. This can be written as $x^3+2(3x-2)=2^{\frac{1}{2}}(3x^2+2)$. By squaring this equation we get $x^6+4x^3(3x-2)+4(9x^2-12x+4)=2(9x^4+12x^2+4)$. This can be written nicely as $p(x)=0$ where $p(x)=x^6-6x^4-8x^3+12x^2-48x+8$. Now, such polynomial does not satisfy Eisenstein's criterion, so we need a different approach. If $a=\sqrt{2}+\sqrt[3]{4}$, then $a-\sqrt{2}=\sqrt[3]{4}$. After cubing this equality we get $a^3-3a^2\sqrt{2}+6a-2\sqrt{2}=4$, i.e. $a^3-3a^2\sqrt{2}+6a-2(\sqrt{2}+2)=0$. We can see that $q(x)\in\Q(\sqrt{2})[x]$, where $q(x)=a^3-3a^2\sqrt{2}+6a-2(\sqrt{2}+2)$. Before we return to $a$ and $p(x)$, let us observe $b=\sqrt{2}$. That implies $b^2-2=0$. Minimal polynomial of $\sqrt{2}$ over $\Q$ is $r(x)=x^2-2$ (it is obviously irreducible). Then, by previous theorem, $\base{\Q(b)}=\{1,b\}$, i.e. $\base{\Q(\sqrt{2}):\Q}=\{1,\sqrt{2}\}$, and we have $\dim{\Q(\sqrt{2})}=2$. The elements $\vek{v}\in\Q(\sqrt{2})$ can be written as $\vek{v}=k_0\cdot 1+k_1\sqrt{2}=k_0+k_1\sqrt{2}$, for some $k_0,k_1\in\Q$. Now let us return to $p(x)$ and $a$. We can see that $p(x)\in\Q(\sqrt{2})$, as coefficients of $p(x)$ are $1$, $-3$, $\sqrt{2}$ ,$6$ and $-2\sqrt{2}-4$, and all are in $\Q(\sqrt{2})$. But, is $p(x)$ minimal polynomial of $a$ over $\Q(\sqrt{2})$? Assume there exist $p_2(x)$ and $p_1(x)$ such that $\deg{p_2(x)}=2$, $\deg{p_1(x)}=1$ and $p_2(a)=p_1(a)=0$. Polynomial $p_1(x)$ cannot be minimal as it would have to be $p_1(x)=x-a$, and $a\notin\Q{\sqrt{2}}$. If $p_2(x)$ is minimal, then $p(a)=a^2+c a+d=0$, i.e. $(\sqrt{2}+\sqrt[3]{4})^2+c(\sqrt{2}+\sqrt[3]{4})+d=0$. From that we have $2+2\sqrt{2}\sqrt[3]{4}+\sqrt[3]{16}+c\sqrt{2}+c\sqrt[3]{4}+d=0$. So, we would need to have $2+\sqrt[3]{16}+c\sqrt[3]{4}+d=0$ and $2\sqrt{3}[4]+c=0$. Thus, we would get $c=-2\sqrt{3}[4]$, which is impossible as $c\in\Q(\sqrt{2})$ and $\sqrt{3}[4]\notin\Q(\sqrt{2})$. We know that, because $\sqrt{3}[4]=e+f\sqrt{2}$, cubed, would yield $4=e^3+e^2 f\sqrt{2}+2 e f^2+2f^3\sqrt{2}$. From that we could get $\sqrt{2}(2f^3+e^2f)=4-e^3+2 e f^2$, i.e.

\begin{equation*}
\sqrt{2}=\frac{4-e^3+2 e f^2}{2f^3+e^2f}.
\end{equation*}

\noindent\newline As $e$ and $f$ are in $\Q$, that would imply that the right-hand side of equality is also in $\Q$, which is impossible, as that would imply $\sqrt{2}\in\Q$. Thus, it must be that $\sqrt{3}[4]\notin\Q(\sqrt{2})$, and $p_1(x)$ and $p_2(x)$ cannot be minimal polynomials. Now, $a$ is root of $p(x)$, and there are no polynomials of lesser degrees, so $p(x)$ has to be minimum polynomial of $a$ over $\Q(\sqrt{2})$. Therefore, basis of $\Q(a)$ over $\Q(\sqrt{2})$ is $\base{\Q(a):\Q(\sqrt{2})}={1,a,a^2}$ and $\dim{\Q(a):\Q(\sqrt{2})}=3$. Finally, it has to be $[\Q(a):\Q]=[\Q(a):\Q(\sqrt{2})][\Q(\sqrt{2}):\Q]=3\cdot 2=6$. We can see that, by previous proposition:

\begin{equation*}
\base{\Q(a):\Q}=\{1,a,\sqrt{2},a\sqrt{2},a^2,a^2\sqrt{2}\}.
\end{equation*}

\noindent\newline So, if we take $\vek{v}\in\Q(a)$ we have:

\begin{eqnarray*}
\vek{v}&=&k_0+k_1(\sqrt{2}+\sqrt[3]{4})+k_2\sqrt{2}+k_3(\sqrt{2}+\sqrt[3]{4})\sqrt{2}\\
&&+k_4(\sqrt{2}+\sqrt[3]{4})^2+k_5(\sqrt{2}+\sqrt[3]{4})^2\sqrt{2}.
\end{eqnarray*}

\noindent\newline This can be reduced to:

\begin{equation*}
\vek{v}=l_0+l_1 2^{\frac{1}{2}}+l_2 2^{\frac{2}{3}}+l_3 2^{\frac{4}{3}}+l_4 2^{\frac{7}{6}}.
\end{equation*}

\noindent\newline{\bf Problem.} Find a basis of $\Q(\sqrt{5},\sqrt{7})$ over $\Q$, and describe the elements of $\Q(\sqrt{5},\sqrt{7})$.

\noindent\newline{\bf Solution.} Let us observe $\Q(\sqrt{5}):\Q$. If $\sqrt{5}=x$, we get $x^2-5=0$. By Eisenstein's criterion it is easy to see that $p(x)=x^2-5$ is irreducible over $\Q$, and is therefore the minimal polynomial of $\sqrt{5}$ over $\Q$. Thus, $\base{\Q(\sqrt{5}):\Q}=\{1,\sqrt{5}\}$ and we have $[\Q(\sqrt{5}):\Q]=2$. Each element in $\Q(\sqrt{5}):\Q$ can be shown as $a+b\sqrt{5}$. Then, let us see if $\sqrt{7}\in\Q(\sqrt{5})$. That would mean $a+b\sqrt{5}=\sqrt{7}$. Squaring this expression gives us $a^2+2a b\sqrt{5}+b^2\sqrt{5}=7$. After some rearrangement that is equivalent to $\sqrt{5}(2a b+b^2)=7-a^2$, i.e. $\sqrt{5}=\frac{2a b+b^2}{7-a^2}$. Now, as $a$, $b$ and $7$ are all rational, then whole right-hand side of equality is also rational, which is impossible as $\sqrt{5}$ is irrational. Now, we have $x=\sqrt{7}$ which gives us $x^2-7=0$. Is $q(x)=x^2-7$ minimum polynomial of $\sqrt{7}$ over $\Q$? If it was $Q(x)=a x+b$, a polynomial of lesser degree, and $Q(\sqrt{7})=0=a\sqrt{7}+b$ we would have $-b=a\sqrt{7}$, for some $a,b\in\Q(\sqrt{5})$. Taking $a^{-1}\in\Q(\sqrt{5})$ (as it is a field such element exists) gives us $-b a^{-1}=\sqrt{7}$. But, $-b a^{-1}\in\Q(\sqrt{5})$, while we just have shown that $\sqrt{7}\notin\Q(\sqrt{5})$. So, as $\sqrt{7}$ is root of $q(x)$, and there does not exist any polynomial of lesser degree whose root is $\sqrt{7}$, as we have shown, then it must be that $q(x)$ is indeed minimum polynomial of $\sqrt{7}$ over $\Q(\sqrt{5})$. So, $\base{\Q(\sqrt{5},\sqrt{7}):\Q(\sqrt{5})}=\{1,\sqrt{7}\}$ and $[\Q(\sqrt{5},\sqrt{7}):\Q(\sqrt{5})]=2$. From that, using previous proposition, we get $[\Q(\sqrt{5},\sqrt{7}):\Q]=[\Q(\sqrt{5},\sqrt{7}):\Q(\sqrt{5})][\Q(\sqrt{5}):\Q]=2\cdot 2=4$, and $\base{\Q(\sqrt{5},\sqrt{7}):\Q}=\{1,\sqrt{5},\sqrt{7},\sqrt{35}\}$. That means that if $\vek{v}\in\Q(\sqrt{5},\sqrt{7}):\Q$, then $\vek{v}=k_0+k_1\sqrt{5}+k_2\sqrt{7}+k_3\sqrt{35}$.

\noindent\newline{\bf Problem.} Find a basis of $\Q(\sqrt{2},\sqrt{3},\sqrt{5})$ over $\Q$ and describe $\Q(\sqrt{2},\sqrt{3},\sqrt{5})$.

\noindent\newline{\bf Solution.} We have $p(x)=x^2-2$ as minimum polynomial of $\sqrt{2}$ over $\Q$ (obviously irreducible). Then, $\base{\Q(\sqrt{2})}=\{1,\sqrt{2}\}$, by previous proposition, and $[\Q(\sqrt{2}):\Q]=2$. Each element in $\Q(\sqrt{2})$ is of the form $a+b\sqrt{2}$. Now, we know that $\sqrt{3}$ is the root of $q(x)=x^2-3$, but is $q(x)$ minimal polynomial of $\sqrt{3}$ over $\Q(\sqrt{2})$. First, we will show that $\sqrt{3}\notin\Q(\sqrt{2})$. If it were so, then there would exist $a,b\in\Q$ such that $a+b\sqrt{2}=\sqrt{3}$. Squaring this equality would give us $a^2+2 a b\sqrt{2}+2 b^2=3$, i.e. $\sqrt{2}=\frac{3-2 b^2-a^2}{2 a b}$. But, as the right-hand side is rational, so it would mean that $\sqrt{2}$ is rational, which is a contradiction. Therefore, $\sqrt{3}\notin\Q$. Assume $q(x)$ is not minimal, and that there exists some $Q(x)$ such that $\deg{Q(x)}<\deg{q(x)}$. That would mean that only possibility is $Q(x)=a x+b$, where $a,b\in\Q(\sqrt{2})$. So, it also must be $Q(\sqrt{3})=a\sqrt{3}+b=0$. That is equivalent to $\sqrt{3}=-b a^{-1}$, where $-b a^{-1}\in\Q(\sqrt{2})$ which is a contradiction. So, as $q(\sqrt{3})=0$ and $q(x)$ has rational coefficients, which are in turn also in extension field, then $q(x)$ must be minimal polynomial. Therefore, $\base{\Q(\sqrt{2},\sqrt{3}):\Q(\sqrt{2})}=\{1,\sqrt{3}\}$ and $[\Q(\sqrt{2},\sqrt{3}):\Q(\sqrt{2})]=2$. By previous proposition, $[\Q(\sqrt{2},\sqrt{3}):\Q]=[\Q(\sqrt{2},\sqrt{3}):\Q(\sqrt{2})][\Q(\sqrt{2}):\Q]=2\cdot 2=4$ along with $\base{\Q(\sqrt{2},\sqrt{3}):\Q}=\{1,\sqrt{2},\sqrt{3},\sqrt{6}\}$. Assume $\sqrt{5}\in\Q(\sqrt{2},\sqrt{3})$. Let us observe that as vector space over $\Q(\sqrt{2})$, with basis vectors $1$ and $\sqrt{3}$. That would mean that $\sqrt{5}=a+b\sqrt{3}$, where $a,b\in\Q(\sqrt{2})$. Squaring this would yield $5=a^2+2a b\sqrt{3}+3b^2$, i.e. $\sqrt{3}=(5-a^2-3b^2)(2 a b)^{-1}$. But, as right-hand side is obviously again in $\Q(\sqrt{2})$, it cannot be that it is equal to $\sqrt{3}$, as we have already shown $\sqrt{3}\notin\Q(\sqrt{2})$. So it must be that $\sqrt{5}\notin\Q(\sqrt{2},\sqrt{3})$. We have $r(x)=x^2-5$ which is a minimal polynomial of $\sqrt{5}$ over $\Q(\sqrt{2},\sqrt{3})$. If it was not minimal, there would exist $R(x)=a x+b$, where $a,b\in\Q(\sqrt{2},\sqrt{3})$. That would imply $R(\sqrt{5})=a\sqrt{5}+b=0$, i.e. $\sqrt{5}=-b a^{-1}$, so the right-hand side is again in $\Q(\sqrt{2},\sqrt{3})$, while we have just shown $\sqrt{5}\notin\Q(\sqrt{2},\sqrt{3})$. By previous proposition, it must be that $\base{\Q(\sqrt{2},\sqrt{3},\sqrt{5}):\Q(\sqrt{2},\sqrt{3})}=\{1,\sqrt{5}\}$ with $[\Q(\sqrt{2},\sqrt{3},\sqrt{5}):\Q(\sqrt{2},\sqrt{3})]=2$. Thus, $[\Q(\sqrt{2},\sqrt{3},\sqrt{5}):\Q]=[\Q(\sqrt{2},\sqrt{3},\sqrt{5}):\Q(\sqrt{2},\sqrt{3})][\Q(\sqrt{2},\sqrt{3}):\Q]=2\cdot 4=8$ and $\base{\Q(\sqrt{2},\sqrt{3},\sqrt{5}):\Q}=\{1,\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{6},\sqrt{10},\sqrt{15},\sqrt{30}\}$. Each $\vek{v}\in\Q(\sqrt{2},\sqrt{3},\sqrt{5})$ can be written as $\vek{v}=k_0+k_1\sqrt{2}+k_2\sqrt{3}+k_3\sqrt{5}+k_4\sqrt{6}+k_5\sqrt{10}+k_6\sqrt{15}+k_7\sqrt{30}$.

\noindent\newline{\bf Problem.} Name an extension of $\Q$ over which $\pi$ is algebraic of degree $3$.

\noindent\newline{\bf Solution.} Let us observe $\pi$ over $\Q(\pi^3)$. We have $p(x)=x^3-\pi^3$, which is a polynomial in $\Q(\pi^3)[x]$, due to $\pi^3\in\Q(\pi^3)$. Also, $p(\pi)=\pi^3-\pi^3=0$. Then, $\Q(\pi)$ over $\Q(\pi^3)$ has basis $\{1,\pi,\pi^2\}$ and $[\Q(\pi):\Q(\pi^3)]=3$.

\noindent\newline{\bf Proposition.} Let $F$ be a field and $\rchar{F}\neq 2$. Let $a,b\in F-\{0\}$ be non-squares and $a\neq b$. Then,

\begin{enumerate}
\item $F\left(\sqrt{a}+\sqrt{b}\right)=F\left(\sqrt{a},\sqrt{b}\right)$;
\item If $b\neq x^2 a$, for all $x\in F$, then $\sqrt{b}\notin F\left(\sqrt{a}\right)$ and $[F\left(\sqrt{a},\sqrt{b}\right):F]=4$;
\item $F\left(\sqrt{a+b+2\sqrt{a b}}\right)=F\left(\sqrt{a},\sqrt{b}\right)$.
\end{enumerate}

\noindent\newline{\bf Proof.} {\it Ad $1$.} By previous proposition we have $F(\sqrt{a}+\sqrt{b})\subseteq F(\sqrt{a},\sqrt{b})$. Let us take $\sqrt{a}+\sqrt{b}\in F(\sqrt{a}+\sqrt{b})=G$. Then, $(\sqrt{a}+\sqrt{b})^2\in G$. But, $(\sqrt{a}+\sqrt{b})^2=a+2\sqrt{a b}+b$, and as $a,b\in F\subseteq G$, then $2\sqrt{a b}=(\sqrt{a}+\sqrt{b})^2-a-b\in G$. Notice that $2\sqrt{a b}=\sqrt{a b}+\sqrt{a+b}=1\sqrt{a b}+1\sqrt{a b}=(1+1)\sqrt{a b}=(2\cdot 1)(\sqrt{a b})$. As $1\in G$ (neutral element), then $(2\cdot 1)\in G$, and so it must be that $(2\cdot 1)^{-1}\in G$ (as $\rchar{F}\neq 2$, then $2\cdot 1\neq 0$ and so such inverse exists). Therefore, $(2\cdot 1)^{-1}(2\cdot 1)\sqrt{a b}\in G$, which implies $\sqrt{a b}\in G=F(\sqrt{a}+\sqrt{b})$. So it also must be that $\sqrt{a b}(\sqrt{a}+\sqrt{b})=\sqrt{a^2 b}+\sqrt{b^2 a}=a\sqrt{b}+b\sqrt{a}\in G$. So, also $\sqrt{a b}(\sqrt{a}+\sqrt{b})-b(\sqrt{a}+\sqrt{b})=a\sqrt{b}+b\sqrt{a}-b\sqrt{a}-b\sqrt{b}=a\sqrt{b}-b\sqrt{b}\in G$, i.e. $\sqrt{b}(a-b)\in G$. As $(a-b)\in F$, then $(a-b)^{-1}$ is also in $F$. Thus, $\sqrt{b}(a-b)(a-b)^{-1}=\sqrt{b}\in G=F(\sqrt{a}+\sqrt{b})$. In the same way we can show $\sqrt{a}\in F(\sqrt{a}+\sqrt{b})$. Thus, as $F(\sqrt{a}+\sqrt{b})$ contains $F$, $\sqrt{a}$ and $\sqrt{b}$, and $F(\sqrt{a},\sqrt{b})$ is smallest such field, it must be that $F(\sqrt{a},\sqrt{b})\subseteq F(\sqrt{a}+\sqrt{b})$, and that implies, along with the first result, that $F(\sqrt{a},\sqrt{b})=F(\sqrt{a}+\sqrt{b})$.

{\it Ad $2$.} Let $b\neq x^2 a$, for all $x\in F$, but $\sqrt{b}\in F(\sqrt{a})$. That implies $\sqrt{b}=k+l\sqrt{a}$, for some $k,l\in F$. Squaring this equality gives us $b=k^2+2k l\sqrt{a}+l^2 a$, i.e. $0=(k^2+l^2 a-b)\cdot 1+(2k l)\sqrt{a}$. As $\{1,\sqrt{a}\}$ is basis of $F(\sqrt{a})$, then $1$ and $\sqrt{a}$ are linearly independent, and it must be that $(k^2+l^2 a-b)=0$ and $2 k l=0$, i.e. $k l=0$. We have three cases. First, if $k=l=0$. Then we simply have $0^2+0^2 a-b=0$ which would mean $b=0$, contrary to our assumption that $a,b\neq 0$. Assume $k=0$ and $l\neq 0$. Then, $0^2+l^2 a-b=0$ gives us $l^2a=b$, which is contrary to assumption that $b\neq x^2 a$, for all $x\in F$. Assume $k\neq 0$ and $l=0$. Then, $k^2+0^2 a-b=0$ gives us $b=k^2$, which is contrary to assumption that $a$ and $b$ are non-squares. Thus it must be that $\sqrt{b}\notin F(\sqrt{a})$. But, there does exist $p(x)=x^2-b$ such that $p(\sqrt{b})=b-b=0$, and it is obviously irreducible, as $q(x)=k x+l$, for $q(\sqrt{b})=0=k\sqrt{b}+l$ would imply $-l k^{-1}=\sqrt{b}$, i.e. that $\sqrt{b}\in F(\sqrt{a})$ (we have just shown it is not the case). Thus, $p(x)$ is minimal polynomial of $\sqrt{b}$ over $F(\sqrt{a})$. So, $\base{F(\sqrt{a},\sqrt{b}):F(\sqrt{a})}=\{1,\sqrt{b}\}$, and by previous proposition, $\base{F(\sqrt{a},\sqrt{b}):F}=\{1,\sqrt{a},\sqrt{b},\sqrt{a b}\}$, i.e. $[F(\sqrt{a},\sqrt{b}):F]=4$.

{\it Ad $3$.} Let $u=\sqrt{a+b+2\sqrt{a b}}$. Then, $u^2=a+b+2\sqrt{a b}$, i.e. $u^2-(a+b)=2\sqrt{a b}$. Squaring that gives us $u^4-2u^2(a+b)+a^2-2a b+b^2=0$. That is equivalent to $u^4-2u^2(a+b)+(a-b)^2=0$. If we took $v=\sqrt{a}+\sqrt{b}$ we would get $v^2=a+2\sqrt{a b}+b$. It is easy to see that by squaring $v^2-(a+b)=2\sqrt{a b}$, we would get $v^4-2v^2(a+b)+(a-b)^2=0$. So, both $u$ and $v$ are roots of $p(x)=x^4-2x^2(a+b)+(a-b)^2$. As $p(u)=p(v)$, it must be that $u^4-2(a+b)u^2+(a-b)^2=v^4-2(a+b)v^2+(a-b)^2$, i.e. $u^4-2(a+b)u^2=v^4-2(a+b)v^2$. That implies $u^4-v^4-(2(a+b)u^2-2(a+b)v^2)=0$, or simplified as $u^4-v^4-2(a+b)(u^2-v^2)=0$. Further simplification gives us $(u^2-v^2)(u^2+v^2)-2(a+b)(u^2-v^2)=0$ and $(u^2-v^2)(u^2+v^2-2(a+b))=0$. Now, using elementary algebra we get $(u-v)(u+v)(u^2+v^2-2(a+b))=0$. From this, $u-v=0$ or $u+v=0$ or $u^2+v^2-2(a+b)=0$. Let us examine the latter case. We would have $(a+b+2\sqrt{a b})+(a+b+2\sqrt{a b})-2(a+b)=0$, which amounts to $4\sqrt{a b}=0$. But that would imply $a b=0$, i.e. $a=0$ or $b=0$, both of which are contradictory to assumption that $a,b\neq 0$. So, it must be $u-v=0$ or $u+v=0$. If $u-v=0$, then obviously $F(u)=F(v)$. If $u+v=0$, then $u=-v$, which would mean $v\in F(u)$. As $F(v)$ is the smallest field containing $F$ and $v$, and also $F\subseteq F(u)$, then it must be that $F(v)\subseteq F(u)$. Also as $v=-u$, we have $u\in F(v)$, which in same way implies $F(u)\subseteq F(v)$. In other words, we have proved that $F(u)=F(v)$, that is $F(\sqrt{a+b+2\sqrt{a b}})=F(\sqrt{a}+\sqrt{b})$. But, $F(\sqrt{a}+\sqrt{b})=F(\sqrt{a},\sqrt{b})$ by a previous proposition, so $F(\sqrt{a+b+2\sqrt{a b}})=F(\sqrt{a},\sqrt{b})$. Previous problem tells us of how the field $F(\sqrt{a+b+2\sqrt{a b}})$ looks like, if $a\neq x^2 b$ for all $x\in F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Find an uncomplicated basis for $\Q(d):\Q$ where $d$ is a root of $x^4-14x^2+9$.

\noindent\newline{\bf Solution.} Notice that $-14=-2(a+b)$, that is $7=a+b$ and $9=(a-b)^2$, i.e. $3=a-b$. This gives us $a=5$ and $b=2$, as in previous proposition. Thus, $d=\sqrt{7+2\sqrt{10}}$ and we have $\Q(d)=\Q(\sqrt{7+2\sqrt{10}})=\Q(\sqrt{5},\sqrt{2})$, by previous proposition. Then, $\base{\Q(\sqrt{5},\sqrt{2}):\Q}=\{1,\sqrt{2},\sqrt{5},\sqrt{10}\}$.

\noindent\newline{\bf Proposition.} Let $F$ be a field, and let $c$ be algebraic over $F$ with $[F(c):F]=m$. Then, each element of $F(c)$ can be written uniquely as $a_0+a_1 c+\cdots+a_{m-1}c^{m-1}$, for some $a_0,a_1,\ldots,a_{m-1}\in F$.

\noindent\newline{\bf Proof.} As $[F(c):F]=m$, then $p(x)=c^m+k_{m-1}c^{m-1}+\ldots+k_1 c+k_0$, and so, by previous theorem, $\base{F(c):F}=\{1,c,c^2,\ldots,c^{m-1}\}$. Now, assume $\vek{v}=a_{m-1}c^{m-1}+\ldots+a_1 c+a_0$ and $\vek{v}=b_{m-1}c^{m-1}+\ldots+b_1 c+b_0$. Then, $a_{m-1}c^{m-1}+\ldots+a_1 c+a_0=b_{m-1}c^{m-1}+\ldots+b_1 c+b_0$ and we have $c^{m-1}(a_{m-1}-b_{m-1})+\ldots+c(a_1-b_1)+(a_0-b_0)=\vek{0}$. As $\{1,c,c^2,\ldots,c^{m-1}\}$ is basis of $F(c):F$, they are linearly independent, implying $a_{m-1}-b_{m-1}=\ldots=a_1-b_1=a_0-b_0=0$, i.e. $a_i=b_i$ for all $i\in\{0,1,\ldots,m-1\}$. Thus, each $\vek{v}$ can be written in a unique manner as $\vek{v}=a_{m-1}c^{m-1}+\ldots+a_1 c+a_0$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Problem.} Construct a field of four elements as an extension of $\zmod{2}$. Describe its elements, and supply its addition and multiplication tables.

\noindent\newline{\bf Solution.} Notice that $\overline{0}+\overline{0}+\overline{1}=\overline{1}$ and $\overline{1}+\overline{1}+\overline{1}=\overline{1}$. Thus, $p(x)=x^2+x+\overline{1}$ has no roots, and, as it is of degree $2$, it is irreducible, and also monic. Let $c$ be such element that $c^2+c+\overline{1}=\overline{0}$. Then, $\base{\zmod{2}(c):\zmod{2}}=\{1,c\}$ and if $\vek{v}\in\zmod{2}(c)$, then $\vek{v}=\overline{k}c+\overline{l}$. We can see that $\zmod{2}(c)$ has four elements (two for $\overline{k}$ and two for $\overline{l}$). Addition table for $\zmod{2}(c)$ is:

\begin{center}\begin{tabular}{c|cccc}
  %\hline
  $+$ & $\overline{0}$ & $\overline{1}$ & $c$ & $\overline{1}+c$\\
  \hline
  $\overline{0}$ & $\overline{0}$ & $\overline{1}$ & $c$ & $\overline{1}+c$\\
  $\overline{1}$ & $\overline{1}$ & $\overline{0}$ & $\overline{1}+c$ & $c$\\
	$c$ & $c$ & $\overline{1}+c$ & $\overline{0}$ & $\overline{1}$\\
	$\overline{1}+c$ & $\overline{1}+c$ & $c$ & $\overline{1}$ & $\overline{0}$\\
  %\hline
\end{tabular}\end{center}

\noindent\newline For multiplication table, notice that $c^2=c+\overline{1}$ and $c^2+c=\overline{1}$ which we get from definition $c^2+c+\overline{1}=\overline{0}$. Multiplication table for $\zmod{2}(c)$ is as follows:

\begin{center}\begin{tabular}{c|cccc}
  %\hline
  $\cdot$ & $\overline{0}$ & $\overline{1}$ & $c$ & $\overline{1}+c$\\
  \hline
  $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$ & $\overline{0}$\\
  $\overline{1}$ & $\overline{0}$ & $\overline{1}$ & $c$ & $\overline{1}+c$\\
	$c$ & $\overline{0}$ & $c$ & $\overline{1}+c$ & $\overline{1}$\\
	$\overline{1}+c$ & $\overline{0}$ & $\overline{1}+c$ & $\overline{1}$ & $c$\\
  %\hline
\end{tabular}\end{center}

\noindent\newline{\bf Problem.} Construct a field of eight elements as an extension of $\zmod{2}$.

\noindent\newline{\bf Solution.} We have seen that $|\zmod{2}(c)|=4$, where $c^2+c+\overline{1}=\overline{0}$. Now, if we take some $p(x)\in\zmod{2}(c)$ such that it is irreducible and of degree $2$, we can construct a field of eight elements based upon the former field. Notice that $x^2\in\{\overline{1},\overline{1}+c,c\}$, for all $x\in\zmod{2}(c)$. Using this, we can think of an irreducible polynomial of degree two. Let us observe also $p(x)=x^2+x$. We have $p(\overline{1})=\overline{1}+\overline{1}=\overline{0}$, $p(c)=c+\overline{1}+c=\overline{1}$ and $p(c+\overline{1})=c+c+\overline{1}=\overline{1}$. Of course, $p(\overline{0})=\overline{0}$. As $p(c)=p(\overline{1}+c)=\overline{1}$, adding $\overline{1}$ to $p(x)$ would make it zero, but adding $c$ would make all $p(x)\neq\overline{0}$. So, we are looking for $p(x)=x^2+x+c$, which is monic and irreducible (it is of degree two and has no roots in $\zmod{2}(c)$). Now, taking for a rule $d^2+d+c=\overline{0}$, we arrive at $\base{\zmod{2}(c,d):\zmod{2}(c)}=\{\overline{1},d\}$, but by previous proposition, $\base{\zmod{2}(c,d):\zmod{2}}=\{\overline{1},c,d,c d\}$. Thus, every $\vek{v}\in\zmod{2}(c,d)$ can be shown as $\vek{v}=k_0\overline{1}+k_1 c+k_2 d+k_3 c d$, for some $k_i\in\zmod{2}$, where $i\in\{0,1,2,3\}$. We can see that, we have $2$ choices for each $k_i$, and therefore, all in all, $8$ possible elements in $\zmod{2}(c,d)$.

\noindent\newline{\bf Proposition.} Let $F$ be a finite field such that $|F|=n$. If $c$ is algebraic over $F$ with $[F(c):F]=m$, then $|F(c)|=n^m$.

\noindent\newline{\bf Proof.} As $[F(c):F]=m$, then minimal polynomial of $c$ over $F$ is of degree $m$ and $\base{F(c)}=\{1,c,c^2,\ldots,c^{m-1}\}$. Then every $\vek{v}\in F(c)$ can be shown as $\vek{v}=k_0+k_1 c+k_2 c^2+\ldots+k_{m-1}c^{m-1}$. For each choice of $k_i$ we have $n$ possibilities, as $k_i\in F$ and $|F|=n$. Also, due to previous proposition, this can be done in unique manner. Therefore, in total, we have $n^m$ vectors in $F(c):F$.

\begin{flushright}
$\square$\\
\end{flushright}

\noindent{\bf Proposition.} For every $p\in P$ there exists a field with $p^2$ elements.

\noindent\newline{\bf Proof.} We have already shown the case for $p=2$. Now, we know that $\zmod{p}$ is a field. So, if we took $q(x)=x^2+\overline{c}$, where $\overline{c}\in\zmod{p}$, then $q(x)$ would be irreducible if and only if it had no roots in $\zmod{p}$. How many elements in $\zmod{p}$ are squares? Well, setting aside $\overline{0}$, it leaves $p-1$ elements to consider. As $(\overline{m-a})^2=\overline{m}^2-2\overline{m}\overline{a}+\overline{a}^2=\overline{a}^2$, it is evident that there are $\overline{p-1}{2}$ squares (if not exactly, then surely less than that) in $\zmod{p}$. Adding $\overline{0}$ makes it $\overline{p+1}{2}$ squares. Thus, there exist $\overline{p-1}{2}$ elements that are not squares, and that much elements we can choose for $\overline{c}$ so $q(x)$ will have no roots, and thus irreducible. Assume $d^2=-\overline{c}$. Extension $\zmod{p}(d)$ is then of degree $2$ (due to $q(x)$ being of degree $2$). By previous proposition, as $|\zmod{p}|=p$ and $[\zmod{p}(d):\zmod{p}]=2$, we have $|\zmod{p}(d)|=p^2$.

\begin{flushright}
$\square$\\
\end{flushright}

\newpage

\begin{center}
{\bf Summa Summarum}
\end{center}

\vskip 0.5cm

\noindent\newline Let $C,D\neq\emptyset$.

\noindent\newline{\bf Relation} $\mathcal{R}=\{(x,y):\ x\in D,\ y\in C\}\subseteq D\times C$ (we write $x\mathcal{R}y$ if $(x,y)\in\mathcal{R}$) is:

\begin{itemize}
\item {\bf left-total} if $(\forall x\in D)(\exists y\in C):\ (x,y)\in\mathcal{R}$;
\item {\bf right-total} if $(\forall y\in C)(\exists x\in D):\ (x,y)\in\mathcal{R}$;
\item {\bf one-to-many} if $(\forall y\in C)(\forall x_1,x_2\in D):\ (x_1,y)\in\mathcal{R},(x_2,y)\in\mathcal{R}\rightarrow x_1=x_2$;
\item {\bf many-to-one} if $(\forall x\in D)(\forall y_1,y_2\in C):\ (x,y_1)\in\mathcal{R},(x,y_2)\in\mathcal{R}\rightarrow y_1=y_2$;
\item {\bf one-to-one} if one-to-many and many-to-one.
\end{itemize}

\noindent\newline{\bf Domain of relation} $\mathcal{R}\subseteq D\times C$ is set $D$.

\noindent\newline{\bf Codomain of relation} $\mathcal{R}\subseteq D\times C$ is set $C$.

\noindent\newline{\bf Function} $f:D\rightarrow C$ is a {\it left-total}, {\it many-to-one} relation $f\subseteq D\times C$ called
\begin{itemize}
\item {\bf injective} if one-to-many;
\item {\bf surjective} if right-total;
\item {\bf bijective} if injective and surjective.
\end{itemize}

\noindent\newline{\bf Domain of function} $f:D\rightarrow C$ is set $D$.

\noindent\newline{\bf Codomain of function} $f:D\rightarrow C$ is set $C$.

\noindent\newline{\bf Image of function} $f:D\rightarrow C$ is set $\textnormal{Im}(f)=\{y\in C:\ y=f(x)\}$.

\noindent\newline{\bf Partial binary operation} $\ast$ is a many-to-one relation $\ast\subseteq(D^2\times D)$.

\noindent\newline{\bf Binary operation} $\ast$ is a function $\ast: D^2\rightarrow D$.

\noindent\newline A {\bf partition} of a set $A$ is a family $\{A_i:\ i\in I\}$ of nonempty subsets of $A$ such that:

\begin{enumerate}
\item $\left(\forall x\in A\right)\left(\forall i,j\in I\right)\left(x\in A_i \wedge x\in A_j\right)\Rightarrow A_i=A_j$;
\item $\left(\forall x\in A\right)\left(\exists i\in I\right)\left(x\in A_i\right)$.
\end{enumerate}

\noindent\newline Let $S$ be a non-empty set. We say that $\mathcal{R}=\{(a,b):\ a,b\in S\}$ is an {\bf equivalence relation}, if:

\begin{itemize}
\item $\left(\forall a\in S\right):\ a\mathcal{R}a$ ({\bf reflexivity});
\item $\left(\forall a,b\in S\right):\ a\mathcal{R}b\rightarrow b\mathcal{R}a$ ({\bf symmetry});
\item $\left(\forall a,b,c\in S\right):\ \left(a\mathcal{R}b\wedge b\mathcal{R}c\right)\rightarrow a\mathcal{R} c$ ({\bf transitivity}).
\end{itemize}

\noindent\newline Let $\sim$ be an equivalence relation on $A$ and $x\in A$. {\bf Equivalence class} of $x$ is $[x]=\{y\in A:\ y\sim x\}$.

\noindent\newline Ordered pair $(S,\ast)$ with $S\neq\emptyset$ is called:
\begin{itemize}
\item {\bf partial magma} if $\ast$ is a partial binary operation;
\item {\bf magma} (groupoid) if $\ast:S^2\rightarrow S$ is a binary operation;
\item {\bf semigroup} if $(S,\ast)$ is magma and $(\forall x,y,z\in S):\ x\ast(y\ast z)=(x\ast y)\ast z$;
\item {\bf monoid} if $(S,\ast)$ is semigroup and $(\exists e\in S)(\forall x\in S):\ e\ast x=x\ast e=x$;
\item {\bf group} if $(S,\ast)$ is monoid and $(\forall x\in S)(\exists x^{-1}\in S):\ x^{-1}\ast x=x\ast x^{-1}=e$;
\item {\bf Abelian group} if $(S,\ast)$ is group and $(\forall x,y\in S):\ x\ast y=y\ast x$.
\end{itemize}

\noindent\newline Let $G$ be a group and $S\neq\emptyset$. $S$ is a {\bf subgroup} of $G$ if:
\begin{itemize}
\item $S\subseteq G$,
\item $(\forall x,y\in S):\ x\ast y\in S$,
\item $(\forall x\in S):\ x^{-1}\in S$.
\end{itemize}

\noindent\newline{\bf Center of a group} $G$ is a subgroup $C=\{a\in G:\ x a=a x,\ \forall x\in G\}$.

\noindent\newline{\bf Subgroup of $G$ generated by} $\{a_1,\ldots,a_n\}\subseteq G$ is a subgroup $S$ such that:
\begin{itemize}
\item $a_1,\ldots,a_n,a_1^{-1},\ldots,a_n^{-1}\in S$;
\item $(\forall a\in S):\ a$ is expressible as an arbitrary product of $a_1,\ldots,a_n,a_1^{-1},\ldots,a_n^{-1}$.
\end{itemize}

\noindent\newline{\bf Cyclic subgroup} $\left\langle a\right\rangle$ of $G$ is a subgroup of $G$ generated by $\{a\}\subseteq G$.

\noindent\newline{\bf Permutation} is any bijection of the form $f:A\rightarrow A$.

\noindent\newline Group of permutations $S_A$ on $A$ is called {\bf symmetry group} on $A$.

\noindent\newline Let $p(x_1,\ldots,x_n)=\sum{\prod_{j=1}^{n}{x_j^{y_j}}}$, where $0\leq y_j\leq m$, be a polynomial. The {\bf symmetries of a polynomial} $p$ are all the permutations of subscripts of $x_j$ that leave $p$ unchanged.

\noindent\newline {\bf Cycle} is a permutation $f:A\rightarrow A$ (where $A$ is finite) such that $[\underbrace{f\circ f\circ\cdots\circ f}_{n+1\textnormal{ times}}](a)=a$ and $[f\circ f](a)\neq a$ for some $a\in A$ and $[f\circ f](x)=x$ for all $x\in A\backslash\{a\}$. Number $n$ is called the {\bf length of the cycle} $f$. Cycle of length $2$ is called a {\bf transposition}.

\noindent\newline Subgroup $A_n$ of even permutations in $S_n$ is called {\bf alternating group}.

\noindent\newline Let $(G_1,\odot)$ and $(G_2,\otimes)$ be groups. If there exists a bijection $f:G_1\rightarrow G_2$ such that $f(a\odot b)=f(a)\otimes f(b)$, for all $a,b\in G_1$, we say that group $G_1$ is {\bf isomorphic} to group $G_2$ and write $G_1\cong G_2$. Such function $f$ is called an {\bf isomorphism} from $G_1$ to $G_1$.

\noindent\newline Let $G$ be a group and $G^{\ast}=\{\pi_a:\ a\in G\}$, where $\pi_a:G\rightarrow G$ is a permutation on $G$. We say that $G^{\ast}$ is:

\begin{itemize}
\item {\bf left regular representation} of $G$ if $\pi_a(x)=a x$;
\item {\bf right regular representation} of $G$ if $\pi_a(x)=x a$;
\item {\bf regular representation} of $G$ if $G$ is commutative.
\end{itemize}

\noindent\newline If $G$ is a group, an {\bf automorphism} of $G$ is an isomorphism from $G$ to $G$.

\noindent\newline Let $G$ be a group and $H$ a subgroup of $G$. Let $a\in G$. A {\bf left coset} of $H$ in $G$ is defined as $a H=\{y\in G:\ (\forall h\in H)(y=a h)\}$. A {\bf right coset} of $H$ in $G$ is $H a=\{y\in G:\ (\forall h\in H)(y=h a)\}$.

\noindent\newline If $a\in G$, a {\bf conjugate} of $a$ is any element of the form $x a x^{-1}$, where $x\in G$.

\noindent\newline Let $a\in G$ and $\sim$ an equivalence relation on $G$ such that $a\sim b$ if and only if $a=x b x^{-1}$ for some $x\in G$. {\bf Conjugacy class} of $a$ is the set $[a]_c=\{x a x^{-1}:\ x\in G\}$.

\noindent\newline For any element $a\in G$, the {\bf centralizer} of $a$ is the set $C_a=\{x\in G:\ x a=a x\}=\{x\in G:\ x a x^{-1}=a\}$.

\noindent\newline Let $A$ be a set, and $G$ a subgroup of $S_A$ (group of all the permutations of $A$). We say that $G$ is a {\bf group acting on the set} $A$.

\noindent\newline Let $G$ be a finite group acting on the set $A$. If $a\in A$, then the {\bf orbit} of $a$, with respect to $G$, is the set $O(a)=\left\{g(a)\in A:\ g\in G\right\}$.

\noindent\newline Let $G$ be a group acting on the set $A$. The {\bf stabilizer} of $a\in A$, with respect to $G$, is the set $G_a=\{g\in G:\ g(a)=a\}$.

\noindent\newline Let $G$ and $H$ be groups. Function $f:G\rightarrow H$ is called a {\bf homomorphism} from $G$ to $H$ if $f(a b)=f(a)f(b)$, for all $a,b\in G$. Then, $H$ is called a {\bf homomorphic image} of $G$.

\noindent\newline Let $f:G\rightarrow H$ be a homomorphism from group $G$ to group $H$. Then, the {\bf kernel} of $f$ is the set $\ker{f}=\{a\in G:\ f(a)=e\}$. The {\bf range} of $f$ is the set $\ran{f}=\{f(a)\in H:\ a\in G\}$.

\noindent\newline Let $G$ be a group and $H$ a subgroup of $G$. If $x y x^{-1}\in H$, for all $y\in H$ and $x\in G$, then $H$ is called a {\bf normal subgroup} of $G$.

\noindent\newline Let $G$ be a group. A {\bf commutator} is any product of the form $a b a^{-1} b^{-1}$, where $a,b\in G$.

\noindent\newline Let $G$ be a group, $H$ a subgroup of $G$ and $a\in G$. Set $a H a^{-1}=\{a h a^{-1}:\ h\in H\}$ is a {\bf conjugate} of $H$.

\noindent\newline Let $G$ be a finite group and $H$ a subgroup of $G$. Set

\begin{equation*}
N(H)=\left\{a\in G:\ \left(\forall h\in H\right)\left(a h a^{-1}\in H\right)\right\}
\end{equation*}

\noindent\newline is the {\bf normalizer} of $H$.

\noindent\newline Let $G$ be a group and $H$ a normal subgroup of $G$. Then, $G\slash H=\{H a:\ a\in G\}$ with coset multiplication defined as $H a\cdot H b=H(a b)$ is a {\bf quotient group} (sometimes a {\bf factor group}).

\noindent\newline Group $G$ is called a {\bf $p$-group} if the order of all elements of $G$ is a power of $p$.

\newpage

\begin{center}
{\bf Appendix A. Kantian and Analytic Notions}
\end{center}

\vskip 0.5cm

\noindent{\bf Definition.} A {\it proposition} is any statement whose truth value (true-false being the usual) can be determined. Proposition is composed of a {\it subject}, which determines what the proposition is about, and a {\it predicate} which describes the subject, i.e. tells us something about it. A proposition is said to be {\it analytic} if the predicate is contained in the subject; if the predicate is not contained in the subject, the proposition is said to be {\it synthetic}.

\noindent\newline{\bf Definition.} Knowledge can be thought of as a set of propositions to which an individual can ascribe a truth value. We can say that something is known {\it a priori} if it is known independent of the information derived from our senses; independent of experience. Something is known {\it a posteriori} if the information about it is derived from our senses, i.e. from experience. We can think about these terms in the absolute or in the relative sense. We have defined it so far in the absolute sense. Now, the {\it relative a priori} knowledge is actually something that {\it can} be known through the senses, but is not. {\it Relative a posteriori} knowledge is in opposition to relative a priori knowledge. It is something that {\it can} be and {\it is} known through the senses.

\noindent\newline{\bf Definition.} {\it Type} is the class of things in the universal, abstract sense. It is, actually, derived by considering properties in common to some set of objects. {\it Token} is a concrete incarnation of the class, i.e. type, in reality.

\noindent\newline{\bf Clarification.} Relative a priori knowledge is, furthermore, different from absolute a priori knowledge in the way that the referent of the former is type and of the latter token.

\noindent\newline{\bf Definition.} {\it Representation} is the collection of {\it phenomena} (the way we perceive outside objects). {\it Nomena} is the object-in-itself, as it is outside, not the way it is perceived.

\noindent\newline{\bf Definition.} A subject can be viewed from two different angles in {\it sense-reference categories}. It can be viewed as {\it sense}, in which case it tells us the way something is. If it is viewed from the point of {\it reference}, it tells us just what something is and nothing else.

\noindent\newline{\bf Definition.} A subject can be viewed from two different angles in {\it mention-use categories}. Through {\it mention} we are only talking about the subject in the context of denotation, representation, etc. I use it more broadly, to talk about notation in the more abstract sense, not completely as a string of symbols. Through {\it use} we are talking about what the subject really is; in the context of how it is used (colloquially speaking).

\noindent\newline{\bf Definition.} {\it Abstraction} is a negative movement, while {\it concretization} is a positive movement. Abstraction moves the point of thought from the subject by removing its properties. Concretization moves the point of thought to the subject by further defining and describing it through it's properties. {\it Deduction} is a method of {\it conclusion-making} through the use of concretization. {\it Induction} is an opposite of deduction; we conclude through the use of abstraction. {\it Abduction} 
is the combination of the both, we make a conclusion by concretization, observation and then by abstraction.

\newpage

\begin{center}
{\bf Appendix B. Critique of Classical Logic}
\end{center}

\noindent\newline{\bf On analytic a priori truths.} Rationalists' main argument was that there do exist analytic a priori truths and that all analytic propositions were of the a priori class. The main example were tautologies, law of identity, etc. To contradict them would be irrational, as it would enforce a contradiction. For example, if we say that it is not $A=A$, that would mean that $A\neq A$, which is the breach of the law of non-contradiction. Therefore, some truths must be known independent of experience. Now, Kant has proved that there do exist synthetic a priori truths by using examples in mathematics (he considered mathematics to be a pure science). But, I do argue against rationalists by saying that the breach of non-contradiction does not enforce necessity of existence, for the law of non-contradiction is, as Schopenhauer similarly mentioned, based on our reason to believe that it is not otherwise. It is only an assumption-axiom, one would have to prove that it is absolute. But in order to prove that, one would have to use logic to do so, which is in turn based on the law of non-contradiction, and so it would go against the forbidance of circular reasoning.

\noindent\newline{\bf On synthetic a priori truths.} Now, Kant has argued against rationalists that there do exist some synthetic a priori truths, i.e. not all analytic propositions are a priori. One of the examples was that $5+2=7$ is a synthetic proposition. I have already defined what a synthetic proposition is; we cannot find the predicate in the subject. Now, for this expression one does not need empirical evidence from Kant's point of view. Therefore it is a priori. And one also cannot find $7$ nor in $5$ nor in $2$, nor in the $5+2$. Therefore it must be synthetic. I, on the other hand say that this is true, considering only the semiotic sphere. We indeed, cannot find predicate in the subject, but it's only in the mention-sense, not in the use-sense. Therefore, altough I cannot see $7$ nor in $5$, nor in $2$, I in fact do know, that the expression itself, as a process, yields $7$. The result of the process of addition is $7$. I may say that in the use-sense one can find predicate in this subject. For if we were to say that it is not, we would need to view $5+2$ as a process and $7$ as a number. But, how can a process equal an object, in this case, a number? Then we would need to view the equal sign as "the result of the process on the left-hand side is". In that case, which would, in my opinion be final, one would see that one can see $7$ in $5+2$. Knowing that equality is not the same in language as in mathematics, that it's only a short for the former expression, one could then argue against $A=A$, the law of identity. If $A$ were the process, then how can the result of a process be a process itself, the same as the one before? It would indeed, not say that $A$ equals $A$, but would in fact define $A$ as a never-ending process, the one whose result is itself. The main confusion was in the fact that we usually say "is" or "equals" for short; much better would be "yields" or "gets us", if not the forementioned expression. Now, if Kantian view was indeed correct, then it would mean that $5+2$ is in fact the number $7$ itself. And for that, he would be right in the use-sense, which is incorrect, as the process cannot equal an object. The number $7$ already hides in that process, whose result we do indeed know, but it's as if we were waiting for it to be finished, and can't see the number $7$ inside. But if we already, beforehand, a priori, know the result is $7$, then we indeed must know what the result of the $5+2$ process was. Therefore, in my opinion, there do not exist nor analytic a priori truths, nor synthetic a priori truths. In fact, I would argue against any a priori knowledge, for $5+2=7$ is something that is actually learned from experience. But, the long-use of that fact in our society has led to its absolute acceptance, as something that was obvious. That is true in the mention sense. But, the very values themselves cannot be learned. One cannot learn that $5$ rocks plus $2$ rocks yields $7$ rocks (where numbers now should not be viewed in the mention sense). It also cannot be seen, for we do not know beforehand that $7$ in the mention-sense is actually what is in use-sense. It is our agreement, that we used to denote use-$7$ by symbol "$7$" and to think of it as an abstraction of the use-$7$ as the mention-$7$. That process cannot be learned. Only an agreement of the symbols can be learned. Therefore, if we do indeed view $5+2$ as a process, then it really is a priori. But then cannot be synthetic.

\newpage

\begin{center}
{\bf Appendix C. On Formulaic Mention-Sense}
\end{center}

\vskip 0.5cm

Now, the motivation for the distinction about to be made more clear is due to, what I believe, true misunderstanding of Bertrand Russell's paradox. It's a sort of a magic trick, that allows us to believe that everything goes according to some sort of plan, the magician shows us step by step necessary actions, assuring us that they are valid, but then comes the prestige which reveals something unexpected, leaving us in awe and misbelief into the very prestige itself; yet the point is not contained within the prestige, it is in the misinterpretation of the steps. And one of the crucial steps that were misinterpreted in Russell's paradox is that the formula itself is valid. Not through axiomatics, but through our own interpretation. We would never consider a formula $a b\ast=\langle x:\leftarrow$ valid, as it contains nothing of the sort that we got used to through our experience in mathematics. In addition, it contains symbols that don't have a meaning in standard arithmetic at all, leaving us to not even try to interpret such a formula and we are forced to call it {\it noise}. Now, the fluctuation itself, the deviation from this pattern of thought, is hidden in Russell's paradox. As we are used to, in set-theoretic area and sense, all of the symbols and their combinations in the definition of the set used in paradox, we do not even try to not interpret formula at all, rather believing it to be meaningful, due to interesting outcome, than dismissing it a priori. This further led to one of the most ugly concepts in mathematics and that is ZFC axiomatic system. We are forced to view mathematics as a strict game with strict rules, we are forced to dismiss our intuition in favor of an axiomatic system, and we forget that it was intuition in the first place that brought us ZFC axioms. Modern mathematician is sort of a Jewish analogue; Moses brought ten commandments and they are not questioned but obeyed without hesitation. And, in the age of "raving nihilism" as Zizek called it, one fails to see that this point of view, glorified by modern pseudophilosophers, is actually nihilism. Now, with Putnam's non-sensical thought experiments and with such axiomatics, one fails to see that in both systems, there is a great negation of more aspects of reality that appear to be positive, but are rather negative in nature. If in philosophy, one chooses to replace the idea of God and transcendetality with some vague notion of other-immortality and transcendentality of human mind, then one is negating, not only God, but the opposite fact, and that is that everything is contained without infinity restriction. One is able to speak about philosophical zombies, and that two straight lines intersect, but under condition that it happens in infinity (p-zombie problem is a thought experiment, and I am bound to say that every thought experiment is real in a point in infinity). And, saying that something happens or does not happen at a point in infinity is the equivalent of saying that it does not happen at all. For then, we can talk about Gods of Olympus as everything is true beyond infinity, it is not verifiable. One is led to believe that in mathematics, operations with infinity are absolute, as one of the most misleading facts that one over zero is infinity, or that one over infinity is zero. Here comes the punchline, and it is in misinterpretation of the copula. We must say that it tends to infinity or that it tends to zero, respectively, and not use the copula at all. The notion of limits must be used with strict care, and through such semantical absolutists, mathematician has forgotten to think (like a social science through statistics) and to use expressions technicaly. In order to further my theories, I will use a function to distinguish an alphabetic expression, i.e. string from a meaningful mathematical expression. I can say that $R=\{x:\ x\notin X\}$ is a {\it string} and not a meaningful expression. The same thing goes for $a b\ast=\langle x:\leftarrow$. Now, let $\alpha$ be an alphabet consisting of symbols used within some theory. Then, $\alpha^{*}$ is a set of all possible symbols throught the use of concatenation from $\alpha$. Now, we will say that $\xi\in\alpha*$ iff $\xi\in\alpha^n$, for some $n\in\N$. Now, let $\alpha_{*}$ be the set of all meaningful expressions. We define a partial function $\Xi:\alpha^{*}\rightarrow\alpha_{*}$, that is, a {\it conversion function} from a string to a meaningful mathematical expression. Furthermore, we will define a {\it breaking function} $B:\alpha^{*}\times\alpha\rightarrow\alpha_{*}$ such that, if $\xi=(\xi_1,\xi_2,\ldots,\xi_n)\in\alpha^{*}$ and $\odot\in\alpha$, then $B(\xi,\odot)=\Xi(\xi_1\odot\xi_2\odot\cdots\odot\xi_n)$.

\noindent\newline For further explanations, examples and justifications... {\it Je n'ai pas le temps...} Yet the answer is in my head, clear as sky on a beautiful sunny day...

\end{document}